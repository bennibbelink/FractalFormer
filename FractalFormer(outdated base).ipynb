{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "174f89e8",
   "metadata": {},
   "source": [
    "# FractaFormer\n",
    "\n",
    "this base version is going to be absurdly terribly no-good inefficient because we're taking the biggest computational issue ($O(t^2)$ attention) and making it way worse by doing multiple of them at once. plan is to fix / work around this later such that we only double our memory requirement or even keep it the same at the sacrifice of a marginal amount of context length. \n",
    "\n",
    "# TODO\n",
    "- embedding\n",
    "- ffwd\n",
    "- sa\n",
    "- mha\n",
    "- block\n",
    "- output\n",
    "- loss\n",
    "- body\n",
    "- batch splitup\n",
    "- inference choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5373dd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import time\n",
    "from typing import List\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6021eafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38ddc9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding sizes:  [16, 32, 64]\n",
      "number of nesting levels:  3  (I will frequently refer to this number as 'g')\n",
      "models at base level: 1\n",
      "models at smallest level:  4\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "b = 8 # how many independent sequences will we process in parallel?\n",
    "t = 24 # what is the maximum context length for predictions?\n",
    "max_iters = 10\n",
    "eval_interval = 2\n",
    "lr = 3e-4 # learning rate for each backprop step\n",
    "eval_iters = 20\n",
    "h = 4 # number of attention heads\n",
    "l = 4 # number of transormer layers\n",
    "dropout = 0.1 # % of parameters to ignore every iteration\n",
    "l2 = 0.01 # multiplier for our L2 norm to encourage sparsity\n",
    "\n",
    "# embedding aka hidden dimension. this is the largest that the model will have\n",
    "d = 64 # make sure it is a power of 2\n",
    "power_of_d = int(math.log2(d))\n",
    "\n",
    "# the smallest power of 2 we'll be considering as a matryoshka embedding\n",
    "min_power = 4 # Starting from 2^min_power\n",
    "nesting_list = [2**i for i in range(min_power, int(power_of_d) + 1)]\n",
    "print(\"embedding sizes: \", nesting_list)\n",
    "print(\"number of nesting levels: \", len(nesting_list), \" (I will frequently refer to this number as 'g')\")\n",
    "print(\"models at base level: 1\\nmodels at smallest level: \", 2**(len(nesting_list)-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b34b3cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n"
     ]
    }
   ],
   "source": [
    "# the dataset we'll be using is just TinyShakespeare\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "print(text[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7891c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'] 65\n"
     ]
    }
   ],
   "source": [
    "# here are all the unique characters that occur in this text. we'll do character-wise tokenization\n",
    "chars = sorted(list(set(text)))\n",
    "v = len(chars)\n",
    "print(chars, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e620b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "018a19ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest validation\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a28bc7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - t, (b,))\n",
    "    x = torch.stack([data[i:i+t] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+t+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30ce7acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(): # to use later during the training loop\n",
    "    out = {}\n",
    "    model.eval() # sets model to eval mode\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train() # just resets to training mode\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0401c6",
   "metadata": {},
   "source": [
    "# FEEDFORWARD\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./images/ffwd.jpeg\" width=\"512\"/>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a37edd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FractalFeedFoward(nn.Module):\n",
    "    def __init__(self, nesting_list: List, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        # our list of different potential embedding sizes\n",
    "        self.nesting_list = nesting_list\n",
    "        \n",
    "        # the embedding dimension of the largest model\n",
    "        self.d = nesting_list[-1]\n",
    "\n",
    "        # Initialize only the largest weights and biases\n",
    "        # this is more efficient than using regualr nn.Linear() because we need to splice them frequently\n",
    "        self.w1 = nn.Parameter(torch.Tensor(self.d, 4 * self.d))\n",
    "        self.b1 = nn.Parameter(torch.Tensor(4 * self.d))\n",
    "        self.w2 = nn.Parameter(torch.Tensor(4 * self.d, self.d))\n",
    "        self.b2 = nn.Parameter(torch.Tensor(self.d))\n",
    "\n",
    "        # Initializing parameters\n",
    "        nn.init.normal_(self.w1, std=0.02)  \n",
    "        nn.init.normal_(self.b1, std=0.02)\n",
    "        nn.init.normal_(self.w2, std=0.02)\n",
    "        nn.init.normal_(self.b2, std=0.02)\n",
    "        \n",
    "        # the other parts\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        # notice how in forwardTuple(), the dropout mechanism will actually randomly drop out different weights for\n",
    "        # each model size during a given forward pass. My intuition says this will actually be good for generalizability\n",
    "        # but I suppose the opposite could be true. It'd be interesting to create a custom dropout method that ensures \n",
    "        # consistency in what parameters are dropped out across model sizes, but that's really not worth the effort\n",
    "    \n",
    "    def forwardTuple(self, x):\n",
    "        \"\"\"\n",
    "        input: Tuple of length g containing tuples of varying lengths which themselves have tensors of shape (b,t,d_i) for d_i in nesting_list\n",
    "        operation: 2 linear layers with a 4-times larger hidden depth, a relu nonlinearity in between, and then a dropout\n",
    "        output: Tuple of length g containing tuples of varying lengths which themselves have tensors of shape (b,t,d_i) for d_i in nesting_list\n",
    "        \"\"\"\n",
    "        out = ()\n",
    "        for i, d_i in enumerate(self.nesting_list):\n",
    "            models_in_level = self.d // d_i\n",
    "            level = ()\n",
    "            for m in range(models_in_level):\n",
    "                skip = m * d_i\n",
    "                w1 = self.w1[skip:skip + d_i, 4*skip:4*(skip + d_i)]\n",
    "                b1 = self.b1[4*skip:4*(skip + d_i)]\n",
    "                w2 = self.w2[4*skip:4*(skip+d_i), skip:skip + d_i]\n",
    "                b2 = self.b2[skip:skip + d_i]\n",
    "                level += (self.drop(self.relu(x[i][m] @ w1 + b1) @ w2 + b2), )\n",
    "            #out += (self.drop(self.relu(x[i] @ self.w1[:d_i,:4*d_i] + self.b1[:4*d_i]) @ self.w2[:4*d_i,:d_i] + self.b2[:d_i]),)\n",
    "            out += (level, )\n",
    "        return out\n",
    "\n",
    "    def forwardTensor(self, x, model=0):\n",
    "        \"\"\"\n",
    "        input: tensor of shape (b,t,d_i)\n",
    "        operation: 2 linear layers with a 4-times depth, a relu nonlinearity in between, and then a dropout\n",
    "        output: tensor of shape (b,t,d_i)\n",
    "        \"\"\"\n",
    "        d_i = x.shape[-1]\n",
    "        skip = model * d_i\n",
    "        w1 = self.w1[skip:skip + d_i, 4*skip:4*(skip + d_i)]\n",
    "        b1 = self.b1[4*skip:4*(skip + d_i)]\n",
    "        w2 = self.w2[4*skip:4*(skip + d_i), skip:skip + d_i]\n",
    "        b2 = self.b2[skip:skip + d_i]\n",
    "        return self.relu(x @ w1 + b1) @ w2 + b2\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # forwardTuple() is for training and forwardTensor() is for inference\n",
    "        # that will remain true for the rest of the code as well\n",
    "        return self.forwardTuple(x) if type(x) == tuple else self.forwardTensor(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3127b06c",
   "metadata": {},
   "source": [
    "# ATTENTION\n",
    "\n",
    "To subset the attention heads, we have to not only splice according to the model's embedding dimension but also take into account new smaller head sizes and how they're spaced throughout the matrix. I'm assuming you know how self-attention works well enough to look at this weight matrix and get the idea\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./images/sa.jpeg\" width=\"512\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3692ba",
   "metadata": {},
   "source": [
    "# --------- BOOKMARK ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cede0c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FractalHead(nn.Module):\n",
    "    def __init__(self, nesting_list: List, head_sizes: List):\n",
    "        super().__init__()\n",
    "\n",
    "        # to be used for iterating in forward()\n",
    "        self.nesting_list = nesting_list\n",
    "        self.head_sizes = head_sizes\n",
    "        \n",
    "        # the largest embedding dimension of the model\n",
    "        self.d = nesting_list[-1]\n",
    "        # the largest head size\n",
    "        self.h = head_sizes[-1]\n",
    "\n",
    "        # initialize only the largest. we'll subset later during forward()\n",
    "        self.key = nn.Parameter(torch.Tensor(self.d, self.h)).to(device)\n",
    "        self.query = nn.Parameter(torch.Tensor(self.d, self.h)).to(device)\n",
    "        self.value = nn.Parameter(torch.Tensor(self.d, self.h)).to(device)\n",
    "        \n",
    "        # Initializing parameters\n",
    "        nn.init.normal_(self.key, std=0.02)  \n",
    "        nn.init.normal_(self.query, std=0.02)\n",
    "        nn.init.normal_(self.value, std=0.02)\n",
    "\n",
    "        # the mask so they only look into the past\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(t, t)))\n",
    "\n",
    "    def forwardTuple(self, x):\n",
    "        \"\"\"\n",
    "        input: Tuple of length g containing tuples of varying lengths which themselves have tensors of shape (b,t,d_i) for d_i in nesting_list\n",
    "        operation: masked self-attention\n",
    "        output: Tuple of length g containing tuples of varying lengths which themselves have tensors of shape (b,t,h_i) for h_i in head_sizes \n",
    "        \"\"\"\n",
    "        out = ()\n",
    "        for i, (d_i, h_i) in enumerate(zip(self.nesting_list, self.head_sizes)):\n",
    "            models_in_level = self.d // d_i\n",
    "            k,q,v,wei,level = (),(),(),[],() # wei is a list so i can edit it in-place\n",
    "            for m in range(models_in_level):\n",
    "                skip_d = m * d_i\n",
    "                skip_h = m * h_i\n",
    "                \n",
    "                Wk = self.key[skip_d:skip_d + d_i, skip_h:skip_h + h_i]\n",
    "                Wq = self.query[skip_d:skip_d + d_i, skip_h:skip_h + h_i]\n",
    "                Wv = self.value[skip_d:skip_d + d_i, skip_h:skip_h + h_i]\n",
    "\n",
    "                k += (x[i] @ Wk,)\n",
    "                q += (x[i] @ Wq,)\n",
    "                v += (x[i] @ Wv,)\n",
    "\n",
    "                wei.append(q[i] @ k[i].transpose(-1,-1) * h_i ** -0.5)\n",
    "                wei[i] = wei[i].masked_fill(self.tril[:t,:t] == 0, float('inf')) # i think [:t,:t] is redundant\n",
    "                wei[i] = F.softmax(wei[i], dim=-1)\n",
    "\n",
    "                level += (wei[i] @ v[i],)\n",
    "\n",
    "            out += (level,)\n",
    "        return out\n",
    "\n",
    "    def forwardTensor(self, x, h, i):\n",
    "        \"\"\"\n",
    "        input: \n",
    "            - tensor of shape (b,t,d_i)\n",
    "            - number of heads h\n",
    "            - which head is this i\n",
    "        operation: masked self-attention\n",
    "        output: tensor of shape (b,t,h_i) where h_i = d_i / h\n",
    "        \"\"\"\n",
    "        d_i = x.shape[-1]\n",
    "        h_i = d_i // h # the second / ensures it's an int rather than a float\n",
    "        \n",
    "        skip_d = i * d_i\n",
    "        skip_h = i * h_i\n",
    "\n",
    "        k = x @ self.key[skip_d:skip_d + d_i, skip_h:skip_h + h_i]\n",
    "        q = x @ self.query[skip_d:skip_d + d_i, skip_h:skip_h + h_i]\n",
    "        v = x @ self.value[skip_d:skip_d + d_i, skip_h:skip_h + h_i]\n",
    "\n",
    "        wei = q @ k.transpose(-2,-1) * h_i ** -0.5 # k.shape[-1]**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:t,:t] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "        return wei @ v\n",
    "        \n",
    "    def forward(self, x, h=None, i=0):\n",
    "        return self.forwardTuple(x) if type(x) == tuple else self.forwardTensor(x, h, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63af660d",
   "metadata": {},
   "source": [
    "# MHA\n",
    "\n",
    "then we've gotta concatenate the outputs of each head\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./images/mha_concat.jpeg\" width=\"512\"/>\n",
    "</p>\n",
    "\n",
    "and after that linearly project them\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./images/mha_proj.jpeg\" width=\"512\"/>\n",
    "</p>\n",
    "\n",
    "this is the place where our splicing gets conceptually annoying. instead of just grabbing the matrix in the upper corner, because of the way attention head output concatenation works we actually need to skip over certain parts of the linear projection matrix and then concatenate them together in order to use them. Here's an example of what the matrix multiplication looks like. on the left is a simplified version of the concatenated attention heads where i just showed it as a matrix rather than a tensor, and then on the right is the actual projection matrix. notice how the numbers in the pink output matrix look similar to the first column of the purple output matrix with a positive number, its negative, and then a smaller positive number; that's the self-similarity in action. the yellow arrows point to the parts that get skipped over. obviously this would look a lot uglier with bigger matrices & incorporating the blue/green layer\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./images/mha_proj_matmul.jpeg\" width=\"512\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b48b8874",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FractalMultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, h, nesting_list: List, head_sizes: List, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.nesting_list = nesting_list\n",
    "        self.head_sizes = head_sizes\n",
    "        self.h_count = h # number of heads\n",
    "        self.d_count = len(nesting_list) # number of nesting doll sizes\n",
    "        self.h_max = head_sizes[-1] # size of largest head\n",
    "        self.d_max = nesting_list[-1] # size of largest embedding\n",
    "        \n",
    "        # creating all of our different attention heads, then storing them in a list for use later\n",
    "        self.headsList = nn.ModuleList([FractalHead(self.nesting_list, self.head_sizes) for _ in range(self.h_count)])\n",
    "        \n",
    "        # the linear projection that combines the outputs of all the heads\n",
    "        self.weight = nn.Parameter(torch.Tensor(self.h_max * self.h_count, self.d_max)).to(device)\n",
    "        self.bias = nn.Parameter(torch.Tensor(self.h_max * self.h_count)).to(device)\n",
    "        \n",
    "        # Initializing parameters\n",
    "        nn.init.normal_(self.weight, std=0.02)  \n",
    "        nn.init.normal_(self.bias, std=0.02)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forwardTuple(self, x):\n",
    "        \"\"\"\n",
    "        input: Tuple of length g containing tuples of varying lengths which themselves have tensors of shape (b,t,d_i) for d_i=nesting_list[i]\n",
    "        operation: \n",
    "            - perform self-attention w each head\n",
    "                - input to each head: Tuple of length g containing tuples of varying lengths which themselves have tensors of shape (b,t,d_i) for d_i=nesting_list[i]\n",
    "                - output from each head: Tuple of length g containing tuples of varying lengths which themselves have tensors of shape (b,t,h_i) for h_i=head_sizes[i] where h_i = d_i / h\n",
    "            - then concatenate into Tuple of length g containing tuples of varying lengths which themselves have tensors of shape (b,t,h*h_i). here by design h*h_i=d_i but it need not be that way\n",
    "            - then linearly project each of the g tensors in the tuple back to (b,t,d_i)\n",
    "        output: Tuple of length g containing tuples of varying lengths which themselves have tensors of shape (b,t,d_i) for d_i=nesting_list[i]\n",
    "        \"\"\"\n",
    "        # let's get the outputs of each attention head\n",
    "        # list length h of tuples length g of tensors shape (b,t,h_i) for h_i=d_i/h where d_i = nesting_list[i]\n",
    "        head_outputs = [head(x) for head in self.headsList]\n",
    "\n",
    "        #########################################################################################################\n",
    "        ############################################# bookmark #####################################################\n",
    "        ######################################### ugh this is annoying #############################################\n",
    "        #########################################################################################################\n",
    "        \n",
    "        # now let's reformat our ugly list of tuples into our usual expected Tuple of length g containing tuples of varying lengths which themselves have tensors shape (b,t,d_i)\n",
    "        mid = ()\n",
    "        for i in range(self.d_count):\n",
    "            level = [] # where will store the output of each head for this model size\n",
    "            for j, head in enumerate(head_outputs):\n",
    "                level.append(head[i]) # this head's output for the d_i layer of the model\n",
    "            \n",
    "            # appending the concatenation of all the heads for this d_i layer of the model\n",
    "            mid += (torch.cat(level, dim=-1),) # tuple length g with tensors of shape (b,t,d_i) for d_i=nesting_list[i]\n",
    "        # mid is now a Tuple of length g containing tuples of varying lengths which themselves have tensors shape (b,t,h*h_i)\n",
    "        \n",
    "        # now let's do our linear projection, which is not similar to how we did the matryoshkaFeedForward()\n",
    "        # because we can't just select nested matrices within the primary matrix, we also have to account for the head \n",
    "        # concatenation which means skipping throughout and grabbing specific parts from the projection that match up\n",
    "        #\n",
    "        # so along the vertical of the matrix we want to iterate through self.nesting_list \n",
    "        # and along the horizontal we need to make skips the size of self.h\n",
    "        # and then from those skips as starting points iteratively slice using self.head_sizes\n",
    "        # then we concatenate those multiple spliced pieces along the horizontal\n",
    "        # then we multiply a given output level by its respective projection\n",
    "        out = ()\n",
    "        for i, (d_i, h_i) in enumerate(zip(self.nesting_list, self.head_sizes)):\n",
    "            # h_i is the head size of this iteration\n",
    "            # j*self.h_max is our skip length\n",
    "            this_levels_proj_w = torch.cat([self.weight[j*self.h_max:j*self.h_max+h_i, :d_i] for j in range(self.h_count)], dim=0)\n",
    "\n",
    "            # bias is only one dimension so a bit simpler\n",
    "            this_levels_proj_b = torch.cat([self.bias[j*self.h_max:j*self.h_max+h_i] for j in range(self.h_count)])\n",
    "\n",
    "            # select correct level & multiply by weights then add bias\n",
    "            # and can't forget to dropout\n",
    "            out += (self.dropout(mid[i]@this_levels_proj_w + this_levels_proj_b),)\n",
    "            \n",
    "        return out\n",
    "\n",
    "    def forwardTensor(self, x):\n",
    "        \"\"\"\n",
    "        input: tensor of shape (b,t,d_i)\n",
    "        operation: \n",
    "            - perform self-attention w each head\n",
    "                - input to each head: tensor of shape (b,t,d_i)\n",
    "                - output from each head: tensor of shape (b,t,h_i) where h_i = d_i / h\n",
    "            - then concatenate the head outputs\n",
    "            - then linearly project\n",
    "        output: tensor of shape (b,t,d_i) \n",
    "        \"\"\"\n",
    "        d_i = x.shape[-1]\n",
    "        h_i = d_i // self.h_count\n",
    "        \n",
    "        head_outputs = torch.cat([head(x, h=self.h_count) for head in self.headsList], dim=-1) # (b,t,h*h_i)\n",
    "\n",
    "        spliced_projection_w = torch.cat([self.weight[j*self.h_max:j*self.h_max+h_i,:d_i] for j in range(self.h_count)], dim=0)\n",
    "        spliced_projection_b = torch.cat([self.bias[j*self.h_max:j*self.h_max+h_i] for j in range(self.h_count)])\n",
    "\n",
    "        return head_outputs @ spliced_projection_w + spliced_projection_b\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.forwardTuple(x) if type(x) == tuple else self.forwardTensor(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ce2ef0",
   "metadata": {},
   "source": [
    "# LAYERNORM\n",
    "\n",
    "Layernorm is relatively simple code-wise. However, of note is the fact that during training, the entire full length vector gets normalized whereas during inference we only layernorm the sub-vector we've been given if we're not using the full model size. This probably isn't a big deal since the sub-vectors are still hopefully being drawn from the same distribution during training. However, it wouldn't be surprising if the logits going into the small vectors are characteristically different from the full super-vectors, in which case this certainly might be a difficulty for the model. It might be worth changing this algorithm such that during training sub-vectors get normalized first and then held constant while super-vectors are normalized. something to think about. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "684dcb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class matryoshkaLayerNorm(nn.Module):\n",
    "    def __init__(self, nesting_list: List):\n",
    "        super().__init__()\n",
    "\n",
    "        self.nesting_list = nesting_list\n",
    "        self.d_count = len(nesting_list)\n",
    "\n",
    "        # we need layernorm attributes for each dimension size\n",
    "        for d_i in nesting_list:\n",
    "            setattr(self, f\"ln_{d_i}\", nn.LayerNorm(d_i, elementwise_affine=False))\n",
    "            # we do elementwise_affine=False to remove the linear projection at the end\n",
    "            # the linear projection would be counterproductive since we're layernorming in so many different places\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        a layernorm module that is dynamic to the input of either a single tensor or a tuple of tensors\n",
    "        only works if the dimensions in question are in self.nesting_list\n",
    "\n",
    "        input: either \n",
    "        - a tensor with last dimension equal to some value in self.nesting_list\n",
    "        - a tuple of tensors where the last dimensions of each matches the values in self.nesting_list IN ORDER\n",
    "\n",
    "        output: either of the above, but normalized\n",
    "        \"\"\"\n",
    "        if type(x) == tuple:\n",
    "            out = ()\n",
    "            for i, d_i in enumerate(self.nesting_list):\n",
    "                out += (getattr(self, f\"ln_{d_i}\")(x[i]),)\n",
    "        else:\n",
    "            d_i = x.shape[-1]\n",
    "            out = getattr(self, f\"ln_{d_i}\")(x)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4819ca",
   "metadata": {},
   "source": [
    "# RESIDUAL BLOCK\n",
    "\n",
    "not a whole lot to say here other than the fact that i've chosen to pass everything through in the form of a tuple means that this block structure is HELLA inefficient in terms of memory. that's like 6 different copies of the tensors being forced to stay in memory goddamn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1eca3ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "class matryoshkaBlock(nn.Module):\n",
    "    def __init__(self, h, nesting_list: List, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.nesting_list = nesting_list\n",
    "        self.head_sizes = [d_i // h for d_i in nesting_list]\n",
    "        \n",
    "        self.ln = matryoshkaLayerNorm(nesting_list)\n",
    "        self.mha = matryoshkaMultiHeadAttention(h, nesting_list, self.head_sizes, dropout) \n",
    "        self.ffwd = matryoshkaFeedFoward(nesting_list, dropout)\n",
    "    \n",
    "    def forwardTuple(self, x_i):\n",
    "        \"\"\"\n",
    "        input: length g tuple of shape (b,t,d_i) tensors for d_i in nesting_list\n",
    "        output: length g tuple of shape (b,t,d_i) tensors for d_i in nesting_list\n",
    "        \"\"\"\n",
    "        # please forgive my weird variable naming scheme\n",
    "\n",
    "        # layernorming the input\n",
    "        x_iplus1quart = self.ln(x_i)\n",
    "\n",
    "        # the full multi-head attention\n",
    "        attn = self.mha(x_iplus1quart)\n",
    "\n",
    "        # residual connection for every residual state in our list of models\n",
    "        x_iplus1half = tuple(x_i[j] + attn[j] for j in range(len(self.nesting_list)))\n",
    "\n",
    "        # another layernorm\n",
    "        x_iplus3quart = self.ln(x_iplus1half)\n",
    "\n",
    "        # the feeforward\n",
    "        ffwd = self.ffwd(x_iplus3quart)\n",
    "\n",
    "        # the next residual connection for every residual state in our list of models\n",
    "        x_iplus1 = tuple(x_iplus1half[j] + ffwd[j] for j in range(len(self.nesting_list)))\n",
    "            \n",
    "        return x_iplus1\n",
    "\n",
    "    def forwardTensor(self, x):\n",
    "        \"\"\"\n",
    "        input: tensor of shape (b,t,d_i)\n",
    "        output: tensor of shape (b,t,d_i)\n",
    "        \"\"\"\n",
    "        return x + self.ffwd(self.ln(x + self.mha(self.ln(x))))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.forwardTuple(x) if type(x) == tuple else self.forwardTensor(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7680a0",
   "metadata": {},
   "source": [
    "# OUTPUT\n",
    "\n",
    "this output layer is similar to what you'll find in in [the original paper](https://arxiv.org/abs/2205.13147) except \n",
    "1) i use one output matrix instead of multiple\n",
    "2) that output matrix i use is the transposed token embedding matrix\n",
    "3) i add the option to perform inference rather than just training, which is something they did do in the [matformer paper](https://arxiv.org/pdf/2310.07707.pdf)\n",
    "\n",
    "and then the loss function is the exact same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60901e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class matryoshkaOutputLayer(nn.Module):\n",
    "    def __init__(self, embedding, nesting_list: List, num_classes):\n",
    "        super().__init__()\n",
    "        self.nesting_list = nesting_list\n",
    "        self.num_classes = num_classes  # Number of tokens in the vocabulary\n",
    "        \n",
    "        self.embedding = embedding  # Store reference to the embedding matrix\n",
    "\n",
    "        self.norm = matryoshkaLayerNorm(nesting_list)\n",
    "\n",
    "    def forwardTuple(self, x):\n",
    "        \"\"\"\n",
    "        input: length g tuple of tensors shape (b,t,d_i) for d_i in nesting_list\n",
    "        operation: layernorm then multiply the final residual state by the transposed embedding matrix to get final logits\n",
    "        output: length g tuple of tensors shape (b,t,v) where v is token vocabulary length\n",
    "        \"\"\"\n",
    "        normed_logits = self.norm(x)\n",
    "        normed_embeddings = self.norm(self.embedding).t()\n",
    "        \n",
    "        out = ()\n",
    "        for i, d_i in enumerate(self.nesting_list):\n",
    "            out += (normed_logits[i] @ normed_embeddings[:d_i,:],) \n",
    "            \n",
    "        return out\n",
    "\n",
    "    def forwardTensor(self, x):\n",
    "        \"\"\"\n",
    "        input: tensor shape (b,t,d_i)\n",
    "        operation: layernorm then multiply the final residual state by the transposed embedding matrix to get final logits\n",
    "        output: tensor shape (b,t,v) where v is token vocabulary length\n",
    "        \"\"\"\n",
    "        d_i = x.shape[-1]\n",
    "        normed_logits = self.norm(x)\n",
    "        normed_embeddings = self.norm(self.embedding[:,:d_i]).t()\n",
    "        return normed_logits @ normed_embeddings\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.forwardTuple(x) if type(x) == tuple else self.forwardTensor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9d9d23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class matryoshkaCEL(nn.Module):\n",
    "    '''\n",
    "    Loss function for Matryoshka Representation Learning\n",
    "    we don't need to create a tensor version of the loss function bc training always involves all nesting levels\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # the loss function\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        \"\"\"\n",
    "        input: \n",
    "            - logits are a length g list of lists each of shape [b batch size, t sequence length, v number of classes]\n",
    "            - target is a shape [b batch size, t sequence length] tensor of the indices of the correct tokens\n",
    "        output: a tensor containing a single float\n",
    "        \"\"\"\n",
    "        g = len(logits)\n",
    "        b,t,v = logits[0].shape\n",
    "\n",
    "        # Calculate losses for each output and stack them\n",
    "        losses = torch.stack([self.criterion(logits_i.view(b*t, v), target.view(b*t)) for logits_i in logits])\n",
    "\n",
    "        return losses.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26963e5e",
   "metadata": {},
   "source": [
    "# THE MODEL\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./images/E.jpeg\" width=\"512\"/>\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./images/pos.jpeg\" width=\"512\"/>\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./images/X.jpeg\" width=\"512\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d361260d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class matryoshkaGPT(nn.Module):\n",
    "    def __init__(self, nesting_list: List, v, t, h, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        # the list of dimensions we'll be using\n",
    "        self.nesting_list = nesting_list\n",
    "        \n",
    "        # the embedding size of the largest model\n",
    "        self.d = nesting_list[-1]\n",
    "        \n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(v, self.d).to(device)\n",
    "        \n",
    "        # simple learned positional encodings rather than sine or RoPE\n",
    "        # at some point i'm gonna write up a new model using all the stuff like RoPE that I should be using\n",
    "        self.position_embedding_table = nn.Embedding(t, self.d).to(device)\n",
    "        self.context_len = t\n",
    "\n",
    "        # our special implementation of layernorm\n",
    "        self.ln = matryoshkaLayerNorm(nesting_list)\n",
    "\n",
    "        # bulk of the beast\n",
    "        self.blocks = nn.Sequential(*[matryoshkaBlock(h, nesting_list, dropout) for _ in range(l)]) \n",
    "\n",
    "        # MATRYOSHKA OUTPUT HEADS\n",
    "        self.out_heads = matryoshkaOutputLayer(self.token_embedding_table.weight, nesting_list, num_classes=v)\n",
    "        \n",
    "        # MATRYOSHKA LOSS\n",
    "        self.loss = matryoshkaCEL()\n",
    "\n",
    "    def forward(self, idx, targets=None, desired_d=nesting_list[-1], model=0): \n",
    "        # desired_d is the desired dimension to use when performing inference (not used during training)\n",
    "\n",
    "        # idx and targets are both (b,t) tensor of integers\n",
    "        b, t = idx.shape\n",
    "        \n",
    "        pos_emb = self.position_embedding_table(torch.arange(t, device=device)) # (t,d)\n",
    "        tok_emb = self.token_embedding_table(idx) # (b,t,d)\n",
    "    \n",
    "        if targets is None: \n",
    "            # if we are NOT training AKA just performing inference\n",
    "            # send in a single matrix using desired_d\n",
    "            # select the correct model to use from that level using skip\n",
    "            skip = model*desired_d\n",
    "            x_0 = self.ln(tok_emb[:,:,skip:skip + desired_d]) + pos_emb[:,skip:skip + desired_d]\n",
    "        else:\n",
    "            # if we ARE training\n",
    "            # create list of lists of residual states & send it thru\n",
    "            x_0 = ()\n",
    "            for d_i in self.nesting_list:\n",
    "                models_in_level = self.nesting_list[-1] // d_i\n",
    "                level = ()\n",
    "                for m in range(models_in_level):\n",
    "                    skip = m*d_i\n",
    "                    level += (self.ln(tok_emb[:,:,skip:skip + d_i]) + pos_emb[:,skip:skip + d_i], )\n",
    "                x_0 += (level, )\n",
    "                #x_0 += (self.ln(tok_emb[:,:,:d_i]) + pos_emb[:,:d_i],)\n",
    "\n",
    "        # most of the model is here\n",
    "        x_f = self.blocks(x_0)\n",
    "\n",
    "        # Matryoshka output head\n",
    "        # self.out_heads includes within it the final layernorm\n",
    "        logits = self.out_heads(x_f)\n",
    "\n",
    "        loss = None if targets is None else self.loss(logits, targets) # g*(b,t,d) & (b,t) -> float\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens=100, degree=-1):\n",
    "        \"\"\"\n",
    "        input: \n",
    "            - idx is (b, ?) tensor of indices from the current context\n",
    "            - max_new_tokens sets generation length\n",
    "            - degree determines which model to use. 0 for smallest & -1 for largest\n",
    "        output: idx is (b,?+max_new_tokens) tensor of indices\n",
    "        \"\"\"\n",
    "        # making sure the user specified an actual existing model. 0 is the smallest model\n",
    "        assert degree >= -1 & degree < len(nesting_list)\n",
    "\n",
    "        # getting the actual embedding size of the model we've chosen\n",
    "        desired_d = self.nesting_list[degree]\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -self.context_len:]\n",
    "            \n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond, desired_d=desired_d)\n",
    "            \n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (b, d)\n",
    "            \n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (b, d)\n",
    "            \n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (b, 1)\n",
    "            \n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (b, t+1)\n",
    "            \n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6068ef30",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce5a134d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1595.52 K parameters\n"
     ]
    }
   ],
   "source": [
    "model = matryoshkaGPT(nesting_list, v, t, h, dropout).to(device)\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=l2)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e3, 'K parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "001f8eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 6.8818, val loss 6.9728, time elapsed: 0.23 seconds\n",
      "step 100: train loss 6.8676, val loss 6.9765, time elapsed: 23.09 seconds\n",
      "step 200: train loss 6.8080, val loss 6.8736, time elapsed: 45.84 seconds\n",
      "step 300: train loss 6.8021, val loss 6.9008, time elapsed: 68.68 seconds\n",
      "step 400: train loss 6.8075, val loss 6.9001, time elapsed: 91.29 seconds\n",
      "step 500: train loss 6.7299, val loss 6.8127, time elapsed: 113.99 seconds\n",
      "step 600: train loss 6.7066, val loss 6.8978, time elapsed: 136.54 seconds\n",
      "step 700: train loss 6.7235, val loss 6.8059, time elapsed: 159.59 seconds\n",
      "step 800: train loss 6.7163, val loss 6.7379, time elapsed: 182.33 seconds\n",
      "step 900: train loss 6.6745, val loss 6.8096, time elapsed: 204.89 seconds\n",
      "step 1000: train loss 6.7024, val loss 6.7507, time elapsed: 227.48 seconds\n",
      "step 1100: train loss 6.6677, val loss 6.7020, time elapsed: 250.05 seconds\n",
      "step 1200: train loss 6.6395, val loss 6.6584, time elapsed: 272.63 seconds\n",
      "step 1300: train loss 6.6142, val loss 6.6797, time elapsed: 295.25 seconds\n",
      "step 1400: train loss 6.5610, val loss 6.6513, time elapsed: 317.84 seconds\n",
      "step 1500: train loss 6.5327, val loss 6.6548, time elapsed: 340.52 seconds\n",
      "step 1600: train loss 6.5331, val loss 6.6388, time elapsed: 363.27 seconds\n",
      "step 1700: train loss 6.5527, val loss 6.6898, time elapsed: 385.79 seconds\n",
      "step 1800: train loss 6.5081, val loss 6.6303, time elapsed: 408.32 seconds\n",
      "step 1900: train loss 6.4473, val loss 6.6143, time elapsed: 430.83 seconds\n",
      "step 2000: train loss 6.4156, val loss 6.5794, time elapsed: 453.33 seconds\n",
      "step 2100: train loss 6.3842, val loss 6.5638, time elapsed: 475.96 seconds\n",
      "step 2200: train loss 6.3822, val loss 6.5580, time elapsed: 498.44 seconds\n",
      "step 2300: train loss 6.3821, val loss 6.5082, time elapsed: 521.07 seconds\n",
      "step 2400: train loss 6.3773, val loss 6.5082, time elapsed: 543.59 seconds\n",
      "step 2500: train loss 6.3598, val loss 6.5739, time elapsed: 566.12 seconds\n",
      "step 2600: train loss 6.3271, val loss 6.4791, time elapsed: 588.69 seconds\n",
      "step 2700: train loss 6.2338, val loss 6.4897, time elapsed: 611.24 seconds\n",
      "step 2800: train loss 6.2645, val loss 6.4705, time elapsed: 633.75 seconds\n",
      "step 2900: train loss 6.2624, val loss 6.4660, time elapsed: 656.44 seconds\n",
      "step 3000: train loss 6.2274, val loss 6.4616, time elapsed: 679.21 seconds\n",
      "step 3100: train loss 6.2192, val loss 6.3789, time elapsed: 701.83 seconds\n",
      "step 3200: train loss 6.2698, val loss 6.3800, time elapsed: 724.37 seconds\n",
      "step 3300: train loss 6.1437, val loss 6.3793, time elapsed: 746.97 seconds\n",
      "step 3400: train loss 6.1506, val loss 6.3700, time elapsed: 769.55 seconds\n",
      "step 3500: train loss 6.1434, val loss 6.3671, time elapsed: 792.12 seconds\n",
      "step 3600: train loss 6.1042, val loss 6.2585, time elapsed: 814.63 seconds\n",
      "step 3700: train loss 6.1120, val loss 6.2665, time elapsed: 837.24 seconds\n",
      "step 3800: train loss 6.0468, val loss 6.2703, time elapsed: 859.76 seconds\n",
      "step 3900: train loss 6.0623, val loss 6.3021, time elapsed: 882.28 seconds\n",
      "step 4000: train loss 6.0940, val loss 6.2759, time elapsed: 904.80 seconds\n",
      "step 4100: train loss 6.0350, val loss 6.1601, time elapsed: 927.40 seconds\n",
      "step 4200: train loss 5.9868, val loss 6.3232, time elapsed: 949.93 seconds\n",
      "step 4300: train loss 5.9578, val loss 6.1949, time elapsed: 972.45 seconds\n",
      "step 4400: train loss 5.9798, val loss 6.2349, time elapsed: 995.14 seconds\n",
      "step 4500: train loss 5.9392, val loss 6.2262, time elapsed: 1017.78 seconds\n",
      "step 4600: train loss 6.0069, val loss 6.2060, time elapsed: 1041.11 seconds\n",
      "step 4700: train loss 5.9970, val loss 6.1877, time elapsed: 1063.69 seconds\n",
      "step 4800: train loss 5.9122, val loss 6.2203, time elapsed: 1086.28 seconds\n",
      "step 4900: train loss 6.0020, val loss 6.1433, time elapsed: 1108.87 seconds\n",
      "step 4999: train loss 5.8608, val loss 6.1161, time elapsed: 1131.53 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Enable anomaly detection\n",
    "#torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "    # train\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        current_time = time.time()\n",
    "        elapsed_time = current_time - start_time\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, time elapsed: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Disable anomaly detection after the training loop\n",
    "#torch.autograd.set_detect_anomaly(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "619ddde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the trained model\n",
    "torch.save(model.state_dict(), f'models/{model.__class__.__name__}_b{b}_t{t}_d{d}_h{h}_l{l}_lr{lr}_drop{dropout}_l2-{l2}_min_power{min_power}_{time.strftime(\"%Y-%m-%d|%H-%M-%S\")}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0019de",
   "metadata": {},
   "source": [
    "# Load a saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf51be34",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = matryoshkaGPT().to(device)  # Initialize a model with the same architecture\n",
    "\n",
    "# Load the saved state dictionary\n",
    "model.load_state_dict(torch.load('models/matryoshkaGPT_b16_t64_d2_h4_l8_lr0.0003_drop0.1_l2-0.01_min_power5_2024-02-13|01-55-54.pth'))\n",
    "\n",
    "# If you plan to continue training the model, switch to training mode\n",
    "#model.train()\n",
    "\n",
    "# If you only plan to do inference, switch to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b7fc72",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "18079ca3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------model:  0 ------------------\n",
      "JULIET:\n",
      "O Romeo, Romeo! wherefore art thou Romeo?\n",
      "Deny thy fathe, d.\n",
      "\n",
      "Th,\n",
      "Cw.\n",
      "BBUBBt,\n",
      "Mur pwnd, ming wid wit?\n",
      "Thifck,\n",
      "Y:\n",
      "Thow, wwive,\n",
      "GUBun,\n",
      "ICowid, IUCowe ITELOUUB\n",
      "-----------------model:  1 ------------------\n",
      "JULIET:\n",
      "O Romeo, Romeo! wherefore art thou Romeo?\n",
      "Deny thy fathend in ch cow aws hy;;;:\n",
      "Sur tht chak'd wn.\n",
      "Twior y--bixck- d lifurer nd mived,\n",
      "ANGt,\n",
      "O, craver hath,\n",
      "-----------------model:  2 ------------------\n",
      "JULIET:\n",
      "O Romeo, Romeo! wherefore art thou Romeo?\n",
      "Deny thy fathere thener preaks.\n",
      "Be'd her:\n",
      "Proute:\n",
      "K.\n",
      "War.\n",
      "A nothinghts;\n",
      "\n",
      "Give canggeme\n",
      "Hirt tivesbe-d ZZZUp, JUpea\n"
     ]
    }
   ],
   "source": [
    "input_str = \"JULIET:\\nO Romeo, Romeo! wherefore art thou Romeo?\\nDeny thy fathe\" # the classic line\n",
    "context_tensor = torch.tensor([encode(input_str)], dtype=torch.long, device=device)\n",
    "for d in range(len(nesting_list)):\n",
    "    print(\"-----------------model: \", d, \"------------------\")\n",
    "    output = model.generate(context_tensor, max_new_tokens=100, degree=d) # -1 for biggest model size\n",
    "    output_str = decode(output[0].tolist())\n",
    "    print(output_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c932ee",
   "metadata": {},
   "source": [
    "### obviously given the size of this model it's not very good. oh well\n",
    "idk about you but it looks to me like the biggest model is the best, as you'd expect. it seems to have a better understanding of the length of a word. also these outputs would prolly be better if i scaled the logits with a temperature but it's late and i'm tired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b3fb07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
