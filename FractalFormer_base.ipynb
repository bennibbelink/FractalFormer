{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ccc40e3",
   "metadata": {},
   "source": [
    "# FractaFormer\n",
    "\n",
    "this base version is going to be absurdly terribly no-good inefficient because we're taking the biggest computational issue ($O(t^2)$ attention) and making it way worse by doing MANY of them at once and then having to keep track of each parameter's gradient from MANY different perspectives. This is basically just an extension of [MatFormer+](https://github.com/evintunador/matryoshkaGPT/blob/main/MatFormer%2B.ipynb) where instead of one inner model, we have 2 (or whatever number you specify) models inside 1 at each layer\n",
    "\n",
    "# TODO\n",
    "- ~output~\n",
    "    - ~tensor~\n",
    "    - ~tuple~\n",
    "    - triple check test\n",
    "- ~loss~\n",
    "    - ~tuple~\n",
    "    - triple check test\n",
    "- ~model itself~\n",
    "    - ~tensor~\n",
    "    - ~tuple~\n",
    "    - triple check test\n",
    "- adjust verboseness to be function-specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95a204aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# used for the tokenizer\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Imports used for the config\n",
    "import dataclasses \n",
    "from typing import Optional\n",
    "\n",
    "# Imports used for the model\n",
    "import re\n",
    "from typing import Any, List, Sequence, Tuple, Union\n",
    "import numpy as np\n",
    "\n",
    "# used in the training loop\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e172c181",
   "metadata": {},
   "source": [
    "# The Dataset\n",
    "\n",
    "the dataset we'll be using is just TinyShakespeare for sake of simplicity & ability to do run/train locally on any computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49ca7881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n",
      "\n",
      " ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'] 65\n"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# the first 200 characters. It's just one continuous text document with all of the works of shakespeare back-to-back\n",
    "print(text[:200])\n",
    "\n",
    "# here are all the unique characters that occur in this text and how many there are\n",
    "chars = sorted(list(set(text)))\n",
    "v = len(chars)\n",
    "print('\\n', chars, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d23a7e",
   "metadata": {},
   "source": [
    "# The Tokenizer\n",
    "\n",
    "We'll be using a very simple tokenizer I previoiusly trained off of the TinyShakespeare dataset that has 128 total tokens and ignores stuff like special tokens & regex. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb1e7cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab length:  128\n",
      "Encoded: [22, 33, 24, 21, 17, 32, 71, 27, 1, 30, 53, 83, 53, 66, 30, 53, 83, 53, 2, 1, 61, 87, 93, 105, 43, 1, 77, 58, 1, 65, 67, 1, 30, 53, 83, 53, 12] 37\n",
      "Decoded: JULIET:\n",
      "O Romeo, Romeo! wherefore art thou Romeo? 49\n"
     ]
    }
   ],
   "source": [
    "import classes.simple_tokenizer as simple_tokenizer\n",
    "\n",
    "# Load the tokenizer data using pickle\n",
    "with open('./tokenizers/tokenizer.model', 'rb') as f:\n",
    "    loaded_tokenizer_data = pickle.load(f)\n",
    "\n",
    "# Extract the stoi mapping and merges from the loaded data\n",
    "loaded_stoi = loaded_tokenizer_data['stoi']\n",
    "loaded_merges = loaded_tokenizer_data['merges']\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# Assuming loaded_stoi and loaded_merges are already loaded from the tokenizer.model file\n",
    "\n",
    "tokenizer = simple_tokenizer.SimpleTokenizer(loaded_stoi, loaded_merges)\n",
    "print(\"vocab length: \", tokenizer.vocab_len)\n",
    "\n",
    "# Encoding text\n",
    "encoded_text = tokenizer.encode(\"JULIET:\\nO Romeo, Romeo! wherefore art thou Romeo?\")\n",
    "print(\"Encoded:\", encoded_text, len(encoded_text))\n",
    "\n",
    "# Decoding back\n",
    "decoded_text = tokenizer.decode(encoded_text)\n",
    "print(\"Decoded:\", decoded_text, len(decoded_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac092b1a",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dedbb53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single large model -> hierarchy of many smaller models inside\n",
      "model_count: [1, 2, 4]\n",
      "model_dim_list: [128, 64, 32]\n",
      "head_dim_list: [32, 16, 8]\n",
      "verbose: {'RMSNorm': False, 'MLP': False, 'MQA': False, 'Layer': False, 'OutputLayer': False, 'FractalLoss': False, 'FractalFormer': False, 'Sampler': False, 'Generate': False}\n"
     ]
    }
   ],
   "source": [
    "import classes.ff_config as ff_config\n",
    "config = ff_config.Config(tokenizer.vocab_len)\n",
    "\n",
    "print(\"single large model -> hierarchy of many smaller models inside\")\n",
    "print(f\"model_count: {config.model_count}\")\n",
    "print(f\"model_dim_list: {config.model_dim_list}\")\n",
    "print(f\"head_dim_list: {config.head_dim_list}\")\n",
    "print(f\"verbose: {config.verbose}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52c2b9d",
   "metadata": {},
   "source": [
    "# Rotary Positional Encoding (RoPE)\n",
    "\n",
    "i don't think i need to adjust the code for this one as long as i always call it individually"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac23cbdc",
   "metadata": {},
   "source": [
    "# RMSNorm\n",
    "\n",
    "Layernorm is relatively simple code-wise. However, of note is the fact that during training, the entire full length vector gets normalized whereas during inference we only layernorm the sub-vector we've been given if we're not using the full model size. This is interesting because RMSNorm puts a vector of length $d$ onto a hypersphere of radius $\\sqrt{d}$ which means that while the embeddings of the largest model exist on a hypersphere of the aforementioned size, for each number of layers $i\\in\\mathbb{N}$ s.t. $0 < i \\leq$ `config.model_count` the embeddings are placed onto a hypersphere of radius $\\sqrt{\\frac{d}{s^i}}$ where $s=$`config.split`. I'm not sure yet exactly how to interpret this concatenation of vectors geometrically. When you combine the entries of two hypserspheres to make a larger hypserspheres, what happens to the feature groupings on the surface of the smaller hyperspheres? I presume there are some type of interaction effects or something. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e8bdc0",
   "metadata": {},
   "source": [
    "The following cell was designed to help you visualize what's happening with RMSNorm's splicing. With RMSNorm we'll only have to think about doing this with individual tensors, but with future methods like MLP and MQA we'll have to create an entirely separate forward method used during training that deals with tuples of tensors. The thing to pay attention to here is the size of the scale weights. scale_weights' entries are 0's because we've not yet undergone training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28bbeb79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Micro Hyperparameters -------\n",
      "model_count:  [1, 2, 4]\n",
      "model_dim_list:  [4, 2, 1]\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the big model |-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 2, 4])\n",
      "tensor([[[0.1906, 0.8151, 0.7329, 0.6793],\n",
      "         [0.1616, 0.3718, 0.9625, 0.6126]]])\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 2, 4])\n",
      "tensor([[[0.1906, 0.8151, 0.7329, 0.6793],\n",
      "         [0.1616, 0.3718, 0.9625, 0.6126]]])\n",
      "normed x: torch.Size([1, 2, 4])\n",
      "tensor([[[0.2924, 1.2506, 1.1244, 1.0423],\n",
      "         [0.2669, 0.6142, 1.5899, 1.0118]]])\n",
      "dim: 4\n",
      "skip: 0\n",
      "spliced scale: torch.Size([4])\n",
      "tensor([0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 2, 4])\n",
      "tensor([[[0.2924, 1.2506, 1.1244, 1.0423],\n",
      "         [0.2669, 0.6142, 1.5899, 1.0118]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "y: torch.Size([1, 2, 4])\n",
      "tensor([[[0.2924, 1.2506, 1.1244, 1.0423],\n",
      "         [0.2669, 0.6142, 1.5899, 1.0118]]], grad_fn=<MulBackward0>)\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the first sub-model |-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 2, 2])\n",
      "tensor([[[0.2939, 0.9952],\n",
      "         [0.8182, 0.3735]]])\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 2, 2])\n",
      "tensor([[[0.2939, 0.9952],\n",
      "         [0.8182, 0.3735]]])\n",
      "normed x: torch.Size([1, 2, 2])\n",
      "tensor([[[0.4005, 1.3563],\n",
      "         [1.2865, 0.5872]]])\n",
      "dim: 2\n",
      "skip: 0\n",
      "spliced scale: torch.Size([2])\n",
      "tensor([0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 2, 2])\n",
      "tensor([[[0.4005, 1.3563],\n",
      "         [1.2865, 0.5872]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "y: torch.Size([1, 2, 2])\n",
      "tensor([[[0.4005, 1.3563],\n",
      "         [1.2865, 0.5872]]], grad_fn=<MulBackward0>)\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the second sub-model |-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 2, 2])\n",
      "tensor([[[0.9495, 0.0120],\n",
      "         [0.4208, 0.3156]]])\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 2, 2])\n",
      "tensor([[[0.9495, 0.0120],\n",
      "         [0.4208, 0.3156]]])\n",
      "normed x: torch.Size([1, 2, 2])\n",
      "tensor([[[1.4141, 0.0178],\n",
      "         [1.1314, 0.8485]]])\n",
      "dim: 2\n",
      "skip: 2\n",
      "spliced scale: torch.Size([2])\n",
      "tensor([0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 2, 2])\n",
      "tensor([[[1.4141, 0.0178],\n",
      "         [1.1314, 0.8485]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "y: torch.Size([1, 2, 2])\n",
      "tensor([[[1.4141, 0.0178],\n",
      "         [1.1314, 0.8485]]], grad_fn=<MulBackward0>)\n",
      "---------- RESET CONFIG --------\n",
      "model_count:  [1, 2, 4]\n"
     ]
    }
   ],
   "source": [
    "import classes.rms_norm as rms_norm\n",
    "\n",
    "# Testing our RMSNorm's forward()\n",
    "print(\"--------- Micro Hyperparameters -------\")\n",
    "hold = config.hidden_size\n",
    "config.hidden_size = 4\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the big model |-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,2,config.hidden_size)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "norm = rms_norm.RMSNorm(config.hidden_size)\n",
    "norm.verbose = True\n",
    "y = norm(x)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the first sub-model |-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,2,config.hidden_size//2)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "norm = rms_norm.RMSNorm(config.hidden_size)\n",
    "norm.verbose = True\n",
    "y = norm(x)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the second sub-model |-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,2,config.hidden_size//2)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "norm = rms_norm.RMSNorm(config.hidden_size)\n",
    "norm.verbose = True\n",
    "y = norm(x, model=1)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "print(\"---------- RESET CONFIG --------\")\n",
    "config.hidden_size = hold\n",
    "print(\"model_count: \", config.model_count)\n",
    "\n",
    "# clear up memory\n",
    "del hold, x, y, norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3d5428",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./images/ffwd.jpeg\" width=\"512\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5cf879",
   "metadata": {},
   "source": [
    "The following two cells are designed to help you comprehend what's happening. If you walk through every single print statement and follow along even down to watching what happens to each weight, you'll be able to clearly see what's happening with the odd splicing behavior. In order to make this somewhat feasible, I've set very small matrices for these examples. However I will admit it is still inevitably a pain, which is why I included the drawings above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "949b1e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Micro Hyperparameters -------\n",
      "model_count:  [1, 2, 4]\n",
      "model_dim_list:  [4, 2, 1]\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the big model |-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 2, 4])\n",
      "tensor([[[0.9599, 0.7425, 0.4285, 0.6888],\n",
      "         [0.4940, 0.4207, 0.9316, 0.2704]]])\n",
      "y: torch.Size([1, 2, 4])\n",
      "tensor([[[ 0.2219, -0.0904,  0.1551,  0.0908],\n",
      "         [ 0.3040, -0.0764,  0.0677,  0.1387]]], grad_fn=<AddBackward0>)\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the first sub-model |-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 2, 2])\n",
      "tensor([[[0.1443, 0.7489],\n",
      "         [0.0171, 0.4325]]])\n",
      "y: torch.Size([1, 2, 2])\n",
      "tensor([[[0.0676, 0.3493],\n",
      "         [0.0720, 0.3463]]], grad_fn=<AddBackward0>)\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the second sub-model |-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 2, 2])\n",
      "tensor([[[0.4050, 0.4628],\n",
      "         [0.0385, 0.7982]]])\n",
      "y: torch.Size([1, 2, 2])\n",
      "tensor([[[-0.3495,  0.3693],\n",
      "         [-0.3573,  0.3609]]], grad_fn=<AddBackward0>)\n",
      "---------- RESET CONFIG --------\n",
      "model_count:  [1, 2, 4]\n"
     ]
    }
   ],
   "source": [
    "import classes.mlp as my_mlp\n",
    "# Testing our MLP's forwardTensor()\n",
    "verbose = True\n",
    "print(\"--------- Micro Hyperparameters -------\")\n",
    "hold = config.hidden_size\n",
    "config.hidden_size = 4\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the big model |-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,2,4)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "mlp = my_mlp.MLP(4,8)\n",
    "y = mlp(x)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the first sub-model |-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,2,2)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "mlp = my_mlp.MLP(4,8)\n",
    "y = mlp(x)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the second sub-model |-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,2,2)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "mlp = my_mlp.MLP(4,8)\n",
    "y = mlp(x, model=1)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "verbose = False\n",
    "print(\"---------- RESET CONFIG --------\")\n",
    "config.hidden_size = hold\n",
    "print(\"model_count: \", config.model_count)\n",
    "\n",
    "# clear up memory\n",
    "del hold, x, y, mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b0a4ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Micro Hyperparameters -------\n",
      "model_count:  [1, 2]\n",
      "model_dim_list:  [4, 2]\n",
      "x: ((tensor([[[-1.7272e-02,  1.7605e+00,  1.3065e+00,  1.0095e-01],\n",
      "         [ 2.2699e+00,  1.1732e-03, -5.2974e-01, -9.1841e-03]]]),), (tensor([[[-0.4960,  0.9603],\n",
      "         [ 0.4245,  1.8475]]]), tensor([[[ 0.7208,  0.4512],\n",
      "         [-1.1678,  1.6859]]])))\n",
      "out: ((tensor([[[ 0.0859, -0.5522, -0.5765, -0.2148],\n",
      "         [ 0.0753,  0.2515, -0.0000,  0.5056]]], grad_fn=<MulBackward0>),), (tensor([[[ 0.2315, -0.3805],\n",
      "         [ 0.0000, -0.4768]]], grad_fn=<MulBackward0>), tensor([[[-0.0000, 0.1465],\n",
      "         [-0.0000, 0.1279]]], grad_fn=<MulBackward0>)))\n",
      "---------- RESET CONFIG --------\n",
      "model_count:  [1, 2, 4]\n",
      "model_dim_list:  [128, 64, 32]\n"
     ]
    }
   ],
   "source": [
    "# Testing our MLP's forwardTuple()\n",
    "verbose = True\n",
    "print(\"--------- Micro Hyperparameters -------\")\n",
    "hold1, hold2 = config.hidden_size, config.levels\n",
    "config.hidden_size = 4\n",
    "config.levels = 2\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "\n",
    "mlp = my_mlp.MLP(4,8)\n",
    "x = ((torch.randn((1,2,4)),),\n",
    "     (torch.randn((1,2,2)),torch.randn((1,2,2)))\n",
    "    )\n",
    "print(f\"x: {x}\")\n",
    "out = mlp(x)\n",
    "print(f\"out: {out}\")\n",
    "\n",
    "verbose = False\n",
    "print(\"---------- RESET CONFIG --------\")\n",
    "config.hidden_size = hold1\n",
    "config.levels = hold2\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "\n",
    "# clear up memory\n",
    "del hold1, hold2, x, out, mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14cf359",
   "metadata": {},
   "source": [
    "# Attention\n",
    "\n",
    "To subset the attention heads, we have to not only splice according to the model's embedding dimension but also take into account new smaller head sizes and how they're spaced throughout the matrix. I'm assuming you know how self-attention works well enough to look at this weight matrix and get the idea\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./images/sa.jpeg\" width=\"512\"/>\n",
    "</p>\n",
    "\n",
    "then we've gotta concatenate the outputs of each head\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./images/mha_concat.jpeg\" width=\"512\"/>\n",
    "</p>\n",
    "\n",
    "and after that linearly project them\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./images/mha_proj.jpeg\" width=\"512\"/>\n",
    "</p>\n",
    "\n",
    "this is the place where our splicing gets conceptually annoying. instead of just grabbing the matrix in the upper corner, because of the way attention head output concatenation works we actually need to skip over certain parts of the linear projection matrix and then concatenate them together in order to use them. Here's an example of what the matrix multiplication looks like. on the left is a simplified version of the concatenated attention heads where i just showed it as a matrix rather than a tensor, and then on the right is the actual projection matrix. notice how the numbers in the pink output matrix look similar to the first column of the purple output matrix with a positive number, its negative, and then a smaller positive number; that's the self-similarity in action. the yellow arrows point to the parts that get skipped over. obviously this would look a lot uglier with bigger matrices & incorporating the blue/green layer\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./images/mha_proj_matmul.jpeg\" width=\"512\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c01ce03",
   "metadata": {},
   "source": [
    "And here are the detailed print statements for the attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa877863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Micro Hyperparameters -------\n",
      "model_count:  [1, 2, 4]\n",
      "model_dim_list:  [8, 4, 2]\n",
      "head_dim_list:  [4, 2, 1]\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the big model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 3, 8])\n",
      "tensor([[[0.0263, 0.9244, 0.4639, 0.6496, 0.1376, 0.2457, 0.0823, 0.5641],\n",
      "         [0.0526, 0.2470, 0.2450, 0.9060, 0.0645, 0.1045, 0.1379, 0.9632],\n",
      "         [0.7057, 0.2345, 0.5654, 0.4798, 0.5494, 0.4583, 0.3758, 0.6214]]])\n",
      "y: torch.Size([1, 3, 8])\n",
      "tensor([[[ 0.0852, -0.2793, -0.0199, -0.0007, -0.1245,  0.2309, -0.1641,\n",
      "           0.2679],\n",
      "         [ 0.1774, -0.1397, -0.0198, -0.0094, -0.0488,  0.2022, -0.1168,\n",
      "           0.1838],\n",
      "         [ 0.2226, -0.1038, -0.0217,  0.0018, -0.0344,  0.2372, -0.1085,\n",
      "           0.2067]]], grad_fn=<UnsafeViewBackward0>)\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the first sub-model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[0.3838, 0.1565, 0.9070, 0.4414],\n",
      "         [0.7674, 0.9071, 0.7957, 0.8152],\n",
      "         [0.6029, 0.6673, 0.2373, 0.3254]]])\n",
      "y: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.0659, -0.0848,  0.0320,  0.0086],\n",
      "         [-0.0380, -0.0684,  0.0302,  0.0176],\n",
      "         [-0.0248, -0.0533,  0.0249,  0.0170]]], grad_fn=<UnsafeViewBackward0>)\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the second sub-model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[0.6098, 0.2635, 0.1094, 0.9237],\n",
      "         [0.2642, 0.5399, 0.0368, 0.9004],\n",
      "         [0.7280, 0.2402, 0.3648, 0.6906]]])\n",
      "y: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.0335,  0.0992,  0.1042, -0.1659],\n",
      "         [ 0.0064,  0.1042,  0.1105, -0.1555],\n",
      "         [-0.0267,  0.0942,  0.0989, -0.1555]]], grad_fn=<UnsafeViewBackward0>)\n",
      "---------- RESET CONFIG --------\n",
      "model_count:  [1, 2, 4]\n",
      "model_dim_list:  [128, 64, 32]\n",
      "head_dim_list:  [32, 16, 8]\n"
     ]
    }
   ],
   "source": [
    "import classes.multi_query_attention as mqa\n",
    "\n",
    "# Testing our Attention's forwardTensor()\n",
    "verbose = True\n",
    "\n",
    "print(\"--------- Micro Hyperparameters -------\")\n",
    "hold1, hold2, hold3, hold4 = config.hidden_size, config.num_attention_heads, config.head_dim, config.max_position_embeddings\n",
    "config.hidden_size = 8\n",
    "config.num_attention_heads = 2\n",
    "config.head_dim = 4\n",
    "config.max_position_embeddings = 3\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "print(\"head_dim_list: \", config.head_dim_list)\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the big model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,3,8)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "att = mqa.MultiQueryAttention(config)\n",
    "y = att(x)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the first sub-model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,3,4)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "att = mqa.MultiQueryAttention(config)\n",
    "y = att(x)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the second sub-model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,3,4)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "att = mqa.MultiQueryAttention(config)\n",
    "y = att(x, model=1)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "verbose = False\n",
    "print(\"---------- RESET CONFIG --------\")\n",
    "config.hidden_size = hold1\n",
    "config.num_attention_heads = hold2\n",
    "config.head_dim = hold3\n",
    "config.max_position_embeddings = hold4\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "print(\"head_dim_list: \", config.head_dim_list)\n",
    "\n",
    "# clear up memory\n",
    "del hold1, hold2, hold3, hold4, x, att, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c22983d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Micro Hyperparameters -------\n",
      "model_count:  [1, 2]\n",
      "model_dim_list:  [8, 4]\n",
      "head_dim_list:  [4, 2]\n",
      "x: ((tensor([[[-2.2259e+00,  4.0993e-01, -1.0243e+00,  1.2434e+00,  9.5531e-01,\n",
      "          -8.1069e-02,  2.4555e-01,  5.9443e-01],\n",
      "         [-6.1069e-01,  7.2299e-01, -3.8122e-01,  9.6473e-02,  4.5401e-01,\n",
      "           5.4707e-01,  1.5144e+00, -1.6327e+00],\n",
      "         [ 4.8189e-01, -9.3894e-01, -1.5024e+00, -4.4929e-04, -3.3427e-01,\n",
      "          -2.0939e+00, -7.9109e-02,  1.4455e+00]]]),), (tensor([[[ 0.1748, -2.0631,  0.1800, -0.4377],\n",
      "         [-0.1954, -2.6433, -2.1828, -0.4821],\n",
      "         [ 0.9416, -0.8894, -0.9836, -0.0447]]]), tensor([[[-1.9438, -0.5415,  0.7859,  2.4925],\n",
      "         [-0.9194, -0.2384,  0.5221, -0.0625],\n",
      "         [-0.8240,  0.1747,  0.8672,  0.9473]]])))\n",
      "out: ((tensor([[[-0.0224, -0.1037, -0.0299, -0.3154, -0.0000,  0.1995, -0.0606,\n",
      "          -0.1362],\n",
      "         [ 0.1232,  0.2063, -0.2081, -0.1087, -0.1401,  0.3522,  0.0790,\n",
      "          -0.2274],\n",
      "         [-0.1498, -0.2215,  0.0000, -0.0925, -0.2532,  0.1175, -0.0000,\n",
      "           0.0285]]], grad_fn=<MulBackward0>),), (tensor([[[-0.0212, -0.1095,  0.0000,  0.2789],\n",
      "         [-0.1155, -0.2901,  0.3103,  0.0000],\n",
      "         [-0.1348, -0.3131,  0.2922,  0.0469]]], grad_fn=<MulBackward0>), tensor([[[ 0.0707, -0.1352,  0.2829,  0.1861],\n",
      "         [ 0.0498, -0.0958,  0.1777,  0.1090],\n",
      "         [ 0.0527, -0.0000,  0.1837,  0.1102]]], grad_fn=<MulBackward0>)))\n",
      "---------- RESET CONFIG --------\n",
      "model_count:  [1, 2, 4]\n",
      "model_dim_list:  [128, 64, 32]\n",
      "head_dim_list:  [32, 16, 8]\n"
     ]
    }
   ],
   "source": [
    "# Testing our Attention's forwardTuple()\n",
    "verbose = True\n",
    "\n",
    "print(\"--------- Micro Hyperparameters -------\")\n",
    "hold1, hold2, hold3, hold4, hold5 = config.hidden_size, config.num_attention_heads, config.head_dim, config.levels, config.max_position_embeddings\n",
    "config.hidden_size = 8\n",
    "config.num_attention_heads = 2\n",
    "config.head_dim = 4\n",
    "config.levels = 2\n",
    "config.max_position_embeddings = 3\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "print(\"head_dim_list: \", config.head_dim_list)\n",
    "\n",
    "att = mqa.MultiQueryAttention(config)\n",
    "# we need to make sure to send in a tuple of the expected size. above we set hidden_size=4 and levels=2\n",
    "x = ((torch.randn((1,3,8)),),(torch.randn((1,3,4)),torch.randn((1,3,4))))\n",
    "print(f\"x: {x}\")\n",
    "out = att(x)\n",
    "print(f\"out: {out}\")\n",
    "\n",
    "verbose = False\n",
    "print(\"---------- RESET CONFIG --------\")\n",
    "config.hidden_size = hold1\n",
    "config.num_attention_heads = hold2\n",
    "config.head_dim = hold3\n",
    "config.levels = hold4\n",
    "config.max_position_embeddings = hold5\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "print(\"head_dim_list: \", config.head_dim_list)\n",
    "\n",
    "# clear up memory\n",
    "del hold1, hold2, hold3, hold4, hold5, x, att, out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67bb47d",
   "metadata": {},
   "source": [
    "# Layer\n",
    "\n",
    "nothing too interesting here besides the absurd amount of memory we're probably taking up with these tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b27eb45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Micro Hyperparameters -------\n",
      "model_count:  [1, 2, 4]\n",
      "model_dim_list:  [8, 4, 2]\n",
      "head_dim_list:  [4, 2, 1]\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the big model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 3, 8])\n",
      "tensor([[[0.2900, 0.3502, 0.3640, 0.6985, 0.3658, 0.9914, 0.2533, 0.3429],\n",
      "         [0.6997, 0.9056, 0.1009, 0.3460, 0.8976, 0.8859, 0.0885, 0.6918],\n",
      "         [0.5278, 0.8558, 0.6953, 0.0507, 0.0577, 0.9630, 0.4264, 0.3923]]])\n",
      "y: torch.Size([1, 3, 8])\n",
      "tensor([[[ 0.4562,  0.2389,  0.4436,  0.5384,  0.1791,  0.7488,  0.5640,\n",
      "          -0.0758],\n",
      "         [ 0.9391,  0.4394,  0.1791,  0.1711,  0.6333,  0.5040,  0.4082,\n",
      "           0.0405],\n",
      "         [ 0.6622,  0.6216,  0.7912, -0.1079, -0.2573,  0.6049,  0.6932,\n",
      "          -0.0120]]], grad_fn=<AddBackward0>)\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the first sub-model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[0.6970, 0.5876, 0.6012, 0.4722],\n",
      "         [0.0642, 0.0082, 0.2088, 0.1844],\n",
      "         [0.0217, 0.4466, 0.2522, 0.7789]]])\n",
      "y: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.6859,  0.7358,  0.5705,  0.4057],\n",
      "         [ 0.0116,  0.0381,  0.2444,  0.0165],\n",
      "         [-0.0440,  0.5756,  0.1413,  0.6696]]], grad_fn=<AddBackward0>)\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the second sub-model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[0.6262, 0.0524, 0.6069, 0.5118],\n",
      "         [0.8672, 0.7293, 0.1528, 0.7094],\n",
      "         [0.1103, 0.2524, 0.2834, 0.3841]]])\n",
      "y: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.4893, -0.1271,  0.7982,  0.2071],\n",
      "         [ 0.6906,  0.5626,  0.3198,  0.4031],\n",
      "         [-0.0250,  0.0143,  0.4297,  0.1083]]], grad_fn=<AddBackward0>)\n",
      "---------- RESET CONFIG --------\n",
      "model_count:  [1, 2, 4]\n",
      "model_dim_list:  [128, 64, 32]\n",
      "head_dim_list:  [32, 16, 8]\n"
     ]
    }
   ],
   "source": [
    "import classes.layer as layyyyer\n",
    "# Testing our Layer's forwardTensor()\n",
    "verbose = True\n",
    "\n",
    "print(\"--------- Micro Hyperparameters -------\")\n",
    "hold1, hold2, hold3, hold4 = config.hidden_size, config.num_attention_heads, config.head_dim, config.max_position_embeddings\n",
    "config.hidden_size = 8\n",
    "config.num_attention_heads = 2\n",
    "config.head_dim = 4\n",
    "config.max_position_embeddings = 3\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "print(\"head_dim_list: \", config.head_dim_list)\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the big model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,3,config.hidden_size)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "layer = layyyyer.Layer(config)\n",
    "y = layer(x)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the first sub-model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,3,config.hidden_size//config.split)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "y = layer(x)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the second sub-model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,3,config.hidden_size//config.split)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "y = layer(x, model=1)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "verbose = False\n",
    "print(\"---------- RESET CONFIG --------\")\n",
    "config.hidden_size = hold1\n",
    "config.num_attention_heads = hold2\n",
    "config.head_dim = hold3\n",
    "config.max_position_embeddings = hold4\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "print(\"head_dim_list: \", config.head_dim_list)\n",
    "\n",
    "# clear up memory\n",
    "del hold1, hold2, hold3, hold4, x, layer, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0531758f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Micro Hyperparameters -------\n",
      "model_count:  [1, 2]\n",
      "model_dim_list:  [8, 4]\n",
      "head_dim_list:  [4, 2]\n",
      "x: ((tensor([[[-0.4899,  0.6559,  0.8784, -0.1362,  0.4753,  0.3774,  1.1159,\n",
      "           0.1347],\n",
      "         [-1.1374,  1.5822, -1.3046,  0.2054, -0.1641, -0.8444,  0.9152,\n",
      "          -1.2537],\n",
      "         [-1.6206, -0.2979, -0.5589, -1.7389,  0.2237,  0.8430,  0.2355,\n",
      "          -0.6535]]]),), (tensor([[[ 0.1224,  1.3570, -0.3866, -1.7016],\n",
      "         [ 0.5770, -0.3415,  0.8091,  1.2086],\n",
      "         [-0.0276, -0.4859, -0.2800,  1.2966]]]), tensor([[[-1.6194, -1.2758,  0.4798, -1.4096],\n",
      "         [ 1.1032, -0.4800,  0.1265, -0.7214],\n",
      "         [ 1.0496, -0.6565, -0.1099,  0.6584]]])))\n",
      "out: ((tensor([[[-1.4147,  0.4076,  1.6376, -0.7523,  0.6936, -0.1867,  1.3497,\n",
      "           0.4808],\n",
      "         [-1.5719,  1.1740, -0.7212, -0.0457,  0.3139, -1.3633,  1.0313,\n",
      "          -0.6909],\n",
      "         [-1.9420, -0.5451, -0.0535, -1.9322,  0.5954,  0.0571,  0.5031,\n",
      "          -0.7269]]], grad_fn=<AddBackward0>),), (tensor([[[-0.1365,  1.1174, -0.3261, -1.6862],\n",
      "         [ 0.5191, -0.5736,  0.6389,  1.1992],\n",
      "         [-0.0438, -0.6743, -0.4817,  1.2253]]], grad_fn=<AddBackward0>), tensor([[[-1.3031, -1.7703,  0.3454, -1.3247],\n",
      "         [ 1.3375, -0.7603,  0.0507, -0.6917],\n",
      "         [ 1.0199, -0.9543, -0.2460,  0.7515]]], grad_fn=<AddBackward0>)))\n",
      "---------- RESET CONFIG --------\n",
      "model_count:  [1, 2, 4]\n",
      "model_dim_list:  [128, 64, 32]\n",
      "head_dim_list:  [32, 16, 8]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Testing our Layer's forwardTuple()\n",
    "verbose = True\n",
    "\n",
    "print(\"--------- Micro Hyperparameters -------\")\n",
    "hold1, hold2, hold3, hold4, hold5 = config.hidden_size, config.num_attention_heads, config.head_dim, config.levels, config.max_position_embeddings\n",
    "config.hidden_size = 8\n",
    "config.num_attention_heads = 2\n",
    "config.head_dim = 4\n",
    "config.levels = 2\n",
    "config.max_position_embeddings = 3\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "print(\"head_dim_list: \", config.head_dim_list)\n",
    "\n",
    "layer = layyyyer.Layer(config)\n",
    "# we need to make sure to send in a tuple of the expected size. above we set hidden_size=4 and levels=2\n",
    "x = ((torch.randn((1,3,8)),),(torch.randn((1,3,4)),torch.randn((1,3,4))))\n",
    "print(f\"x: {x}\")\n",
    "out = layer(x)\n",
    "print(f\"out: {out}\")\n",
    "\n",
    "verbose = False\n",
    "print(\"---------- RESET CONFIG --------\")\n",
    "config.hidden_size = hold1\n",
    "config.num_attention_heads = hold2\n",
    "config.head_dim = hold3\n",
    "config.levels = hold4\n",
    "config.max_position_embeddings = hold5\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "print(\"head_dim_list: \", config.head_dim_list)\n",
    "\n",
    "# clear up memory\n",
    "del hold1, hold2, hold3, hold4, hold5, x, layer, out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b302f731",
   "metadata": {},
   "source": [
    "# Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "feeb1bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Micro Hyperparameters -------\n",
      "model_count:  [1, 2, 4]\n",
      "model_dim_list:  [4, 2, 1]\n",
      "embedding: torch.Size([5, 4])\n",
      "tensor([[ 1.3118, -0.6339, -1.6035, -1.1829],\n",
      "        [ 0.1080,  0.9321,  0.3213,  1.3060],\n",
      "        [-0.3626, -1.0572,  0.1246,  0.0184],\n",
      "        [ 0.5701, -1.2204, -1.1347,  0.6017],\n",
      "        [-1.7379, -0.3902,  0.6982, -0.8460]])\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the big model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[0.5452, 0.3534, 0.4683, 0.2556],\n",
      "         [0.3696, 0.0615, 0.8479, 0.3796],\n",
      "         [0.0931, 0.4082, 0.7007, 0.8249]]])\n",
      "y: torch.Size([1, 3, 5])\n",
      "tensor([[[-1.0834,  2.5314, -2.1498, -1.2731, -2.2169],\n",
      "         [-2.2045,  2.1072, -0.3068, -1.2832, -0.7550],\n",
      "         [-3.1223,  3.5583, -1.1117, -1.3775, -0.8727]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the first sub-model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 3, 2])\n",
      "tensor([[[0.4236, 0.9295],\n",
      "         [0.7633, 0.7300],\n",
      "         [0.4454, 0.7053]]])\n",
      "y: torch.Size([1, 3, 5])\n",
      "tensor([[[-0.0451,  1.9033, -1.9906, -1.2978, -1.2079],\n",
      "         [ 0.7000,  1.5394, -1.7765, -0.6406, -1.7131],\n",
      "         [ 0.2258,  1.8027, -1.9460, -1.0801, -1.4124]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the second sub-model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 3, 2])\n",
      "tensor([[[0.7081, 0.3744],\n",
      "         [0.7021, 0.1233],\n",
      "         [0.9494, 0.5169]]])\n",
      "y: torch.Size([1, 3, 5])\n",
      "tensor([[[-1.9778,  1.3302,  1.8853, -1.1240,  0.4043],\n",
      "         [-1.7906,  0.8064,  1.9991, -1.5782,  0.9870],\n",
      "         [-1.9813,  1.3482,  1.8771, -1.1038,  0.3805]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "---------- RESET CONFIG --------\n",
      "model_count:  [1, 2, 4]\n",
      "model_dim_list:  [128, 64, 32]\n"
     ]
    }
   ],
   "source": [
    "import classes.output_layer as output_layer\n",
    "# Testing our OutputLayer's forwardTensor()\n",
    "verbose = True\n",
    "\n",
    "print(\"--------- Micro Hyperparameters -------\")\n",
    "hold1, hold2 = config.hidden_size, config.vocab_size\n",
    "config.hidden_size = 4\n",
    "config.vocab_size = 5\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "\n",
    "embedding = torch.randn(config.vocab_size, config.hidden_size)\n",
    "print(f\"embedding: {embedding.shape}\\n{embedding}\")\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the big model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,3,config.hidden_size)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "layer = output_layer.OutputLayer(embedding, config)\n",
    "y = layer(x)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the first sub-model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,3,config.hidden_size//config.split)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "y = layer(x)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the second sub-model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,3,config.hidden_size//config.split)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "y = layer(x, model=1)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "verbose = False\n",
    "print(\"---------- RESET CONFIG --------\")\n",
    "config.hidden_size = hold1\n",
    "config.vocab_size = hold2\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "\n",
    "# clear up memory\n",
    "del hold1, hold2, x, layer, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87ca01f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Micro Hyperparameters -------\n",
      "model_count:  [1, 2]\n",
      "model_dim_list:  [4, 2]\n",
      "embedding: torch.Size([5, 4])\n",
      "tensor([[ 0.2224, -0.1793, -0.4423,  0.8017],\n",
      "        [-0.1385, -1.2339, -0.8531,  0.1575],\n",
      "        [-0.5926,  2.3769,  0.0477, -0.0497],\n",
      "        [ 2.3957,  0.3885, -1.6190,  0.4254],\n",
      "        [ 0.2385,  0.6624, -0.7998,  0.4367]])\n",
      "x: ((tensor([[[-0.4031,  1.4628, -1.1429, -0.2040],\n",
      "         [ 0.2421,  1.5902,  2.5933,  0.8067],\n",
      "         [-1.5138,  1.1025,  1.0453, -1.1641]]]),), (tensor([[[-0.9932, -2.0957],\n",
      "         [-0.3885, -0.8978],\n",
      "         [-0.5158,  1.3748]]]), tensor([[[-0.7641, -1.2413],\n",
      "         [-1.6640, -1.5790],\n",
      "         [ 0.3750, -0.1341]]])))\n",
      "out: ((tensor([[[-1.0999,  2.1041, -1.2098, -0.8285],\n",
      "         [-0.1051,  1.9027,  2.5249,  0.7481],\n",
      "         [-1.9295,  1.6160,  0.8707, -1.1355]]], grad_fn=<AddBackward0>),), (tensor([[[-1.0618, -2.1726],\n",
      "         [-0.4744, -0.9711],\n",
      "         [-0.6836,  1.4032]]], grad_fn=<AddBackward0>), tensor([[[-0.4680, -1.5190],\n",
      "         [-1.3557, -1.8700],\n",
      "         [ 0.6341, -0.3317]]], grad_fn=<AddBackward0>)))\n",
      "---------- RESET CONFIG --------\n",
      "model_count:  [1, 2, 4]\n",
      "model_dim_list:  [128, 64, 32]\n"
     ]
    }
   ],
   "source": [
    "# Testing our Layer's forwardTuple()\n",
    "verbose = True\n",
    "\n",
    "print(\"--------- Micro Hyperparameters -------\")\n",
    "hold1, hold2, hold3, hold4 = config.hidden_size, config.levels, config.max_position_embeddings, config.hidden_size\n",
    "config.hidden_size = 4\n",
    "config.levels = 2\n",
    "config.max_position_embeddings = 3\n",
    "config.vocab_size = 5\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "\n",
    "embedding = torch.randn(config.vocab_size, config.hidden_size)\n",
    "print(f\"embedding: {embedding.shape}\\n{embedding}\")\n",
    "\n",
    "layer = layyyyer.Layer(config)\n",
    "# we need to make sure to send in a tuple of the expected size. above we set hidden_size=4 and levels=2\n",
    "x = ((torch.randn((1,3,config.hidden_size)),),\n",
    "     (torch.randn((1,3,config.hidden_size//config.split)),torch.randn((1,3,config.hidden_size//config.split))))\n",
    "print(f\"x: {x}\")\n",
    "out = layer(x)\n",
    "print(f\"out: {out}\")\n",
    "\n",
    "verbose = False\n",
    "print(\"---------- RESET CONFIG --------\")\n",
    "config.hidden_size = hold1\n",
    "config.levels = hold2\n",
    "config.max_position_embeddings = hold3\n",
    "config.vocab_size = hold4\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "\n",
    "# clear up memory\n",
    "del hold1, hold2, hold3, hold4, x, layer, out, embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9089c1",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4151cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Micro Hyperparameters -------\n",
      "model_count:  [1, 2]\n",
      "model_dim_list:  [4, 2]\n",
      "embedding: torch.Size([5, 4])\n",
      "tensor([[ 0.4059, -2.0473, -0.2735, -0.5570],\n",
      "        [ 0.7993, -0.7838,  1.1809,  1.2022],\n",
      "        [ 0.1442,  1.4582,  0.6399, -0.5694],\n",
      "        [-1.1310, -0.1624,  0.3523, -0.3711],\n",
      "        [-0.3034,  0.2289, -0.0687,  2.2043]])\n",
      "logits: ((tensor([[[ 0.6436,  0.3571,  0.1013,  0.2318, -0.1090],\n",
      "         [ 0.1568,  0.2087, -2.3688,  0.8231,  0.9686],\n",
      "         [ 0.6816, -1.4875,  0.2660, -1.7342,  0.5034]],\n",
      "\n",
      "        [[-1.2039,  1.9883, -0.2778,  1.4461,  0.3314],\n",
      "         [-0.3510,  0.1949,  1.0903, -0.1730, -1.4147],\n",
      "         [-1.0661,  0.9731, -0.8389, -0.3800,  0.1361]]]),), (tensor([[[ 0.7224, -1.7460, -1.7500,  0.1530,  0.7799],\n",
      "         [ 0.7485,  1.1115, -0.2012, -0.4903,  0.0189],\n",
      "         [ 1.0612,  0.8323, -0.1647, -3.4104,  0.6588]],\n",
      "\n",
      "        [[ 0.2231,  0.1398,  1.6119,  0.4783, -1.0464],\n",
      "         [-0.5405, -0.1441,  1.0267,  0.5986, -0.3172],\n",
      "         [-0.5817, -1.4768, -1.6596,  2.7159, -0.1850]]]), tensor([[[-0.3199,  1.9540,  0.0074,  1.1688,  0.5408],\n",
      "         [ 1.0300,  2.2741,  0.0834,  1.1861,  2.0600],\n",
      "         [-1.5180,  0.9339, -0.4956,  0.5590,  0.1129]],\n",
      "\n",
      "        [[ 0.8110,  0.9933, -0.1834,  0.6517,  0.2008],\n",
      "         [-1.2754,  0.2945, -0.1473, -0.4980,  0.8842],\n",
      "         [ 1.2235,  1.6399,  0.4467,  0.3820, -0.4537]]])))\n",
      "target: tensor([[[0, 3, 2],\n",
      "         [1, 4, 3]]])\n",
      "out: 5.230819225311279\n",
      "---------- RESET CONFIG --------\n",
      "model_count:  [1, 2, 4]\n",
      "model_dim_list:  [128, 64, 32]\n"
     ]
    }
   ],
   "source": [
    "import classes.fractal_loss as fractal_loss\n",
    "\n",
    "# Testing our FractalLoss\n",
    "verbose = True\n",
    "\n",
    "print(\"--------- Micro Hyperparameters -------\")\n",
    "hold1, hold2, hold3, hold4 = config.hidden_size, config.levels, config.max_position_embeddings, config.hidden_size\n",
    "config.hidden_size = 4\n",
    "config.levels = 2\n",
    "config.max_position_embeddings = 3\n",
    "config.vocab_size = 5\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "\n",
    "embedding = torch.randn(config.vocab_size, config.hidden_size)\n",
    "print(f\"embedding: {embedding.shape}\\n{embedding}\")\n",
    "\n",
    "loss = fractal_loss.FractalLoss(config)\n",
    "# we need to make sure to send in a tuple of the expected size. above we set hidden_size=4 and levels=2\n",
    "logits = ((torch.randn((2,3,config.vocab_size)),),\n",
    "     (torch.randn((2,3,config.vocab_size)),torch.randn((2,3,config.vocab_size))))\n",
    "print(f\"logits: {logits}\")\n",
    "target = torch.randint(config.vocab_size, (2,3)).unsqueeze(0)\n",
    "print(f\"target: {target}\")\n",
    "out = loss(logits, target)\n",
    "print(f\"out: {out}\")\n",
    "\n",
    "verbose = False\n",
    "print(\"---------- RESET CONFIG --------\")\n",
    "config.hidden_size = hold1\n",
    "config.levels = hold2\n",
    "config.max_position_embeddings = hold3\n",
    "config.vocab_size = hold4\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "\n",
    "# clear up memory\n",
    "del hold1, hold2, hold3, hold4, embedding, loss, logits, target, out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d511fec",
   "metadata": {},
   "source": [
    "# The Model itself"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7641ae",
   "metadata": {},
   "source": [
    "# Training-related Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b4604bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test splits\n",
    "data = torch.tensor(tokenizer.encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be our training dataset, the rest for validation\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d76bf9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading for training which generates a small batch of data of inputs x and targets y\n",
    "def get_batch(split, batch_size):\n",
    "    # whether we grab from our training or validation dataset\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - config.max_position_embeddings, (batch_size,))\n",
    "    x = torch.stack([data[i:i+config.max_position_embeddings] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+config.max_position_embeddings+1] for i in ix])\n",
    "    x, y = x.to(config.device), y.to(config.device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74f0d059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 37,   1,  34,  21,  75,  23,  21,  26,  19,   1,  17,  16,  35,  13,\n",
      "          30,  16,   1,  21,  34,  71,  26, 103,   1,  87,  93,   1,  39,   1,\n",
      "          54,  68,  47,  53,  42,   1,  94,   1,  58,  59,  51,  59,  50,  58,\n",
      "          59,  67,  57,   1,  40, 115,  47,  50,  57,   8,   0,  13,  61, 106,\n",
      "           1, 100,  65,   1,  27,  62, 105,  42,   1,  84,   1,  20,  39,  83,\n",
      "          57,   1,  15, 107,  58,  99,   1,  80,  56,  39,  47, 122,  58,  71,\n",
      "          18,  73,   1,  31,  53,  83,  56,  91,  58,  66,  94,  44,   1, 100,\n",
      "          65,   1, 102,  57,   1,  45,  59,  47,  50,  58,  63,   1,  87,  39,\n",
      "          42,   8,   0,  19,  53,  66,  98,  77,   1,  72,  51,   1,  87,  52,\n",
      "         104, 125,  21,   1, 100,  81,   1, 116,   1,  87,  77,   1,  72,  51,\n",
      "           1,  57,  54,  43,  39,  49,  85,  27,  36,  18,  27,  30,  16,  71,\n",
      "          18,  73,   1, 101,   1,  54,  77,  58,  66,  21,   5,  81,   1, 116,\n",
      "           1,  58, 115,  59,  40,  99,   1,  72,  43,   1, 100,  65,   1,  61,\n",
      "          73,  42,  57,  85,  31,  27,  25,  17,  30,  31,  17,  32,  71,  26,\n",
      "          73,   1,  21,  66,  40, 114,   1,  80, 126,  54,   1, 100,  65,   1,\n",
      "          54,  78,  47,  76, 104,   1,  84,   1, 101,   1, 105,  58,  59,  52,\n",
      "          43,  85,  29,  33,  17,  17,  26,   1,  25,  13,  30,  19,  13,  30,\n",
      "          17,  32,  71,  31,  53,   1,  54,  77,  58,   1, 119,   1,  57,  39,\n",
      "          42,  50,  63,   1]])\n",
      "Y VI\n",
      "\n",
      "KING EDWARD IV:\n",
      "Now here a period of tumultuous broils.\n",
      "Away with Oxford to Hames Castle straight:\n",
      "For Somerset, off with his guilty head.\n",
      "Go, bear them hence; I will not hear them speak.\n",
      "\n",
      "OXFORD:\n",
      "For my part, I'll not trouble thee with words.\n",
      "\n",
      "SOMERSET:\n",
      "Nor I, but stoop with patience to my fortune.\n",
      "\n",
      "QUEEN MARGARET:\n",
      "So part we sadly \n",
      "-------\n",
      "tensor([[  1,  34,  21,  75,  23,  21,  26,  19,   1,  17,  16,  35,  13,  30,\n",
      "          16,   1,  21,  34,  71,  26, 103,   1,  87,  93,   1,  39,   1,  54,\n",
      "          68,  47,  53,  42,   1,  94,   1,  58,  59,  51,  59,  50,  58,  59,\n",
      "          67,  57,   1,  40, 115,  47,  50,  57,   8,   0,  13,  61, 106,   1,\n",
      "         100,  65,   1,  27,  62, 105,  42,   1,  84,   1,  20,  39,  83,  57,\n",
      "           1,  15, 107,  58,  99,   1,  80,  56,  39,  47, 122,  58,  71,  18,\n",
      "          73,   1,  31,  53,  83,  56,  91,  58,  66,  94,  44,   1, 100,  65,\n",
      "           1, 102,  57,   1,  45,  59,  47,  50,  58,  63,   1,  87,  39,  42,\n",
      "           8,   0,  19,  53,  66,  98,  77,   1,  72,  51,   1,  87,  52, 104,\n",
      "         125,  21,   1, 100,  81,   1, 116,   1,  87,  77,   1,  72,  51,   1,\n",
      "          57,  54,  43,  39,  49,  85,  27,  36,  18,  27,  30,  16,  71,  18,\n",
      "          73,   1, 101,   1,  54,  77,  58,  66,  21,   5,  81,   1, 116,   1,\n",
      "          58, 115,  59,  40,  99,   1,  72,  43,   1, 100,  65,   1,  61,  73,\n",
      "          42,  57,  85,  31,  27,  25,  17,  30,  31,  17,  32,  71,  26,  73,\n",
      "           1,  21,  66,  40, 114,   1,  80, 126,  54,   1, 100,  65,   1,  54,\n",
      "          78,  47,  76, 104,   1,  84,   1, 101,   1, 105,  58,  59,  52,  43,\n",
      "          85,  29,  33,  17,  17,  26,   1,  25,  13,  30,  19,  13,  30,  17,\n",
      "          32,  71,  31,  53,   1,  54,  77,  58,   1, 119,   1,  57,  39,  42,\n",
      "          50,  63,   1,  69]])\n",
      " VI\n",
      "\n",
      "KING EDWARD IV:\n",
      "Now here a period of tumultuous broils.\n",
      "Away with Oxford to Hames Castle straight:\n",
      "For Somerset, off with his guilty head.\n",
      "Go, bear them hence; I will not hear them speak.\n",
      "\n",
      "OXFORD:\n",
      "For my part, I'll not trouble thee with words.\n",
      "\n",
      "SOMERSET:\n",
      "Nor I, but stoop with patience to my fortune.\n",
      "\n",
      "QUEEN MARGARET:\n",
      "So part we sadly in\n"
     ]
    }
   ],
   "source": [
    "# a demonstration of what a batch with batch_size=1 looks like. Notice the one-token offset in characters\n",
    "xb, yb = get_batch('train', 1)\n",
    "print(xb)\n",
    "print(tokenizer.decode(xb.squeeze(0).tolist()))\n",
    "print(\"-------\")\n",
    "print(yb)\n",
    "print(tokenizer.decode(yb.squeeze(0).tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78a7c1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, batch_size, eval_iters = 10): # to estimate loss during the training loop\n",
    "    out = {}\n",
    "    model.eval() # sets model to eval mode\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split, batch_size)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train() # just resets to training mode\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fc68c9",
   "metadata": {},
   "source": [
    "# Instantiating a brand new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06118186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config(max_position_embeddings=256, num_hidden_layers=4, num_attention_heads=4, num_key_value_heads=1, hidden_size=128, head_dim=32, rms_norm_eps=1e-06)\n",
      "972.672 K parameters\n",
      "FractalFormer_base(\n",
      "  (embedder): Embedding(128, 128)\n",
      "  (embedder_norm): RMSNorm()\n",
      "  (layers): ModuleList(\n",
      "    (0-3): 4 x Layer(\n",
      "      (self_attn): MultiQueryAttention(\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (input_layernorm): RMSNorm()\n",
      "      (post_attention_layernorm): RMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (output_layer): OutputLayer(\n",
      "    (embedding_norm): RMSNorm()\n",
      "    (final_norm): RMSNorm()\n",
      "  )\n",
      "  (criterion): FractalLoss(\n",
      "    (criterion): CrossEntropyLoss()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import classes.fractal_former_base as ff_base\n",
    "\n",
    "# just to make sure nothing got messed up above. \n",
    "# if an error gets thrown in one of the test cells then the config values won't reset\n",
    "print(config)\n",
    "\n",
    "model = ff_base.FractalFormer_base(config, tokenizer).to(config.device)\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e3, 'K parameters')\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3003f7",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06f3d856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "# this is not what they used, but this learning rate & weight decay work for our tiny minGemma\n",
    "learning_rate = 3e-5\n",
    "weight_decay = 0.01\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# how long we want to train for\n",
    "max_iters = 5000\n",
    "\n",
    "# how often we want to check & see how our loss is doing\n",
    "eval_interval = 250\n",
    "\n",
    "# batch size to use\n",
    "batch_size = 12\n",
    "\n",
    "# if you want to do debugging\n",
    "config.verbose['RMSNorm'] = False\n",
    "config.verbose['MLP'] = False\n",
    "config.verbose['MQA'] = False\n",
    "config.verbose['Layer'] = False\n",
    "config.verbose['OutputLayer'] = False\n",
    "config.verbose['FractalLoss'] = False\n",
    "config.verbose['FractalFormer'] = False\n",
    "config.verbose['Sampler'] = False\n",
    "config.verbose['Generate'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9f76c3",
   "metadata": {},
   "source": [
    "# ------------ BOOKMARK ----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ca3e922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iters: 5000\n",
      "step 0: train loss 646.7269, val loss 647.3767, time elapsed: 1.78 seconds\n",
      "step 10: train loss 638.0670, val loss 639.3229, time elapsed: 28.80 seconds\n",
      "step 20: train loss 626.4349, val loss 628.4929, time elapsed: 56.47 seconds\n",
      "step 30: train loss 610.5033, val loss 613.5657, time elapsed: 83.83 seconds\n",
      "step 40: train loss 590.6318, val loss 593.1642, time elapsed: 113.13 seconds\n",
      "step 50: train loss 565.5765, val loss 570.2756, time elapsed: 140.79 seconds\n",
      "step 60: train loss 532.3127, val loss 537.9030, time elapsed: 170.35 seconds\n",
      "step 70: train loss 495.9375, val loss 502.2664, time elapsed: 197.25 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m         current_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     23\u001b[0m         elapsed_time \u001b[38;5;241m=\u001b[39m current_time \u001b[38;5;241m-\u001b[39m start_time\n\u001b[0;32m---> 24\u001b[0m         losses \u001b[38;5;241m=\u001b[39m \u001b[43mestimate_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28miter\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: train loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, val loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, time elapsed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00melapsed_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Disable anomaly detection after the training loop\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#torch.autograd.set_detect_anomaly(False)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/fractalformer/lib/python3.12/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 9\u001b[0m, in \u001b[0;36mestimate_loss\u001b[0;34m(model, batch_size, eval_iters)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(eval_iters):\n\u001b[1;32m      8\u001b[0m     X, Y \u001b[38;5;241m=\u001b[39m get_batch(split, batch_size)\n\u001b[0;32m----> 9\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     losses[k] \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     11\u001b[0m out[split] \u001b[38;5;241m=\u001b[39m losses\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[0;32m~/miniconda3/envs/fractalformer/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/fractalformer/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/FractalFormer/classes/fractal_former_base.py:166\u001b[0m, in \u001b[0;36mFractalFormer_base.forward\u001b[0;34m(self, input_token_ids, target_token_ids, level, model)\u001b[0m\n\u001b[1;32m    162\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# if we are training\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# training uses a tuple of tuples of tensors\u001b[39;00m\n\u001b[0;32m--> 166\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforwardTuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_token_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_token_ids\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# -> Tuple[Tuple[Tensor shape (batch_size, max_seq_len, vocab_size)]]\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# custom Fractal CELoss function\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(logits, target_token_ids) \n",
      "File \u001b[0;32m~/FractalFormer/classes/fractal_former_base.py:137\u001b[0m, in \u001b[0;36mFractalFormer_base.forwardTuple\u001b[0;34m(self, input_token_ids, target_token_ids)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers):\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegin layer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 137\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput of layer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    140\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/fractalformer/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/fractalformer/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/FractalFormer/classes/layer.py:131\u001b[0m, in \u001b[0;36mLayer.forward\u001b[0;34m(self, x, model)\u001b[0m\n\u001b[1;32m    129\u001b[0m train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(x) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mtuple\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m---------- Layer Input: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTuple\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mtrain\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch.Tensor\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforwardTuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m train \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforwardTensor(x, model)\n",
      "File \u001b[0;32m~/FractalFormer/classes/layer.py:115\u001b[0m, in \u001b[0;36mLayer.forwardTuple\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(models_per_level[i]):\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from range(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodels_per_level[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 115\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforwardTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop_bool\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforwardTensor() output: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    118\u001b[0m     out_lvl \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (output,)\n",
      "File \u001b[0;32m~/FractalFormer/classes/layer.py:64\u001b[0m, in \u001b[0;36mLayer.forwardTensor\u001b[0;34m(self, x, model, drop_bool)\u001b[0m\n\u001b[1;32m     62\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(x, model)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Transforms the normalized attention output through the MLP, introducing additional non-linearity and capacity to the model.\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop_bool\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Another residual connection\u001b[39;00m\n\u001b[1;32m     66\u001b[0m x \u001b[38;5;241m=\u001b[39m residual_connection \u001b[38;5;241m+\u001b[39m x\n",
      "File \u001b[0;32m~/miniconda3/envs/fractalformer/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/fractalformer/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/FractalFormer/classes/mlp.py:190\u001b[0m, in \u001b[0;36mMLP.forward\u001b[0;34m(self, x, model, drop_bool)\u001b[0m\n\u001b[1;32m    188\u001b[0m train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(x) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mtuple\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m---------- MLP Input: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTuple\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mtrain\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch.Tensor\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforwardTuple(x, drop_bool) \u001b[38;5;28;01mif\u001b[39;00m train \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforwardTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/FractalFormer/classes/mlp.py:126\u001b[0m, in \u001b[0;36mMLP.forwardTensor\u001b[0;34m(self, x, model)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXfuse: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mXfuse\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mXfuse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# Applies the final linear transformation to project the modulated tensor back to the hidden size.\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m Wdown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWdown\u001b[49m[i_skip:i_skip \u001b[38;5;241m+\u001b[39m i_dim, d_skip:d_skip \u001b[38;5;241m+\u001b[39m d_dim]\n\u001b[1;32m    127\u001b[0m Bdown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBdown[d_skip:d_skip \u001b[38;5;241m+\u001b[39m d_dim]\n\u001b[1;32m    128\u001b[0m outputs \u001b[38;5;241m=\u001b[39m Xfuse \u001b[38;5;241m@\u001b[39m Wdown \u001b[38;5;241m+\u001b[39m Bdown\n",
      "File \u001b[0;32m~/miniconda3/envs/fractalformer/lib/python3.12/site-packages/torch/nn/modules/module.py:1675\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1666\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[1;32m   1668\u001b[0m \u001b[38;5;66;03m# On the return type:\u001b[39;00m\n\u001b[1;32m   1669\u001b[0m \u001b[38;5;66;03m# We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\u001b[39;00m\n\u001b[1;32m   1670\u001b[0m \u001b[38;5;66;03m# This is done for better interop with various type checkers for the end users.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1673\u001b[0m \u001b[38;5;66;03m# See full discussion on the problems with returning `Union` here\u001b[39;00m\n\u001b[1;32m   1674\u001b[0m \u001b[38;5;66;03m# https://github.com/microsoft/pyright/issues/4213\u001b[39;00m\n\u001b[0;32m-> 1675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   1676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1677\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "start_time = time.time()\n",
    "\n",
    "# Enable anomaly detection. uncomment these lines if you need to do extensive debugging\n",
    "#torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "print(f\"max_iters: {max_iters}\")\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train', batch_size)\n",
    "    \n",
    "    # train\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1 or iter % 10 == 0:\n",
    "        current_time = time.time()\n",
    "        elapsed_time = current_time - start_time\n",
    "        losses = estimate_loss(model, batch_size)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, time elapsed: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Disable anomaly detection after the training loop\n",
    "#torch.autograd.set_detect_anomaly(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac6744d",
   "metadata": {},
   "source": [
    "# Saving your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090948b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model currently held in memory\n",
    "# the filename specifies the model's class, hyperparameters, and date/time it was saved\n",
    "import os\n",
    "\n",
    "# Ensure the directory exists\n",
    "model_dir = 'models'\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "# Create a shorter, more concise filename\n",
    "filename = (f'{model.__class__.__name__}'\n",
    "           f'-v{config.vocab_size}'\n",
    "           f'-max_t{config.max_position_embeddings}'\n",
    "           f'-layers{config.num_hidden_layers}'\n",
    "           f'-heads{config.num_attention_heads}'\n",
    "           f'-kv_heads{config.num_key_value_heads}'\n",
    "           f'-hidden{config.hidden_size}'\n",
    "           f'-intermediate{config.intermediate_size}'\n",
    "           f'-head_dim{config.head_dim}'\n",
    "           f'-theta{config.rope_theta}'\n",
    "           f'-levels{config.levels}'\n",
    "           f'-split{config.split}'\n",
    "           f'-lr{learning_rate}'\n",
    "           f'-decay{weight_decay}'\n",
    "           f'-batch{batch_size}'\n",
    "            f'-train_iter{15000}'\n",
    "           f'--{time.strftime(\"%Y-%m-%d|%H-%M-%S\")}.pth')\n",
    "\n",
    "# Save the model\n",
    "model_path = os.path.join(model_dir, filename)\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859ce832",
   "metadata": {},
   "source": [
    "# Load a Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21eea23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a blank model\n",
    "model = FractalFormer_base(config, tokenizer).to(config.device)  \n",
    "\n",
    "# here's the path to a minGemma model that i've trained with roughly 1m parameters\n",
    "path = 'models/FractalFormer_base-v128-max_t256-layers4-heads4-kv_heads1-hidden128-intermediate512-head_dim32-theta100.0-levels3-split2-lr0.0003-decay0.01-batch12--2024-03-06|07-14-57.pth'\n",
    "\n",
    "# Load the saved state dictionary\n",
    "model.load_state_dict(torch.load(path))\n",
    "# REMEMBER TO CHANGE VALUES IN CONFIG TO MATCH THE MODEL YOU'VE LOADED\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e3, 'K parameters')\n",
    "\n",
    "# If you only plan to do inference, switch to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# If you plan to continue training the model, switch to training mode\n",
    "#model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12b8c0b",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1923eecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() # sets model to eval mode\n",
    "input_str = \"JULIET:\\nO Romeo, Romeo! wherefore art thou R\" # the classic line\n",
    "max_useable_output_len = config.max_position_embeddings - len(input_str)\n",
    "\n",
    "for i in range(config.levels):\n",
    "    for j in range(config.model_count[i]):\n",
    "        print(f\"level: {i}, model: {j}\")\n",
    "        output = model.generate(input_str, \n",
    "                                output_len = max_useable_output_len, \n",
    "                                temperature=0.7, \n",
    "                                top_k = 3, \n",
    "                                top_p = 0.95,\n",
    "                               level = i,\n",
    "                               model = j)\n",
    "        print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a887a1",
   "metadata": {},
   "source": [
    "so there's almost definitely something wrong happening here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a9b78b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
