{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffd66297",
   "metadata": {},
   "source": [
    "# FractaFormer\n",
    "\n",
    "this base version is going to be absurdly terribly no-good inefficient because we're taking the biggest computational issue ($O(t^2)$ attention) and making it way worse by doing MANY of them at once and then having to keep track of each parameter's gradient from MANY different perspectives. This is basically just an extension of [MatFormer+](https://github.com/evintunador/matryoshkaGPT/blob/main/MatFormer%2B.ipynb) where instead of one inner model, we have 2 (or whatever number you specify) models inside 1 at each layer\n",
    "\n",
    "# TODO\n",
    "- ~output~\n",
    "    - ~tensor~\n",
    "    - ~tuple~\n",
    "    - triple check test\n",
    "- ~loss~\n",
    "    - ~tuple~\n",
    "    - triple check test\n",
    "- ~model itself~\n",
    "    - ~tensor~\n",
    "    - ~tuple~\n",
    "    - triple check test\n",
    "- adjust verboseness to be function-specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2125763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# used for the tokenizer\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Imports used for the config\n",
    "import dataclasses \n",
    "from typing import Optional\n",
    "\n",
    "# Imports used for the model\n",
    "import re\n",
    "from typing import Any, List, Sequence, Tuple, Union\n",
    "import numpy as np\n",
    "\n",
    "# used in the training loop\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbf790c",
   "metadata": {},
   "source": [
    "# The Dataset\n",
    "\n",
    "the dataset we'll be using is just TinyShakespeare for sake of simplicity & ability to do run/train locally on any computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8123fdfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n",
      "\n",
      " ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'] 65\n"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# the first 200 characters. It's just one continuous text document with all of the works of shakespeare back-to-back\n",
    "print(text[:200])\n",
    "\n",
    "# here are all the unique characters that occur in this text and how many there are\n",
    "chars = sorted(list(set(text)))\n",
    "v = len(chars)\n",
    "print('\\n', chars, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4158e7c",
   "metadata": {},
   "source": [
    "# The Tokenizer\n",
    "\n",
    "We'll be using a very simple tokenizer I previoiusly trained off of the TinyShakespeare dataset that has 128 total tokens and ignores stuff like special tokens & regex. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a1ee552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab length:  128\n",
      "Encoded: [22, 33, 24, 21, 17, 32, 71, 27, 1, 30, 53, 83, 53, 66, 30, 53, 83, 53, 2, 1, 61, 87, 93, 105, 43, 1, 77, 58, 1, 65, 67, 1, 30, 53, 83, 53, 12] 37\n",
      "Decoded: JULIET:\n",
      "O Romeo, Romeo! wherefore art thou Romeo? 49\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer data using pickle\n",
    "with open('./tokenizers/tokenizer.model', 'rb') as f:\n",
    "    loaded_tokenizer_data = pickle.load(f)\n",
    "\n",
    "# Extract the stoi mapping and merges from the loaded data\n",
    "loaded_stoi = loaded_tokenizer_data['stoi']\n",
    "loaded_merges = loaded_tokenizer_data['merges']\n",
    "\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self, stoi, merges):\n",
    "        self.stoi = stoi\n",
    "        self.merges = merges\n",
    "        self.itos = {i: s for s, i in stoi.items()}  # Inverse mapping for decoding\n",
    "\n",
    "        self.vocab_len = len(stoi) + len(merges)\n",
    "\n",
    "    def encode(self, text):\n",
    "        # Convert the text to a list of token IDs, using space for unknown characters\n",
    "        tokens = [self.stoi.get(c, self.stoi[' ']) for c in text]\n",
    "\n",
    "        # Perform merging with the possibility of nested merges\n",
    "        i = 0\n",
    "        while i < len(tokens) - 1:\n",
    "            pair = (tokens[i], tokens[i + 1])\n",
    "            if pair in self.merges:\n",
    "                # Replace the current pair with its merged token\n",
    "                merged_token = self.merges[pair]\n",
    "                tokens[i] = merged_token\n",
    "                del tokens[i + 1]\n",
    "\n",
    "                # Move back to handle possible nested merges\n",
    "                if i > 0:\n",
    "                    i -= 1\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        def expand_token(token):\n",
    "            # Base case: if the token is a direct mapping, return its character\n",
    "            if token in self.itos:\n",
    "                return self.itos[token]\n",
    "            # Recursive case: if the token is a merged token, expand its constituents\n",
    "            elif token in self.merges.values():\n",
    "                pair = next(key for key, value in self.merges.items() if value == token)\n",
    "                return ''.join(expand_token(t) for t in pair)\n",
    "            # Fallback for unknown tokens\n",
    "            else:\n",
    "                return ''\n",
    "\n",
    "        # Decode each token in the list, handling nested merges recursively\n",
    "        return ''.join(expand_token(token) for token in tokens)\n",
    "        \n",
    "# Example usage\n",
    "# Assuming loaded_stoi and loaded_merges are already loaded from the tokenizer.model file\n",
    "\n",
    "tokenizer = SimpleTokenizer(loaded_stoi, loaded_merges)\n",
    "print(\"vocab length: \", tokenizer.vocab_len)\n",
    "\n",
    "# Encoding text\n",
    "encoded_text = tokenizer.encode(\"JULIET:\\nO Romeo, Romeo! wherefore art thou Romeo?\")\n",
    "print(\"Encoded:\", encoded_text, len(encoded_text))\n",
    "\n",
    "# Decoding back\n",
    "decoded_text = tokenizer.decode(encoded_text)\n",
    "print(\"Decoded:\", decoded_text, len(decoded_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f1a588",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8da0d14c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single large model -> hierarchy of many smaller models inside\n",
      "model_count: [1, 2, 4]\n",
      "model_dim_list: [128, 64, 32]\n",
      "head_dim_list: [32, 16, 8]\n",
      "verbose: {'RMSNorm': False, 'MLP': False, 'MQA': False, 'Layer': False, 'OutputLayer': False, 'FractalLoss': False, 'FractalFormer': False, 'Sampler': False, 'Generate': False}\n"
     ]
    }
   ],
   "source": [
    "@dataclasses.dataclass # a class meant specifically to just hold data\n",
    "class Config:\n",
    "    \"\"\" \n",
    "    The default configuration & hyperparameters for FractalFormer\n",
    "    \"\"\"\n",
    "    # The number of tokens in the vocabulary.\n",
    "    vocab_size: int = tokenizer.vocab_len\n",
    "    \n",
    "    # The maximum sequence length that this model might ever be used with.\n",
    "    max_position_embeddings: int = 256\n",
    "    \n",
    "    # The number of layers in the model.\n",
    "    num_hidden_layers: int = 4\n",
    "    \n",
    "    # The number of attention heads used in the attention layers of the model.\n",
    "    num_attention_heads: int = 4\n",
    "    \n",
    "    # The number of key-value heads for implementing multi-query attention.\n",
    "    num_key_value_heads: int = 1\n",
    "    # Ensures that the number of query heads is evenly divisible by the number of KV heads.\n",
    "    assert num_attention_heads % num_key_value_heads == 0\n",
    "    \n",
    "    # The hidden size of the model, AKA the embedding dimension\n",
    "    hidden_size: int = 128\n",
    "    # the attention heads need to cleanly divide up the hidden_size of the model for MQA\n",
    "    assert hidden_size % num_attention_heads == 0\n",
    "\n",
    "    # how much larger the inner dimension of the MLP should be than the hidden size of the model\n",
    "    intermediate_multiplier = 4\n",
    "    # The inner dimension of the MLP part of the decoder layer\n",
    "    @property\n",
    "    def intermediate_size(self):\n",
    "        return self.intermediate_multiplier * self.hidden_size\n",
    "    \n",
    "    # The number of head dimensions\n",
    "    head_dim: int = 32\n",
    "    \n",
    "    # The epsilon used by the rms normalization layers.\n",
    "    rms_norm_eps: float = 1e-6 # this is to promote numerical stability & prevent dividing by 0\n",
    "    \n",
    "    # the scaling factor that determines the frequencies for the rotary positional encodings\n",
    "    rope_theta = 100.0\n",
    "    # smaller models should use a smaller theta, but I'm just guessing here. 1000 might work too. 10,000 is the usual\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # the % of neurons to dropout in the MLP\n",
    "    dropout = 0.1\n",
    "\n",
    "    ####### for debugging & visualization\n",
    "    verbose = {\n",
    "    'RMSNorm': False,\n",
    "    'MLP': False,\n",
    "    'MQA': False,\n",
    "    'Layer': False,\n",
    "    'OutputLayer': False,\n",
    "    'FractalLoss': False,\n",
    "    'FractalFormer': False,\n",
    "    'Sampler': False,\n",
    "    'Generate': False\n",
    "    }\n",
    "\n",
    "    ####### FractalFormer-specific hyperparameters\n",
    "\n",
    "    # the number of levels for sub-models to exist on\n",
    "    levels = 3\n",
    "    \n",
    "    # the number of splits to make at a given level\n",
    "    split = 2 # i don't recommend choosing any value other than 2\n",
    "    # needs to be divisible by 2 in order to splice cleanly\n",
    "    assert split % 2 == 0\n",
    "    # RoPE requires a head dimension of length larger than 1 in order to work\n",
    "    assert head_dim // (split * (levels-1)) > 1\n",
    "    # really though you shouldn't be getting anywhere near that small of a head dimension even at the lowest level, that'd be useless\n",
    "\n",
    "    @property\n",
    "    def model_count(self):\n",
    "        return [self.split**i for i in range(self.levels)]\n",
    "\n",
    "    @property\n",
    "    def model_dim_list(self):\n",
    "        return [self.hidden_size // (self.split**i) for i in range(self.levels)]\n",
    "\n",
    "    @property\n",
    "    def head_dim_list(self):\n",
    "        return [self.head_dim // (self.split**i) for i in range(self.levels)]\n",
    "\n",
    "config = Config()\n",
    "\n",
    "print(\"single large model -> hierarchy of many smaller models inside\")\n",
    "print(f\"model_count: {config.model_count}\")\n",
    "print(f\"model_dim_list: {config.model_dim_list}\")\n",
    "print(f\"head_dim_list: {config.head_dim_list}\")\n",
    "print(f\"verbose: {config.verbose}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b8d06d",
   "metadata": {},
   "source": [
    "# Rotary Positional Encoding (RoPE)\n",
    "\n",
    "i don't think i need to adjust the code for this one as long as i always call it individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab56e84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotary_emb(x: torch.Tensor, dim: int, theta: float = 10000.0) -> torch.Tensor:\n",
    "    \"\"\"Applies the rotary embedding to the inputted query or key tensor\"\"\"\n",
    "    # Get sequence length\n",
    "    seq_len = x.size(1)\n",
    "    device = x.device\n",
    "    \n",
    "    # Dynamically compute frequency cis based on the input sequence length\n",
    "    # dynamic is less efficient but pre-computed was giving me trouble so whatever\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2, device=device).float() / dim))\n",
    "    t = torch.arange(seq_len, device=device)\n",
    "    freqs = torch.outer(t, freqs).float()\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "\n",
    "    # Apply rotary embeddings to the input tensor\n",
    "    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis.unsqueeze(0)).type_as(x)  # Ensure batch dimension is handled\n",
    "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "    x_out = x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "\n",
    "    return x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67bf0ee",
   "metadata": {},
   "source": [
    "# RMSNorm\n",
    "\n",
    "Layernorm is relatively simple code-wise. However, of note is the fact that during training, the entire full length vector gets normalized whereas during inference we only layernorm the sub-vector we've been given if we're not using the full model size. This is interesting because RMSNorm puts a vector of length $d$ onto a hypersphere of radius $\\sqrt{d}$ which means that while the embeddings of the largest model exist on a hypersphere of the aforementioned size, for each number of layers $i\\in\\mathbb{N}$ s.t. $0 < i \\leq$ `config.model_count` the embeddings are placed onto a hypersphere of radius $\\sqrt{\\frac{d}{s^i}}$ where $s=$`config.split`. I'm not sure yet exactly how to interpret this concatenation of vectors geometrically. When you combine the entries of two hypserspheres to make a larger hypserspheres, what happens to the feature groupings on the surface of the smaller hyperspheres? I presume there are some type of interaction effects or something. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89f37bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the RMS Normalization (Root Mean Square Normalization) layer.\n",
    "    RMSNorm is a variant of layer normalization that normalizes the activations\n",
    "    of the previous layer based on their root mean square value.\n",
    "\n",
    "    Parameters:\n",
    "    - dim (int): The dimension of the input features the normalization is applied to.\n",
    "    - eps (float): A small value added to the denominator for numerical stability. Default is 1e-6.\n",
    "    - add_unit_offset (bool): If True, adds a unit (1) to the learned scaling coefficient, effectively\n",
    "      starting with no scaling. If False, the scaling coefficient starts from zero. Default is True.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        eps: float = 1e-6,\n",
    "        #add_unit_offset: bool = True,\n",
    "    ):\n",
    "        super().__init__() \n",
    "        self.eps = eps  # Small epsilon value for numerical stability since you can't divide by 0\n",
    "        #self.add_unit_offset = add_unit_offset  # Flag to determine if a unit should be added to the weight\n",
    "        \n",
    "        # Initialize the weight parameter with zeros, which will be learned during training.\n",
    "        # The shape of the weight is [dim], meaning one weight per feature dimension.\n",
    "        self.weight = nn.Parameter(torch.zeros(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        \"\"\"\n",
    "        Private helper function to normalize the input tensor.\n",
    "\n",
    "        Parameters:\n",
    "        - x (Tensor): The input tensor to normalize.\n",
    "\n",
    "        Returns:\n",
    "        - Tensor: The normalized tensor.\n",
    "        \"\"\"\n",
    "        # Calculate the root mean square value for each feature (across the last dimension),\n",
    "        # then use reciprocal square root (rsqrt) for normalization.\n",
    "        # Add self.eps to the denominator for numerical stability.\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, model: int = 0) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the RMSNorm layer\n",
    "\n",
    "        Parameters:\n",
    "        - x (Tensor): The input tensor to normalize.\n",
    "        - model (int): the index indicating the model being used in this layer. used for splicing self.weight\n",
    "\n",
    "        Returns:\n",
    "        - output: The normalized and scaled tensor.\n",
    "        \"\"\"\n",
    "        global verbose\n",
    "        if verbose: \n",
    "            print(\"------------- RMSNorm.forward() ------------\")\n",
    "            print(f\"x: {x.shape}\\n{x}\")\n",
    "            \n",
    "        # Normalize the input tensor using the _norm function and ensure the data type matches the input.\n",
    "        x = self._norm(x.float()).type_as(x)\n",
    "        if verbose: print(f\"normed x: {x.shape}\\n{x}\")\n",
    "        \n",
    "        # grabbing x's dimension to use for splicing\n",
    "        dim = x.shape[-1]\n",
    "        \n",
    "        # calculating skip for our splice\n",
    "        skip = model * dim\n",
    "        if verbose: \n",
    "            print(f\"dim: {dim}\")\n",
    "            print(f\"skip: {skip}\")\n",
    "        \n",
    "        # scale the normalized tensor by (1 + self.weight), which effectively starts with no scaling\n",
    "        spliced_scale = self.weight[skip:skip + dim]\n",
    "        output = x * (1 + spliced_scale)\n",
    "        if verbose:\n",
    "            print(f\"spliced scale: {spliced_scale.shape}\\n{spliced_scale}\")\n",
    "            print(f\"scaled normed x: {output.shape}\\n{output}\")\n",
    "            print(\"------------- END RMSNorm.forward() ------------\")\n",
    "                          \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecec2e6",
   "metadata": {},
   "source": [
    "The following cell was designed to help you visualize what's happening with RMSNorm's splicing. With RMSNorm we'll only have to think about doing this with individual tensors, but with future methods like MLP and MQA we'll have to create an entirely separate forward method used during training that deals with tuples of tensors. The thing to pay attention to here is the size of the scale weights. scale_weights' entries are 0's because we've not yet undergone training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57fa35c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Micro Hyperparameters -------\n",
      "model_count:  [1, 2, 4]\n",
      "model_dim_list:  [4, 2, 1]\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the big model |-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 2, 4])\n",
      "tensor([[[0.6458, 0.3538, 0.0057, 0.7264],\n",
      "         [0.7516, 0.3169, 0.3356, 0.6935]]])\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 2, 4])\n",
      "tensor([[[0.6458, 0.3538, 0.0057, 0.7264],\n",
      "         [0.7516, 0.3169, 0.3356, 0.6935]]])\n",
      "normed x: torch.Size([1, 2, 4])\n",
      "tensor([[[1.2487, 0.6840, 0.0110, 1.4045],\n",
      "         [1.3397, 0.5649, 0.5982, 1.2362]]])\n",
      "dim: 4\n",
      "skip: 0\n",
      "spliced scale: torch.Size([4])\n",
      "tensor([0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 2, 4])\n",
      "tensor([[[1.2487, 0.6840, 0.0110, 1.4045],\n",
      "         [1.3397, 0.5649, 0.5982, 1.2362]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "y: torch.Size([1, 2, 4])\n",
      "tensor([[[1.2487, 0.6840, 0.0110, 1.4045],\n",
      "         [1.3397, 0.5649, 0.5982, 1.2362]]], grad_fn=<MulBackward0>)\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the first sub-model |-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 2, 2])\n",
      "tensor([[[0.7586, 0.0252],\n",
      "         [0.4962, 0.1003]]])\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 2, 2])\n",
      "tensor([[[0.7586, 0.0252],\n",
      "         [0.4962, 0.1003]]])\n",
      "normed x: torch.Size([1, 2, 2])\n",
      "tensor([[[1.4134, 0.0470],\n",
      "         [1.3861, 0.2803]]])\n",
      "dim: 2\n",
      "skip: 0\n",
      "spliced scale: torch.Size([2])\n",
      "tensor([0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 2, 2])\n",
      "tensor([[[1.4134, 0.0470],\n",
      "         [1.3861, 0.2803]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "y: torch.Size([1, 2, 2])\n",
      "tensor([[[1.4134, 0.0470],\n",
      "         [1.3861, 0.2803]]], grad_fn=<MulBackward0>)\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the second sub-model |-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 2, 2])\n",
      "tensor([[[0.4798, 0.2543],\n",
      "         [0.5496, 0.4717]]])\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 2, 2])\n",
      "tensor([[[0.4798, 0.2543],\n",
      "         [0.5496, 0.4717]]])\n",
      "normed x: torch.Size([1, 2, 2])\n",
      "tensor([[[1.2495, 0.6623],\n",
      "         [1.0732, 0.9210]]])\n",
      "dim: 2\n",
      "skip: 2\n",
      "spliced scale: torch.Size([2])\n",
      "tensor([0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 2, 2])\n",
      "tensor([[[1.2495, 0.6623],\n",
      "         [1.0732, 0.9210]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "y: torch.Size([1, 2, 2])\n",
      "tensor([[[1.2495, 0.6623],\n",
      "         [1.0732, 0.9210]]], grad_fn=<MulBackward0>)\n",
      "---------- RESET CONFIG --------\n",
      "model_count:  [1, 2, 4]\n"
     ]
    }
   ],
   "source": [
    "# Testing our RMSNorm's forward()\n",
    "verbose = True\n",
    "print(\"--------- Micro Hyperparameters -------\")\n",
    "hold = config.hidden_size\n",
    "config.hidden_size = 4\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the big model |-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,2,config.hidden_size)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "norm = RMSNorm(config.hidden_size)\n",
    "y = norm(x)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the first sub-model |-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,2,config.hidden_size//2)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "norm = RMSNorm(config.hidden_size)\n",
    "y = norm(x)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the second sub-model |-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,2,config.hidden_size//2)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "norm = RMSNorm(config.hidden_size)\n",
    "y = norm(x, model=1)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "verbose = False\n",
    "print(\"---------- RESET CONFIG --------\")\n",
    "config.hidden_size = hold\n",
    "print(\"model_count: \", config.model_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f07034e",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./images/ffwd.jpeg\" width=\"512\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c30de56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    This class implements a multi-layer perceptron with a GeGLU gating mechanism. The GeGLU\n",
    "    activation combines a standard GeLU activation with a learned gating mechanism, enabling\n",
    "    the network to control the flow of information more dynamically.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        intermediate_size: int,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the GemmaMLP module.\n",
    "\n",
    "        Parameters:\n",
    "            hidden_size (int): The size of the input and output tensors.\n",
    "            intermediate_size (int): The size of the tensor after the initial transformation\n",
    "                                     and before the gating and final projection. This is typically\n",
    "                                     larger than the hidden size to allow for a richer representation.\n",
    "            dropout (float): the dropout rate to use during training in forwardTuple()\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.intermediate_size = intermediate_size\n",
    "        assert intermediate_size % hidden_size == 0\n",
    "        self.intermediate_multiplier = intermediate_size // hidden_size\n",
    "\n",
    "        # Linear transformation for the gating mechanism, projecting input to an intermediate size.\n",
    "        self.Wgate = nn.Parameter(torch.Tensor(hidden_size, intermediate_size))\n",
    "        self.Bgate = nn.Parameter(torch.Tensor(intermediate_size))\n",
    "\n",
    "        # Linear transformation for the input tensor, also projecting to the intermediate size but\n",
    "        # intended for element-wise multiplication with the gated output.\n",
    "        self.Wup = nn.Parameter(torch.Tensor(hidden_size, intermediate_size))\n",
    "        self.Bup = nn.Parameter(torch.Tensor(intermediate_size))\n",
    "\n",
    "        # Linear transformation to project the gated and combined tensor back to the original\n",
    "        # hidden size, completing the MLP structure.\n",
    "        self.Wdown = nn.Parameter(torch.Tensor(intermediate_size, hidden_size))\n",
    "        self.Bdown = nn.Parameter(torch.Tensor(hidden_size))\n",
    "\n",
    "        # Initialize weights with uniform distribution\n",
    "        # For gate & up, where in_features is hidden_size\n",
    "        limit_gateup = 1 / np.sqrt(hidden_size)\n",
    "        nn.init.uniform_(self.Wgate, -limit_gateup, limit_gateup)\n",
    "        nn.init.uniform_(self.Bgate, -limit_gateup, limit_gateup)\n",
    "        nn.init.uniform_(self.Wup, -limit_gateup, limit_gateup)\n",
    "        nn.init.uniform_(self.Bup, -limit_gateup, limit_gateup)\n",
    "        \n",
    "        # For down, where in_features is intermediate_size\n",
    "        limit_down = 1 / np.sqrt(intermediate_size)\n",
    "        nn.init.uniform_(self.Wdown, -limit_down, limit_down)\n",
    "        nn.init.uniform_(self.Bdown, -limit_down, limit_down)\n",
    "        \n",
    "        # defining our dropout for training in forwardTuple()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forwardTensor(self, x, model:int=0):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the MLP module during inference.\n",
    "\n",
    "        Parameters:\n",
    "            x (Tensor): The input tensor to the MLP. \n",
    "                        shape (batch size, sequence length, hidden dimension) where hidden dimension changes by which model was used\n",
    "            model (int): the indicator of which model we're using. \n",
    "                        used in calculating our skip length for splicing. \n",
    "                        defaults to the equivalent of what's used in MatFormer+, meaning no skip, aka we use the top-left-most splice\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The output tensor after applying the GeGLU gating mechanism and the MLP transformations.\n",
    "        \"\"\"\n",
    "        global verbose\n",
    "        if verbose: \n",
    "            print(\"------------- MLP.forwardTensor() ------------\")\n",
    "            print(f\"x: {x.shape}\\n{x}\")\n",
    "            \n",
    "        # figuring out how we should do our splicing\n",
    "        d_dim = x.shape[-1]\n",
    "        d_skip = model * d_dim\n",
    "        i_dim = d_dim * self.intermediate_multiplier\n",
    "        i_skip = model * i_dim\n",
    "        if verbose: \n",
    "            print(f\"d_dim: {d_dim}\")\n",
    "            print(f\"d_skip: {d_skip}\")\n",
    "            print(f\"i_dim: {i_dim}\")\n",
    "            print(f\"i_skip: {i_skip}\")\n",
    "        \n",
    "        # Applies linear transformation for gating.\n",
    "        Wgate = self.Wgate[d_skip:d_skip + d_dim, i_skip:i_skip + i_dim]\n",
    "        Bgate = self.Bgate[i_skip:i_skip + i_dim]\n",
    "        Xgate = x @ Wgate + Bgate\n",
    "        if verbose: \n",
    "            print(f\"Wgate: {self.Wgate.shape}\\n{self.Wgate}\")\n",
    "            print(f\"Wgate spliced: {Wgate.shape}\\n{Wgate}\")\n",
    "            print(f\"Bgate: {self.Bgate.shape}\\n{self.Bgate}\")\n",
    "            print(f\"Bgate spliced: {Bgate.shape}\\n{Bgate}\")\n",
    "            print(f\"Xgate: {Xgate.shape}\\n{Xgate}\")\n",
    "\n",
    "        # Applies GeLU activation to the gate, introducing non-linearity and enabling the gating mechanism.\n",
    "        Xgate = F.gelu(Xgate)\n",
    "        if verbose: print(f\"GeLU'ed Xgate: {Xgate.shape}\\n{Xgate}\")\n",
    "\n",
    "        # Applies another linear transformation to the input tensor for subsequent combination with the gate.\n",
    "        Wup = self.Wup[d_skip:d_skip + d_dim, i_skip:i_skip + i_dim]\n",
    "        Bup = self.Bup[i_skip:i_skip + i_dim]\n",
    "        Xup = x @ Wup + Bup\n",
    "        if verbose: \n",
    "            print(f\"Wup: {self.Wup.shape}\\n{self.Wup}\")\n",
    "            print(f\"Wup spliced: {Wup.shape}\\n{Wup}\")\n",
    "            print(f\"Bup: {self.Bup.shape}\\n{self.Bup}\")\n",
    "            print(f\"Bup spliced: {Bup.shape}\\n{Bup}\")\n",
    "            print(f\"Xup: {Xup.shape}\\n{Xup}\")\n",
    "\n",
    "        # Element-wise multiplication of the gated tensor with the transformed input tensor, modulating\n",
    "        # the input based on the gate's activation.\n",
    "        Xfuse = Xgate * Xup\n",
    "        if verbose: print(f\"Xfuse: {Xfuse.shape}\\n{Xfuse}\")\n",
    "\n",
    "        # Applies the final linear transformation to project the modulated tensor back to the hidden size.\n",
    "        Wdown = self.Wdown[i_skip:i_skip + i_dim, d_skip:d_skip + d_dim]\n",
    "        Bdown = self.Bdown[d_skip:d_skip + d_dim]\n",
    "        outputs = Xfuse @ Wdown + Bdown\n",
    "        if verbose: \n",
    "            print(f\"Wdown: {self.Wdown.shape}\\n{self.Wdown}\")\n",
    "            print(f\"Wdown spliced: {Wdown.shape}\\n{Wdown}\")\n",
    "            print(f\"Bdown: {self.Bdown.shape}\\n{self.Bdown}\")\n",
    "            print(f\"Bdown spliced: {Bdown.shape}\\n{Bdown}\")\n",
    "            print(f\"outputs: {outputs.shape}\\n{outputs}\") \n",
    "            print(\"------------- END MLP.forwardTensor() ------------\")\n",
    "\n",
    "        # Returns the final output tensor of the MLP, after gating and modulation.\n",
    "        return outputs\n",
    "\n",
    "    def forwardTuple(self, x, drop_bool: bool = True):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the MLP module during training.\n",
    "\n",
    "        Parameters:\n",
    "            x (Tuple[Tuple[Tensor]]): \n",
    "                The input tuple of tuples of tensors to the MLP. \n",
    "                first tuple is of length config.levels and second layer of tuples have lengths of config.model_count\n",
    "                tensors are shape (batch size, sequence length, hidden dimension) where hidden dimension changes by which model was used\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Tuple[Tensor]]: \n",
    "                The output tuple of tuples of tensors after applying the GeGLU gating mechanism and the MLP transformations.\n",
    "        \"\"\"\n",
    "        global verbose\n",
    "        if verbose: \n",
    "            print(\"------------- MLP.forwardTuple() ------------\")\n",
    "            print(f\"x: {x}\")\n",
    "\n",
    "        # if we had sent through the config we could've just grabbed these values from there but too late now\n",
    "        num_levels = len(x)\n",
    "        models_per_level = [len(x[i]) for i in range(num_levels)]\n",
    "        if verbose: \n",
    "            print(f\"num_levels: {num_levels}\")\n",
    "            print(f\"models_per_level: {models_per_level}\")\n",
    "        \n",
    "        out = ()\n",
    "        for i in range(num_levels):\n",
    "            if verbose: print(f\"i: {i}\")\n",
    "            \n",
    "            out_lvl = ()\n",
    "            for j in range(models_per_level[i]):\n",
    "                if verbose: print(f\"j: {j}\")\n",
    "\n",
    "                output = self.forwardTensor(x[i][j], model=j)\n",
    "                if verbose: print(f\"forwardTensor() output: {output.shape}\\n{output}\")\n",
    "                    \n",
    "                out_lvl += (self.drop(output),) if drop_bool else (output,)\n",
    "\n",
    "            # pretty sure i have to save & store everything without overwriting to prevent in-place arguments. so annoying\n",
    "            if verbose: print(f\"out_lvl: {out_lvl}\")\n",
    "            out += (out_lvl,)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"out: {out}\")\n",
    "            print(\"------------- END MLP.forwardTuple() ------------\")\n",
    "        return out\n",
    "        \n",
    "    def forward(self, x, model=0, drop_bool = True):\n",
    "        train = True if type(x) == tuple else False\n",
    "        if verbose: print(f\"---------- MLP Input: {'Tuple' if train else 'torch.Tensor'} ------------\")\n",
    "        return self.forwardTuple(x, drop_bool) if train else self.forwardTensor(x, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95fe6ab",
   "metadata": {},
   "source": [
    "The following two cells are designed to help you comprehend what's happening. If you walk through every single print statement and follow along even down to watching what happens to each weight, you'll be able to clearly see what's happening with the odd splicing behavior. In order to make this somewhat feasible, I've set very small matrices for these examples. However I will admit it is still inevitably a pain, which is why I included the drawings above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b26bd23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Micro Hyperparameters -------\n",
      "model_count:  [1, 2, 4]\n",
      "model_dim_list:  [4, 2, 1]\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the big model |-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 2, 4])\n",
      "tensor([[[0.8347, 0.5287, 0.9298, 0.1402],\n",
      "         [0.5776, 0.0385, 0.3836, 0.4818]]])\n",
      "---------- MLP Input: torch.Tensor ------------\n",
      "------------- MLP.forwardTensor() ------------\n",
      "x: torch.Size([1, 2, 4])\n",
      "tensor([[[0.8347, 0.5287, 0.9298, 0.1402],\n",
      "         [0.5776, 0.0385, 0.3836, 0.4818]]])\n",
      "d_dim: 4\n",
      "d_skip: 0\n",
      "i_dim: 8\n",
      "i_skip: 0\n",
      "Wgate: torch.Size([4, 8])\n",
      "Parameter containing:\n",
      "tensor([[ 0.4615, -0.0088,  0.4671,  0.4855,  0.4548,  0.2415, -0.4020,  0.1582],\n",
      "        [-0.4374,  0.1926,  0.0292, -0.2386,  0.0410, -0.3829,  0.1208, -0.2454],\n",
      "        [ 0.2027, -0.3426,  0.1122, -0.2824,  0.4950,  0.1808, -0.4797, -0.3143],\n",
      "        [-0.0300, -0.4899, -0.1638, -0.1777, -0.3709, -0.0491, -0.4678,  0.0134]],\n",
      "       requires_grad=True)\n",
      "Wgate spliced: torch.Size([4, 8])\n",
      "tensor([[ 0.4615, -0.0088,  0.4671,  0.4855,  0.4548,  0.2415, -0.4020,  0.1582],\n",
      "        [-0.4374,  0.1926,  0.0292, -0.2386,  0.0410, -0.3829,  0.1208, -0.2454],\n",
      "        [ 0.2027, -0.3426,  0.1122, -0.2824,  0.4950,  0.1808, -0.4797, -0.3143],\n",
      "        [-0.0300, -0.4899, -0.1638, -0.1777, -0.3709, -0.0491, -0.4678,  0.0134]],\n",
      "       grad_fn=<AliasBackward0>)\n",
      "Bgate: torch.Size([8])\n",
      "Parameter containing:\n",
      "tensor([ 0.1166,  0.0290, -0.2558, -0.4975, -0.0726, -0.2367,  0.4477,  0.2171],\n",
      "       requires_grad=True)\n",
      "Bgate spliced: torch.Size([8])\n",
      "tensor([ 0.1166,  0.0290, -0.2558, -0.4975, -0.0726, -0.2367,  0.4477,  0.2171],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xgate: torch.Size([1, 2, 8])\n",
      "tensor([[[ 0.4548, -0.2638,  0.2308, -0.5058,  0.7369, -0.0764, -0.3355,\n",
      "          -0.0710],\n",
      "         [ 0.4296, -0.3361, -0.0208, -0.4201,  0.2028, -0.0662, -0.1891,\n",
      "           0.1849]]], grad_fn=<AddBackward0>)\n",
      "GeLU'ed Xgate: torch.Size([1, 2, 8])\n",
      "tensor([[[ 0.3071, -0.1045,  0.1365, -0.1550,  0.5670, -0.0359, -0.1237,\n",
      "          -0.0335],\n",
      "         [ 0.2862, -0.1238, -0.0102, -0.1417,  0.1177, -0.0314, -0.0804,\n",
      "           0.1060]]], grad_fn=<GeluBackward0>)\n",
      "Wup: torch.Size([4, 8])\n",
      "Parameter containing:\n",
      "tensor([[-0.0861, -0.0312, -0.2111,  0.0977, -0.3424,  0.2408, -0.1428,  0.3768],\n",
      "        [ 0.3924,  0.3742, -0.4731, -0.4267,  0.0582, -0.0965,  0.2959, -0.1052],\n",
      "        [-0.0598, -0.4831,  0.2540,  0.0133,  0.2806,  0.4311, -0.4072, -0.2018],\n",
      "        [ 0.0716, -0.3571,  0.2423,  0.2938, -0.4781,  0.1325,  0.4119, -0.3926]],\n",
      "       requires_grad=True)\n",
      "Wup spliced: torch.Size([4, 8])\n",
      "tensor([[-0.0861, -0.0312, -0.2111,  0.0977, -0.3424,  0.2408, -0.1428,  0.3768],\n",
      "        [ 0.3924,  0.3742, -0.4731, -0.4267,  0.0582, -0.0965,  0.2959, -0.1052],\n",
      "        [-0.0598, -0.4831,  0.2540,  0.0133,  0.2806,  0.4311, -0.4072, -0.2018],\n",
      "        [ 0.0716, -0.3571,  0.2423,  0.2938, -0.4781,  0.1325,  0.4119, -0.3926]],\n",
      "       grad_fn=<AliasBackward0>)\n",
      "Bup: torch.Size([8])\n",
      "Parameter containing:\n",
      "tensor([ 0.4279,  0.2028, -0.4692, -0.1107,  0.4830,  0.3856,  0.4569,  0.4891],\n",
      "       requires_grad=True)\n",
      "Bup spliced: torch.Size([8])\n",
      "tensor([ 0.4279,  0.2028, -0.4692, -0.1107,  0.4830,  0.3856,  0.4569,  0.4891],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xup: torch.Size([1, 2, 8])\n",
      "tensor([[[ 0.5180, -0.1246, -0.6254, -0.2013,  0.4219,  0.9550,  0.1732,\n",
      "           0.5053],\n",
      "         [ 0.4048, -0.1581, -0.3952,  0.0759,  0.1648,  0.7502,  0.4280,\n",
      "           0.4361]]], grad_fn=<AddBackward0>)\n",
      "Xfuse: torch.Size([1, 2, 8])\n",
      "tensor([[[ 0.1591,  0.0130, -0.0854,  0.0312,  0.2392, -0.0342, -0.0214,\n",
      "          -0.0169],\n",
      "         [ 0.1159,  0.0196,  0.0040, -0.0107,  0.0194, -0.0235, -0.0344,\n",
      "           0.0462]]], grad_fn=<MulBackward0>)\n",
      "Wdown: torch.Size([8, 4])\n",
      "Parameter containing:\n",
      "tensor([[-0.2191,  0.0919,  0.3423,  0.1058],\n",
      "        [-0.1166,  0.2224,  0.0011,  0.0458],\n",
      "        [-0.3162,  0.0515, -0.3032,  0.1917],\n",
      "        [-0.3443,  0.1187, -0.2539,  0.1852],\n",
      "        [-0.2977,  0.3306, -0.2303, -0.2367],\n",
      "        [ 0.1267, -0.0857, -0.1458,  0.2261],\n",
      "        [-0.2237, -0.2397, -0.1827, -0.0744],\n",
      "        [-0.0760,  0.1879, -0.1251,  0.3091]], requires_grad=True)\n",
      "Wdown spliced: torch.Size([8, 4])\n",
      "tensor([[-0.2191,  0.0919,  0.3423,  0.1058],\n",
      "        [-0.1166,  0.2224,  0.0011,  0.0458],\n",
      "        [-0.3162,  0.0515, -0.3032,  0.1917],\n",
      "        [-0.3443,  0.1187, -0.2539,  0.1852],\n",
      "        [-0.2977,  0.3306, -0.2303, -0.2367],\n",
      "        [ 0.1267, -0.0857, -0.1458,  0.2261],\n",
      "        [-0.2237, -0.2397, -0.1827, -0.0744],\n",
      "        [-0.0760,  0.1879, -0.1251,  0.3091]], grad_fn=<AliasBackward0>)\n",
      "Bdown: torch.Size([4])\n",
      "Parameter containing:\n",
      "tensor([ 0.2803,  0.1454,  0.2085, -0.2056], requires_grad=True)\n",
      "Bdown spliced: torch.Size([4])\n",
      "tensor([ 0.2803,  0.1454,  0.2085, -0.2056], grad_fn=<SliceBackward0>)\n",
      "outputs: torch.Size([1, 2, 4])\n",
      "tensor([[[ 0.1907,  0.2462,  0.2369, -0.2668],\n",
      "         [ 0.2505,  0.1847,  0.2492, -0.1868]]], grad_fn=<AddBackward0>)\n",
      "------------- END MLP.forwardTensor() ------------\n",
      "y: torch.Size([1, 2, 4])\n",
      "tensor([[[ 0.1907,  0.2462,  0.2369, -0.2668],\n",
      "         [ 0.2505,  0.1847,  0.2492, -0.1868]]], grad_fn=<AddBackward0>)\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the first sub-model |-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 2, 2])\n",
      "tensor([[[0.6887, 0.6154],\n",
      "         [0.6017, 0.4009]]])\n",
      "---------- MLP Input: torch.Tensor ------------\n",
      "------------- MLP.forwardTensor() ------------\n",
      "x: torch.Size([1, 2, 2])\n",
      "tensor([[[0.6887, 0.6154],\n",
      "         [0.6017, 0.4009]]])\n",
      "d_dim: 2\n",
      "d_skip: 0\n",
      "i_dim: 4\n",
      "i_skip: 0\n",
      "Wgate: torch.Size([4, 8])\n",
      "Parameter containing:\n",
      "tensor([[ 0.2132, -0.3335, -0.0488, -0.2362, -0.3467, -0.1633, -0.1228,  0.4644],\n",
      "        [ 0.4443,  0.2435, -0.4186, -0.3434, -0.0212,  0.4677,  0.1542, -0.2718],\n",
      "        [-0.0320,  0.1716,  0.2655, -0.1649,  0.0847,  0.1664, -0.0820,  0.4477],\n",
      "        [ 0.1468, -0.3053, -0.1810,  0.4837,  0.0331, -0.2801,  0.4456, -0.1641]],\n",
      "       requires_grad=True)\n",
      "Wgate spliced: torch.Size([2, 4])\n",
      "tensor([[ 0.2132, -0.3335, -0.0488, -0.2362],\n",
      "        [ 0.4443,  0.2435, -0.4186, -0.3434]], grad_fn=<SliceBackward0>)\n",
      "Bgate: torch.Size([8])\n",
      "Parameter containing:\n",
      "tensor([-0.4927, -0.4654, -0.4737,  0.2454, -0.3266, -0.0603, -0.0776, -0.0455],\n",
      "       requires_grad=True)\n",
      "Bgate spliced: torch.Size([4])\n",
      "tensor([-0.4927, -0.4654, -0.4737,  0.2454], grad_fn=<SliceBackward0>)\n",
      "Xgate: torch.Size([1, 2, 4])\n",
      "tensor([[[-0.0724, -0.5453, -0.7649, -0.1286],\n",
      "         [-0.1863, -0.5685, -0.6709, -0.0344]]], grad_fn=<AddBackward0>)\n",
      "GeLU'ed Xgate: torch.Size([1, 2, 4])\n",
      "tensor([[[-0.0341, -0.1596, -0.1699, -0.0577],\n",
      "         [-0.0794, -0.1619, -0.1685, -0.0167]]], grad_fn=<GeluBackward0>)\n",
      "Wup: torch.Size([4, 8])\n",
      "Parameter containing:\n",
      "tensor([[-0.1647,  0.0362, -0.2329,  0.4972,  0.1526, -0.1668,  0.4186, -0.4212],\n",
      "        [ 0.1650,  0.3888, -0.3622, -0.2470, -0.3473, -0.0710, -0.4183,  0.4372],\n",
      "        [-0.1491,  0.0809, -0.2346,  0.0272, -0.4654, -0.4484, -0.1985, -0.4311],\n",
      "        [ 0.0601, -0.2406, -0.1637,  0.4748, -0.2932,  0.3213,  0.3004,  0.3846]],\n",
      "       requires_grad=True)\n",
      "Wup spliced: torch.Size([2, 4])\n",
      "tensor([[-0.1647,  0.0362, -0.2329,  0.4972],\n",
      "        [ 0.1650,  0.3888, -0.3622, -0.2470]], grad_fn=<SliceBackward0>)\n",
      "Bup: torch.Size([8])\n",
      "Parameter containing:\n",
      "tensor([-0.1552,  0.0534,  0.3633,  0.4263, -0.2165,  0.0928,  0.2410, -0.2366],\n",
      "       requires_grad=True)\n",
      "Bup spliced: torch.Size([4])\n",
      "tensor([-0.1552,  0.0534,  0.3633,  0.4263], grad_fn=<SliceBackward0>)\n",
      "Xup: torch.Size([1, 2, 4])\n",
      "tensor([[[-0.1671,  0.3176, -0.0200,  0.6167],\n",
      "         [-0.1882,  0.2310,  0.0779,  0.6265]]], grad_fn=<AddBackward0>)\n",
      "Xfuse: torch.Size([1, 2, 4])\n",
      "tensor([[[ 0.0057, -0.0507,  0.0034, -0.0356],\n",
      "         [ 0.0149, -0.0374, -0.0131, -0.0105]]], grad_fn=<MulBackward0>)\n",
      "Wdown: torch.Size([8, 4])\n",
      "Parameter containing:\n",
      "tensor([[-0.0841,  0.3393,  0.2671, -0.2535],\n",
      "        [ 0.2968, -0.1236, -0.0071,  0.2527],\n",
      "        [ 0.0840, -0.3022,  0.1410, -0.0136],\n",
      "        [-0.3486, -0.1830, -0.0878, -0.3474],\n",
      "        [-0.1491,  0.3416, -0.2614,  0.2003],\n",
      "        [ 0.0355,  0.1408,  0.1988,  0.3229],\n",
      "        [-0.1392, -0.0187,  0.0666,  0.1105],\n",
      "        [ 0.1506,  0.2652,  0.3517,  0.2978]], requires_grad=True)\n",
      "Wdown spliced: torch.Size([4, 2])\n",
      "tensor([[-0.0841,  0.3393],\n",
      "        [ 0.2968, -0.1236],\n",
      "        [ 0.0840, -0.3022],\n",
      "        [-0.3486, -0.1830]], grad_fn=<SliceBackward0>)\n",
      "Bdown: torch.Size([4])\n",
      "Parameter containing:\n",
      "tensor([ 0.0605,  0.0457,  0.0589, -0.3448], requires_grad=True)\n",
      "Bdown spliced: torch.Size([2])\n",
      "tensor([0.0605, 0.0457], grad_fn=<SliceBackward0>)\n",
      "outputs: torch.Size([1, 2, 2])\n",
      "tensor([[[0.0577, 0.0594],\n",
      "         [0.0507, 0.0613]]], grad_fn=<AddBackward0>)\n",
      "------------- END MLP.forwardTensor() ------------\n",
      "y: torch.Size([1, 2, 2])\n",
      "tensor([[[0.0577, 0.0594],\n",
      "         [0.0507, 0.0613]]], grad_fn=<AddBackward0>)\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the second sub-model |-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 2, 2])\n",
      "tensor([[[0.3224, 0.8645],\n",
      "         [0.7715, 0.7544]]])\n",
      "---------- MLP Input: torch.Tensor ------------\n",
      "------------- MLP.forwardTensor() ------------\n",
      "x: torch.Size([1, 2, 2])\n",
      "tensor([[[0.3224, 0.8645],\n",
      "         [0.7715, 0.7544]]])\n",
      "d_dim: 2\n",
      "d_skip: 2\n",
      "i_dim: 4\n",
      "i_skip: 4\n",
      "Wgate: torch.Size([4, 8])\n",
      "Parameter containing:\n",
      "tensor([[-0.0468,  0.4184,  0.4936,  0.0300,  0.2411, -0.3558, -0.4593, -0.1323],\n",
      "        [ 0.2544, -0.1617, -0.3531,  0.4524,  0.0260,  0.2537, -0.3914,  0.2556],\n",
      "        [ 0.0510, -0.4352, -0.4416,  0.3981, -0.3839, -0.4907,  0.3921, -0.2343],\n",
      "        [-0.0537,  0.2913,  0.0149, -0.0415,  0.0051, -0.4176,  0.0788,  0.0580]],\n",
      "       requires_grad=True)\n",
      "Wgate spliced: torch.Size([2, 4])\n",
      "tensor([[-0.3839, -0.4907,  0.3921, -0.2343],\n",
      "        [ 0.0051, -0.4176,  0.0788,  0.0580]], grad_fn=<SliceBackward0>)\n",
      "Bgate: torch.Size([8])\n",
      "Parameter containing:\n",
      "tensor([-0.1359, -0.3931, -0.2250, -0.1175,  0.2714,  0.1008,  0.2693, -0.3750],\n",
      "       requires_grad=True)\n",
      "Bgate spliced: torch.Size([4])\n",
      "tensor([ 0.2714,  0.1008,  0.2693, -0.3750], grad_fn=<SliceBackward0>)\n",
      "Xgate: torch.Size([1, 2, 4])\n",
      "tensor([[[ 0.1521, -0.4184,  0.4639, -0.4005],\n",
      "         [-0.0209, -0.5927,  0.6312, -0.5121]]], grad_fn=<AddBackward0>)\n",
      "GeLU'ed Xgate: torch.Size([1, 2, 4])\n",
      "tensor([[[ 0.0852, -0.1413,  0.3148, -0.1379],\n",
      "         [-0.0103, -0.1640,  0.4646, -0.1558]]], grad_fn=<GeluBackward0>)\n",
      "Wup: torch.Size([4, 8])\n",
      "Parameter containing:\n",
      "tensor([[ 0.2070,  0.4299, -0.0835, -0.0610, -0.1702, -0.3226,  0.2165, -0.1509],\n",
      "        [ 0.4281,  0.0768, -0.4000, -0.1080, -0.4993,  0.2964, -0.0865, -0.2516],\n",
      "        [-0.0770,  0.2066, -0.2714,  0.3962, -0.1914,  0.4970,  0.2260,  0.0101],\n",
      "        [-0.3287, -0.1920, -0.4686, -0.4077, -0.1329,  0.0072,  0.3331,  0.2366]],\n",
      "       requires_grad=True)\n",
      "Wup spliced: torch.Size([2, 4])\n",
      "tensor([[-0.1914,  0.4970,  0.2260,  0.0101],\n",
      "        [-0.1329,  0.0072,  0.3331,  0.2366]], grad_fn=<SliceBackward0>)\n",
      "Bup: torch.Size([8])\n",
      "Parameter containing:\n",
      "tensor([ 0.1861, -0.2634,  0.4914,  0.2665, -0.1315, -0.0633,  0.3100,  0.2522],\n",
      "       requires_grad=True)\n",
      "Bup spliced: torch.Size([4])\n",
      "tensor([-0.1315, -0.0633,  0.3100,  0.2522], grad_fn=<SliceBackward0>)\n",
      "Xup: torch.Size([1, 2, 4])\n",
      "tensor([[[-0.3081,  0.1032,  0.6709,  0.4600],\n",
      "         [-0.3794,  0.3256,  0.7357,  0.4385]]], grad_fn=<AddBackward0>)\n",
      "Xfuse: torch.Size([1, 2, 4])\n",
      "tensor([[[-0.0263, -0.0146,  0.2112, -0.0634],\n",
      "         [ 0.0039, -0.0534,  0.3418, -0.0683]]], grad_fn=<MulBackward0>)\n",
      "Wdown: torch.Size([8, 4])\n",
      "Parameter containing:\n",
      "tensor([[ 0.2321,  0.2468, -0.2338, -0.3264],\n",
      "        [ 0.2978,  0.2978,  0.2022,  0.0222],\n",
      "        [-0.2457,  0.2116,  0.2656, -0.2635],\n",
      "        [ 0.1775, -0.1576, -0.1795, -0.0456],\n",
      "        [ 0.2873,  0.2375,  0.0927,  0.0200],\n",
      "        [ 0.2189, -0.1405,  0.1872, -0.1385],\n",
      "        [-0.1362, -0.1568, -0.2789,  0.0855],\n",
      "        [-0.3121,  0.0651, -0.0153,  0.1960]], requires_grad=True)\n",
      "Wdown spliced: torch.Size([4, 2])\n",
      "tensor([[ 0.0927,  0.0200],\n",
      "        [ 0.1872, -0.1385],\n",
      "        [-0.2789,  0.0855],\n",
      "        [-0.0153,  0.1960]], grad_fn=<SliceBackward0>)\n",
      "Bdown: torch.Size([4])\n",
      "Parameter containing:\n",
      "tensor([-0.2748, -0.0682,  0.0190, -0.1049], requires_grad=True)\n",
      "Bdown spliced: torch.Size([2])\n",
      "tensor([ 0.0190, -0.1049], grad_fn=<SliceBackward0>)\n",
      "outputs: torch.Size([1, 2, 2])\n",
      "tensor([[[-0.0441, -0.0977],\n",
      "         [-0.0849, -0.0816]]], grad_fn=<AddBackward0>)\n",
      "------------- END MLP.forwardTensor() ------------\n",
      "y: torch.Size([1, 2, 2])\n",
      "tensor([[[-0.0441, -0.0977],\n",
      "         [-0.0849, -0.0816]]], grad_fn=<AddBackward0>)\n",
      "---------- RESET CONFIG --------\n",
      "model_count:  [1, 2, 4]\n"
     ]
    }
   ],
   "source": [
    "# Testing our MLP's forwardTensor()\n",
    "verbose = True\n",
    "print(\"--------- Micro Hyperparameters -------\")\n",
    "hold = config.hidden_size\n",
    "config.hidden_size = 4\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the big model |-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,2,4)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "mlp = MLP(4,8)\n",
    "y = mlp(x)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the first sub-model |-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,2,2)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "mlp = MLP(4,8)\n",
    "y = mlp(x)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the second sub-model |-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,2,2)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "mlp = MLP(4,8)\n",
    "y = mlp(x, model=1)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "verbose = False\n",
    "print(\"---------- RESET CONFIG --------\")\n",
    "config.hidden_size = hold\n",
    "print(\"model_count: \", config.model_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46a6916d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Micro Hyperparameters -------\n",
      "model_count:  [1, 2]\n",
      "model_dim_list:  [4, 2]\n",
      "x: ((tensor([[[-0.2928, -2.5093, -1.3731,  0.0879],\n",
      "         [-1.5967,  1.8499,  0.8346,  0.2128]]]),), (tensor([[[-0.8850, -0.4616],\n",
      "         [-0.8655, -0.0881]]]), tensor([[[ 0.8532, -0.1943],\n",
      "         [-1.2403, -0.8701]]])))\n",
      "---------- MLP Input: Tuple ------------\n",
      "------------- MLP.forwardTuple() ------------\n",
      "x: ((tensor([[[-0.2928, -2.5093, -1.3731,  0.0879],\n",
      "         [-1.5967,  1.8499,  0.8346,  0.2128]]]),), (tensor([[[-0.8850, -0.4616],\n",
      "         [-0.8655, -0.0881]]]), tensor([[[ 0.8532, -0.1943],\n",
      "         [-1.2403, -0.8701]]])))\n",
      "num_levels: 2\n",
      "models_per_level: [1, 2]\n",
      "i: 0\n",
      "j: 0\n",
      "------------- MLP.forwardTensor() ------------\n",
      "x: torch.Size([1, 2, 4])\n",
      "tensor([[[-0.2928, -2.5093, -1.3731,  0.0879],\n",
      "         [-1.5967,  1.8499,  0.8346,  0.2128]]])\n",
      "d_dim: 4\n",
      "d_skip: 0\n",
      "i_dim: 8\n",
      "i_skip: 0\n",
      "Wgate: torch.Size([4, 8])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0915, -0.4108,  0.1247, -0.0138, -0.2908,  0.3725,  0.3344,  0.4037],\n",
      "        [-0.0284,  0.2760,  0.0754, -0.1862,  0.4906,  0.1263, -0.4868,  0.4304],\n",
      "        [-0.1280,  0.0214,  0.2709,  0.3086,  0.2050,  0.1937, -0.4975,  0.1646],\n",
      "        [-0.0589, -0.2336, -0.1387,  0.4541, -0.0097, -0.0769, -0.4054,  0.1996]],\n",
      "       requires_grad=True)\n",
      "Wgate spliced: torch.Size([4, 8])\n",
      "tensor([[ 0.0915, -0.4108,  0.1247, -0.0138, -0.2908,  0.3725,  0.3344,  0.4037],\n",
      "        [-0.0284,  0.2760,  0.0754, -0.1862,  0.4906,  0.1263, -0.4868,  0.4304],\n",
      "        [-0.1280,  0.0214,  0.2709,  0.3086,  0.2050,  0.1937, -0.4975,  0.1646],\n",
      "        [-0.0589, -0.2336, -0.1387,  0.4541, -0.0097, -0.0769, -0.4054,  0.1996]],\n",
      "       grad_fn=<AliasBackward0>)\n",
      "Bgate: torch.Size([8])\n",
      "Parameter containing:\n",
      "tensor([ 0.4254, -0.2634,  0.2554, -0.4313, -0.0499, -0.0754, -0.4920,  0.2795],\n",
      "       requires_grad=True)\n",
      "Bgate spliced: torch.Size([8])\n",
      "tensor([ 0.4254, -0.2634,  0.2554, -0.4313, -0.0499, -0.0754, -0.4920,  0.2795],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xgate: torch.Size([1, 2, 8])\n",
      "tensor([[[ 0.6405, -0.8856, -0.3545, -0.3439, -1.4782, -0.7741,  1.2791,\n",
      "          -1.1272],\n",
      "         [ 0.1074,  0.8714,  0.3923, -0.3996,  1.4909, -0.2914, -2.4281,\n",
      "           0.6109]]], grad_fn=<AddBackward0>)\n",
      "GeLU'ed Xgate: torch.Size([1, 2, 8])\n",
      "tensor([[[ 0.4734, -0.1664, -0.1281, -0.1257, -0.1030, -0.1699,  1.1506,\n",
      "          -0.1463],\n",
      "         [ 0.0583,  0.7042,  0.2560, -0.1378,  1.3896, -0.1123, -0.0184,\n",
      "           0.4456]]], grad_fn=<GeluBackward0>)\n",
      "Wup: torch.Size([4, 8])\n",
      "Parameter containing:\n",
      "tensor([[-0.4226, -0.3866,  0.1093, -0.4012, -0.4684,  0.0925,  0.1151,  0.2215],\n",
      "        [-0.1194, -0.1408,  0.1217, -0.0437,  0.0620,  0.1206,  0.0745,  0.3171],\n",
      "        [ 0.0845, -0.4458,  0.4656,  0.1543, -0.4919,  0.3747,  0.4895, -0.0916],\n",
      "        [ 0.2456, -0.3102, -0.2267, -0.1938,  0.3095, -0.4544,  0.4943,  0.1458]],\n",
      "       requires_grad=True)\n",
      "Wup spliced: torch.Size([4, 8])\n",
      "tensor([[-0.4226, -0.3866,  0.1093, -0.4012, -0.4684,  0.0925,  0.1151,  0.2215],\n",
      "        [-0.1194, -0.1408,  0.1217, -0.0437,  0.0620,  0.1206,  0.0745,  0.3171],\n",
      "        [ 0.0845, -0.4458,  0.4656,  0.1543, -0.4919,  0.3747,  0.4895, -0.0916],\n",
      "        [ 0.2456, -0.3102, -0.2267, -0.1938,  0.3095, -0.4544,  0.4943,  0.1458]],\n",
      "       grad_fn=<AliasBackward0>)\n",
      "Bup: torch.Size([8])\n",
      "Parameter containing:\n",
      "tensor([-0.2670, -0.2387, -0.3355, -0.3630,  0.4801,  0.2352,  0.2024, -0.1721],\n",
      "       requires_grad=True)\n",
      "Bup spliced: torch.Size([8])\n",
      "tensor([-0.2670, -0.2387, -0.3355, -0.3630,  0.4801,  0.2352,  0.2024, -0.1721],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xup: torch.Size([1, 2, 8])\n",
      "tensor([[[ 0.0620,  0.8125, -1.3321, -0.3649,  1.1644, -0.6490, -0.6468,\n",
      "          -0.8941],\n",
      "         [ 0.3095, -0.3198,  0.0554,  0.2843,  0.9979,  0.5267,  0.6701,\n",
      "           0.0154]]], grad_fn=<AddBackward0>)\n",
      "Xfuse: torch.Size([1, 2, 8])\n",
      "tensor([[[ 0.0293, -0.1352,  0.1707,  0.0459, -0.1199,  0.1102, -0.7442,\n",
      "           0.1308],\n",
      "         [ 0.0180, -0.2252,  0.0142, -0.0392,  1.3867, -0.0591, -0.0123,\n",
      "           0.0069]]], grad_fn=<MulBackward0>)\n",
      "Wdown: torch.Size([8, 4])\n",
      "Parameter containing:\n",
      "tensor([[-0.0587, -0.0691, -0.1855,  0.2738],\n",
      "        [-0.0400,  0.3109, -0.0571,  0.2973],\n",
      "        [-0.2211, -0.0138,  0.2115,  0.0928],\n",
      "        [ 0.2388, -0.0659,  0.0365, -0.0493],\n",
      "        [ 0.2974,  0.0448, -0.0455, -0.1362],\n",
      "        [ 0.0378,  0.1691, -0.2889, -0.2223],\n",
      "        [-0.2327,  0.1655, -0.1326, -0.1297],\n",
      "        [-0.2917, -0.3170,  0.3517,  0.2367]], requires_grad=True)\n",
      "Wdown spliced: torch.Size([8, 4])\n",
      "tensor([[-0.0587, -0.0691, -0.1855,  0.2738],\n",
      "        [-0.0400,  0.3109, -0.0571,  0.2973],\n",
      "        [-0.2211, -0.0138,  0.2115,  0.0928],\n",
      "        [ 0.2388, -0.0659,  0.0365, -0.0493],\n",
      "        [ 0.2974,  0.0448, -0.0455, -0.1362],\n",
      "        [ 0.0378,  0.1691, -0.2889, -0.2223],\n",
      "        [-0.2327,  0.1655, -0.1326, -0.1297],\n",
      "        [-0.2917, -0.3170,  0.3517,  0.2367]], grad_fn=<AliasBackward0>)\n",
      "Bdown: torch.Size([4])\n",
      "Parameter containing:\n",
      "tensor([-0.1174, -0.0472,  0.2702,  0.0127], requires_grad=True)\n",
      "Bdown spliced: torch.Size([4])\n",
      "tensor([-0.1174, -0.0472,  0.2702,  0.0127], grad_fn=<SliceBackward0>)\n",
      "outputs: torch.Size([1, 2, 4])\n",
      "tensor([[[-0.0370, -0.2481,  0.4285,  0.1135],\n",
      "         [ 0.2892, -0.0682,  0.2393, -0.2186]]], grad_fn=<AddBackward0>)\n",
      "------------- END MLP.forwardTensor() ------------\n",
      "forwardTensor() output: torch.Size([1, 2, 4])\n",
      "tensor([[[-0.0370, -0.2481,  0.4285,  0.1135],\n",
      "         [ 0.2892, -0.0682,  0.2393, -0.2186]]], grad_fn=<AddBackward0>)\n",
      "out_lvl: (tensor([[[-0.0411, -0.2757,  0.4761,  0.1261],\n",
      "         [ 0.3213, -0.0758,  0.2659, -0.2429]]], grad_fn=<MulBackward0>),)\n",
      "i: 1\n",
      "j: 0\n",
      "------------- MLP.forwardTensor() ------------\n",
      "x: torch.Size([1, 2, 2])\n",
      "tensor([[[-0.8850, -0.4616],\n",
      "         [-0.8655, -0.0881]]])\n",
      "d_dim: 2\n",
      "d_skip: 0\n",
      "i_dim: 4\n",
      "i_skip: 0\n",
      "Wgate: torch.Size([4, 8])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0915, -0.4108,  0.1247, -0.0138, -0.2908,  0.3725,  0.3344,  0.4037],\n",
      "        [-0.0284,  0.2760,  0.0754, -0.1862,  0.4906,  0.1263, -0.4868,  0.4304],\n",
      "        [-0.1280,  0.0214,  0.2709,  0.3086,  0.2050,  0.1937, -0.4975,  0.1646],\n",
      "        [-0.0589, -0.2336, -0.1387,  0.4541, -0.0097, -0.0769, -0.4054,  0.1996]],\n",
      "       requires_grad=True)\n",
      "Wgate spliced: torch.Size([2, 4])\n",
      "tensor([[ 0.0915, -0.4108,  0.1247, -0.0138],\n",
      "        [-0.0284,  0.2760,  0.0754, -0.1862]], grad_fn=<SliceBackward0>)\n",
      "Bgate: torch.Size([8])\n",
      "Parameter containing:\n",
      "tensor([ 0.4254, -0.2634,  0.2554, -0.4313, -0.0499, -0.0754, -0.4920,  0.2795],\n",
      "       requires_grad=True)\n",
      "Bgate spliced: torch.Size([4])\n",
      "tensor([ 0.4254, -0.2634,  0.2554, -0.4313], grad_fn=<SliceBackward0>)\n",
      "Xgate: torch.Size([1, 2, 4])\n",
      "tensor([[[ 0.3576, -0.0272,  0.1102, -0.3332],\n",
      "         [ 0.3487,  0.0679,  0.1408, -0.4030]]], grad_fn=<AddBackward0>)\n",
      "GeLU'ed Xgate: torch.Size([1, 2, 4])\n",
      "tensor([[[ 0.2287, -0.0133,  0.0600, -0.1231],\n",
      "         [ 0.2219,  0.0358,  0.0783, -0.1384]]], grad_fn=<GeluBackward0>)\n",
      "Wup: torch.Size([4, 8])\n",
      "Parameter containing:\n",
      "tensor([[-0.4226, -0.3866,  0.1093, -0.4012, -0.4684,  0.0925,  0.1151,  0.2215],\n",
      "        [-0.1194, -0.1408,  0.1217, -0.0437,  0.0620,  0.1206,  0.0745,  0.3171],\n",
      "        [ 0.0845, -0.4458,  0.4656,  0.1543, -0.4919,  0.3747,  0.4895, -0.0916],\n",
      "        [ 0.2456, -0.3102, -0.2267, -0.1938,  0.3095, -0.4544,  0.4943,  0.1458]],\n",
      "       requires_grad=True)\n",
      "Wup spliced: torch.Size([2, 4])\n",
      "tensor([[-0.4226, -0.3866,  0.1093, -0.4012],\n",
      "        [-0.1194, -0.1408,  0.1217, -0.0437]], grad_fn=<SliceBackward0>)\n",
      "Bup: torch.Size([8])\n",
      "Parameter containing:\n",
      "tensor([-0.2670, -0.2387, -0.3355, -0.3630,  0.4801,  0.2352,  0.2024, -0.1721],\n",
      "       requires_grad=True)\n",
      "Bup spliced: torch.Size([4])\n",
      "tensor([-0.2670, -0.2387, -0.3355, -0.3630], grad_fn=<SliceBackward0>)\n",
      "Xup: torch.Size([1, 2, 4])\n",
      "tensor([[[ 0.1621,  0.1685, -0.4884,  0.0122],\n",
      "         [ 0.1092,  0.1084, -0.4408, -0.0119]]], grad_fn=<AddBackward0>)\n",
      "Xfuse: torch.Size([1, 2, 4])\n",
      "tensor([[[ 0.0371, -0.0022, -0.0293, -0.0015],\n",
      "         [ 0.0242,  0.0039, -0.0345,  0.0017]]], grad_fn=<MulBackward0>)\n",
      "Wdown: torch.Size([8, 4])\n",
      "Parameter containing:\n",
      "tensor([[-0.0587, -0.0691, -0.1855,  0.2738],\n",
      "        [-0.0400,  0.3109, -0.0571,  0.2973],\n",
      "        [-0.2211, -0.0138,  0.2115,  0.0928],\n",
      "        [ 0.2388, -0.0659,  0.0365, -0.0493],\n",
      "        [ 0.2974,  0.0448, -0.0455, -0.1362],\n",
      "        [ 0.0378,  0.1691, -0.2889, -0.2223],\n",
      "        [-0.2327,  0.1655, -0.1326, -0.1297],\n",
      "        [-0.2917, -0.3170,  0.3517,  0.2367]], requires_grad=True)\n",
      "Wdown spliced: torch.Size([4, 2])\n",
      "tensor([[-0.0587, -0.0691],\n",
      "        [-0.0400,  0.3109],\n",
      "        [-0.2211, -0.0138],\n",
      "        [ 0.2388, -0.0659]], grad_fn=<SliceBackward0>)\n",
      "Bdown: torch.Size([4])\n",
      "Parameter containing:\n",
      "tensor([-0.1174, -0.0472,  0.2702,  0.0127], requires_grad=True)\n",
      "Bdown spliced: torch.Size([2])\n",
      "tensor([-0.1174, -0.0472], grad_fn=<SliceBackward0>)\n",
      "outputs: torch.Size([1, 2, 2])\n",
      "tensor([[[-0.1133, -0.0500],\n",
      "         [-0.1109, -0.0473]]], grad_fn=<AddBackward0>)\n",
      "------------- END MLP.forwardTensor() ------------\n",
      "forwardTensor() output: torch.Size([1, 2, 2])\n",
      "tensor([[[-0.1133, -0.0500],\n",
      "         [-0.1109, -0.0473]]], grad_fn=<AddBackward0>)\n",
      "j: 1\n",
      "------------- MLP.forwardTensor() ------------\n",
      "x: torch.Size([1, 2, 2])\n",
      "tensor([[[ 0.8532, -0.1943],\n",
      "         [-1.2403, -0.8701]]])\n",
      "d_dim: 2\n",
      "d_skip: 2\n",
      "i_dim: 4\n",
      "i_skip: 4\n",
      "Wgate: torch.Size([4, 8])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0915, -0.4108,  0.1247, -0.0138, -0.2908,  0.3725,  0.3344,  0.4037],\n",
      "        [-0.0284,  0.2760,  0.0754, -0.1862,  0.4906,  0.1263, -0.4868,  0.4304],\n",
      "        [-0.1280,  0.0214,  0.2709,  0.3086,  0.2050,  0.1937, -0.4975,  0.1646],\n",
      "        [-0.0589, -0.2336, -0.1387,  0.4541, -0.0097, -0.0769, -0.4054,  0.1996]],\n",
      "       requires_grad=True)\n",
      "Wgate spliced: torch.Size([2, 4])\n",
      "tensor([[ 0.2050,  0.1937, -0.4975,  0.1646],\n",
      "        [-0.0097, -0.0769, -0.4054,  0.1996]], grad_fn=<SliceBackward0>)\n",
      "Bgate: torch.Size([8])\n",
      "Parameter containing:\n",
      "tensor([ 0.4254, -0.2634,  0.2554, -0.4313, -0.0499, -0.0754, -0.4920,  0.2795],\n",
      "       requires_grad=True)\n",
      "Bgate spliced: torch.Size([4])\n",
      "tensor([-0.0499, -0.0754, -0.4920,  0.2795], grad_fn=<SliceBackward0>)\n",
      "Xgate: torch.Size([1, 2, 4])\n",
      "tensor([[[ 0.1269,  0.1048, -0.8377,  0.3812],\n",
      "         [-0.2958, -0.2488,  0.4777, -0.0984]]], grad_fn=<AddBackward0>)\n",
      "GeLU'ed Xgate: torch.Size([1, 2, 4])\n",
      "tensor([[[ 0.0699,  0.0568, -0.1685,  0.2472],\n",
      "         [-0.1135, -0.0999,  0.3265, -0.0453]]], grad_fn=<GeluBackward0>)\n",
      "Wup: torch.Size([4, 8])\n",
      "Parameter containing:\n",
      "tensor([[-0.4226, -0.3866,  0.1093, -0.4012, -0.4684,  0.0925,  0.1151,  0.2215],\n",
      "        [-0.1194, -0.1408,  0.1217, -0.0437,  0.0620,  0.1206,  0.0745,  0.3171],\n",
      "        [ 0.0845, -0.4458,  0.4656,  0.1543, -0.4919,  0.3747,  0.4895, -0.0916],\n",
      "        [ 0.2456, -0.3102, -0.2267, -0.1938,  0.3095, -0.4544,  0.4943,  0.1458]],\n",
      "       requires_grad=True)\n",
      "Wup spliced: torch.Size([2, 4])\n",
      "tensor([[-0.4919,  0.3747,  0.4895, -0.0916],\n",
      "        [ 0.3095, -0.4544,  0.4943,  0.1458]], grad_fn=<SliceBackward0>)\n",
      "Bup: torch.Size([8])\n",
      "Parameter containing:\n",
      "tensor([-0.2670, -0.2387, -0.3355, -0.3630,  0.4801,  0.2352,  0.2024, -0.1721],\n",
      "       requires_grad=True)\n",
      "Bup spliced: torch.Size([4])\n",
      "tensor([ 0.4801,  0.2352,  0.2024, -0.1721], grad_fn=<SliceBackward0>)\n",
      "Xup: torch.Size([1, 2, 4])\n",
      "tensor([[[ 2.8861e-04,  6.4323e-01,  5.2402e-01, -2.7858e-01],\n",
      "         [ 8.2091e-01,  1.6576e-01, -8.3480e-01, -1.8527e-01]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "Xfuse: torch.Size([1, 2, 4])\n",
      "tensor([[[ 2.0162e-05,  3.6509e-02, -8.8278e-02, -6.8856e-02],\n",
      "         [-9.3164e-02, -1.6568e-02, -2.7260e-01,  8.4000e-03]]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "Wdown: torch.Size([8, 4])\n",
      "Parameter containing:\n",
      "tensor([[-0.0587, -0.0691, -0.1855,  0.2738],\n",
      "        [-0.0400,  0.3109, -0.0571,  0.2973],\n",
      "        [-0.2211, -0.0138,  0.2115,  0.0928],\n",
      "        [ 0.2388, -0.0659,  0.0365, -0.0493],\n",
      "        [ 0.2974,  0.0448, -0.0455, -0.1362],\n",
      "        [ 0.0378,  0.1691, -0.2889, -0.2223],\n",
      "        [-0.2327,  0.1655, -0.1326, -0.1297],\n",
      "        [-0.2917, -0.3170,  0.3517,  0.2367]], requires_grad=True)\n",
      "Wdown spliced: torch.Size([4, 2])\n",
      "tensor([[-0.0455, -0.1362],\n",
      "        [-0.2889, -0.2223],\n",
      "        [-0.1326, -0.1297],\n",
      "        [ 0.3517,  0.2367]], grad_fn=<SliceBackward0>)\n",
      "Bdown: torch.Size([4])\n",
      "Parameter containing:\n",
      "tensor([-0.1174, -0.0472,  0.2702,  0.0127], requires_grad=True)\n",
      "Bdown spliced: torch.Size([2])\n",
      "tensor([0.2702, 0.0127], grad_fn=<SliceBackward0>)\n",
      "outputs: torch.Size([1, 2, 2])\n",
      "tensor([[[ 2.4711e-01, -2.5595e-04],\n",
      "         [ 3.1829e-01,  6.6433e-02]]], grad_fn=<AddBackward0>)\n",
      "------------- END MLP.forwardTensor() ------------\n",
      "forwardTensor() output: torch.Size([1, 2, 2])\n",
      "tensor([[[ 2.4711e-01, -2.5595e-04],\n",
      "         [ 3.1829e-01,  6.6433e-02]]], grad_fn=<AddBackward0>)\n",
      "out_lvl: (tensor([[[-0.1259, -0.0000],\n",
      "         [-0.1232, -0.0526]]], grad_fn=<MulBackward0>), tensor([[[ 2.7457e-01, -2.8439e-04],\n",
      "         [ 3.5366e-01,  7.3815e-02]]], grad_fn=<MulBackward0>))\n",
      "out: ((tensor([[[-0.0411, -0.2757,  0.4761,  0.1261],\n",
      "         [ 0.3213, -0.0758,  0.2659, -0.2429]]], grad_fn=<MulBackward0>),), (tensor([[[-0.1259, -0.0000],\n",
      "         [-0.1232, -0.0526]]], grad_fn=<MulBackward0>), tensor([[[ 2.7457e-01, -2.8439e-04],\n",
      "         [ 3.5366e-01,  7.3815e-02]]], grad_fn=<MulBackward0>)))\n",
      "------------- END MLP.forwardTuple() ------------\n",
      "out: ((tensor([[[-0.0411, -0.2757,  0.4761,  0.1261],\n",
      "         [ 0.3213, -0.0758,  0.2659, -0.2429]]], grad_fn=<MulBackward0>),), (tensor([[[-0.1259, -0.0000],\n",
      "         [-0.1232, -0.0526]]], grad_fn=<MulBackward0>), tensor([[[ 2.7457e-01, -2.8439e-04],\n",
      "         [ 3.5366e-01,  7.3815e-02]]], grad_fn=<MulBackward0>)))\n",
      "---------- RESET CONFIG --------\n",
      "model_count:  [1, 2, 4]\n",
      "model_dim_list:  [128, 64, 32]\n"
     ]
    }
   ],
   "source": [
    "# Testing our MLP's forwardTuple()\n",
    "verbose = True\n",
    "print(\"--------- Micro Hyperparameters -------\")\n",
    "hold1, hold2 = config.hidden_size, config.levels\n",
    "config.hidden_size = 4\n",
    "config.levels = 2\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "\n",
    "mlp = MLP(4,8)\n",
    "x = ((torch.randn((1,2,4)),),\n",
    "     (torch.randn((1,2,2)),torch.randn((1,2,2)))\n",
    "    )\n",
    "print(f\"x: {x}\")\n",
    "out = mlp(x)\n",
    "print(f\"out: {out}\")\n",
    "\n",
    "verbose = False\n",
    "print(\"---------- RESET CONFIG --------\")\n",
    "config.hidden_size = hold1\n",
    "config.levels = hold2\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f269924",
   "metadata": {},
   "source": [
    "# Attention\n",
    "\n",
    "To subset the attention heads, we have to not only splice according to the model's embedding dimension but also take into account new smaller head sizes and how they're spaced throughout the matrix. I'm assuming you know how self-attention works well enough to look at this weight matrix and get the idea\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./images/sa.jpeg\" width=\"512\"/>\n",
    "</p>\n",
    "\n",
    "then we've gotta concatenate the outputs of each head\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./images/mha_concat.jpeg\" width=\"512\"/>\n",
    "</p>\n",
    "\n",
    "and after that linearly project them\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./images/mha_proj.jpeg\" width=\"512\"/>\n",
    "</p>\n",
    "\n",
    "this is the place where our splicing gets conceptually annoying. instead of just grabbing the matrix in the upper corner, because of the way attention head output concatenation works we actually need to skip over certain parts of the linear projection matrix and then concatenate them together in order to use them. Here's an example of what the matrix multiplication looks like. on the left is a simplified version of the concatenated attention heads where i just showed it as a matrix rather than a tensor, and then on the right is the actual projection matrix. notice how the numbers in the pink output matrix look similar to the first column of the purple output matrix with a positive number, its negative, and then a smaller positive number; that's the self-similarity in action. the yellow arrows point to the parts that get skipped over. obviously this would look a lot uglier with bigger matrices & incorporating the blue/green layer\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./images/mha_proj_matmul.jpeg\" width=\"512\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a66e2155",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiQueryAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements Multi-Query Attention which supports a distinct number of attention heads for queries and key-values (KV).\n",
    "    In the case where the same number of queries and key-values are used, this implemenation is equivalent to regular Multi-Head Attention.  \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.num_kv_heads = config.num_key_value_heads\n",
    "        \n",
    "        # Determines the number of query heads associated with each KV head.\n",
    "        self.num_queries_per_kv = self.num_heads // self.num_kv_heads\n",
    "\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.head_dim = config.head_dim\n",
    "        self.theta = config.rope_theta\n",
    "\n",
    "        # Calculates the total size for all query projections.\n",
    "        self.q_size = self.num_heads * self.head_dim\n",
    "        # Calculates the total size for all key and value projections.\n",
    "        self.kv_size = self.num_kv_heads * self.head_dim\n",
    "        \n",
    "        # Initialize our learnable matrices\n",
    "        # the linear projection layer for queries, keys, and values\n",
    "        # no real reason why we're creating one matrix instead of separate ones. cleaner model summary view?\n",
    "        self.Wqkv = nn.Parameter(torch.Tensor(self.hidden_size,\n",
    "                                              (self.num_heads + 2 * self.num_kv_heads) * self.head_dim))\n",
    "        # the output projection layer, mapping the concatenated attention outputs back to the hidden size.\n",
    "        self.Wo = nn.Parameter(torch.Tensor(self.num_heads * self.head_dim, self.hidden_size))\n",
    "        \n",
    "        # Initialize weights with uniform distribution\n",
    "        # For qkv_proj, where in_features is hidden_size\n",
    "        limit_Wqkv = 1 / np.sqrt(self.hidden_size)\n",
    "        nn.init.uniform_(self.Wqkv, -limit_Wqkv, limit_Wqkv)\n",
    "        # for o_proj, where in_features is self.num_heads * self.head_dim\n",
    "        limit_Wo = 1 / np.sqrt(self.num_heads * self.head_dim)\n",
    "        nn.init.uniform_(self.Wo, -limit_Wo, limit_Wo)\n",
    "        \n",
    "        # for our attention mask we'll use very large negative values to prevent attending to certain tokens\n",
    "        mask_negatives = torch.full((1, 1, config.max_position_embeddings, config.max_position_embeddings),\n",
    "                                 -2.3819763e38).to(torch.float)\n",
    "        # then we'll replace the lower triangular ones with 0's to allow attention to see past tokens\n",
    "        mask = torch.triu(mask_negatives, diagonal=1).to(config.device)\n",
    "        # to define self.mask as a tensor that shouldn't undergo gradient descent\n",
    "        self.register_buffer('mask', mask)\n",
    "        \n",
    "        # defining our dropout\n",
    "        self.drop = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forwardTensor(self,\n",
    "                      x: torch.Tensor,\n",
    "                      model: int = 0,\n",
    "                     ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            x (torch.Tensor): Te input tensor to the attention mechanism.\n",
    "                        shape (batch_size, input_len, hidden_size)\n",
    "            model (int): the indicator of which model we're using. \n",
    "                        used in calculating our skip length for splicing. \n",
    "                        defaults to the equivalent of what's used in MatFormer+, meaning no skip, aka we use the top-left-most splice\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: The output tensor after applying the attention mechanism\n",
    "        \"\"\"\n",
    "        global verbose\n",
    "        if verbose: print(\"----------------- MultiQueryAttention.forwardTensor() --------------------\")\n",
    "        \n",
    "        # Ensures the input tensor is 3-dimensional (batch_size, input_len, hidden_size).\n",
    "        x_shape = x.shape\n",
    "        assert len(x_shape) == 3\n",
    "        if verbose: print(f\"x shape: {x_shape}\")\n",
    "\n",
    "        # Extracts input sequence length and embedding dimension length from the hidden states tensor.\n",
    "        batch_size, input_len, d_dim = x_shape\n",
    "        \n",
    "        # figuring out how we should do our splicing\n",
    "        # first along the embedding dimension\n",
    "        d_skip = model * d_dim  # the size of our skip along the model's embedding dimension\n",
    "        if verbose: print(f\"d_skip: {d_skip}\")\n",
    "        \n",
    "        # then for splicing along the head sizes dimension\n",
    "        index = config.model_dim_list.index(d_dim)\n",
    "        models_in_this_level = config.model_count[index] # how many models are in this level\n",
    "        h_dim = config.head_dim_list[index] # the head dimension size of this model in this level\n",
    "        h_skip = model * h_dim # the size of our skip along the head dimension\n",
    "        if verbose: \n",
    "            print(f\"models_in_this_level: {models_in_this_level}\")\n",
    "            print(f\"h_dim: {h_dim}\")\n",
    "            print(f\"h_skip: {h_skip}\")\n",
    "\n",
    "        # Splits the Wqkv tensor into separate tensors for queries, keys, and values based on their respective sizes.\n",
    "        if verbose: print(f\"self.Wqkv: {self.Wqkv.shape}\\n{self.Wqkv}\")\n",
    "        Wq, Wk, Wv = self.Wqkv.split([self.q_size,\n",
    "                                      self.kv_size,\n",
    "                                      self.kv_size],dim=-1)\n",
    "        if verbose: \n",
    "            print(f\"Wq: {Wq.shape}\\n{Wq}\")\n",
    "            print(f\"Wk: {Wk.shape}\\n{Wk}\")\n",
    "            print(f\"Wv: {Wv.shape}\\n{Wv}\")\n",
    "        \n",
    "        # splicing to get our correct weight matrices for each respective head\n",
    "        # d_dim is relatively self-explanatory\n",
    "        # i*self.head_dim is bc we initialized one single q, k, and v matrix for all heads so we have to\n",
    "        # iterate through said matrix to get to the correct head\n",
    "        Wq = torch.cat([Wq[d_skip:d_skip + d_dim,\\\n",
    "                               i*self.head_dim + h_skip:i*self.head_dim + h_skip + h_dim] \\\n",
    "                               for i in range(self.num_heads)], dim=1)\n",
    "        Wk = torch.cat([Wk[d_skip:d_skip + d_dim,\\\n",
    "                               i*self.head_dim + h_skip:i*self.head_dim + h_skip + h_dim] \\\n",
    "                               for i in range(self.num_kv_heads)], dim=1)\n",
    "        Wv = torch.cat([Wv[d_skip:d_skip + d_dim,\\\n",
    "                               i*self.head_dim + h_skip:i*self.head_dim + h_skip + h_dim] \\\n",
    "                               for i in range(self.num_kv_heads)], dim=1)\n",
    "        if verbose:\n",
    "            print(f\"Wq spliced: {Wq.shape}\\n{Wq}\")\n",
    "            print(f\"Wk spliced: {Wk.shape}\\n{Wk}\")\n",
    "            print(f\"Wv spliced: {Wv.shape}\\n{Wv}\")\n",
    "        \n",
    "        # this needs to be size (d_dim, (self.num_heads + 2 * self.num_kv_heads) * h_dim) aka (32,24)\n",
    "        # recombine the spliced Wq Wk and Wv. Now they're the right size for matmul against x\n",
    "        Wqkv_spliced = torch.cat((Wq, Wk, Wv), dim=-1)\n",
    "        if verbose:\n",
    "            print(f\"Wqkv_spliced: {Wqkv_spliced.shape}\\n{Wqkv_spliced}\")\n",
    "        \n",
    "\n",
    "        # finally we can project x to get our queries, keys and values\n",
    "        xqkv = x @ Wqkv_spliced\n",
    "        if verbose: print(f\"xqkv: {xqkv.shape}\\n{xqkv}\")\n",
    "            \n",
    "        # Splits the combined Xqkv tensor into separate tensors for queries (xq), keys (xk), and values (xv) based on their respective sizes.\n",
    "        xq, xk, xv = xqkv.split([self.q_size // models_in_this_level,\n",
    "                                 self.kv_size // models_in_this_level,\n",
    "                                 self.kv_size // models_in_this_level],dim=-1)\n",
    "        if verbose:\n",
    "            print(f\"xq: {xq.shape}\\n{xq}\")\n",
    "            print(f\"xk: {xk.shape}\\n{xk}\")\n",
    "            print(f\"xv: {xv.shape}\\n{xv}\")\n",
    "\n",
    "        # Reshapes each of the Q, K, and V tensors to separate the heads and align the dimensions for attention operations.\n",
    "        xq = xq.view(batch_size, input_len, self.num_heads, h_dim)#, self.head_dim)\n",
    "        xk = xk.view(batch_size, input_len, self.num_kv_heads, h_dim)#, self.head_dim)\n",
    "        xv = xv.view(batch_size, input_len, self.num_kv_heads, h_dim)#, self.head_dim)\n",
    "        if verbose:\n",
    "            print(f\"xq reshaped: {xq.shape}\\n{xq}\")\n",
    "            print(f\"xk reshaped: {xk.shape}\\n{xk}\")\n",
    "            print(f\"xv reshaped: {xv.shape}\\n{xv}\")\n",
    "\n",
    "        # Applies rotary positional embeddings to queries and keys to incorporate positional information.\n",
    "        xq = apply_rotary_emb(xq, h_dim, self.theta)#self.head_dim\n",
    "        xk = apply_rotary_emb(xk, h_dim, self.theta)#self.head_dim\n",
    "        # is the differring head dimension going to mess with RoPE? Not sure\n",
    "        if verbose:\n",
    "            print(f\"rotated xq: {xq.shape}\\n{xq}\")\n",
    "            print(f\"rotated xk: {xk.shape}\\n{xk}\")\n",
    "\n",
    "        # If the number of KV heads is different from the number of query heads, adjusts keys and values to match the query heads count.\n",
    "        if self.num_kv_heads != self.num_heads:\n",
    "            # [batch_size, input_len, n_local_heads, head_dim]\n",
    "            xk = torch.repeat_interleave(xk, self.num_queries_per_kv, dim=2)\n",
    "            xv = torch.repeat_interleave(xv, self.num_queries_per_kv, dim=2)\n",
    "            if verbose:\n",
    "                print(f\"repeat_interleaved xk: {xk.shape}\\n{xk}\")\n",
    "                print(f\"repeat_interleaved xv: {xv.shape}\\n{xv}\")\n",
    "\n",
    "        # Transposes Q, K, and V tensors to align them for the batch matrix multiplication in attention calculation.\n",
    "        # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        q = xq.transpose(1, 2)\n",
    "        k = xk.transpose(1, 2)\n",
    "        v = xv.transpose(1, 2)\n",
    "        if verbose:\n",
    "            print(f\"transposed xq: {q.shape}\\n{q}\")\n",
    "            print(f\"transposed xk: {k.shape}\\n{k}\")\n",
    "            print(f\"transposed xv: {v.shape}\\n{v}\")\n",
    "\n",
    "        # Calculates attention scores by performing a batch matrix multiplication between queries and keys, followed by scaling.\n",
    "        # [batch_size, n_local_heads, input_len, input_len]\n",
    "        scores = torch.matmul(q, k.transpose(2, 3)) * h_dim**-0.5#self.scaling\n",
    "        if verbose: print(f\"scores: {scores.shape}\\n{scores}\")\n",
    "        \n",
    "        # Applies the lower-triangular mask to the attention scores\n",
    "        if verbose: print(f\"mask: {self.mask[...,:input_len, :input_len].shape}\\n{self.mask[...,:input_len, :input_len]}\")\n",
    "        scores = scores + self.mask[...,:input_len, :input_len] # make sure mask is the correct size. input_len <= max_seq_len\n",
    "        if verbose: print(f\"masked scores: {scores.shape}\\n{scores}\")\n",
    "\n",
    "        # Applies softmax to the scores to obtain attention probabilities\n",
    "        scores = F.softmax(scores, dim=-1)\n",
    "        if verbose: print(f\"softmaxed scores: {scores.shape}\\n{scores}\")\n",
    "        \n",
    "        # Computes the weighted sum of values based on the attention scores to obtain the output of the attention mechanism.\n",
    "        # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        attention = torch.matmul(scores, v)\n",
    "        if verbose: print(f\"attention: {attention.shape}\\n{attention}\")\n",
    "\n",
    "        # Reshapes the attention output to match the expected output dimensions, combining the heads back into the hidden dimension.\n",
    "        # [batch_size, input_len, hidden_dim]\n",
    "        attention = attention.transpose(1, 2).contiguous().view(batch_size, input_len, -1)\n",
    "        if verbose: print(f\"reshaped attention: {attention.shape}\\n{attention}\")\n",
    "\n",
    "        # Splice the output projection\n",
    "        Wo = torch.cat([self.Wo[i*self.head_dim + h_skip:i*self.head_dim + h_skip + h_dim,\\\n",
    "                                d_skip:d_skip + d_dim,\\\n",
    "                               ] for i in range(self.num_heads)], dim=0)\n",
    "        if verbose: \n",
    "            print(f\"self.Wo: {self.Wo.shape}\\n{self.Wo}\")\n",
    "            print(f\"spliced Wo: {Wo.shape}\\n{Wo}\")\n",
    "            \n",
    "        # Applies the final linear projection to the attention output, mapping it back to the hidden size dimension.\n",
    "        output = attention @ Wo\n",
    "        if verbose: \n",
    "            print(f\"projected output: {output.shape}\\n{output}\")\n",
    "            print(\"----------------- END MultiQueryAttention.forwardTensor() --------------------\")\n",
    "            \n",
    "        return output\n",
    "\n",
    "    def forwardTuple(self,\n",
    "                     x: Tuple[Tuple[torch.Tensor]],\n",
    "                     drop_bool: bool = True\n",
    "                    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the Attention module during training.\n",
    "\n",
    "        Parameters:\n",
    "            x (Tuple[Tuple[Tensor]]): \n",
    "                The input tuple of tuples of tensors \n",
    "                first tuple is of length config.levels and second layer of tuples have lengths of config.model_count\n",
    "                tensors are shape (batch size, sequence length, hidden dimension) where hidden dimension changes by which model was used\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Tuple[Tensor]]: \n",
    "                The output tuple of tuples of tensors after applying the MQA mechanism\n",
    "        \"\"\"\n",
    "        global verbose\n",
    "        if verbose: \n",
    "            print(\"------------- MultiQueryAttention.forwardTuple() ------------\")\n",
    "            print(f\"x: {x}\")\n",
    "            \n",
    "        # forwardTuple() should only be used during training, so we assert input_len == max_position_embeddings\n",
    "        input_len = x[0][0].shape[1]\n",
    "        if verbose: print(f\"input_len: {input_len}\")\n",
    "        assert input_len == config.max_position_embeddings\n",
    "\n",
    "        # we could define these from the config but this way the method is more flexible to testing\n",
    "        num_levels = len(x)\n",
    "        models_per_level = [len(x[i]) for i in range(num_levels)]\n",
    "        if verbose: \n",
    "            print(f\"num_levels: {num_levels}\")\n",
    "            print(f\"models_per_level: {models_per_level}\")\n",
    "\n",
    "        # the loop that iterates over levels, aka the different potential sizes of models\n",
    "        out = ()\n",
    "        for i in range(num_levels):\n",
    "            if verbose: print(f\"Level {i} from range({num_levels})\")\n",
    "\n",
    "            # now for the loop that iterates over models in this level\n",
    "            out_lvl = ()\n",
    "            for j in range(models_per_level[i]):\n",
    "                if verbose: print(f\"Model {j} from range({models_per_level[i]})\")\n",
    "\n",
    "                output = self.forwardTensor(x[i][j], model=j)\n",
    "                if verbose: print(f\"forwardTensor() output: {output.shape}\\n{output}\")\n",
    "                \n",
    "                out_lvl += (self.drop(output),) if drop_bool else (output,)\n",
    "            \n",
    "            out += (out_lvl,)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"final output: {out}\")\n",
    "            print(\"------------- END MultiQueryAttention.forwardTuple() ------------\")\n",
    "\n",
    "        return out\n",
    "        \n",
    "    def forward(self, x, model=0, drop_bool = True):\n",
    "        train = True if type(x) == tuple else False\n",
    "        if verbose: print(f\"---------- Attention Input: {'Tuple' if train else 'torch.Tensor'} ------------\")\n",
    "        return self.forwardTuple(x, drop_bool) if train else self.forwardTensor(x, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad02319",
   "metadata": {},
   "source": [
    "And here are the detailed print statements for the attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41c59f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Micro Hyperparameters -------\n",
      "model_count:  [1, 2, 4]\n",
      "model_dim_list:  [8, 4, 2]\n",
      "head_dim_list:  [4, 2, 1]\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the big model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 3, 8])\n",
      "tensor([[[0.1519, 0.3537, 0.8843, 0.1288, 0.1360, 0.0713, 0.4278, 0.2955],\n",
      "         [0.7735, 0.7697, 0.5463, 0.6070, 0.6501, 0.1923, 0.0536, 0.3405],\n",
      "         [0.2467, 0.2990, 0.6230, 0.6324, 0.2659, 0.2712, 0.6693, 0.0847]]])\n",
      "---------- Attention Input: torch.Tensor ------------\n",
      "----------------- MultiQueryAttention.forwardTensor() --------------------\n",
      "x shape: torch.Size([1, 3, 8])\n",
      "d_skip: 0\n",
      "models_in_this_level: 1\n",
      "h_dim: 4\n",
      "h_skip: 0\n",
      "self.Wqkv: torch.Size([8, 16])\n",
      "Parameter containing:\n",
      "tensor([[ 0.1423, -0.0180, -0.1410,  0.0382,  0.0122, -0.1372,  0.3468,  0.1498,\n",
      "         -0.0661, -0.1451,  0.0516,  0.1634, -0.1407, -0.1390, -0.0870,  0.0576],\n",
      "        [ 0.1708, -0.0454, -0.1510,  0.0768,  0.2447,  0.0084,  0.0574,  0.3502,\n",
      "          0.2398,  0.1825, -0.1435,  0.2742, -0.3526, -0.1488,  0.0089,  0.0093],\n",
      "        [-0.0559, -0.0638,  0.3046,  0.1827,  0.1695, -0.2933, -0.2888,  0.1723,\n",
      "          0.2539, -0.0585, -0.0680,  0.0940,  0.1993,  0.1260, -0.2973, -0.2448],\n",
      "        [-0.2156,  0.0308, -0.2504,  0.2014, -0.1899,  0.1746,  0.2946, -0.2698,\n",
      "          0.2007, -0.2258,  0.1053, -0.3314, -0.1319,  0.0056,  0.0038, -0.1428],\n",
      "        [ 0.0559, -0.2970, -0.0035, -0.2646,  0.3506,  0.0886,  0.1032, -0.2012,\n",
      "         -0.2960,  0.0270,  0.1776,  0.3277, -0.2423, -0.0275, -0.1076, -0.0077],\n",
      "        [-0.3189,  0.0907, -0.3154,  0.1249, -0.0932, -0.1950,  0.0143,  0.2305,\n",
      "          0.1915,  0.1152,  0.3369, -0.1412,  0.1376,  0.2973, -0.3134, -0.0245],\n",
      "        [ 0.1012, -0.2597, -0.2295, -0.2149, -0.2420,  0.0781, -0.1078,  0.1118,\n",
      "         -0.3208,  0.2114,  0.0822, -0.3503, -0.0117, -0.2041,  0.2922,  0.1498],\n",
      "        [-0.1838, -0.1134,  0.0580,  0.2759, -0.2583,  0.2506, -0.2030, -0.0248,\n",
      "         -0.0919,  0.0405, -0.2035, -0.0151,  0.2938, -0.2220,  0.2284, -0.0035]],\n",
      "       requires_grad=True)\n",
      "Wq: torch.Size([8, 8])\n",
      "tensor([[ 0.1423, -0.0180, -0.1410,  0.0382,  0.0122, -0.1372,  0.3468,  0.1498],\n",
      "        [ 0.1708, -0.0454, -0.1510,  0.0768,  0.2447,  0.0084,  0.0574,  0.3502],\n",
      "        [-0.0559, -0.0638,  0.3046,  0.1827,  0.1695, -0.2933, -0.2888,  0.1723],\n",
      "        [-0.2156,  0.0308, -0.2504,  0.2014, -0.1899,  0.1746,  0.2946, -0.2698],\n",
      "        [ 0.0559, -0.2970, -0.0035, -0.2646,  0.3506,  0.0886,  0.1032, -0.2012],\n",
      "        [-0.3189,  0.0907, -0.3154,  0.1249, -0.0932, -0.1950,  0.0143,  0.2305],\n",
      "        [ 0.1012, -0.2597, -0.2295, -0.2149, -0.2420,  0.0781, -0.1078,  0.1118],\n",
      "        [-0.1838, -0.1134,  0.0580,  0.2759, -0.2583,  0.2506, -0.2030, -0.0248]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wk: torch.Size([8, 4])\n",
      "tensor([[-0.0661, -0.1451,  0.0516,  0.1634],\n",
      "        [ 0.2398,  0.1825, -0.1435,  0.2742],\n",
      "        [ 0.2539, -0.0585, -0.0680,  0.0940],\n",
      "        [ 0.2007, -0.2258,  0.1053, -0.3314],\n",
      "        [-0.2960,  0.0270,  0.1776,  0.3277],\n",
      "        [ 0.1915,  0.1152,  0.3369, -0.1412],\n",
      "        [-0.3208,  0.2114,  0.0822, -0.3503],\n",
      "        [-0.0919,  0.0405, -0.2035, -0.0151]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wv: torch.Size([8, 4])\n",
      "tensor([[-0.1407, -0.1390, -0.0870,  0.0576],\n",
      "        [-0.3526, -0.1488,  0.0089,  0.0093],\n",
      "        [ 0.1993,  0.1260, -0.2973, -0.2448],\n",
      "        [-0.1319,  0.0056,  0.0038, -0.1428],\n",
      "        [-0.2423, -0.0275, -0.1076, -0.0077],\n",
      "        [ 0.1376,  0.2973, -0.3134, -0.0245],\n",
      "        [-0.0117, -0.2041,  0.2922,  0.1498],\n",
      "        [ 0.2938, -0.2220,  0.2284, -0.0035]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wq spliced: torch.Size([8, 8])\n",
      "tensor([[ 0.1423, -0.0180, -0.1410,  0.0382,  0.0122, -0.1372,  0.3468,  0.1498],\n",
      "        [ 0.1708, -0.0454, -0.1510,  0.0768,  0.2447,  0.0084,  0.0574,  0.3502],\n",
      "        [-0.0559, -0.0638,  0.3046,  0.1827,  0.1695, -0.2933, -0.2888,  0.1723],\n",
      "        [-0.2156,  0.0308, -0.2504,  0.2014, -0.1899,  0.1746,  0.2946, -0.2698],\n",
      "        [ 0.0559, -0.2970, -0.0035, -0.2646,  0.3506,  0.0886,  0.1032, -0.2012],\n",
      "        [-0.3189,  0.0907, -0.3154,  0.1249, -0.0932, -0.1950,  0.0143,  0.2305],\n",
      "        [ 0.1012, -0.2597, -0.2295, -0.2149, -0.2420,  0.0781, -0.1078,  0.1118],\n",
      "        [-0.1838, -0.1134,  0.0580,  0.2759, -0.2583,  0.2506, -0.2030, -0.0248]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Wk spliced: torch.Size([8, 4])\n",
      "tensor([[-0.0661, -0.1451,  0.0516,  0.1634],\n",
      "        [ 0.2398,  0.1825, -0.1435,  0.2742],\n",
      "        [ 0.2539, -0.0585, -0.0680,  0.0940],\n",
      "        [ 0.2007, -0.2258,  0.1053, -0.3314],\n",
      "        [-0.2960,  0.0270,  0.1776,  0.3277],\n",
      "        [ 0.1915,  0.1152,  0.3369, -0.1412],\n",
      "        [-0.3208,  0.2114,  0.0822, -0.3503],\n",
      "        [-0.0919,  0.0405, -0.2035, -0.0151]], grad_fn=<CatBackward0>)\n",
      "Wv spliced: torch.Size([8, 4])\n",
      "tensor([[-0.1407, -0.1390, -0.0870,  0.0576],\n",
      "        [-0.3526, -0.1488,  0.0089,  0.0093],\n",
      "        [ 0.1993,  0.1260, -0.2973, -0.2448],\n",
      "        [-0.1319,  0.0056,  0.0038, -0.1428],\n",
      "        [-0.2423, -0.0275, -0.1076, -0.0077],\n",
      "        [ 0.1376,  0.2973, -0.3134, -0.0245],\n",
      "        [-0.0117, -0.2041,  0.2922,  0.1498],\n",
      "        [ 0.2938, -0.2220,  0.2284, -0.0035]], grad_fn=<CatBackward0>)\n",
      "Wqkv_spliced: torch.Size([8, 16])\n",
      "tensor([[ 0.1423, -0.0180, -0.1410,  0.0382,  0.0122, -0.1372,  0.3468,  0.1498,\n",
      "         -0.0661, -0.1451,  0.0516,  0.1634, -0.1407, -0.1390, -0.0870,  0.0576],\n",
      "        [ 0.1708, -0.0454, -0.1510,  0.0768,  0.2447,  0.0084,  0.0574,  0.3502,\n",
      "          0.2398,  0.1825, -0.1435,  0.2742, -0.3526, -0.1488,  0.0089,  0.0093],\n",
      "        [-0.0559, -0.0638,  0.3046,  0.1827,  0.1695, -0.2933, -0.2888,  0.1723,\n",
      "          0.2539, -0.0585, -0.0680,  0.0940,  0.1993,  0.1260, -0.2973, -0.2448],\n",
      "        [-0.2156,  0.0308, -0.2504,  0.2014, -0.1899,  0.1746,  0.2946, -0.2698,\n",
      "          0.2007, -0.2258,  0.1053, -0.3314, -0.1319,  0.0056,  0.0038, -0.1428],\n",
      "        [ 0.0559, -0.2970, -0.0035, -0.2646,  0.3506,  0.0886,  0.1032, -0.2012,\n",
      "         -0.2960,  0.0270,  0.1776,  0.3277, -0.2423, -0.0275, -0.1076, -0.0077],\n",
      "        [-0.3189,  0.0907, -0.3154,  0.1249, -0.0932, -0.1950,  0.0143,  0.2305,\n",
      "          0.1915,  0.1152,  0.3369, -0.1412,  0.1376,  0.2973, -0.3134, -0.0245],\n",
      "        [ 0.1012, -0.2597, -0.2295, -0.2149, -0.2420,  0.0781, -0.1078,  0.1118,\n",
      "         -0.3208,  0.2114,  0.0822, -0.3503, -0.0117, -0.2041,  0.2922,  0.1498],\n",
      "        [-0.1838, -0.1134,  0.0580,  0.2759, -0.2583,  0.2506, -0.2030, -0.0248,\n",
      "         -0.0919,  0.0405, -0.2035, -0.0151,  0.2938, -0.2220,  0.2284, -0.0035]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "xqkv: torch.Size([1, 3, 16])\n",
      "tensor([[[-0.0213, -0.2498,  0.0582,  0.1830,  0.0751, -0.1491, -0.2355,\n",
      "           0.2937,  0.1341,  0.0760, -0.0662,  0.0424,  0.0718, -0.0971,\n",
      "          -0.1170, -0.1626],\n",
      "         [-0.0019, -0.2932, -0.2664,  0.2451,  0.2842, -0.0442,  0.3284,\n",
      "           0.2268,  0.1898, -0.0759,  0.0716,  0.3496, -0.3831, -0.1971,\n",
      "          -0.2573, -0.1715],\n",
      "         [-0.1044, -0.2761, -0.2838,  0.1166, -0.0542, -0.0595,  0.0511,\n",
      "           0.1601,  0.0912,  0.0229,  0.1705, -0.2157, -0.1094, -0.0788,\n",
      "          -0.1003, -0.1346]]], grad_fn=<UnsafeViewBackward0>)\n",
      "xq: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.0213, -0.2498,  0.0582,  0.1830,  0.0751, -0.1491, -0.2355,\n",
      "           0.2937],\n",
      "         [-0.0019, -0.2932, -0.2664,  0.2451,  0.2842, -0.0442,  0.3284,\n",
      "           0.2268],\n",
      "         [-0.1044, -0.2761, -0.2838,  0.1166, -0.0542, -0.0595,  0.0511,\n",
      "           0.1601]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xk: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.1341,  0.0760, -0.0662,  0.0424],\n",
      "         [ 0.1898, -0.0759,  0.0716,  0.3496],\n",
      "         [ 0.0912,  0.0229,  0.1705, -0.2157]]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "xv: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.0718, -0.0971, -0.1170, -0.1626],\n",
      "         [-0.3831, -0.1971, -0.2573, -0.1715],\n",
      "         [-0.1094, -0.0788, -0.1003, -0.1346]]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "xq reshaped: torch.Size([1, 3, 2, 4])\n",
      "tensor([[[[-0.0213, -0.2498,  0.0582,  0.1830],\n",
      "          [ 0.0751, -0.1491, -0.2355,  0.2937]],\n",
      "\n",
      "         [[-0.0019, -0.2932, -0.2664,  0.2451],\n",
      "          [ 0.2842, -0.0442,  0.3284,  0.2268]],\n",
      "\n",
      "         [[-0.1044, -0.2761, -0.2838,  0.1166],\n",
      "          [-0.0542, -0.0595,  0.0511,  0.1601]]]], grad_fn=<ViewBackward0>)\n",
      "xk reshaped: torch.Size([1, 3, 1, 4])\n",
      "tensor([[[[ 0.1341,  0.0760, -0.0662,  0.0424]],\n",
      "\n",
      "         [[ 0.1898, -0.0759,  0.0716,  0.3496]],\n",
      "\n",
      "         [[ 0.0912,  0.0229,  0.1705, -0.2157]]]], grad_fn=<ViewBackward0>)\n",
      "xv reshaped: torch.Size([1, 3, 1, 4])\n",
      "tensor([[[[ 0.0718, -0.0971, -0.1170, -0.1626]],\n",
      "\n",
      "         [[-0.3831, -0.1971, -0.2573, -0.1715]],\n",
      "\n",
      "         [[-0.1094, -0.0788, -0.1003, -0.1346]]]], grad_fn=<ViewBackward0>)\n",
      "rotated xq: torch.Size([1, 3, 2, 4])\n",
      "tensor([[[[-0.0213, -0.2498,  0.0582,  0.1830],\n",
      "          [ 0.0751, -0.1491, -0.2355,  0.2937]],\n",
      "\n",
      "         [[ 0.2231, -0.3162, -0.1456,  0.2146],\n",
      "          [-0.1228, -0.0667,  0.4166,  0.2212]],\n",
      "\n",
      "         [[ 0.3015, -0.2937,  0.0231,  0.0594],\n",
      "          [-0.0239, -0.0901, -0.0706,  0.1451]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "rotated xk: torch.Size([1, 3, 1, 4])\n",
      "tensor([[[[ 0.1341,  0.0760, -0.0662,  0.0424]],\n",
      "\n",
      "         [[ 0.0423, -0.1105,  0.1984,  0.3403]],\n",
      "\n",
      "         [[-0.1930,  0.0653,  0.0119, -0.2068]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "repeat_interleaved xk: torch.Size([1, 3, 2, 4])\n",
      "tensor([[[[ 0.1341,  0.0760, -0.0662,  0.0424],\n",
      "          [ 0.1341,  0.0760, -0.0662,  0.0424]],\n",
      "\n",
      "         [[ 0.0423, -0.1105,  0.1984,  0.3403],\n",
      "          [ 0.0423, -0.1105,  0.1984,  0.3403]],\n",
      "\n",
      "         [[-0.1930,  0.0653,  0.0119, -0.2068],\n",
      "          [-0.1930,  0.0653,  0.0119, -0.2068]]]], grad_fn=<ViewBackward0>)\n",
      "repeat_interleaved xv: torch.Size([1, 3, 2, 4])\n",
      "tensor([[[[ 0.0718, -0.0971, -0.1170, -0.1626],\n",
      "          [ 0.0718, -0.0971, -0.1170, -0.1626]],\n",
      "\n",
      "         [[-0.3831, -0.1971, -0.2573, -0.1715],\n",
      "          [-0.3831, -0.1971, -0.2573, -0.1715]],\n",
      "\n",
      "         [[-0.1094, -0.0788, -0.1003, -0.1346],\n",
      "          [-0.1094, -0.0788, -0.1003, -0.1346]]]], grad_fn=<ViewBackward0>)\n",
      "transposed xq: torch.Size([1, 2, 3, 4])\n",
      "tensor([[[[-0.0213, -0.2498,  0.0582,  0.1830],\n",
      "          [ 0.2231, -0.3162, -0.1456,  0.2146],\n",
      "          [ 0.3015, -0.2937,  0.0231,  0.0594]],\n",
      "\n",
      "         [[ 0.0751, -0.1491, -0.2355,  0.2937],\n",
      "          [-0.1228, -0.0667,  0.4166,  0.2212],\n",
      "          [-0.0239, -0.0901, -0.0706,  0.1451]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "transposed xk: torch.Size([1, 2, 3, 4])\n",
      "tensor([[[[ 0.1341,  0.0760, -0.0662,  0.0424],\n",
      "          [ 0.0423, -0.1105,  0.1984,  0.3403],\n",
      "          [-0.1930,  0.0653,  0.0119, -0.2068]],\n",
      "\n",
      "         [[ 0.1341,  0.0760, -0.0662,  0.0424],\n",
      "          [ 0.0423, -0.1105,  0.1984,  0.3403],\n",
      "          [-0.1930,  0.0653,  0.0119, -0.2068]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "transposed xv: torch.Size([1, 2, 3, 4])\n",
      "tensor([[[[ 0.0718, -0.0971, -0.1170, -0.1626],\n",
      "          [-0.3831, -0.1971, -0.2573, -0.1715],\n",
      "          [-0.1094, -0.0788, -0.1003, -0.1346]],\n",
      "\n",
      "         [[ 0.0718, -0.0971, -0.1170, -0.1626],\n",
      "          [-0.3831, -0.1971, -0.2573, -0.1715],\n",
      "          [-0.1094, -0.0788, -0.1003, -0.1346]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[-0.0090,  0.0503, -0.0247],\n",
      "          [ 0.0123,  0.0443, -0.0549],\n",
      "          [ 0.0095,  0.0350, -0.0447]],\n",
      "\n",
      "         [[ 0.0134,  0.0364, -0.0439],\n",
      "          [-0.0199,  0.0801, -0.0107],\n",
      "          [ 0.0004,  0.0222, -0.0161]]]], grad_fn=<MulBackward0>)\n",
      "mask: torch.Size([1, 1, 3, 3])\n",
      "tensor([[[[ 0.0000e+00, -2.3820e+38, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]]]])\n",
      "masked scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[-8.9717e-03, -2.3820e+38, -2.3820e+38],\n",
      "          [ 1.2310e-02,  4.4259e-02, -2.3820e+38],\n",
      "          [ 9.5482e-03,  3.4997e-02, -4.4686e-02]],\n",
      "\n",
      "         [[ 1.3386e-02, -2.3820e+38, -2.3820e+38],\n",
      "          [-1.9869e-02,  8.0057e-02, -2.3820e+38],\n",
      "          [ 3.8400e-04,  2.2157e-02, -1.6060e-02]]]], grad_fn=<AddBackward0>)\n",
      "softmaxed scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [0.4920, 0.5080, 0.0000],\n",
      "          [0.3364, 0.3450, 0.3186]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.4750, 0.5250, 0.0000],\n",
      "          [0.3327, 0.3400, 0.3273]]]], grad_fn=<SoftmaxBackward0>)\n",
      "attention: torch.Size([1, 2, 3, 4])\n",
      "tensor([[[[ 0.0718, -0.0971, -0.1170, -0.1626],\n",
      "          [-0.1593, -0.1479, -0.1882, -0.1671],\n",
      "          [-0.1429, -0.1258, -0.1600, -0.1568]],\n",
      "\n",
      "         [[ 0.0718, -0.0971, -0.1170, -0.1626],\n",
      "          [-0.1670, -0.1496, -0.1906, -0.1673],\n",
      "          [-0.1422, -0.1251, -0.1592, -0.1565]]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "reshaped attention: torch.Size([1, 3, 8])\n",
      "tensor([[[ 0.0718, -0.0971, -0.1170, -0.1626,  0.0718, -0.0971, -0.1170,\n",
      "          -0.1626],\n",
      "         [-0.1593, -0.1479, -0.1882, -0.1671, -0.1670, -0.1496, -0.1906,\n",
      "          -0.1673],\n",
      "         [-0.1429, -0.1258, -0.1600, -0.1568, -0.1422, -0.1251, -0.1592,\n",
      "          -0.1565]]], grad_fn=<ViewBackward0>)\n",
      "self.Wo: torch.Size([8, 8])\n",
      "Parameter containing:\n",
      "tensor([[-0.1600,  0.0353,  0.2748, -0.0703,  0.2290, -0.0809, -0.2417,  0.3120],\n",
      "        [-0.1720,  0.0868, -0.0900, -0.2775, -0.2473, -0.0708,  0.1809, -0.2659],\n",
      "        [-0.3520, -0.1391,  0.0245, -0.1816,  0.1016,  0.2165,  0.1334, -0.2820],\n",
      "        [-0.3450,  0.0448,  0.2293,  0.1915,  0.2190,  0.3236, -0.2818, -0.2200],\n",
      "        [ 0.0172,  0.0775,  0.1654, -0.0925,  0.2246, -0.0209,  0.2707, -0.2102],\n",
      "        [-0.2484,  0.1949, -0.0923, -0.0441,  0.2083, -0.3014, -0.1869,  0.0176],\n",
      "        [ 0.0363,  0.2836, -0.1605, -0.1427, -0.0581, -0.3494, -0.1014,  0.3427],\n",
      "        [-0.2158,  0.0691, -0.0254,  0.0790, -0.0917,  0.0146,  0.1019,  0.3397]],\n",
      "       requires_grad=True)\n",
      "spliced Wo: torch.Size([8, 8])\n",
      "tensor([[-0.1600,  0.0353,  0.2748, -0.0703,  0.2290, -0.0809, -0.2417,  0.3120],\n",
      "        [-0.1720,  0.0868, -0.0900, -0.2775, -0.2473, -0.0708,  0.1809, -0.2659],\n",
      "        [-0.3520, -0.1391,  0.0245, -0.1816,  0.1016,  0.2165,  0.1334, -0.2820],\n",
      "        [-0.3450,  0.0448,  0.2293,  0.1915,  0.2190,  0.3236, -0.2818, -0.2200],\n",
      "        [ 0.0172,  0.0775,  0.1654, -0.0925,  0.2246, -0.0209,  0.2707, -0.2102],\n",
      "        [-0.2484,  0.1949, -0.0923, -0.0441,  0.2083, -0.3014, -0.1869,  0.0176],\n",
      "        [ 0.0363,  0.2836, -0.1605, -0.1427, -0.0581, -0.3494, -0.1014,  0.3427],\n",
      "        [-0.2158,  0.0691, -0.0254,  0.0790, -0.0917,  0.0146,  0.1019,  0.3397]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "projected output: torch.Size([1, 3, 8])\n",
      "tensor([[[ 0.1587, -0.0547,  0.0321,  0.0135,  0.0106, -0.0107,  0.0282,\n",
      "           0.0048],\n",
      "         [ 0.2383, -0.1075, -0.0523,  0.0904, -0.0979,  0.0412,  0.0188,\n",
      "          -0.0102],\n",
      "         [ 0.2115, -0.0921, -0.0502,  0.0730, -0.0866,  0.0291,  0.0197,\n",
      "          -0.0115]]], grad_fn=<UnsafeViewBackward0>)\n",
      "----------------- END MultiQueryAttention.forwardTensor() --------------------\n",
      "y: torch.Size([1, 3, 8])\n",
      "tensor([[[ 0.1587, -0.0547,  0.0321,  0.0135,  0.0106, -0.0107,  0.0282,\n",
      "           0.0048],\n",
      "         [ 0.2383, -0.1075, -0.0523,  0.0904, -0.0979,  0.0412,  0.0188,\n",
      "          -0.0102],\n",
      "         [ 0.2115, -0.0921, -0.0502,  0.0730, -0.0866,  0.0291,  0.0197,\n",
      "          -0.0115]]], grad_fn=<UnsafeViewBackward0>)\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the first sub-model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[0.2143, 0.5407, 0.4930, 0.2441],\n",
      "         [0.3531, 0.4151, 0.8769, 0.8747],\n",
      "         [0.2421, 0.8687, 0.9623, 0.4272]]])\n",
      "---------- Attention Input: torch.Tensor ------------\n",
      "----------------- MultiQueryAttention.forwardTensor() --------------------\n",
      "x shape: torch.Size([1, 3, 4])\n",
      "d_skip: 0\n",
      "models_in_this_level: 2\n",
      "h_dim: 2\n",
      "h_skip: 0\n",
      "self.Wqkv: torch.Size([8, 16])\n",
      "Parameter containing:\n",
      "tensor([[ 0.2027, -0.1009,  0.2900,  0.0703,  0.0358, -0.1315, -0.1431, -0.1686,\n",
      "          0.1184,  0.1066, -0.1082, -0.0333,  0.0118, -0.1273,  0.0585,  0.1444],\n",
      "        [ 0.3046, -0.0327, -0.2225, -0.0384,  0.0211,  0.0682, -0.0919, -0.1968,\n",
      "          0.1685,  0.0748,  0.0260, -0.0363,  0.3358,  0.2142, -0.2853,  0.1830],\n",
      "        [-0.1366,  0.1827, -0.0219, -0.2496,  0.1346,  0.0891, -0.0134,  0.1533,\n",
      "          0.3156,  0.2165,  0.0363,  0.2031, -0.0074,  0.1421, -0.1736,  0.1522],\n",
      "        [-0.2991,  0.1157, -0.2578,  0.0839,  0.0157,  0.1496, -0.2028,  0.2829,\n",
      "         -0.1279,  0.1028, -0.3412,  0.3471, -0.0982,  0.0119, -0.2737,  0.2805],\n",
      "        [-0.2673,  0.3088,  0.3113, -0.2155, -0.1215, -0.2693,  0.2135, -0.0022,\n",
      "         -0.2134,  0.0916, -0.0186,  0.1132,  0.2638, -0.1873,  0.1921, -0.3473],\n",
      "        [-0.2526, -0.2527,  0.2065,  0.2769, -0.3009,  0.3140,  0.1665,  0.2639,\n",
      "         -0.1494, -0.0393, -0.2053,  0.2971, -0.0156, -0.2235, -0.0362,  0.0347],\n",
      "        [ 0.1971,  0.2066, -0.2979,  0.2531,  0.0802,  0.0316, -0.0757,  0.0525,\n",
      "         -0.3454, -0.3462, -0.2154,  0.0492,  0.2736,  0.0239, -0.0760, -0.2628],\n",
      "        [ 0.1921,  0.3021, -0.0473, -0.3233, -0.1685,  0.0207,  0.1537, -0.1999,\n",
      "         -0.1355,  0.0968, -0.2893, -0.3472,  0.0571,  0.1568, -0.2335, -0.1011]],\n",
      "       requires_grad=True)\n",
      "Wq: torch.Size([8, 8])\n",
      "tensor([[ 0.2027, -0.1009,  0.2900,  0.0703,  0.0358, -0.1315, -0.1431, -0.1686],\n",
      "        [ 0.3046, -0.0327, -0.2225, -0.0384,  0.0211,  0.0682, -0.0919, -0.1968],\n",
      "        [-0.1366,  0.1827, -0.0219, -0.2496,  0.1346,  0.0891, -0.0134,  0.1533],\n",
      "        [-0.2991,  0.1157, -0.2578,  0.0839,  0.0157,  0.1496, -0.2028,  0.2829],\n",
      "        [-0.2673,  0.3088,  0.3113, -0.2155, -0.1215, -0.2693,  0.2135, -0.0022],\n",
      "        [-0.2526, -0.2527,  0.2065,  0.2769, -0.3009,  0.3140,  0.1665,  0.2639],\n",
      "        [ 0.1971,  0.2066, -0.2979,  0.2531,  0.0802,  0.0316, -0.0757,  0.0525],\n",
      "        [ 0.1921,  0.3021, -0.0473, -0.3233, -0.1685,  0.0207,  0.1537, -0.1999]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wk: torch.Size([8, 4])\n",
      "tensor([[ 0.1184,  0.1066, -0.1082, -0.0333],\n",
      "        [ 0.1685,  0.0748,  0.0260, -0.0363],\n",
      "        [ 0.3156,  0.2165,  0.0363,  0.2031],\n",
      "        [-0.1279,  0.1028, -0.3412,  0.3471],\n",
      "        [-0.2134,  0.0916, -0.0186,  0.1132],\n",
      "        [-0.1494, -0.0393, -0.2053,  0.2971],\n",
      "        [-0.3454, -0.3462, -0.2154,  0.0492],\n",
      "        [-0.1355,  0.0968, -0.2893, -0.3472]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wv: torch.Size([8, 4])\n",
      "tensor([[ 0.0118, -0.1273,  0.0585,  0.1444],\n",
      "        [ 0.3358,  0.2142, -0.2853,  0.1830],\n",
      "        [-0.0074,  0.1421, -0.1736,  0.1522],\n",
      "        [-0.0982,  0.0119, -0.2737,  0.2805],\n",
      "        [ 0.2638, -0.1873,  0.1921, -0.3473],\n",
      "        [-0.0156, -0.2235, -0.0362,  0.0347],\n",
      "        [ 0.2736,  0.0239, -0.0760, -0.2628],\n",
      "        [ 0.0571,  0.1568, -0.2335, -0.1011]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wq spliced: torch.Size([4, 4])\n",
      "tensor([[ 0.2027, -0.1009,  0.0358, -0.1315],\n",
      "        [ 0.3046, -0.0327,  0.0211,  0.0682],\n",
      "        [-0.1366,  0.1827,  0.1346,  0.0891],\n",
      "        [-0.2991,  0.1157,  0.0157,  0.1496]], grad_fn=<CatBackward0>)\n",
      "Wk spliced: torch.Size([4, 2])\n",
      "tensor([[ 0.1184,  0.1066],\n",
      "        [ 0.1685,  0.0748],\n",
      "        [ 0.3156,  0.2165],\n",
      "        [-0.1279,  0.1028]], grad_fn=<CatBackward0>)\n",
      "Wv spliced: torch.Size([4, 2])\n",
      "tensor([[ 0.0118, -0.1273],\n",
      "        [ 0.3358,  0.2142],\n",
      "        [-0.0074,  0.1421],\n",
      "        [-0.0982,  0.0119]], grad_fn=<CatBackward0>)\n",
      "Wqkv_spliced: torch.Size([4, 8])\n",
      "tensor([[ 0.2027, -0.1009,  0.0358, -0.1315,  0.1184,  0.1066,  0.0118, -0.1273],\n",
      "        [ 0.3046, -0.0327,  0.0211,  0.0682,  0.1685,  0.0748,  0.3358,  0.2142],\n",
      "        [-0.1366,  0.1827,  0.1346,  0.0891,  0.3156,  0.2165, -0.0074,  0.1421],\n",
      "        [-0.2991,  0.1157,  0.0157,  0.1496, -0.1279,  0.1028, -0.0982,  0.0119]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "xqkv: torch.Size([1, 3, 8])\n",
      "tensor([[[ 0.0678,  0.0790,  0.0893,  0.0892,  0.2409,  0.1951,  0.1565,\n",
      "           0.1615],\n",
      "         [-0.1834,  0.2122,  0.1531,  0.1909,  0.2767,  0.3485,  0.0512,\n",
      "           0.1790],\n",
      "         [ 0.0545,  0.1724,  0.1632,  0.1771,  0.4242,  0.3431,  0.2455,\n",
      "           0.2971]]], grad_fn=<UnsafeViewBackward0>)\n",
      "xq: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.0678,  0.0790,  0.0893,  0.0892],\n",
      "         [-0.1834,  0.2122,  0.1531,  0.1909],\n",
      "         [ 0.0545,  0.1724,  0.1632,  0.1771]]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "xk: torch.Size([1, 3, 2])\n",
      "tensor([[[0.2409, 0.1951],\n",
      "         [0.2767, 0.3485],\n",
      "         [0.4242, 0.3431]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xv: torch.Size([1, 3, 2])\n",
      "tensor([[[0.1565, 0.1615],\n",
      "         [0.0512, 0.1790],\n",
      "         [0.2455, 0.2971]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xq reshaped: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[ 0.0678,  0.0790],\n",
      "          [ 0.0893,  0.0892]],\n",
      "\n",
      "         [[-0.1834,  0.2122],\n",
      "          [ 0.1531,  0.1909]],\n",
      "\n",
      "         [[ 0.0545,  0.1724],\n",
      "          [ 0.1632,  0.1771]]]], grad_fn=<ViewBackward0>)\n",
      "xk reshaped: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[0.2409, 0.1951]],\n",
      "\n",
      "         [[0.2767, 0.3485]],\n",
      "\n",
      "         [[0.4242, 0.3431]]]], grad_fn=<ViewBackward0>)\n",
      "xv reshaped: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[0.1565, 0.1615]],\n",
      "\n",
      "         [[0.0512, 0.1790]],\n",
      "\n",
      "         [[0.2455, 0.2971]]]], grad_fn=<ViewBackward0>)\n",
      "rotated xq: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[ 0.0678,  0.0790],\n",
      "          [ 0.0893,  0.0892]],\n",
      "\n",
      "         [[-0.2777, -0.0396],\n",
      "          [-0.0779,  0.2320]],\n",
      "\n",
      "         [[-0.1795, -0.0222],\n",
      "          [-0.2290,  0.0747]]]], grad_fn=<TransposeBackward0>)\n",
      "rotated xk: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[ 0.2409,  0.1951]],\n",
      "\n",
      "         [[-0.1437,  0.4211]],\n",
      "\n",
      "         [[-0.4884,  0.2429]]]], grad_fn=<TransposeBackward0>)\n",
      "repeat_interleaved xk: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[ 0.2409,  0.1951],\n",
      "          [ 0.2409,  0.1951]],\n",
      "\n",
      "         [[-0.1437,  0.4211],\n",
      "          [-0.1437,  0.4211]],\n",
      "\n",
      "         [[-0.4884,  0.2429],\n",
      "          [-0.4884,  0.2429]]]], grad_fn=<ViewBackward0>)\n",
      "repeat_interleaved xv: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[0.1565, 0.1615],\n",
      "          [0.1565, 0.1615]],\n",
      "\n",
      "         [[0.0512, 0.1790],\n",
      "          [0.0512, 0.1790]],\n",
      "\n",
      "         [[0.2455, 0.2971],\n",
      "          [0.2455, 0.2971]]]], grad_fn=<ViewBackward0>)\n",
      "transposed xq: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[ 0.0678,  0.0790],\n",
      "          [-0.2777, -0.0396],\n",
      "          [-0.1795, -0.0222]],\n",
      "\n",
      "         [[ 0.0893,  0.0892],\n",
      "          [-0.0779,  0.2320],\n",
      "          [-0.2290,  0.0747]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xk: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[ 0.2409,  0.1951],\n",
      "          [-0.1437,  0.4211],\n",
      "          [-0.4884,  0.2429]],\n",
      "\n",
      "         [[ 0.2409,  0.1951],\n",
      "          [-0.1437,  0.4211],\n",
      "          [-0.4884,  0.2429]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xv: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[0.1565, 0.1615],\n",
      "          [0.0512, 0.1790],\n",
      "          [0.2455, 0.2971]],\n",
      "\n",
      "         [[0.1565, 0.1615],\n",
      "          [0.0512, 0.1790],\n",
      "          [0.2455, 0.2971]]]], grad_fn=<TransposeBackward0>)\n",
      "scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[ 0.0224,  0.0166, -0.0098],\n",
      "          [-0.0528,  0.0164,  0.0891],\n",
      "          [-0.0336,  0.0116,  0.0582]],\n",
      "\n",
      "         [[ 0.0275,  0.0175, -0.0155],\n",
      "          [ 0.0187,  0.0770,  0.0668],\n",
      "          [-0.0287,  0.0455,  0.0919]]]], grad_fn=<MulBackward0>)\n",
      "mask: torch.Size([1, 1, 3, 3])\n",
      "tensor([[[[ 0.0000e+00, -2.3820e+38, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]]]])\n",
      "masked scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[ 2.2447e-02, -2.3820e+38, -2.3820e+38],\n",
      "          [-5.2765e-02,  1.6417e-02, -2.3820e+38],\n",
      "          [-3.3635e-02,  1.1613e-02,  5.8160e-02]],\n",
      "\n",
      "         [[ 2.7503e-02, -2.3820e+38, -2.3820e+38],\n",
      "          [ 1.8740e-02,  7.6997e-02, -2.3820e+38],\n",
      "          [-2.8690e-02,  4.5512e-02,  9.1910e-02]]]], grad_fn=<AddBackward0>)\n",
      "softmaxed scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [0.4827, 0.5173, 0.0000],\n",
      "          [0.3182, 0.3330, 0.3488]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.4854, 0.5146, 0.0000],\n",
      "          [0.3120, 0.3360, 0.3520]]]], grad_fn=<SoftmaxBackward0>)\n",
      "attention: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[0.1565, 0.1615],\n",
      "          [0.1020, 0.1705],\n",
      "          [0.1525, 0.2146]],\n",
      "\n",
      "         [[0.1565, 0.1615],\n",
      "          [0.1023, 0.1705],\n",
      "          [0.1524, 0.2151]]]], grad_fn=<UnsafeViewBackward0>)\n",
      "reshaped attention: torch.Size([1, 3, 4])\n",
      "tensor([[[0.1565, 0.1615, 0.1565, 0.1615],\n",
      "         [0.1020, 0.1705, 0.1023, 0.1705],\n",
      "         [0.1525, 0.2146, 0.1524, 0.2151]]], grad_fn=<ViewBackward0>)\n",
      "self.Wo: torch.Size([8, 8])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0784, -0.3268,  0.2983,  0.1253,  0.3400, -0.1834,  0.0707, -0.2136],\n",
      "        [ 0.2190, -0.1915,  0.1892,  0.2796,  0.1571,  0.0874, -0.0685, -0.0626],\n",
      "        [ 0.0842, -0.2819,  0.1108, -0.1603, -0.2882,  0.3298, -0.0923,  0.0629],\n",
      "        [ 0.3141, -0.2450,  0.2078,  0.2210,  0.0418,  0.0443, -0.2068,  0.0147],\n",
      "        [-0.0125, -0.0572, -0.0503,  0.2338, -0.3302, -0.2435, -0.0959,  0.2052],\n",
      "        [-0.1563, -0.0682,  0.1997,  0.1145, -0.0208,  0.0643,  0.0412,  0.2484],\n",
      "        [ 0.1454,  0.0578, -0.2178, -0.1463,  0.0617,  0.2122, -0.3488,  0.3351],\n",
      "        [ 0.1552,  0.2546, -0.3376, -0.2187, -0.1093,  0.0802, -0.2185, -0.1143]],\n",
      "       requires_grad=True)\n",
      "spliced Wo: torch.Size([4, 4])\n",
      "tensor([[ 0.0784, -0.3268,  0.2983,  0.1253],\n",
      "        [ 0.2190, -0.1915,  0.1892,  0.2796],\n",
      "        [-0.0125, -0.0572, -0.0503,  0.2338],\n",
      "        [-0.1563, -0.0682,  0.1997,  0.1145]], grad_fn=<CatBackward0>)\n",
      "projected output: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.0204, -0.1020,  0.1016,  0.1198],\n",
      "         [ 0.0174, -0.0835,  0.0916,  0.1039],\n",
      "         [ 0.0234, -0.1143,  0.1214,  0.1394]]], grad_fn=<UnsafeViewBackward0>)\n",
      "----------------- END MultiQueryAttention.forwardTensor() --------------------\n",
      "y: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.0204, -0.1020,  0.1016,  0.1198],\n",
      "         [ 0.0174, -0.0835,  0.0916,  0.1039],\n",
      "         [ 0.0234, -0.1143,  0.1214,  0.1394]]], grad_fn=<UnsafeViewBackward0>)\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the second sub-model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[0.2728, 0.2854, 0.1188, 0.0773],\n",
      "         [0.9998, 0.0244, 0.0642, 0.4580],\n",
      "         [0.5026, 0.7913, 0.2469, 0.8469]]])\n",
      "---------- Attention Input: torch.Tensor ------------\n",
      "----------------- MultiQueryAttention.forwardTensor() --------------------\n",
      "x shape: torch.Size([1, 3, 4])\n",
      "d_skip: 4\n",
      "models_in_this_level: 2\n",
      "h_dim: 2\n",
      "h_skip: 2\n",
      "self.Wqkv: torch.Size([8, 16])\n",
      "Parameter containing:\n",
      "tensor([[-0.2668,  0.3029, -0.3189,  0.0326, -0.3257, -0.1201, -0.1609, -0.0973,\n",
      "         -0.3246, -0.0407,  0.3407,  0.0369, -0.3136,  0.1406, -0.2188,  0.1409],\n",
      "        [ 0.2330, -0.1614,  0.2137,  0.2202, -0.2309, -0.0629, -0.2081,  0.0892,\n",
      "         -0.0477,  0.2670, -0.0743,  0.2346, -0.0967, -0.3001, -0.0996, -0.2306],\n",
      "        [-0.0855, -0.2462,  0.1882, -0.3493,  0.3378,  0.1519,  0.2100,  0.1016,\n",
      "          0.2328, -0.2207, -0.3283,  0.2388, -0.2215,  0.1384,  0.2506, -0.2697],\n",
      "        [ 0.1240,  0.2133,  0.1712, -0.0177, -0.0726,  0.3268, -0.1711, -0.1903,\n",
      "          0.1730, -0.1664, -0.1661,  0.2497,  0.1787,  0.1325, -0.2684,  0.3205],\n",
      "        [ 0.0979,  0.0521,  0.0783, -0.0102, -0.0722, -0.1619,  0.3216, -0.0578,\n",
      "          0.1147,  0.2648,  0.2453, -0.2675, -0.0725,  0.3201, -0.1306, -0.1918],\n",
      "        [ 0.1309,  0.2804,  0.2947,  0.2929, -0.3527, -0.2093, -0.2560, -0.1949,\n",
      "         -0.1467, -0.2091,  0.0882, -0.1230,  0.0288, -0.0319,  0.1361, -0.1435],\n",
      "        [ 0.1603,  0.1621,  0.3091,  0.2392, -0.1760,  0.0201, -0.0723,  0.2684,\n",
      "          0.1198,  0.2792,  0.3465,  0.3132, -0.1131, -0.0255,  0.3202,  0.3281],\n",
      "        [ 0.3283, -0.0572, -0.3336, -0.2981,  0.0922, -0.2888, -0.2266,  0.0980,\n",
      "         -0.0278,  0.1571,  0.2413, -0.0481,  0.2719, -0.1029,  0.2777,  0.2993]],\n",
      "       requires_grad=True)\n",
      "Wq: torch.Size([8, 8])\n",
      "tensor([[-0.2668,  0.3029, -0.3189,  0.0326, -0.3257, -0.1201, -0.1609, -0.0973],\n",
      "        [ 0.2330, -0.1614,  0.2137,  0.2202, -0.2309, -0.0629, -0.2081,  0.0892],\n",
      "        [-0.0855, -0.2462,  0.1882, -0.3493,  0.3378,  0.1519,  0.2100,  0.1016],\n",
      "        [ 0.1240,  0.2133,  0.1712, -0.0177, -0.0726,  0.3268, -0.1711, -0.1903],\n",
      "        [ 0.0979,  0.0521,  0.0783, -0.0102, -0.0722, -0.1619,  0.3216, -0.0578],\n",
      "        [ 0.1309,  0.2804,  0.2947,  0.2929, -0.3527, -0.2093, -0.2560, -0.1949],\n",
      "        [ 0.1603,  0.1621,  0.3091,  0.2392, -0.1760,  0.0201, -0.0723,  0.2684],\n",
      "        [ 0.3283, -0.0572, -0.3336, -0.2981,  0.0922, -0.2888, -0.2266,  0.0980]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wk: torch.Size([8, 4])\n",
      "tensor([[-0.3246, -0.0407,  0.3407,  0.0369],\n",
      "        [-0.0477,  0.2670, -0.0743,  0.2346],\n",
      "        [ 0.2328, -0.2207, -0.3283,  0.2388],\n",
      "        [ 0.1730, -0.1664, -0.1661,  0.2497],\n",
      "        [ 0.1147,  0.2648,  0.2453, -0.2675],\n",
      "        [-0.1467, -0.2091,  0.0882, -0.1230],\n",
      "        [ 0.1198,  0.2792,  0.3465,  0.3132],\n",
      "        [-0.0278,  0.1571,  0.2413, -0.0481]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wv: torch.Size([8, 4])\n",
      "tensor([[-0.3136,  0.1406, -0.2188,  0.1409],\n",
      "        [-0.0967, -0.3001, -0.0996, -0.2306],\n",
      "        [-0.2215,  0.1384,  0.2506, -0.2697],\n",
      "        [ 0.1787,  0.1325, -0.2684,  0.3205],\n",
      "        [-0.0725,  0.3201, -0.1306, -0.1918],\n",
      "        [ 0.0288, -0.0319,  0.1361, -0.1435],\n",
      "        [-0.1131, -0.0255,  0.3202,  0.3281],\n",
      "        [ 0.2719, -0.1029,  0.2777,  0.2993]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wq spliced: torch.Size([4, 4])\n",
      "tensor([[ 0.0783, -0.0102,  0.3216, -0.0578],\n",
      "        [ 0.2947,  0.2929, -0.2560, -0.1949],\n",
      "        [ 0.3091,  0.2392, -0.0723,  0.2684],\n",
      "        [-0.3336, -0.2981, -0.2266,  0.0980]], grad_fn=<CatBackward0>)\n",
      "Wk spliced: torch.Size([4, 2])\n",
      "tensor([[ 0.2453, -0.2675],\n",
      "        [ 0.0882, -0.1230],\n",
      "        [ 0.3465,  0.3132],\n",
      "        [ 0.2413, -0.0481]], grad_fn=<CatBackward0>)\n",
      "Wv spliced: torch.Size([4, 2])\n",
      "tensor([[-0.1306, -0.1918],\n",
      "        [ 0.1361, -0.1435],\n",
      "        [ 0.3202,  0.3281],\n",
      "        [ 0.2777,  0.2993]], grad_fn=<CatBackward0>)\n",
      "Wqkv_spliced: torch.Size([4, 8])\n",
      "tensor([[ 0.0783, -0.0102,  0.3216, -0.0578,  0.2453, -0.2675, -0.1306, -0.1918],\n",
      "        [ 0.2947,  0.2929, -0.2560, -0.1949,  0.0882, -0.1230,  0.1361, -0.1435],\n",
      "        [ 0.3091,  0.2392, -0.0723,  0.2684,  0.3465,  0.3132,  0.3202,  0.3281],\n",
      "        [-0.3336, -0.2981, -0.2266,  0.0980,  0.2413, -0.0481,  0.2777,  0.2993]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "xqkv: torch.Size([1, 3, 8])\n",
      "tensor([[[ 1.1640e-01,  8.6191e-02, -1.1429e-02, -3.1928e-02,  1.5191e-01,\n",
      "          -7.4581e-02,  6.2717e-02, -3.1139e-02],\n",
      "         [-4.7446e-02, -1.2424e-01,  2.0686e-01, -4.4557e-04,  3.8017e-01,\n",
      "          -2.7236e-01,  2.0462e-02, -3.7071e-02],\n",
      "         [ 6.6315e-02,  3.3262e-02, -2.5069e-01, -3.3998e-02,  4.8304e-01,\n",
      "          -1.9517e-01,  3.5634e-01,  1.2464e-01]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "xq: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.1164,  0.0862, -0.0114, -0.0319],\n",
      "         [-0.0474, -0.1242,  0.2069, -0.0004],\n",
      "         [ 0.0663,  0.0333, -0.2507, -0.0340]]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "xk: torch.Size([1, 3, 2])\n",
      "tensor([[[ 0.1519, -0.0746],\n",
      "         [ 0.3802, -0.2724],\n",
      "         [ 0.4830, -0.1952]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xv: torch.Size([1, 3, 2])\n",
      "tensor([[[ 0.0627, -0.0311],\n",
      "         [ 0.0205, -0.0371],\n",
      "         [ 0.3563,  0.1246]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xq reshaped: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[ 0.1164,  0.0862],\n",
      "          [-0.0114, -0.0319]],\n",
      "\n",
      "         [[-0.0474, -0.1242],\n",
      "          [ 0.2069, -0.0004]],\n",
      "\n",
      "         [[ 0.0663,  0.0333],\n",
      "          [-0.2507, -0.0340]]]], grad_fn=<ViewBackward0>)\n",
      "xk reshaped: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[ 0.1519, -0.0746]],\n",
      "\n",
      "         [[ 0.3802, -0.2724]],\n",
      "\n",
      "         [[ 0.4830, -0.1952]]]], grad_fn=<ViewBackward0>)\n",
      "xv reshaped: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[ 0.0627, -0.0311]],\n",
      "\n",
      "         [[ 0.0205, -0.0371]],\n",
      "\n",
      "         [[ 0.3563,  0.1246]]]], grad_fn=<ViewBackward0>)\n",
      "rotated xq: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[ 0.1164,  0.0862],\n",
      "          [-0.0114, -0.0319]],\n",
      "\n",
      "         [[ 0.0789, -0.1071],\n",
      "          [ 0.1121,  0.1738]],\n",
      "\n",
      "         [[-0.0578,  0.0465],\n",
      "          [ 0.1352, -0.2138]]]], grad_fn=<TransposeBackward0>)\n",
      "rotated xk: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[ 0.1519, -0.0746]],\n",
      "\n",
      "         [[ 0.4346,  0.1727]],\n",
      "\n",
      "         [[-0.0236,  0.5204]]]], grad_fn=<TransposeBackward0>)\n",
      "repeat_interleaved xk: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[ 0.1519, -0.0746],\n",
      "          [ 0.1519, -0.0746]],\n",
      "\n",
      "         [[ 0.4346,  0.1727],\n",
      "          [ 0.4346,  0.1727]],\n",
      "\n",
      "         [[-0.0236,  0.5204],\n",
      "          [-0.0236,  0.5204]]]], grad_fn=<ViewBackward0>)\n",
      "repeat_interleaved xv: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[ 0.0627, -0.0311],\n",
      "          [ 0.0627, -0.0311]],\n",
      "\n",
      "         [[ 0.0205, -0.0371],\n",
      "          [ 0.0205, -0.0371]],\n",
      "\n",
      "         [[ 0.3563,  0.1246],\n",
      "          [ 0.3563,  0.1246]]]], grad_fn=<ViewBackward0>)\n",
      "transposed xq: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[ 0.1164,  0.0862],\n",
      "          [ 0.0789, -0.1071],\n",
      "          [-0.0578,  0.0465]],\n",
      "\n",
      "         [[-0.0114, -0.0319],\n",
      "          [ 0.1121,  0.1738],\n",
      "          [ 0.1352, -0.2138]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xk: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[ 0.1519, -0.0746],\n",
      "          [ 0.4346,  0.1727],\n",
      "          [-0.0236,  0.5204]],\n",
      "\n",
      "         [[ 0.1519, -0.0746],\n",
      "          [ 0.4346,  0.1727],\n",
      "          [-0.0236,  0.5204]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xv: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[ 0.0627, -0.0311],\n",
      "          [ 0.0205, -0.0371],\n",
      "          [ 0.3563,  0.1246]],\n",
      "\n",
      "         [[ 0.0627, -0.0311],\n",
      "          [ 0.0205, -0.0371],\n",
      "          [ 0.3563,  0.1246]]]], grad_fn=<TransposeBackward0>)\n",
      "scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[ 0.0080,  0.0463,  0.0298],\n",
      "          [ 0.0141,  0.0112, -0.0407],\n",
      "          [-0.0087, -0.0121,  0.0181]],\n",
      "\n",
      "         [[ 0.0005, -0.0074, -0.0116],\n",
      "          [ 0.0029,  0.0557,  0.0621],\n",
      "          [ 0.0258,  0.0154, -0.0809]]]], grad_fn=<MulBackward0>)\n",
      "mask: torch.Size([1, 1, 3, 3])\n",
      "tensor([[[[ 0.0000e+00, -2.3820e+38, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]]]])\n",
      "masked scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[ 7.9585e-03, -2.3820e+38, -2.3820e+38],\n",
      "          [ 1.4121e-02,  1.1172e-02, -2.3820e+38],\n",
      "          [-8.6633e-03, -1.2100e-02,  1.8060e-02]],\n",
      "\n",
      "         [[ 4.5613e-04, -2.3820e+38, -2.3820e+38],\n",
      "          [ 2.8789e-03,  5.5694e-02, -2.3820e+38],\n",
      "          [ 2.5803e-02,  1.5443e-02, -8.0936e-02]]]], grad_fn=<AddBackward0>)\n",
      "softmaxed scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [0.5007, 0.4993, 0.0000],\n",
      "          [0.3307, 0.3296, 0.3397]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.4868, 0.5132, 0.0000],\n",
      "          [0.3462, 0.3426, 0.3112]]]], grad_fn=<SoftmaxBackward0>)\n",
      "attention: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[ 0.0627, -0.0311],\n",
      "          [ 0.0416, -0.0341],\n",
      "          [ 0.1485,  0.0198]],\n",
      "\n",
      "         [[ 0.0627, -0.0311],\n",
      "          [ 0.0410, -0.0342],\n",
      "          [ 0.1396,  0.0153]]]], grad_fn=<UnsafeViewBackward0>)\n",
      "reshaped attention: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.0627, -0.0311,  0.0627, -0.0311],\n",
      "         [ 0.0416, -0.0341,  0.0410, -0.0342],\n",
      "         [ 0.1485,  0.0198,  0.1396,  0.0153]]], grad_fn=<ViewBackward0>)\n",
      "self.Wo: torch.Size([8, 8])\n",
      "Parameter containing:\n",
      "tensor([[ 0.2843,  0.1595, -0.0114,  0.3344, -0.1141, -0.0004,  0.0337,  0.3354],\n",
      "        [ 0.2165, -0.1517,  0.1123, -0.0115, -0.1128,  0.3314,  0.0313, -0.1033],\n",
      "        [-0.1586,  0.1660,  0.0208, -0.2023, -0.1290,  0.1355,  0.0065, -0.0824],\n",
      "        [ 0.1879,  0.1110,  0.3007, -0.0078, -0.1387,  0.0518,  0.0646,  0.0277],\n",
      "        [-0.2609,  0.0823,  0.0021, -0.0343,  0.3259, -0.0327,  0.0500, -0.0982],\n",
      "        [ 0.0889,  0.1543,  0.3200, -0.2993, -0.0446,  0.1357,  0.2407, -0.0571],\n",
      "        [ 0.3374,  0.0303,  0.1435, -0.2687,  0.3197,  0.2232, -0.3470,  0.1009],\n",
      "        [-0.1074,  0.3260,  0.2242,  0.2229, -0.2605, -0.1802, -0.2792, -0.1940]],\n",
      "       requires_grad=True)\n",
      "spliced Wo: torch.Size([4, 4])\n",
      "tensor([[-0.1290,  0.1355,  0.0065, -0.0824],\n",
      "        [-0.1387,  0.0518,  0.0646,  0.0277],\n",
      "        [ 0.3197,  0.2232, -0.3470,  0.1009],\n",
      "        [-0.2605, -0.1802, -0.2792, -0.1940]], grad_fn=<CatBackward0>)\n",
      "projected output: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.0244,  0.0265, -0.0147,  0.0063],\n",
      "         [ 0.0214,  0.0192, -0.0066,  0.0064],\n",
      "         [ 0.0187,  0.0496, -0.0505, -0.0006]]], grad_fn=<UnsafeViewBackward0>)\n",
      "----------------- END MultiQueryAttention.forwardTensor() --------------------\n",
      "y: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.0244,  0.0265, -0.0147,  0.0063],\n",
      "         [ 0.0214,  0.0192, -0.0066,  0.0064],\n",
      "         [ 0.0187,  0.0496, -0.0505, -0.0006]]], grad_fn=<UnsafeViewBackward0>)\n",
      "---------- RESET CONFIG --------\n",
      "model_count:  [1, 2, 4]\n",
      "model_dim_list:  [128, 64, 32]\n",
      "head_dim_list:  [32, 16, 8]\n"
     ]
    }
   ],
   "source": [
    "# Testing our Attention's forwardTensor()\n",
    "verbose = True\n",
    "\n",
    "print(\"--------- Micro Hyperparameters -------\")\n",
    "hold1, hold2, hold3, hold4 = config.hidden_size, config.num_attention_heads, config.head_dim, config.max_position_embeddings\n",
    "config.hidden_size = 8\n",
    "config.num_attention_heads = 2\n",
    "config.head_dim = 4\n",
    "config.max_position_embeddings = 3\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "print(\"head_dim_list: \", config.head_dim_list)\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the big model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,3,8)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "att = MultiQueryAttention(config)\n",
    "y = att(x)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the first sub-model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,3,4)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "att = MultiQueryAttention(config)\n",
    "y = att(x)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the second sub-model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,3,4)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "att = MultiQueryAttention(config)\n",
    "y = att(x, model=1)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "verbose = False\n",
    "print(\"---------- RESET CONFIG --------\")\n",
    "config.hidden_size = hold1\n",
    "config.num_attention_heads = hold2\n",
    "config.head_dim = hold3\n",
    "config.max_position_embeddings = hold4\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "print(\"head_dim_list: \", config.head_dim_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ffcead5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Micro Hyperparameters -------\n",
      "model_count:  [1, 2]\n",
      "model_dim_list:  [8, 4]\n",
      "head_dim_list:  [4, 2]\n",
      "x: ((tensor([[[-0.1019,  0.2428, -0.3732, -1.5328,  0.5193, -1.9946,  1.2929,\n",
      "          -1.2588],\n",
      "         [ 0.1088,  0.6013, -0.2455, -0.2865, -0.7261,  0.4209,  0.2447,\n",
      "          -0.1067],\n",
      "         [-0.3173, -1.3407, -1.8709,  0.7280, -1.6094,  1.5309, -0.3970,\n",
      "           0.3737]]]),), (tensor([[[ 0.4299, -0.3087, -0.6267, -0.0480],\n",
      "         [ 0.6544,  1.2164, -0.5778, -0.0486],\n",
      "         [ 0.0654,  0.6517,  0.1242, -0.9419]]]), tensor([[[-1.2690,  0.7162, -0.4014,  0.5533],\n",
      "         [-0.3125,  0.4083,  0.3263,  1.8385],\n",
      "         [ 0.0162, -1.2446, -0.5478,  1.7539]]])))\n",
      "---------- Attention Input: Tuple ------------\n",
      "------------- MultiQueryAttention.forwardTuple() ------------\n",
      "x: ((tensor([[[-0.1019,  0.2428, -0.3732, -1.5328,  0.5193, -1.9946,  1.2929,\n",
      "          -1.2588],\n",
      "         [ 0.1088,  0.6013, -0.2455, -0.2865, -0.7261,  0.4209,  0.2447,\n",
      "          -0.1067],\n",
      "         [-0.3173, -1.3407, -1.8709,  0.7280, -1.6094,  1.5309, -0.3970,\n",
      "           0.3737]]]),), (tensor([[[ 0.4299, -0.3087, -0.6267, -0.0480],\n",
      "         [ 0.6544,  1.2164, -0.5778, -0.0486],\n",
      "         [ 0.0654,  0.6517,  0.1242, -0.9419]]]), tensor([[[-1.2690,  0.7162, -0.4014,  0.5533],\n",
      "         [-0.3125,  0.4083,  0.3263,  1.8385],\n",
      "         [ 0.0162, -1.2446, -0.5478,  1.7539]]])))\n",
      "input_len: 3\n",
      "num_levels: 2\n",
      "models_per_level: [1, 2]\n",
      "Level 0 from range(2)\n",
      "Model 0 from range(1)\n",
      "----------------- MultiQueryAttention.forwardTensor() --------------------\n",
      "x shape: torch.Size([1, 3, 8])\n",
      "d_skip: 0\n",
      "models_in_this_level: 1\n",
      "h_dim: 4\n",
      "h_skip: 0\n",
      "self.Wqkv: torch.Size([8, 16])\n",
      "Parameter containing:\n",
      "tensor([[-0.1735,  0.2315,  0.1117,  0.1047, -0.1489,  0.2376,  0.1229,  0.3110,\n",
      "          0.0873, -0.2684, -0.1096, -0.0097,  0.3097,  0.2150,  0.2990,  0.3137],\n",
      "        [ 0.1431,  0.1412, -0.0838,  0.2646,  0.3242, -0.2271, -0.2130, -0.3368,\n",
      "         -0.0050,  0.2183, -0.0016,  0.3343,  0.1920,  0.3316, -0.2514, -0.0077],\n",
      "        [ 0.0443, -0.1020,  0.0601, -0.2298,  0.2458,  0.0583,  0.2098,  0.3069,\n",
      "         -0.2419,  0.3114,  0.3513,  0.0985, -0.1354, -0.0416, -0.0551, -0.3424],\n",
      "        [-0.2932,  0.0489, -0.2404, -0.3425, -0.1447, -0.1952,  0.2017, -0.2510,\n",
      "          0.2124,  0.1749, -0.1075, -0.2291, -0.3099,  0.0309,  0.3235,  0.3422],\n",
      "        [ 0.3169, -0.0661, -0.2328,  0.2331, -0.2754,  0.3143, -0.0073,  0.1731,\n",
      "          0.0169, -0.1440,  0.1663, -0.0258,  0.1103,  0.2496, -0.0560,  0.2469],\n",
      "        [-0.2366,  0.2698, -0.0968, -0.2891, -0.2942, -0.1853, -0.3053,  0.0728,\n",
      "         -0.0625,  0.2472,  0.2964, -0.1388,  0.2694,  0.1347, -0.2692,  0.1232],\n",
      "        [ 0.0215,  0.1852,  0.2982,  0.0872,  0.2097,  0.3356,  0.2573,  0.2475,\n",
      "         -0.0317, -0.1558, -0.1693,  0.1063, -0.2679, -0.0946,  0.0275,  0.1280],\n",
      "        [ 0.0351, -0.1243, -0.1558,  0.3483,  0.2635,  0.0277, -0.0599,  0.2833,\n",
      "         -0.0971,  0.3065,  0.2610, -0.2330,  0.0905, -0.1111, -0.3418, -0.1117]],\n",
      "       requires_grad=True)\n",
      "Wq: torch.Size([8, 8])\n",
      "tensor([[-0.1735,  0.2315,  0.1117,  0.1047, -0.1489,  0.2376,  0.1229,  0.3110],\n",
      "        [ 0.1431,  0.1412, -0.0838,  0.2646,  0.3242, -0.2271, -0.2130, -0.3368],\n",
      "        [ 0.0443, -0.1020,  0.0601, -0.2298,  0.2458,  0.0583,  0.2098,  0.3069],\n",
      "        [-0.2932,  0.0489, -0.2404, -0.3425, -0.1447, -0.1952,  0.2017, -0.2510],\n",
      "        [ 0.3169, -0.0661, -0.2328,  0.2331, -0.2754,  0.3143, -0.0073,  0.1731],\n",
      "        [-0.2366,  0.2698, -0.0968, -0.2891, -0.2942, -0.1853, -0.3053,  0.0728],\n",
      "        [ 0.0215,  0.1852,  0.2982,  0.0872,  0.2097,  0.3356,  0.2573,  0.2475],\n",
      "        [ 0.0351, -0.1243, -0.1558,  0.3483,  0.2635,  0.0277, -0.0599,  0.2833]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wk: torch.Size([8, 4])\n",
      "tensor([[ 0.0873, -0.2684, -0.1096, -0.0097],\n",
      "        [-0.0050,  0.2183, -0.0016,  0.3343],\n",
      "        [-0.2419,  0.3114,  0.3513,  0.0985],\n",
      "        [ 0.2124,  0.1749, -0.1075, -0.2291],\n",
      "        [ 0.0169, -0.1440,  0.1663, -0.0258],\n",
      "        [-0.0625,  0.2472,  0.2964, -0.1388],\n",
      "        [-0.0317, -0.1558, -0.1693,  0.1063],\n",
      "        [-0.0971,  0.3065,  0.2610, -0.2330]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wv: torch.Size([8, 4])\n",
      "tensor([[ 0.3097,  0.2150,  0.2990,  0.3137],\n",
      "        [ 0.1920,  0.3316, -0.2514, -0.0077],\n",
      "        [-0.1354, -0.0416, -0.0551, -0.3424],\n",
      "        [-0.3099,  0.0309,  0.3235,  0.3422],\n",
      "        [ 0.1103,  0.2496, -0.0560,  0.2469],\n",
      "        [ 0.2694,  0.1347, -0.2692,  0.1232],\n",
      "        [-0.2679, -0.0946,  0.0275,  0.1280],\n",
      "        [ 0.0905, -0.1111, -0.3418, -0.1117]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wq spliced: torch.Size([8, 8])\n",
      "tensor([[-0.1735,  0.2315,  0.1117,  0.1047, -0.1489,  0.2376,  0.1229,  0.3110],\n",
      "        [ 0.1431,  0.1412, -0.0838,  0.2646,  0.3242, -0.2271, -0.2130, -0.3368],\n",
      "        [ 0.0443, -0.1020,  0.0601, -0.2298,  0.2458,  0.0583,  0.2098,  0.3069],\n",
      "        [-0.2932,  0.0489, -0.2404, -0.3425, -0.1447, -0.1952,  0.2017, -0.2510],\n",
      "        [ 0.3169, -0.0661, -0.2328,  0.2331, -0.2754,  0.3143, -0.0073,  0.1731],\n",
      "        [-0.2366,  0.2698, -0.0968, -0.2891, -0.2942, -0.1853, -0.3053,  0.0728],\n",
      "        [ 0.0215,  0.1852,  0.2982,  0.0872,  0.2097,  0.3356,  0.2573,  0.2475],\n",
      "        [ 0.0351, -0.1243, -0.1558,  0.3483,  0.2635,  0.0277, -0.0599,  0.2833]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Wk spliced: torch.Size([8, 4])\n",
      "tensor([[ 0.0873, -0.2684, -0.1096, -0.0097],\n",
      "        [-0.0050,  0.2183, -0.0016,  0.3343],\n",
      "        [-0.2419,  0.3114,  0.3513,  0.0985],\n",
      "        [ 0.2124,  0.1749, -0.1075, -0.2291],\n",
      "        [ 0.0169, -0.1440,  0.1663, -0.0258],\n",
      "        [-0.0625,  0.2472,  0.2964, -0.1388],\n",
      "        [-0.0317, -0.1558, -0.1693,  0.1063],\n",
      "        [-0.0971,  0.3065,  0.2610, -0.2330]], grad_fn=<CatBackward0>)\n",
      "Wv spliced: torch.Size([8, 4])\n",
      "tensor([[ 0.3097,  0.2150,  0.2990,  0.3137],\n",
      "        [ 0.1920,  0.3316, -0.2514, -0.0077],\n",
      "        [-0.1354, -0.0416, -0.0551, -0.3424],\n",
      "        [-0.3099,  0.0309,  0.3235,  0.3422],\n",
      "        [ 0.1103,  0.2496, -0.0560,  0.2469],\n",
      "        [ 0.2694,  0.1347, -0.2692,  0.1232],\n",
      "        [-0.2679, -0.0946,  0.0275,  0.1280],\n",
      "        [ 0.0905, -0.1111, -0.3418, -0.1117]], grad_fn=<CatBackward0>)\n",
      "Wqkv_spliced: torch.Size([8, 16])\n",
      "tensor([[-0.1735,  0.2315,  0.1117,  0.1047, -0.1489,  0.2376,  0.1229,  0.3110,\n",
      "          0.0873, -0.2684, -0.1096, -0.0097,  0.3097,  0.2150,  0.2990,  0.3137],\n",
      "        [ 0.1431,  0.1412, -0.0838,  0.2646,  0.3242, -0.2271, -0.2130, -0.3368,\n",
      "         -0.0050,  0.2183, -0.0016,  0.3343,  0.1920,  0.3316, -0.2514, -0.0077],\n",
      "        [ 0.0443, -0.1020,  0.0601, -0.2298,  0.2458,  0.0583,  0.2098,  0.3069,\n",
      "         -0.2419,  0.3114,  0.3513,  0.0985, -0.1354, -0.0416, -0.0551, -0.3424],\n",
      "        [-0.2932,  0.0489, -0.2404, -0.3425, -0.1447, -0.1952,  0.2017, -0.2510,\n",
      "          0.2124,  0.1749, -0.1075, -0.2291, -0.3099,  0.0309,  0.3235,  0.3422],\n",
      "        [ 0.3169, -0.0661, -0.2328,  0.2331, -0.2754,  0.3143, -0.0073,  0.1731,\n",
      "          0.0169, -0.1440,  0.1663, -0.0258,  0.1103,  0.2496, -0.0560,  0.2469],\n",
      "        [-0.2366,  0.2698, -0.0968, -0.2891, -0.2942, -0.1853, -0.3053,  0.0728,\n",
      "         -0.0625,  0.2472,  0.2964, -0.1388,  0.2694,  0.1347, -0.2692,  0.1232],\n",
      "        [ 0.0215,  0.1852,  0.2982,  0.0872,  0.2097,  0.3356,  0.2573,  0.2475,\n",
      "         -0.0317, -0.1558, -0.1693,  0.1063, -0.2679, -0.0946,  0.0275,  0.1280],\n",
      "        [ 0.0351, -0.1243, -0.1558,  0.3483,  0.2635,  0.0277, -0.0599,  0.2833,\n",
      "         -0.0971,  0.3065,  0.2610, -0.2330,  0.0905, -0.1111, -0.3418, -0.1117]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "xqkv: torch.Size([1, 3, 16])\n",
      "tensor([[[ 1.1052, -0.2027,  0.9681,  1.0365,  0.6072,  1.1299,  0.5617,\n",
      "           0.0647, -0.0307, -1.4591, -1.0078,  1.0908, -0.3996, -0.0948,\n",
      "           0.4069, -0.2421],\n",
      "         [-0.1879,  0.3413,  0.2337,  0.0182,  0.2592, -0.2961, -0.2779,\n",
      "          -0.2368, -0.0310,  0.1132, -0.1335,  0.2526,  0.2292,  0.0883,\n",
      "          -0.2272, -0.0686],\n",
      "         [-1.3006,  0.3632, -0.1606, -0.9296, -0.9446, -0.9343, -0.5794,\n",
      "          -0.5635,  0.4395,  0.1237, -0.3477, -1.0967,  0.0470, -0.6118,\n",
      "           0.1202,  0.4994]]], grad_fn=<UnsafeViewBackward0>)\n",
      "xq: torch.Size([1, 3, 8])\n",
      "tensor([[[ 1.1052, -0.2027,  0.9681,  1.0365,  0.6072,  1.1299,  0.5617,\n",
      "           0.0647],\n",
      "         [-0.1879,  0.3413,  0.2337,  0.0182,  0.2592, -0.2961, -0.2779,\n",
      "          -0.2368],\n",
      "         [-1.3006,  0.3632, -0.1606, -0.9296, -0.9446, -0.9343, -0.5794,\n",
      "          -0.5635]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xk: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.0307, -1.4591, -1.0078,  1.0908],\n",
      "         [-0.0310,  0.1132, -0.1335,  0.2526],\n",
      "         [ 0.4395,  0.1237, -0.3477, -1.0967]]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "xv: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.3996, -0.0948,  0.4069, -0.2421],\n",
      "         [ 0.2292,  0.0883, -0.2272, -0.0686],\n",
      "         [ 0.0470, -0.6118,  0.1202,  0.4994]]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "xq reshaped: torch.Size([1, 3, 2, 4])\n",
      "tensor([[[[ 1.1052, -0.2027,  0.9681,  1.0365],\n",
      "          [ 0.6072,  1.1299,  0.5617,  0.0647]],\n",
      "\n",
      "         [[-0.1879,  0.3413,  0.2337,  0.0182],\n",
      "          [ 0.2592, -0.2961, -0.2779, -0.2368]],\n",
      "\n",
      "         [[-1.3006,  0.3632, -0.1606, -0.9296],\n",
      "          [-0.9446, -0.9343, -0.5794, -0.5635]]]], grad_fn=<ViewBackward0>)\n",
      "xk reshaped: torch.Size([1, 3, 1, 4])\n",
      "tensor([[[[-0.0307, -1.4591, -1.0078,  1.0908]],\n",
      "\n",
      "         [[-0.0310,  0.1132, -0.1335,  0.2526]],\n",
      "\n",
      "         [[ 0.4395,  0.1237, -0.3477, -1.0967]]]], grad_fn=<ViewBackward0>)\n",
      "xv reshaped: torch.Size([1, 3, 1, 4])\n",
      "tensor([[[[-0.3996, -0.0948,  0.4069, -0.2421]],\n",
      "\n",
      "         [[ 0.2292,  0.0883, -0.2272, -0.0686]],\n",
      "\n",
      "         [[ 0.0470, -0.6118,  0.1202,  0.4994]]]], grad_fn=<ViewBackward0>)\n",
      "rotated xq: torch.Size([1, 3, 2, 4])\n",
      "tensor([[[[ 1.1052, -0.2027,  0.9681,  1.0365],\n",
      "          [ 0.6072,  1.1299,  0.5617,  0.0647]],\n",
      "\n",
      "         [[-0.2982,  0.3377, -0.0318,  0.0522],\n",
      "          [ 0.3739, -0.2710,  0.0680, -0.2652]],\n",
      "\n",
      "         [[ 0.6873,  0.5406, -1.1158, -0.8390],\n",
      "          [ 0.9200, -0.8037, -0.6178, -0.7379]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "rotated xk: torch.Size([1, 3, 1, 4])\n",
      "tensor([[[[-0.0307, -1.4591, -1.0078,  1.0908]],\n",
      "\n",
      "         [[ 0.0956,  0.0875, -0.0982,  0.2626]],\n",
      "\n",
      "         [[ 0.1333,  0.3391,  0.5443, -1.0502]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "repeat_interleaved xk: torch.Size([1, 3, 2, 4])\n",
      "tensor([[[[-0.0307, -1.4591, -1.0078,  1.0908],\n",
      "          [-0.0307, -1.4591, -1.0078,  1.0908]],\n",
      "\n",
      "         [[ 0.0956,  0.0875, -0.0982,  0.2626],\n",
      "          [ 0.0956,  0.0875, -0.0982,  0.2626]],\n",
      "\n",
      "         [[ 0.1333,  0.3391,  0.5443, -1.0502],\n",
      "          [ 0.1333,  0.3391,  0.5443, -1.0502]]]], grad_fn=<ViewBackward0>)\n",
      "repeat_interleaved xv: torch.Size([1, 3, 2, 4])\n",
      "tensor([[[[-0.3996, -0.0948,  0.4069, -0.2421],\n",
      "          [-0.3996, -0.0948,  0.4069, -0.2421]],\n",
      "\n",
      "         [[ 0.2292,  0.0883, -0.2272, -0.0686],\n",
      "          [ 0.2292,  0.0883, -0.2272, -0.0686]],\n",
      "\n",
      "         [[ 0.0470, -0.6118,  0.1202,  0.4994],\n",
      "          [ 0.0470, -0.6118,  0.1202,  0.4994]]]], grad_fn=<ViewBackward0>)\n",
      "transposed xq: torch.Size([1, 2, 3, 4])\n",
      "tensor([[[[ 1.1052, -0.2027,  0.9681,  1.0365],\n",
      "          [-0.2982,  0.3377, -0.0318,  0.0522],\n",
      "          [ 0.6873,  0.5406, -1.1158, -0.8390]],\n",
      "\n",
      "         [[ 0.6072,  1.1299,  0.5617,  0.0647],\n",
      "          [ 0.3739, -0.2710,  0.0680, -0.2652],\n",
      "          [ 0.9200, -0.8037, -0.6178, -0.7379]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "transposed xk: torch.Size([1, 2, 3, 4])\n",
      "tensor([[[[-0.0307, -1.4591, -1.0078,  1.0908],\n",
      "          [ 0.0956,  0.0875, -0.0982,  0.2626],\n",
      "          [ 0.1333,  0.3391,  0.5443, -1.0502]],\n",
      "\n",
      "         [[-0.0307, -1.4591, -1.0078,  1.0908],\n",
      "          [ 0.0956,  0.0875, -0.0982,  0.2626],\n",
      "          [ 0.1333,  0.3391,  0.5443, -1.0502]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "transposed xv: torch.Size([1, 2, 3, 4])\n",
      "tensor([[[[-0.3996, -0.0948,  0.4069, -0.2421],\n",
      "          [ 0.2292,  0.0883, -0.2272, -0.0686],\n",
      "          [ 0.0470, -0.6118,  0.1202,  0.4994]],\n",
      "\n",
      "         [[-0.3996, -0.0948,  0.4069, -0.2421],\n",
      "          [ 0.2292,  0.0883, -0.2272, -0.0686],\n",
      "          [ 0.0470, -0.6118,  0.1202,  0.4994]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[ 0.2084,  0.1325, -0.2415],\n",
      "          [-0.1973,  0.0089,  0.0013],\n",
      "          [-0.3003,  0.0011,  0.2743]],\n",
      "\n",
      "         [[-1.0814,  0.0594,  0.3510],\n",
      "          [ 0.0131, -0.0321,  0.1367],\n",
      "          [ 0.4811, -0.0577,  0.1443]]]], grad_fn=<MulBackward0>)\n",
      "mask: torch.Size([1, 1, 3, 3])\n",
      "tensor([[[[ 0.0000e+00, -2.3820e+38, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]]]])\n",
      "masked scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[ 2.0840e-01, -2.3820e+38, -2.3820e+38],\n",
      "          [-1.9734e-01,  8.9236e-03, -2.3820e+38],\n",
      "          [-3.0029e-01,  1.1437e-03,  2.7434e-01]],\n",
      "\n",
      "         [[-1.0814e+00, -2.3820e+38, -2.3820e+38],\n",
      "          [ 1.3088e-02, -3.2131e-02, -2.3820e+38],\n",
      "          [ 4.8113e-01, -5.7702e-02,  1.4434e-01]]]], grad_fn=<AddBackward0>)\n",
      "softmaxed scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [0.4486, 0.5514, 0.0000],\n",
      "          [0.2422, 0.3274, 0.4303]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.5113, 0.4887, 0.0000],\n",
      "          [0.4353, 0.2539, 0.3108]]]], grad_fn=<SoftmaxBackward0>)\n",
      "attention: torch.Size([1, 2, 3, 4])\n",
      "tensor([[[[-0.3996, -0.0948,  0.4069, -0.2421],\n",
      "          [-0.0529,  0.0062,  0.0573, -0.1464],\n",
      "          [-0.0015, -0.2573,  0.0759,  0.1338]],\n",
      "\n",
      "         [[-0.3996, -0.0948,  0.4069, -0.2421],\n",
      "          [-0.0923, -0.0053,  0.0970, -0.1573],\n",
      "          [-0.1011, -0.2090,  0.1568,  0.0324]]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "reshaped attention: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.3996, -0.0948,  0.4069, -0.2421, -0.3996, -0.0948,  0.4069,\n",
      "          -0.2421],\n",
      "         [-0.0529,  0.0062,  0.0573, -0.1464, -0.0923, -0.0053,  0.0970,\n",
      "          -0.1573],\n",
      "         [-0.0015, -0.2573,  0.0759,  0.1338, -0.1011, -0.2090,  0.1568,\n",
      "           0.0324]]], grad_fn=<ViewBackward0>)\n",
      "self.Wo: torch.Size([8, 8])\n",
      "Parameter containing:\n",
      "tensor([[-0.0496, -0.3459,  0.2673, -0.1144, -0.2102, -0.2353, -0.3515, -0.2918],\n",
      "        [-0.1161, -0.3058, -0.0305, -0.2624, -0.0753, -0.0889,  0.1168, -0.0650],\n",
      "        [-0.0702, -0.2040,  0.3528,  0.2734, -0.1198,  0.3052, -0.0174, -0.0780],\n",
      "        [-0.2187, -0.0797, -0.0517,  0.2273,  0.1223, -0.3305,  0.1861,  0.2571],\n",
      "        [ 0.1604, -0.0069,  0.0352, -0.3231,  0.1954,  0.1479,  0.0266,  0.2748],\n",
      "        [-0.2671, -0.2954, -0.2005, -0.3106,  0.1900, -0.0619,  0.0551,  0.2309],\n",
      "        [ 0.2418,  0.2926,  0.1091,  0.2340, -0.2686,  0.0718, -0.0098,  0.2020],\n",
      "        [ 0.0466,  0.1870, -0.1680, -0.0113, -0.0536, -0.3473, -0.2148,  0.2712]],\n",
      "       requires_grad=True)\n",
      "spliced Wo: torch.Size([8, 8])\n",
      "tensor([[-0.0496, -0.3459,  0.2673, -0.1144, -0.2102, -0.2353, -0.3515, -0.2918],\n",
      "        [-0.1161, -0.3058, -0.0305, -0.2624, -0.0753, -0.0889,  0.1168, -0.0650],\n",
      "        [-0.0702, -0.2040,  0.3528,  0.2734, -0.1198,  0.3052, -0.0174, -0.0780],\n",
      "        [-0.2187, -0.0797, -0.0517,  0.2273,  0.1223, -0.3305,  0.1861,  0.2571],\n",
      "        [ 0.1604, -0.0069,  0.0352, -0.3231,  0.1954,  0.1479,  0.0266,  0.2748],\n",
      "        [-0.2671, -0.2954, -0.2005, -0.3106,  0.1900, -0.0619,  0.0551,  0.2309],\n",
      "        [ 0.2418,  0.2926,  0.1091,  0.2340, -0.2686,  0.0718, -0.0098,  0.2020],\n",
      "        [ 0.0466,  0.1870, -0.1680, -0.0113, -0.0536, -0.3473, -0.2148,  0.2712]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "projected output: torch.Size([1, 3, 8])\n",
      "tensor([[[ 1.0351e-01,  2.0805e-01,  1.4216e-01,  3.8332e-01, -1.7959e-01,\n",
      "           3.6672e-01,  1.0941e-01, -8.6390e-02],\n",
      "         [ 3.2654e-02,  1.7555e-02,  4.8280e-02,  4.2744e-02, -5.0774e-02,\n",
      "           1.2605e-01,  2.1150e-02, -7.6747e-02],\n",
      "         [ 7.4342e-02,  1.6744e-01,  7.7295e-02,  2.5276e-01, -7.6336e-02,\n",
      "           1.4825e-04, -2.8650e-02,  1.0060e-02]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "----------------- END MultiQueryAttention.forwardTensor() --------------------\n",
      "forwardTensor() output: torch.Size([1, 3, 8])\n",
      "tensor([[[ 1.0351e-01,  2.0805e-01,  1.4216e-01,  3.8332e-01, -1.7959e-01,\n",
      "           3.6672e-01,  1.0941e-01, -8.6390e-02],\n",
      "         [ 3.2654e-02,  1.7555e-02,  4.8280e-02,  4.2744e-02, -5.0774e-02,\n",
      "           1.2605e-01,  2.1150e-02, -7.6747e-02],\n",
      "         [ 7.4342e-02,  1.6744e-01,  7.7295e-02,  2.5276e-01, -7.6336e-02,\n",
      "           1.4825e-04, -2.8650e-02,  1.0060e-02]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "Level 1 from range(2)\n",
      "Model 0 from range(2)\n",
      "----------------- MultiQueryAttention.forwardTensor() --------------------\n",
      "x shape: torch.Size([1, 3, 4])\n",
      "d_skip: 0\n",
      "models_in_this_level: 2\n",
      "h_dim: 2\n",
      "h_skip: 0\n",
      "self.Wqkv: torch.Size([8, 16])\n",
      "Parameter containing:\n",
      "tensor([[-0.1735,  0.2315,  0.1117,  0.1047, -0.1489,  0.2376,  0.1229,  0.3110,\n",
      "          0.0873, -0.2684, -0.1096, -0.0097,  0.3097,  0.2150,  0.2990,  0.3137],\n",
      "        [ 0.1431,  0.1412, -0.0838,  0.2646,  0.3242, -0.2271, -0.2130, -0.3368,\n",
      "         -0.0050,  0.2183, -0.0016,  0.3343,  0.1920,  0.3316, -0.2514, -0.0077],\n",
      "        [ 0.0443, -0.1020,  0.0601, -0.2298,  0.2458,  0.0583,  0.2098,  0.3069,\n",
      "         -0.2419,  0.3114,  0.3513,  0.0985, -0.1354, -0.0416, -0.0551, -0.3424],\n",
      "        [-0.2932,  0.0489, -0.2404, -0.3425, -0.1447, -0.1952,  0.2017, -0.2510,\n",
      "          0.2124,  0.1749, -0.1075, -0.2291, -0.3099,  0.0309,  0.3235,  0.3422],\n",
      "        [ 0.3169, -0.0661, -0.2328,  0.2331, -0.2754,  0.3143, -0.0073,  0.1731,\n",
      "          0.0169, -0.1440,  0.1663, -0.0258,  0.1103,  0.2496, -0.0560,  0.2469],\n",
      "        [-0.2366,  0.2698, -0.0968, -0.2891, -0.2942, -0.1853, -0.3053,  0.0728,\n",
      "         -0.0625,  0.2472,  0.2964, -0.1388,  0.2694,  0.1347, -0.2692,  0.1232],\n",
      "        [ 0.0215,  0.1852,  0.2982,  0.0872,  0.2097,  0.3356,  0.2573,  0.2475,\n",
      "         -0.0317, -0.1558, -0.1693,  0.1063, -0.2679, -0.0946,  0.0275,  0.1280],\n",
      "        [ 0.0351, -0.1243, -0.1558,  0.3483,  0.2635,  0.0277, -0.0599,  0.2833,\n",
      "         -0.0971,  0.3065,  0.2610, -0.2330,  0.0905, -0.1111, -0.3418, -0.1117]],\n",
      "       requires_grad=True)\n",
      "Wq: torch.Size([8, 8])\n",
      "tensor([[-0.1735,  0.2315,  0.1117,  0.1047, -0.1489,  0.2376,  0.1229,  0.3110],\n",
      "        [ 0.1431,  0.1412, -0.0838,  0.2646,  0.3242, -0.2271, -0.2130, -0.3368],\n",
      "        [ 0.0443, -0.1020,  0.0601, -0.2298,  0.2458,  0.0583,  0.2098,  0.3069],\n",
      "        [-0.2932,  0.0489, -0.2404, -0.3425, -0.1447, -0.1952,  0.2017, -0.2510],\n",
      "        [ 0.3169, -0.0661, -0.2328,  0.2331, -0.2754,  0.3143, -0.0073,  0.1731],\n",
      "        [-0.2366,  0.2698, -0.0968, -0.2891, -0.2942, -0.1853, -0.3053,  0.0728],\n",
      "        [ 0.0215,  0.1852,  0.2982,  0.0872,  0.2097,  0.3356,  0.2573,  0.2475],\n",
      "        [ 0.0351, -0.1243, -0.1558,  0.3483,  0.2635,  0.0277, -0.0599,  0.2833]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wk: torch.Size([8, 4])\n",
      "tensor([[ 0.0873, -0.2684, -0.1096, -0.0097],\n",
      "        [-0.0050,  0.2183, -0.0016,  0.3343],\n",
      "        [-0.2419,  0.3114,  0.3513,  0.0985],\n",
      "        [ 0.2124,  0.1749, -0.1075, -0.2291],\n",
      "        [ 0.0169, -0.1440,  0.1663, -0.0258],\n",
      "        [-0.0625,  0.2472,  0.2964, -0.1388],\n",
      "        [-0.0317, -0.1558, -0.1693,  0.1063],\n",
      "        [-0.0971,  0.3065,  0.2610, -0.2330]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wv: torch.Size([8, 4])\n",
      "tensor([[ 0.3097,  0.2150,  0.2990,  0.3137],\n",
      "        [ 0.1920,  0.3316, -0.2514, -0.0077],\n",
      "        [-0.1354, -0.0416, -0.0551, -0.3424],\n",
      "        [-0.3099,  0.0309,  0.3235,  0.3422],\n",
      "        [ 0.1103,  0.2496, -0.0560,  0.2469],\n",
      "        [ 0.2694,  0.1347, -0.2692,  0.1232],\n",
      "        [-0.2679, -0.0946,  0.0275,  0.1280],\n",
      "        [ 0.0905, -0.1111, -0.3418, -0.1117]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wq spliced: torch.Size([4, 4])\n",
      "tensor([[-0.1735,  0.2315, -0.1489,  0.2376],\n",
      "        [ 0.1431,  0.1412,  0.3242, -0.2271],\n",
      "        [ 0.0443, -0.1020,  0.2458,  0.0583],\n",
      "        [-0.2932,  0.0489, -0.1447, -0.1952]], grad_fn=<CatBackward0>)\n",
      "Wk spliced: torch.Size([4, 2])\n",
      "tensor([[ 0.0873, -0.2684],\n",
      "        [-0.0050,  0.2183],\n",
      "        [-0.2419,  0.3114],\n",
      "        [ 0.2124,  0.1749]], grad_fn=<CatBackward0>)\n",
      "Wv spliced: torch.Size([4, 2])\n",
      "tensor([[ 0.3097,  0.2150],\n",
      "        [ 0.1920,  0.3316],\n",
      "        [-0.1354, -0.0416],\n",
      "        [-0.3099,  0.0309]], grad_fn=<CatBackward0>)\n",
      "Wqkv_spliced: torch.Size([4, 8])\n",
      "tensor([[-0.1735,  0.2315, -0.1489,  0.2376,  0.0873, -0.2684,  0.3097,  0.2150],\n",
      "        [ 0.1431,  0.1412,  0.3242, -0.2271, -0.0050,  0.2183,  0.1920,  0.3316],\n",
      "        [ 0.0443, -0.1020,  0.2458,  0.0583, -0.2419,  0.3114, -0.1354, -0.0416],\n",
      "        [-0.2932,  0.0489, -0.1447, -0.1952,  0.2124,  0.1749, -0.3099,  0.0309]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "xqkv: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.1325,  0.1175, -0.3112,  0.1451,  0.1805, -0.3863,  0.1736,\n",
      "           0.0147],\n",
      "         [ 0.0491,  0.3798,  0.1620, -0.1450,  0.1805, -0.0985,  0.5294,\n",
      "           0.5666],\n",
      "         [ 0.3635,  0.0484,  0.3684,  0.0586, -0.2277, -0.0014,  0.4205,\n",
      "           0.1959]]], grad_fn=<UnsafeViewBackward0>)\n",
      "xq: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.1325,  0.1175, -0.3112,  0.1451],\n",
      "         [ 0.0491,  0.3798,  0.1620, -0.1450],\n",
      "         [ 0.3635,  0.0484,  0.3684,  0.0586]]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "xk: torch.Size([1, 3, 2])\n",
      "tensor([[[ 0.1805, -0.3863],\n",
      "         [ 0.1805, -0.0985],\n",
      "         [-0.2277, -0.0014]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xv: torch.Size([1, 3, 2])\n",
      "tensor([[[0.1736, 0.0147],\n",
      "         [0.5294, 0.5666],\n",
      "         [0.4205, 0.1959]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xq reshaped: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[-0.1325,  0.1175],\n",
      "          [-0.3112,  0.1451]],\n",
      "\n",
      "         [[ 0.0491,  0.3798],\n",
      "          [ 0.1620, -0.1450]],\n",
      "\n",
      "         [[ 0.3635,  0.0484],\n",
      "          [ 0.3684,  0.0586]]]], grad_fn=<ViewBackward0>)\n",
      "xk reshaped: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[ 0.1805, -0.3863]],\n",
      "\n",
      "         [[ 0.1805, -0.0985]],\n",
      "\n",
      "         [[-0.2277, -0.0014]]]], grad_fn=<ViewBackward0>)\n",
      "xv reshaped: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[0.1736, 0.0147]],\n",
      "\n",
      "         [[0.5294, 0.5666]],\n",
      "\n",
      "         [[0.4205, 0.1959]]]], grad_fn=<ViewBackward0>)\n",
      "rotated xq: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[-0.1325,  0.1175],\n",
      "          [-0.3112,  0.1451]],\n",
      "\n",
      "         [[-0.2930,  0.2465],\n",
      "          [ 0.2095,  0.0580]],\n",
      "\n",
      "         [[-0.1953,  0.3104],\n",
      "          [-0.2066,  0.3106]]]], grad_fn=<TransposeBackward0>)\n",
      "rotated xk: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[ 0.1805, -0.3863]],\n",
      "\n",
      "         [[ 0.1805,  0.0987]],\n",
      "\n",
      "         [[ 0.0960, -0.2064]]]], grad_fn=<TransposeBackward0>)\n",
      "repeat_interleaved xk: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[ 0.1805, -0.3863],\n",
      "          [ 0.1805, -0.3863]],\n",
      "\n",
      "         [[ 0.1805,  0.0987],\n",
      "          [ 0.1805,  0.0987]],\n",
      "\n",
      "         [[ 0.0960, -0.2064],\n",
      "          [ 0.0960, -0.2064]]]], grad_fn=<ViewBackward0>)\n",
      "repeat_interleaved xv: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[0.1736, 0.0147],\n",
      "          [0.1736, 0.0147]],\n",
      "\n",
      "         [[0.5294, 0.5666],\n",
      "          [0.5294, 0.5666]],\n",
      "\n",
      "         [[0.4205, 0.1959],\n",
      "          [0.4205, 0.1959]]]], grad_fn=<ViewBackward0>)\n",
      "transposed xq: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[-0.1325,  0.1175],\n",
      "          [-0.2930,  0.2465],\n",
      "          [-0.1953,  0.3104]],\n",
      "\n",
      "         [[-0.3112,  0.1451],\n",
      "          [ 0.2095,  0.0580],\n",
      "          [-0.2066,  0.3106]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xk: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[ 0.1805, -0.3863],\n",
      "          [ 0.1805,  0.0987],\n",
      "          [ 0.0960, -0.2064]],\n",
      "\n",
      "         [[ 0.1805, -0.3863],\n",
      "          [ 0.1805,  0.0987],\n",
      "          [ 0.0960, -0.2064]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xv: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[0.1736, 0.0147],\n",
      "          [0.5294, 0.5666],\n",
      "          [0.4205, 0.1959]],\n",
      "\n",
      "         [[0.1736, 0.0147],\n",
      "          [0.5294, 0.5666],\n",
      "          [0.4205, 0.1959]]]], grad_fn=<TransposeBackward0>)\n",
      "scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[-0.0490, -0.0087, -0.0262],\n",
      "          [-0.1047, -0.0202, -0.0559],\n",
      "          [-0.1097, -0.0033, -0.0586]],\n",
      "\n",
      "         [[-0.0794, -0.0296, -0.0423],\n",
      "          [ 0.0109,  0.0308,  0.0058],\n",
      "          [-0.1112, -0.0047, -0.0594]]]], grad_fn=<MulBackward0>)\n",
      "mask: torch.Size([1, 1, 3, 3])\n",
      "tensor([[[[ 0.0000e+00, -2.3820e+38, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]]]])\n",
      "masked scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[-4.9015e-02, -2.3820e+38, -2.3820e+38],\n",
      "          [-1.0473e-01, -2.0198e-02, -2.3820e+38],\n",
      "          [-1.0971e-01, -3.2730e-03, -5.8567e-02]],\n",
      "\n",
      "         [[-7.9353e-02, -2.3820e+38, -2.3820e+38],\n",
      "          [ 1.0905e-02,  3.0780e-02, -2.3820e+38],\n",
      "          [-1.1121e-01, -4.6954e-03, -5.9363e-02]]]], grad_fn=<AddBackward0>)\n",
      "softmaxed scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [0.4789, 0.5211, 0.0000],\n",
      "          [0.3160, 0.3515, 0.3326]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.4950, 0.5050, 0.0000],\n",
      "          [0.3159, 0.3514, 0.3327]]]], grad_fn=<SoftmaxBackward0>)\n",
      "attention: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[0.1736, 0.0147],\n",
      "          [0.3590, 0.3023],\n",
      "          [0.3808, 0.2689]],\n",
      "\n",
      "         [[0.1736, 0.0147],\n",
      "          [0.3533, 0.2934],\n",
      "          [0.3808, 0.2689]]]], grad_fn=<UnsafeViewBackward0>)\n",
      "reshaped attention: torch.Size([1, 3, 4])\n",
      "tensor([[[0.1736, 0.0147, 0.1736, 0.0147],\n",
      "         [0.3590, 0.3023, 0.3533, 0.2934],\n",
      "         [0.3808, 0.2689, 0.3808, 0.2689]]], grad_fn=<ViewBackward0>)\n",
      "self.Wo: torch.Size([8, 8])\n",
      "Parameter containing:\n",
      "tensor([[-0.0496, -0.3459,  0.2673, -0.1144, -0.2102, -0.2353, -0.3515, -0.2918],\n",
      "        [-0.1161, -0.3058, -0.0305, -0.2624, -0.0753, -0.0889,  0.1168, -0.0650],\n",
      "        [-0.0702, -0.2040,  0.3528,  0.2734, -0.1198,  0.3052, -0.0174, -0.0780],\n",
      "        [-0.2187, -0.0797, -0.0517,  0.2273,  0.1223, -0.3305,  0.1861,  0.2571],\n",
      "        [ 0.1604, -0.0069,  0.0352, -0.3231,  0.1954,  0.1479,  0.0266,  0.2748],\n",
      "        [-0.2671, -0.2954, -0.2005, -0.3106,  0.1900, -0.0619,  0.0551,  0.2309],\n",
      "        [ 0.2418,  0.2926,  0.1091,  0.2340, -0.2686,  0.0718, -0.0098,  0.2020],\n",
      "        [ 0.0466,  0.1870, -0.1680, -0.0113, -0.0536, -0.3473, -0.2148,  0.2712]],\n",
      "       requires_grad=True)\n",
      "spliced Wo: torch.Size([4, 4])\n",
      "tensor([[-0.0496, -0.3459,  0.2673, -0.1144],\n",
      "        [-0.1161, -0.3058, -0.0305, -0.2624],\n",
      "        [ 0.1604, -0.0069,  0.0352, -0.3231],\n",
      "        [-0.2671, -0.2954, -0.2005, -0.3106]], grad_fn=<CatBackward0>)\n",
      "projected output: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.0136, -0.0701,  0.0491, -0.0844],\n",
      "         [-0.0746, -0.3057,  0.0403, -0.3257],\n",
      "         [-0.0608, -0.2960,  0.0530, -0.3207]]], grad_fn=<UnsafeViewBackward0>)\n",
      "----------------- END MultiQueryAttention.forwardTensor() --------------------\n",
      "forwardTensor() output: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.0136, -0.0701,  0.0491, -0.0844],\n",
      "         [-0.0746, -0.3057,  0.0403, -0.3257],\n",
      "         [-0.0608, -0.2960,  0.0530, -0.3207]]], grad_fn=<UnsafeViewBackward0>)\n",
      "Model 1 from range(2)\n",
      "----------------- MultiQueryAttention.forwardTensor() --------------------\n",
      "x shape: torch.Size([1, 3, 4])\n",
      "d_skip: 4\n",
      "models_in_this_level: 2\n",
      "h_dim: 2\n",
      "h_skip: 2\n",
      "self.Wqkv: torch.Size([8, 16])\n",
      "Parameter containing:\n",
      "tensor([[-0.1735,  0.2315,  0.1117,  0.1047, -0.1489,  0.2376,  0.1229,  0.3110,\n",
      "          0.0873, -0.2684, -0.1096, -0.0097,  0.3097,  0.2150,  0.2990,  0.3137],\n",
      "        [ 0.1431,  0.1412, -0.0838,  0.2646,  0.3242, -0.2271, -0.2130, -0.3368,\n",
      "         -0.0050,  0.2183, -0.0016,  0.3343,  0.1920,  0.3316, -0.2514, -0.0077],\n",
      "        [ 0.0443, -0.1020,  0.0601, -0.2298,  0.2458,  0.0583,  0.2098,  0.3069,\n",
      "         -0.2419,  0.3114,  0.3513,  0.0985, -0.1354, -0.0416, -0.0551, -0.3424],\n",
      "        [-0.2932,  0.0489, -0.2404, -0.3425, -0.1447, -0.1952,  0.2017, -0.2510,\n",
      "          0.2124,  0.1749, -0.1075, -0.2291, -0.3099,  0.0309,  0.3235,  0.3422],\n",
      "        [ 0.3169, -0.0661, -0.2328,  0.2331, -0.2754,  0.3143, -0.0073,  0.1731,\n",
      "          0.0169, -0.1440,  0.1663, -0.0258,  0.1103,  0.2496, -0.0560,  0.2469],\n",
      "        [-0.2366,  0.2698, -0.0968, -0.2891, -0.2942, -0.1853, -0.3053,  0.0728,\n",
      "         -0.0625,  0.2472,  0.2964, -0.1388,  0.2694,  0.1347, -0.2692,  0.1232],\n",
      "        [ 0.0215,  0.1852,  0.2982,  0.0872,  0.2097,  0.3356,  0.2573,  0.2475,\n",
      "         -0.0317, -0.1558, -0.1693,  0.1063, -0.2679, -0.0946,  0.0275,  0.1280],\n",
      "        [ 0.0351, -0.1243, -0.1558,  0.3483,  0.2635,  0.0277, -0.0599,  0.2833,\n",
      "         -0.0971,  0.3065,  0.2610, -0.2330,  0.0905, -0.1111, -0.3418, -0.1117]],\n",
      "       requires_grad=True)\n",
      "Wq: torch.Size([8, 8])\n",
      "tensor([[-0.1735,  0.2315,  0.1117,  0.1047, -0.1489,  0.2376,  0.1229,  0.3110],\n",
      "        [ 0.1431,  0.1412, -0.0838,  0.2646,  0.3242, -0.2271, -0.2130, -0.3368],\n",
      "        [ 0.0443, -0.1020,  0.0601, -0.2298,  0.2458,  0.0583,  0.2098,  0.3069],\n",
      "        [-0.2932,  0.0489, -0.2404, -0.3425, -0.1447, -0.1952,  0.2017, -0.2510],\n",
      "        [ 0.3169, -0.0661, -0.2328,  0.2331, -0.2754,  0.3143, -0.0073,  0.1731],\n",
      "        [-0.2366,  0.2698, -0.0968, -0.2891, -0.2942, -0.1853, -0.3053,  0.0728],\n",
      "        [ 0.0215,  0.1852,  0.2982,  0.0872,  0.2097,  0.3356,  0.2573,  0.2475],\n",
      "        [ 0.0351, -0.1243, -0.1558,  0.3483,  0.2635,  0.0277, -0.0599,  0.2833]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wk: torch.Size([8, 4])\n",
      "tensor([[ 0.0873, -0.2684, -0.1096, -0.0097],\n",
      "        [-0.0050,  0.2183, -0.0016,  0.3343],\n",
      "        [-0.2419,  0.3114,  0.3513,  0.0985],\n",
      "        [ 0.2124,  0.1749, -0.1075, -0.2291],\n",
      "        [ 0.0169, -0.1440,  0.1663, -0.0258],\n",
      "        [-0.0625,  0.2472,  0.2964, -0.1388],\n",
      "        [-0.0317, -0.1558, -0.1693,  0.1063],\n",
      "        [-0.0971,  0.3065,  0.2610, -0.2330]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wv: torch.Size([8, 4])\n",
      "tensor([[ 0.3097,  0.2150,  0.2990,  0.3137],\n",
      "        [ 0.1920,  0.3316, -0.2514, -0.0077],\n",
      "        [-0.1354, -0.0416, -0.0551, -0.3424],\n",
      "        [-0.3099,  0.0309,  0.3235,  0.3422],\n",
      "        [ 0.1103,  0.2496, -0.0560,  0.2469],\n",
      "        [ 0.2694,  0.1347, -0.2692,  0.1232],\n",
      "        [-0.2679, -0.0946,  0.0275,  0.1280],\n",
      "        [ 0.0905, -0.1111, -0.3418, -0.1117]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wq spliced: torch.Size([4, 4])\n",
      "tensor([[-0.2328,  0.2331, -0.0073,  0.1731],\n",
      "        [-0.0968, -0.2891, -0.3053,  0.0728],\n",
      "        [ 0.2982,  0.0872,  0.2573,  0.2475],\n",
      "        [-0.1558,  0.3483, -0.0599,  0.2833]], grad_fn=<CatBackward0>)\n",
      "Wk spliced: torch.Size([4, 2])\n",
      "tensor([[ 0.1663, -0.0258],\n",
      "        [ 0.2964, -0.1388],\n",
      "        [-0.1693,  0.1063],\n",
      "        [ 0.2610, -0.2330]], grad_fn=<CatBackward0>)\n",
      "Wv spliced: torch.Size([4, 2])\n",
      "tensor([[-0.0560,  0.2469],\n",
      "        [-0.2692,  0.1232],\n",
      "        [ 0.0275,  0.1280],\n",
      "        [-0.3418, -0.1117]], grad_fn=<CatBackward0>)\n",
      "Wqkv_spliced: torch.Size([4, 8])\n",
      "tensor([[-0.2328,  0.2331, -0.0073,  0.1731,  0.1663, -0.0258, -0.0560,  0.2469],\n",
      "        [-0.0968, -0.2891, -0.3053,  0.0728,  0.2964, -0.1388, -0.2692,  0.1232],\n",
      "        [ 0.2982,  0.0872,  0.2573,  0.2475, -0.1693,  0.1063,  0.0275,  0.1280],\n",
      "        [-0.1558,  0.3483, -0.0599,  0.2833,  0.2610, -0.2330, -0.3418, -0.1117]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "xqkv: torch.Size([1, 3, 8])\n",
      "tensor([[[ 0.0202, -0.3452, -0.3459, -0.1101,  0.2136, -0.2383, -0.3219,\n",
      "          -0.3382],\n",
      "         [-0.1559,  0.4778, -0.1486,  0.5773,  0.4937, -0.4423, -0.7118,\n",
      "          -0.1904],\n",
      "         [-0.3199,  0.9267,  0.1339,  0.2736,  0.1843, -0.2945, -0.2804,\n",
      "          -0.4153]]], grad_fn=<UnsafeViewBackward0>)\n",
      "xq: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.0202, -0.3452, -0.3459, -0.1101],\n",
      "         [-0.1559,  0.4778, -0.1486,  0.5773],\n",
      "         [-0.3199,  0.9267,  0.1339,  0.2736]]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "xk: torch.Size([1, 3, 2])\n",
      "tensor([[[ 0.2136, -0.2383],\n",
      "         [ 0.4937, -0.4423],\n",
      "         [ 0.1843, -0.2945]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xv: torch.Size([1, 3, 2])\n",
      "tensor([[[-0.3219, -0.3382],\n",
      "         [-0.7118, -0.1904],\n",
      "         [-0.2804, -0.4153]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xq reshaped: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[ 0.0202, -0.3452],\n",
      "          [-0.3459, -0.1101]],\n",
      "\n",
      "         [[-0.1559,  0.4778],\n",
      "          [-0.1486,  0.5773]],\n",
      "\n",
      "         [[-0.3199,  0.9267],\n",
      "          [ 0.1339,  0.2736]]]], grad_fn=<ViewBackward0>)\n",
      "xk reshaped: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[ 0.2136, -0.2383]],\n",
      "\n",
      "         [[ 0.4937, -0.4423]],\n",
      "\n",
      "         [[ 0.1843, -0.2945]]]], grad_fn=<ViewBackward0>)\n",
      "xv reshaped: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[-0.3219, -0.3382]],\n",
      "\n",
      "         [[-0.7118, -0.1904]],\n",
      "\n",
      "         [[-0.2804, -0.4153]]]], grad_fn=<ViewBackward0>)\n",
      "rotated xq: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[ 0.0202, -0.3452],\n",
      "          [-0.3459, -0.1101]],\n",
      "\n",
      "         [[-0.4863,  0.1270],\n",
      "          [-0.5661,  0.1869]],\n",
      "\n",
      "         [[-0.7095, -0.6765],\n",
      "          [-0.3045,  0.0079]]]], grad_fn=<TransposeBackward0>)\n",
      "rotated xk: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[ 0.2136, -0.2383]],\n",
      "\n",
      "         [[ 0.6389,  0.1764]],\n",
      "\n",
      "         [[ 0.1911,  0.2901]]]], grad_fn=<TransposeBackward0>)\n",
      "repeat_interleaved xk: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[ 0.2136, -0.2383],\n",
      "          [ 0.2136, -0.2383]],\n",
      "\n",
      "         [[ 0.6389,  0.1764],\n",
      "          [ 0.6389,  0.1764]],\n",
      "\n",
      "         [[ 0.1911,  0.2901],\n",
      "          [ 0.1911,  0.2901]]]], grad_fn=<ViewBackward0>)\n",
      "repeat_interleaved xv: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[-0.3219, -0.3382],\n",
      "          [-0.3219, -0.3382]],\n",
      "\n",
      "         [[-0.7118, -0.1904],\n",
      "          [-0.7118, -0.1904]],\n",
      "\n",
      "         [[-0.2804, -0.4153],\n",
      "          [-0.2804, -0.4153]]]], grad_fn=<ViewBackward0>)\n",
      "transposed xq: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[ 0.0202, -0.3452],\n",
      "          [-0.4863,  0.1270],\n",
      "          [-0.7095, -0.6765]],\n",
      "\n",
      "         [[-0.3459, -0.1101],\n",
      "          [-0.5661,  0.1869],\n",
      "          [-0.3045,  0.0079]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xk: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[ 0.2136, -0.2383],\n",
      "          [ 0.6389,  0.1764],\n",
      "          [ 0.1911,  0.2901]],\n",
      "\n",
      "         [[ 0.2136, -0.2383],\n",
      "          [ 0.6389,  0.1764],\n",
      "          [ 0.1911,  0.2901]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xv: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[-0.3219, -0.3382],\n",
      "          [-0.7118, -0.1904],\n",
      "          [-0.2804, -0.4153]],\n",
      "\n",
      "         [[-0.3219, -0.3382],\n",
      "          [-0.7118, -0.1904],\n",
      "          [-0.2804, -0.4153]]]], grad_fn=<TransposeBackward0>)\n",
      "scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[ 0.0612, -0.0339, -0.0681],\n",
      "          [-0.0949, -0.2039, -0.0397],\n",
      "          [ 0.0068, -0.4049, -0.2346]],\n",
      "\n",
      "         [[-0.0337, -0.1700, -0.0693],\n",
      "          [-0.1170, -0.2324, -0.0381],\n",
      "          [-0.0473, -0.1366, -0.0395]]]], grad_fn=<MulBackward0>)\n",
      "mask: torch.Size([1, 1, 3, 3])\n",
      "tensor([[[[ 0.0000e+00, -2.3820e+38, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]]]])\n",
      "masked scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[ 6.1223e-02, -2.3820e+38, -2.3820e+38],\n",
      "          [-9.4865e-02, -2.0386e-01, -2.3820e+38],\n",
      "          [ 6.8109e-03, -4.0495e-01, -2.3463e-01]],\n",
      "\n",
      "         [[-3.3707e-02, -2.3820e+38, -2.3820e+38],\n",
      "          [-1.1701e-01, -2.3242e-01, -2.3820e+38],\n",
      "          [-4.7323e-02, -1.3658e-01, -3.9524e-02]]]], grad_fn=<AddBackward0>)\n",
      "softmaxed scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [0.5272, 0.4728, 0.0000],\n",
      "          [0.4085, 0.2706, 0.3209]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.5288, 0.4712, 0.0000],\n",
      "          [0.3422, 0.3130, 0.3449]]]], grad_fn=<SoftmaxBackward0>)\n",
      "attention: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[-0.3219, -0.3382],\n",
      "          [-0.5063, -0.2683],\n",
      "          [-0.4141, -0.3229]],\n",
      "\n",
      "         [[-0.3219, -0.3382],\n",
      "          [-0.5056, -0.2685],\n",
      "          [-0.4296, -0.3185]]]], grad_fn=<UnsafeViewBackward0>)\n",
      "reshaped attention: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.3219, -0.3382, -0.3219, -0.3382],\n",
      "         [-0.5063, -0.2683, -0.5056, -0.2685],\n",
      "         [-0.4141, -0.3229, -0.4296, -0.3185]]], grad_fn=<ViewBackward0>)\n",
      "self.Wo: torch.Size([8, 8])\n",
      "Parameter containing:\n",
      "tensor([[-0.0496, -0.3459,  0.2673, -0.1144, -0.2102, -0.2353, -0.3515, -0.2918],\n",
      "        [-0.1161, -0.3058, -0.0305, -0.2624, -0.0753, -0.0889,  0.1168, -0.0650],\n",
      "        [-0.0702, -0.2040,  0.3528,  0.2734, -0.1198,  0.3052, -0.0174, -0.0780],\n",
      "        [-0.2187, -0.0797, -0.0517,  0.2273,  0.1223, -0.3305,  0.1861,  0.2571],\n",
      "        [ 0.1604, -0.0069,  0.0352, -0.3231,  0.1954,  0.1479,  0.0266,  0.2748],\n",
      "        [-0.2671, -0.2954, -0.2005, -0.3106,  0.1900, -0.0619,  0.0551,  0.2309],\n",
      "        [ 0.2418,  0.2926,  0.1091,  0.2340, -0.2686,  0.0718, -0.0098,  0.2020],\n",
      "        [ 0.0466,  0.1870, -0.1680, -0.0113, -0.0536, -0.3473, -0.2148,  0.2712]],\n",
      "       requires_grad=True)\n",
      "spliced Wo: torch.Size([4, 4])\n",
      "tensor([[-0.1198,  0.3052, -0.0174, -0.0780],\n",
      "        [ 0.1223, -0.3305,  0.1861,  0.2571],\n",
      "        [-0.2686,  0.0718, -0.0098,  0.2020],\n",
      "        [-0.0536, -0.3473, -0.2148,  0.2712]], grad_fn=<CatBackward0>)\n",
      "projected output: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.1018,  0.1078,  0.0185, -0.2186],\n",
      "         [ 0.1780, -0.0089,  0.0215, -0.2044],\n",
      "         [ 0.1426,  0.0601,  0.0197, -0.2239]]], grad_fn=<UnsafeViewBackward0>)\n",
      "----------------- END MultiQueryAttention.forwardTensor() --------------------\n",
      "forwardTensor() output: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.1018,  0.1078,  0.0185, -0.2186],\n",
      "         [ 0.1780, -0.0089,  0.0215, -0.2044],\n",
      "         [ 0.1426,  0.0601,  0.0197, -0.2239]]], grad_fn=<UnsafeViewBackward0>)\n",
      "final output: ((tensor([[[ 1.1501e-01,  2.3116e-01,  1.5795e-01,  4.2591e-01, -1.9954e-01,\n",
      "           0.0000e+00,  1.2156e-01, -9.5989e-02],\n",
      "         [ 3.6282e-02,  1.9505e-02,  5.3645e-02,  4.7493e-02, -5.6415e-02,\n",
      "           1.4005e-01,  2.3500e-02, -8.5274e-02],\n",
      "         [ 8.2602e-02,  1.8604e-01,  8.5883e-02,  2.8084e-01, -8.4818e-02,\n",
      "           1.6472e-04, -0.0000e+00,  1.1178e-02]]], grad_fn=<MulBackward0>),), (tensor([[[ 0.0151, -0.0779,  0.0546, -0.0938],\n",
      "         [-0.0828, -0.3397,  0.0448, -0.3619],\n",
      "         [-0.0676, -0.3289,  0.0000, -0.3563]]], grad_fn=<MulBackward0>), tensor([[[ 0.1131,  0.1198,  0.0205, -0.2429],\n",
      "         [ 0.1978, -0.0099,  0.0239, -0.0000],\n",
      "         [ 0.1584,  0.0668,  0.0219, -0.2488]]], grad_fn=<MulBackward0>)))\n",
      "------------- END MultiQueryAttention.forwardTuple() ------------\n",
      "out: ((tensor([[[ 1.1501e-01,  2.3116e-01,  1.5795e-01,  4.2591e-01, -1.9954e-01,\n",
      "           0.0000e+00,  1.2156e-01, -9.5989e-02],\n",
      "         [ 3.6282e-02,  1.9505e-02,  5.3645e-02,  4.7493e-02, -5.6415e-02,\n",
      "           1.4005e-01,  2.3500e-02, -8.5274e-02],\n",
      "         [ 8.2602e-02,  1.8604e-01,  8.5883e-02,  2.8084e-01, -8.4818e-02,\n",
      "           1.6472e-04, -0.0000e+00,  1.1178e-02]]], grad_fn=<MulBackward0>),), (tensor([[[ 0.0151, -0.0779,  0.0546, -0.0938],\n",
      "         [-0.0828, -0.3397,  0.0448, -0.3619],\n",
      "         [-0.0676, -0.3289,  0.0000, -0.3563]]], grad_fn=<MulBackward0>), tensor([[[ 0.1131,  0.1198,  0.0205, -0.2429],\n",
      "         [ 0.1978, -0.0099,  0.0239, -0.0000],\n",
      "         [ 0.1584,  0.0668,  0.0219, -0.2488]]], grad_fn=<MulBackward0>)))\n",
      "---------- RESET CONFIG --------\n",
      "model_count:  [1, 2, 4]\n",
      "model_dim_list:  [128, 64, 32]\n",
      "head_dim_list:  [32, 16, 8]\n"
     ]
    }
   ],
   "source": [
    "# Testing our Attention's forwardTuple()\n",
    "verbose = True\n",
    "\n",
    "print(\"--------- Micro Hyperparameters -------\")\n",
    "hold1, hold2, hold3, hold4, hold5 = config.hidden_size, config.num_attention_heads, config.head_dim, config.levels, config.max_position_embeddings\n",
    "config.hidden_size = 8\n",
    "config.num_attention_heads = 2\n",
    "config.head_dim = 4\n",
    "config.levels = 2\n",
    "config.max_position_embeddings = 3\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "print(\"head_dim_list: \", config.head_dim_list)\n",
    "\n",
    "att = MultiQueryAttention(config)\n",
    "# we need to make sure to send in a tuple of the expected size. above we set hidden_size=4 and levels=2\n",
    "x = ((torch.randn((1,3,8)),),(torch.randn((1,3,4)),torch.randn((1,3,4))))\n",
    "print(f\"x: {x}\")\n",
    "out = att(x)\n",
    "print(f\"out: {out}\")\n",
    "\n",
    "verbose = False\n",
    "print(\"---------- RESET CONFIG --------\")\n",
    "config.hidden_size = hold1\n",
    "config.num_attention_heads = hold2\n",
    "config.head_dim = hold3\n",
    "config.levels = hold4\n",
    "config.max_position_embeddings = hold5\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "print(\"head_dim_list: \", config.head_dim_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391da4a9",
   "metadata": {},
   "source": [
    "# Layer\n",
    "\n",
    "nothing too interesting here besides the absurd amount of memory we're probably taking up with these tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb6e171e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(nn.Module):\n",
    "    \"\"\"\n",
    "    A decoder layer that integrates the MultiQueryAttention and MLP. It includes\n",
    "    normalization steps both before and after the attention mechanism to stabilize and accelerate training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initializes the GemmaAttention mechanism with parameters from the config, enabling self-attention within the decoder layer.\n",
    "        self.self_attn = MultiQueryAttention(config)\n",
    "        \n",
    "        # Initializes the GemmaMLP module, providing a non-linear transformation after the attention mechanism.\n",
    "        self.mlp = MLP(\n",
    "            # the hidden dimension of the model\n",
    "            hidden_size = config.hidden_size,\n",
    "            # the number of nodes in the center of the two feedforward layers\n",
    "            intermediate_size = config.intermediate_size,\n",
    "            # the % of neurons to set to 0 during training\n",
    "            dropout = config.dropout,\n",
    "        )\n",
    "        \n",
    "        # Applies RMSNorm normalization to the input of the decoder layer for stable training dynamics.\n",
    "        self.input_layernorm = RMSNorm(config.hidden_size,\n",
    "                                       eps = config.rms_norm_eps)\n",
    "        \n",
    "        # Applies RMSNorm after the attention mechanism and before the MLP to ensure the output is well-conditioned for further processing.\n",
    "        self.post_attention_layernorm = RMSNorm(config.hidden_size,\n",
    "                                                eps = config.rms_norm_eps)\n",
    "\n",
    "    def forwardTensor(self,\n",
    "                # The input tensor to the decoder layer. shape (batch_size, input_len, hidden_size)\n",
    "                x: torch.Tensor,\n",
    "                model: int = 0,\n",
    "                drop_bool: bool = False\n",
    "                ) -> torch.Tensor:\n",
    "        global verbose\n",
    "        if verbose: print(\"----------------- Layer.forwardTensor() --------------------\")\n",
    "        \n",
    "        # Self Attention Block\n",
    "        # Stores the original input for use as a residual connection, aiding in mitigating the vanishing gradient problem\n",
    "        residual_connection = x\n",
    "        # Normalizes the input before processing by the attention mechanism.\n",
    "        x = self.input_layernorm(x, model)\n",
    "        # Processes the normalized input through the GemmaAttention mechanism\n",
    "        x = self.self_attn(x, model, drop_bool)\n",
    "        # The aforementioned residual connection\n",
    "        x = residual_connection + x\n",
    "        if verbose: print(f\"x in layer after MQA & resid connection and before MLP:\\n{x}\")\n",
    "\n",
    "        # MLP Block\n",
    "        # Again, stores the output of the attention block for use as a residual connection before processing by the MLP.\n",
    "        residual_connection = x\n",
    "        # Normalizes the output of the attention block before passing it to the MLP, ensuring a stable input distribution.\n",
    "        x = self.post_attention_layernorm(x, model)\n",
    "        # Transforms the normalized attention output through the MLP, introducing additional non-linearity and capacity to the model.\n",
    "        x = self.mlp(x, model, drop_bool)\n",
    "        # Another residual connection\n",
    "        x = residual_connection + x\n",
    "        if verbose: \n",
    "            print(f\"layer's final residual state:\\n{x}\")\n",
    "            print(\"----------------- END Layer.forwardTensor() --------------------\")\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forwardTuple(self,\n",
    "                     x: Tuple[Tuple[torch.Tensor]],\n",
    "                    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Defines the forward pass of a decoder layer during training.\n",
    "\n",
    "        Parameters:\n",
    "            x (Tuple[Tuple[Tensor]]): \n",
    "                The input tuple of tuples of tensors \n",
    "                first tuple is of length config.levels and second layer of tuples have lengths of config.model_count\n",
    "                tensors are shape (batch size, sequence length, hidden dimension) where hidden dimension changes by which model was used\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Tuple[Tensor]]: \n",
    "                The output tuple of tuples of tensors after applying the decoder layer\n",
    "        \"\"\"\n",
    "        global verbose\n",
    "        if verbose: \n",
    "            print(\"------------- Layer.forwardTuple() ------------\")\n",
    "            print(f\"x:\\n{x}\")\n",
    "            \n",
    "        # forwardTuple() should only be used during training, so we assert input_len == max_position_embeddings\n",
    "        input_len = x[0][0].shape[1]\n",
    "        if verbose: print(f\"input_len: {input_len}\")\n",
    "        assert input_len == config.max_position_embeddings\n",
    "\n",
    "        # we could define these from the config but this way the method is more flexible to testing\n",
    "        num_levels = len(x)\n",
    "        models_per_level = [len(x[i]) for i in range(num_levels)]\n",
    "        if verbose: \n",
    "            print(f\"num_levels: {num_levels}\")\n",
    "            print(f\"models_per_level: {models_per_level}\")\n",
    "\n",
    "        # the loop that iterates over levels, aka the different potential sizes of models\n",
    "        out = ()\n",
    "        for i in range(num_levels):\n",
    "            if verbose: print(f\"Level {i} from range({num_levels})\")\n",
    "\n",
    "            # now for the loop that iterates over models in this level\n",
    "            out_lvl = ()\n",
    "            for j in range(models_per_level[i]):\n",
    "                if verbose: print(f\"Model {j} from range({models_per_level[i]})\")\n",
    "\n",
    "                output = self.forwardTensor(x[i][j], model = j, drop_bool = True)\n",
    "                if verbose: print(f\"forwardTensor() output: {output.shape}\\n{output}\")\n",
    "                \n",
    "                out_lvl += (output,)\n",
    "            \n",
    "            out += (out_lvl,)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"final output: {out}\")\n",
    "            print(\"------------- END Layer.forwardTuple() ------------\")\n",
    "\n",
    "        return out\n",
    "        \n",
    "    def forward(self, x, model=0):\n",
    "        train = True if type(x) == tuple else False\n",
    "        if verbose: print(f\"---------- Layer Input: {'Tuple' if train else 'torch.Tensor'} ------------\")\n",
    "        return self.forwardTuple(x) if train else self.forwardTensor(x, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e674a900-16d8-451f-8425-5ec924de603a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Micro Hyperparameters -------\n",
      "model_count:  [1, 2, 4]\n",
      "model_dim_list:  [8, 4, 2]\n",
      "head_dim_list:  [4, 2, 1]\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the big model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 3, 8])\n",
      "tensor([[[0.4252, 0.5358, 0.5203, 0.3575, 0.6646, 0.4463, 0.0593, 0.4805],\n",
      "         [0.2765, 0.2705, 0.9595, 0.0708, 0.5833, 0.0015, 0.8442, 0.3101],\n",
      "         [0.9782, 0.5889, 0.7047, 0.9204, 0.0966, 0.6561, 0.8882, 0.1908]]])\n",
      "---------- Layer Input: torch.Tensor ------------\n",
      "----------------- Layer.forwardTensor() --------------------\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 3, 8])\n",
      "tensor([[[0.4252, 0.5358, 0.5203, 0.3575, 0.6646, 0.4463, 0.0593, 0.4805],\n",
      "         [0.2765, 0.2705, 0.9595, 0.0708, 0.5833, 0.0015, 0.8442, 0.3101],\n",
      "         [0.9782, 0.5889, 0.7047, 0.9204, 0.0966, 0.6561, 0.8882, 0.1908]]])\n",
      "normed x: torch.Size([1, 3, 8])\n",
      "tensor([[[0.9113, 1.1484, 1.1150, 0.7661, 1.4244, 0.9565, 0.1271, 1.0299],\n",
      "         [0.5244, 0.5130, 1.8197, 0.1342, 1.1062, 0.0028, 1.6009, 0.5881],\n",
      "         [1.3987, 0.8421, 1.0077, 1.3162, 0.1382, 0.9382, 1.2701, 0.2729]]])\n",
      "dim: 8\n",
      "skip: 0\n",
      "spliced scale: torch.Size([8])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 3, 8])\n",
      "tensor([[[0.9113, 1.1484, 1.1150, 0.7661, 1.4244, 0.9565, 0.1271, 1.0299],\n",
      "         [0.5244, 0.5130, 1.8197, 0.1342, 1.1062, 0.0028, 1.6009, 0.5881],\n",
      "         [1.3987, 0.8421, 1.0077, 1.3162, 0.1382, 0.9382, 1.2701, 0.2729]]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "---------- Attention Input: torch.Tensor ------------\n",
      "----------------- MultiQueryAttention.forwardTensor() --------------------\n",
      "x shape: torch.Size([1, 3, 8])\n",
      "d_skip: 0\n",
      "models_in_this_level: 1\n",
      "h_dim: 4\n",
      "h_skip: 0\n",
      "self.Wqkv: torch.Size([8, 16])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0596,  0.1926,  0.1416,  0.1543, -0.2403, -0.1886,  0.0078, -0.2750,\n",
      "         -0.2458,  0.2633,  0.0717, -0.3379, -0.0550, -0.2145, -0.2263, -0.1891],\n",
      "        [ 0.0133, -0.3318,  0.1303,  0.2509, -0.2420, -0.2944, -0.2999, -0.0378,\n",
      "          0.3185, -0.2420,  0.1715, -0.3286, -0.3181,  0.3192, -0.2656, -0.0685],\n",
      "        [-0.0784,  0.2418, -0.3332, -0.2701,  0.1462,  0.1656,  0.2593, -0.2085,\n",
      "          0.1567,  0.2266,  0.1648, -0.0409, -0.2899, -0.0026, -0.3158, -0.2046],\n",
      "        [-0.1486, -0.2741,  0.3087, -0.1919, -0.2210,  0.0555,  0.2809, -0.1217,\n",
      "         -0.2563, -0.2128,  0.3361, -0.1418,  0.0329, -0.0038, -0.1683,  0.0916],\n",
      "        [-0.0454, -0.0726,  0.0953,  0.0128, -0.0403,  0.3463,  0.1210, -0.1087,\n",
      "          0.0029,  0.0485,  0.1873, -0.1808,  0.2871,  0.1377, -0.0610, -0.1091],\n",
      "        [-0.3370, -0.1046,  0.0757, -0.1619,  0.1124, -0.0957, -0.2866, -0.2601,\n",
      "          0.2742,  0.0471,  0.1900,  0.2722,  0.3035,  0.2868,  0.1214,  0.2979],\n",
      "        [-0.0918,  0.2254, -0.0081,  0.1744,  0.1780, -0.1682,  0.0331, -0.3179,\n",
      "         -0.1440,  0.2721,  0.0799, -0.0047,  0.1340,  0.2574, -0.2891, -0.3440],\n",
      "        [ 0.3502,  0.1751, -0.0075, -0.2120,  0.2058, -0.0625,  0.0250,  0.1968,\n",
      "          0.1427,  0.0588, -0.3105,  0.1747, -0.0898,  0.1781, -0.3073,  0.0556]],\n",
      "       requires_grad=True)\n",
      "Wq: torch.Size([8, 8])\n",
      "tensor([[ 0.0596,  0.1926,  0.1416,  0.1543, -0.2403, -0.1886,  0.0078, -0.2750],\n",
      "        [ 0.0133, -0.3318,  0.1303,  0.2509, -0.2420, -0.2944, -0.2999, -0.0378],\n",
      "        [-0.0784,  0.2418, -0.3332, -0.2701,  0.1462,  0.1656,  0.2593, -0.2085],\n",
      "        [-0.1486, -0.2741,  0.3087, -0.1919, -0.2210,  0.0555,  0.2809, -0.1217],\n",
      "        [-0.0454, -0.0726,  0.0953,  0.0128, -0.0403,  0.3463,  0.1210, -0.1087],\n",
      "        [-0.3370, -0.1046,  0.0757, -0.1619,  0.1124, -0.0957, -0.2866, -0.2601],\n",
      "        [-0.0918,  0.2254, -0.0081,  0.1744,  0.1780, -0.1682,  0.0331, -0.3179],\n",
      "        [ 0.3502,  0.1751, -0.0075, -0.2120,  0.2058, -0.0625,  0.0250,  0.1968]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wk: torch.Size([8, 4])\n",
      "tensor([[-0.2458,  0.2633,  0.0717, -0.3379],\n",
      "        [ 0.3185, -0.2420,  0.1715, -0.3286],\n",
      "        [ 0.1567,  0.2266,  0.1648, -0.0409],\n",
      "        [-0.2563, -0.2128,  0.3361, -0.1418],\n",
      "        [ 0.0029,  0.0485,  0.1873, -0.1808],\n",
      "        [ 0.2742,  0.0471,  0.1900,  0.2722],\n",
      "        [-0.1440,  0.2721,  0.0799, -0.0047],\n",
      "        [ 0.1427,  0.0588, -0.3105,  0.1747]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wv: torch.Size([8, 4])\n",
      "tensor([[-0.0550, -0.2145, -0.2263, -0.1891],\n",
      "        [-0.3181,  0.3192, -0.2656, -0.0685],\n",
      "        [-0.2899, -0.0026, -0.3158, -0.2046],\n",
      "        [ 0.0329, -0.0038, -0.1683,  0.0916],\n",
      "        [ 0.2871,  0.1377, -0.0610, -0.1091],\n",
      "        [ 0.3035,  0.2868,  0.1214,  0.2979],\n",
      "        [ 0.1340,  0.2574, -0.2891, -0.3440],\n",
      "        [-0.0898,  0.1781, -0.3073,  0.0556]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wq spliced: torch.Size([8, 8])\n",
      "tensor([[ 0.0596,  0.1926,  0.1416,  0.1543, -0.2403, -0.1886,  0.0078, -0.2750],\n",
      "        [ 0.0133, -0.3318,  0.1303,  0.2509, -0.2420, -0.2944, -0.2999, -0.0378],\n",
      "        [-0.0784,  0.2418, -0.3332, -0.2701,  0.1462,  0.1656,  0.2593, -0.2085],\n",
      "        [-0.1486, -0.2741,  0.3087, -0.1919, -0.2210,  0.0555,  0.2809, -0.1217],\n",
      "        [-0.0454, -0.0726,  0.0953,  0.0128, -0.0403,  0.3463,  0.1210, -0.1087],\n",
      "        [-0.3370, -0.1046,  0.0757, -0.1619,  0.1124, -0.0957, -0.2866, -0.2601],\n",
      "        [-0.0918,  0.2254, -0.0081,  0.1744,  0.1780, -0.1682,  0.0331, -0.3179],\n",
      "        [ 0.3502,  0.1751, -0.0075, -0.2120,  0.2058, -0.0625,  0.0250,  0.1968]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Wk spliced: torch.Size([8, 4])\n",
      "tensor([[-0.2458,  0.2633,  0.0717, -0.3379],\n",
      "        [ 0.3185, -0.2420,  0.1715, -0.3286],\n",
      "        [ 0.1567,  0.2266,  0.1648, -0.0409],\n",
      "        [-0.2563, -0.2128,  0.3361, -0.1418],\n",
      "        [ 0.0029,  0.0485,  0.1873, -0.1808],\n",
      "        [ 0.2742,  0.0471,  0.1900,  0.2722],\n",
      "        [-0.1440,  0.2721,  0.0799, -0.0047],\n",
      "        [ 0.1427,  0.0588, -0.3105,  0.1747]], grad_fn=<CatBackward0>)\n",
      "Wv spliced: torch.Size([8, 4])\n",
      "tensor([[-0.0550, -0.2145, -0.2263, -0.1891],\n",
      "        [-0.3181,  0.3192, -0.2656, -0.0685],\n",
      "        [-0.2899, -0.0026, -0.3158, -0.2046],\n",
      "        [ 0.0329, -0.0038, -0.1683,  0.0916],\n",
      "        [ 0.2871,  0.1377, -0.0610, -0.1091],\n",
      "        [ 0.3035,  0.2868,  0.1214,  0.2979],\n",
      "        [ 0.1340,  0.2574, -0.2891, -0.3440],\n",
      "        [-0.0898,  0.1781, -0.3073,  0.0556]], grad_fn=<CatBackward0>)\n",
      "Wqkv_spliced: torch.Size([8, 16])\n",
      "tensor([[ 0.0596,  0.1926,  0.1416,  0.1543, -0.2403, -0.1886,  0.0078, -0.2750,\n",
      "         -0.2458,  0.2633,  0.0717, -0.3379, -0.0550, -0.2145, -0.2263, -0.1891],\n",
      "        [ 0.0133, -0.3318,  0.1303,  0.2509, -0.2420, -0.2944, -0.2999, -0.0378,\n",
      "          0.3185, -0.2420,  0.1715, -0.3286, -0.3181,  0.3192, -0.2656, -0.0685],\n",
      "        [-0.0784,  0.2418, -0.3332, -0.2701,  0.1462,  0.1656,  0.2593, -0.2085,\n",
      "          0.1567,  0.2266,  0.1648, -0.0409, -0.2899, -0.0026, -0.3158, -0.2046],\n",
      "        [-0.1486, -0.2741,  0.3087, -0.1919, -0.2210,  0.0555,  0.2809, -0.1217,\n",
      "         -0.2563, -0.2128,  0.3361, -0.1418,  0.0329, -0.0038, -0.1683,  0.0916],\n",
      "        [-0.0454, -0.0726,  0.0953,  0.0128, -0.0403,  0.3463,  0.1210, -0.1087,\n",
      "          0.0029,  0.0485,  0.1873, -0.1808,  0.2871,  0.1377, -0.0610, -0.1091],\n",
      "        [-0.3370, -0.1046,  0.0757, -0.1619,  0.1124, -0.0957, -0.2866, -0.2601,\n",
      "          0.2742,  0.0471,  0.1900,  0.2722,  0.3035,  0.2868,  0.1214,  0.2979],\n",
      "        [-0.0918,  0.2254, -0.0081,  0.1744,  0.1780, -0.1682,  0.0331, -0.3179,\n",
      "         -0.1440,  0.2721,  0.0799, -0.0047,  0.1340,  0.2574, -0.2891, -0.3440],\n",
      "        [ 0.3502,  0.1751, -0.0075, -0.2120,  0.2058, -0.0625,  0.0250,  0.1968,\n",
      "          0.1427,  0.0588, -0.3105,  0.1747, -0.0898,  0.1781, -0.3073,  0.0556]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "xqkv: torch.Size([1, 3, 16])\n",
      "tensor([[[-0.1698, -0.1404,  0.3430, -0.3522, -0.2187,  0.0331,  0.0952,\n",
      "          -0.8611,  0.5153,  0.2608,  0.8424, -0.6574, -0.0896,  0.8520,\n",
      "          -1.3164, -0.2659],\n",
      "         [-0.1168,  0.7172, -0.3356, -0.1394,  0.3478,  0.1356,  0.5606,\n",
      "          -1.0735,  0.1427,  0.9217,  0.6235, -0.5432, -0.2349,  0.7160,\n",
      "          -1.5628, -1.1322],\n",
      "         [-0.5236,  0.0989,  0.4501, -0.0842, -0.3014, -0.5445,  0.1862,\n",
      "          -1.3959, -0.1412,  0.5252,  1.0740, -0.7052, -0.1235,  0.6248,\n",
      "          -1.4255, -0.5651]]], grad_fn=<UnsafeViewBackward0>)\n",
      "xq: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.1698, -0.1404,  0.3430, -0.3522, -0.2187,  0.0331,  0.0952,\n",
      "          -0.8611],\n",
      "         [-0.1168,  0.7172, -0.3356, -0.1394,  0.3478,  0.1356,  0.5606,\n",
      "          -1.0735],\n",
      "         [-0.5236,  0.0989,  0.4501, -0.0842, -0.3014, -0.5445,  0.1862,\n",
      "          -1.3959]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xk: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.5153,  0.2608,  0.8424, -0.6574],\n",
      "         [ 0.1427,  0.9217,  0.6235, -0.5432],\n",
      "         [-0.1412,  0.5252,  1.0740, -0.7052]]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "xv: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.0896,  0.8520, -1.3164, -0.2659],\n",
      "         [-0.2349,  0.7160, -1.5628, -1.1322],\n",
      "         [-0.1235,  0.6248, -1.4255, -0.5651]]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "xq reshaped: torch.Size([1, 3, 2, 4])\n",
      "tensor([[[[-0.1698, -0.1404,  0.3430, -0.3522],\n",
      "          [-0.2187,  0.0331,  0.0952, -0.8611]],\n",
      "\n",
      "         [[-0.1168,  0.7172, -0.3356, -0.1394],\n",
      "          [ 0.3478,  0.1356,  0.5606, -1.0735]],\n",
      "\n",
      "         [[-0.5236,  0.0989,  0.4501, -0.0842],\n",
      "          [-0.3014, -0.5445,  0.1862, -1.3959]]]], grad_fn=<ViewBackward0>)\n",
      "xk reshaped: torch.Size([1, 3, 1, 4])\n",
      "tensor([[[[ 0.5153,  0.2608,  0.8424, -0.6574]],\n",
      "\n",
      "         [[ 0.1427,  0.9217,  0.6235, -0.5432]],\n",
      "\n",
      "         [[-0.1412,  0.5252,  1.0740, -0.7052]]]], grad_fn=<ViewBackward0>)\n",
      "xv reshaped: torch.Size([1, 3, 1, 4])\n",
      "tensor([[[[-0.0896,  0.8520, -1.3164, -0.2659]],\n",
      "\n",
      "         [[-0.2349,  0.7160, -1.5628, -1.1322]],\n",
      "\n",
      "         [[-0.1235,  0.6248, -1.4255, -0.5651]]]], grad_fn=<ViewBackward0>)\n",
      "rotated xq: torch.Size([1, 3, 2, 4])\n",
      "tensor([[[[-0.1698, -0.1404,  0.3430, -0.3522],\n",
      "          [-0.2187,  0.0331,  0.0952, -0.8611]],\n",
      "\n",
      "         [[ 0.2193,  0.7276, -0.2796, -0.0671],\n",
      "          [-0.2838,  0.2421,  0.5956, -1.0546]],\n",
      "\n",
      "         [[-0.1914,  0.1136, -0.6634, -0.0628],\n",
      "          [-0.0439, -0.2563, -0.3516, -1.4762]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "rotated xk: torch.Size([1, 3, 1, 4])\n",
      "tensor([[[[ 0.5153,  0.2608,  0.8424, -0.6574]],\n",
      "\n",
      "         [[-0.4476,  0.9713,  0.4569, -0.4485]],\n",
      "\n",
      "         [[-0.9178,  0.6548, -0.5754, -0.5868]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "repeat_interleaved xk: torch.Size([1, 3, 2, 4])\n",
      "tensor([[[[ 0.5153,  0.2608,  0.8424, -0.6574],\n",
      "          [ 0.5153,  0.2608,  0.8424, -0.6574]],\n",
      "\n",
      "         [[-0.4476,  0.9713,  0.4569, -0.4485],\n",
      "          [-0.4476,  0.9713,  0.4569, -0.4485]],\n",
      "\n",
      "         [[-0.9178,  0.6548, -0.5754, -0.5868],\n",
      "          [-0.9178,  0.6548, -0.5754, -0.5868]]]], grad_fn=<ViewBackward0>)\n",
      "repeat_interleaved xv: torch.Size([1, 3, 2, 4])\n",
      "tensor([[[[-0.0896,  0.8520, -1.3164, -0.2659],\n",
      "          [-0.0896,  0.8520, -1.3164, -0.2659]],\n",
      "\n",
      "         [[-0.2349,  0.7160, -1.5628, -1.1322],\n",
      "          [-0.2349,  0.7160, -1.5628, -1.1322]],\n",
      "\n",
      "         [[-0.1235,  0.6248, -1.4255, -0.5651],\n",
      "          [-0.1235,  0.6248, -1.4255, -0.5651]]]], grad_fn=<ViewBackward0>)\n",
      "transposed xq: torch.Size([1, 2, 3, 4])\n",
      "tensor([[[[-0.1698, -0.1404,  0.3430, -0.3522],\n",
      "          [ 0.2193,  0.7276, -0.2796, -0.0671],\n",
      "          [-0.1914,  0.1136, -0.6634, -0.0628]],\n",
      "\n",
      "         [[-0.2187,  0.0331,  0.0952, -0.8611],\n",
      "          [-0.2838,  0.2421,  0.5956, -1.0546],\n",
      "          [-0.0439, -0.2563, -0.3516, -1.4762]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "transposed xk: torch.Size([1, 2, 3, 4])\n",
      "tensor([[[[ 0.5153,  0.2608,  0.8424, -0.6574],\n",
      "          [-0.4476,  0.9713,  0.4569, -0.4485],\n",
      "          [-0.9178,  0.6548, -0.5754, -0.5868]],\n",
      "\n",
      "         [[ 0.5153,  0.2608,  0.8424, -0.6574],\n",
      "          [-0.4476,  0.9713,  0.4569, -0.4485],\n",
      "          [-0.9178,  0.6548, -0.5754, -0.5868]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "transposed xv: torch.Size([1, 2, 3, 4])\n",
      "tensor([[[[-0.0896,  0.8520, -1.3164, -0.2659],\n",
      "          [-0.2349,  0.7160, -1.5628, -1.1322],\n",
      "          [-0.1235,  0.6248, -1.4255, -0.5651]],\n",
      "\n",
      "         [[-0.0896,  0.8520, -1.3164, -0.2659],\n",
      "          [-0.2349,  0.7160, -1.5628, -1.1322],\n",
      "          [-0.1235,  0.6248, -1.4255, -0.5651]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[ 0.1982,  0.1272,  0.0366],\n",
      "          [ 0.0557,  0.2555,  0.2377],\n",
      "          [-0.2933, -0.0395,  0.3343]],\n",
      "\n",
      "         [[ 0.2711,  0.2799,  0.3364],\n",
      "          [ 0.5559,  0.5536,  0.3476],\n",
      "          [ 0.2924,  0.1361,  0.4705]]]], grad_fn=<MulBackward0>)\n",
      "mask: torch.Size([1, 1, 3, 3])\n",
      "tensor([[[[ 0.0000e+00, -2.3820e+38, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]]]])\n",
      "masked scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[ 1.9817e-01, -2.3820e+38, -2.3820e+38],\n",
      "          [ 5.5672e-02,  2.5545e-01, -2.3820e+38],\n",
      "          [-2.9327e-01, -3.9469e-02,  3.3432e-01]],\n",
      "\n",
      "         [[ 2.7111e-01, -2.3820e+38, -2.3820e+38],\n",
      "          [ 5.5595e-01,  5.5364e-01, -2.3820e+38],\n",
      "          [ 2.9241e-01,  1.3607e-01,  4.7047e-01]]]], grad_fn=<AddBackward0>)\n",
      "softmaxed scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [0.4502, 0.5498, 0.0000],\n",
      "          [0.2403, 0.3097, 0.4500]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.5006, 0.4994, 0.0000],\n",
      "          [0.3278, 0.2804, 0.3918]]]], grad_fn=<SoftmaxBackward0>)\n",
      "attention: torch.Size([1, 2, 3, 4])\n",
      "tensor([[[[-0.0896,  0.8520, -1.3164, -0.2659],\n",
      "          [-0.1695,  0.7772, -1.4519, -0.7421],\n",
      "          [-0.1499,  0.7076, -1.4418, -0.6688]],\n",
      "\n",
      "         [[-0.0896,  0.8520, -1.3164, -0.2659],\n",
      "          [-0.1622,  0.7840, -1.4394, -0.6985],\n",
      "          [-0.1436,  0.7248, -1.4282, -0.6260]]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "reshaped attention: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.0896,  0.8520, -1.3164, -0.2659, -0.0896,  0.8520, -1.3164,\n",
      "          -0.2659],\n",
      "         [-0.1695,  0.7772, -1.4519, -0.7421, -0.1622,  0.7840, -1.4394,\n",
      "          -0.6985],\n",
      "         [-0.1499,  0.7076, -1.4418, -0.6688, -0.1436,  0.7248, -1.4282,\n",
      "          -0.6260]]], grad_fn=<ViewBackward0>)\n",
      "self.Wo: torch.Size([8, 8])\n",
      "Parameter containing:\n",
      "tensor([[ 0.3231,  0.0792, -0.0726,  0.0410, -0.3169,  0.1623, -0.3217, -0.2931],\n",
      "        [ 0.1463, -0.2797,  0.2616, -0.2893, -0.3044,  0.2440,  0.0803,  0.0721],\n",
      "        [-0.2530,  0.2275,  0.2466,  0.2761, -0.3468, -0.0675, -0.1488,  0.1003],\n",
      "        [-0.1378, -0.2843, -0.1344, -0.1738, -0.2921, -0.1149,  0.2788,  0.0617],\n",
      "        [ 0.3457, -0.1351, -0.2053,  0.2984,  0.1184, -0.0856, -0.0667,  0.1067],\n",
      "        [-0.3129,  0.0139,  0.2070, -0.2315,  0.1270,  0.0176, -0.2821, -0.0126],\n",
      "        [-0.0456, -0.2981,  0.3328, -0.1692,  0.0312, -0.1484, -0.0794, -0.2105],\n",
      "        [ 0.1036, -0.3221, -0.1057, -0.1228, -0.2304,  0.2234,  0.0985,  0.2627]],\n",
      "       requires_grad=True)\n",
      "spliced Wo: torch.Size([8, 8])\n",
      "tensor([[ 0.3231,  0.0792, -0.0726,  0.0410, -0.3169,  0.1623, -0.3217, -0.2931],\n",
      "        [ 0.1463, -0.2797,  0.2616, -0.2893, -0.3044,  0.2440,  0.0803,  0.0721],\n",
      "        [-0.2530,  0.2275,  0.2466,  0.2761, -0.3468, -0.0675, -0.1488,  0.1003],\n",
      "        [-0.1378, -0.2843, -0.1344, -0.1738, -0.2921, -0.1149,  0.2788,  0.0617],\n",
      "        [ 0.3457, -0.1351, -0.2053,  0.2984,  0.1184, -0.0856, -0.0667,  0.1067],\n",
      "        [-0.3129,  0.0139,  0.2070, -0.2315,  0.1270,  0.0176, -0.2821, -0.0126],\n",
      "        [-0.0456, -0.2981,  0.3328, -0.1692,  0.0312, -0.1484, -0.0794, -0.2105],\n",
      "        [ 0.1036, -0.3221, -0.1057, -0.1228, -0.2304,  0.2234,  0.0985,  0.2627]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "projected output: torch.Size([1, 3, 8])\n",
      "tensor([[[ 0.2002,  0.0327, -0.2748, -0.5360,  0.4211,  0.4714,  0.0629,\n",
      "           0.1262],\n",
      "         [ 0.2203,  0.3368, -0.2523, -0.4043,  0.7339,  0.4307, -0.0389,\n",
      "           0.0066],\n",
      "         [ 0.2358,  0.3092, -0.2993, -0.3848,  0.7023,  0.4197, -0.0101,\n",
      "           0.0208]]], grad_fn=<UnsafeViewBackward0>)\n",
      "----------------- END MultiQueryAttention.forwardTensor() --------------------\n",
      "x in layer after MQA & resid connection and before MLP:\n",
      "tensor([[[ 0.6254,  0.5686,  0.2455, -0.1786,  1.0857,  0.9176,  0.1222,\n",
      "           0.6067],\n",
      "         [ 0.4968,  0.6073,  0.7072, -0.3335,  1.3172,  0.4321,  0.8052,\n",
      "           0.3167],\n",
      "         [ 1.2139,  0.8981,  0.4054,  0.5356,  0.7989,  1.0758,  0.8781,\n",
      "           0.2116]]], grad_fn=<AddBackward0>)\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 3, 8])\n",
      "tensor([[[ 0.6254,  0.5686,  0.2455, -0.1786,  1.0857,  0.9176,  0.1222,\n",
      "           0.6067],\n",
      "         [ 0.4968,  0.6073,  0.7072, -0.3335,  1.3172,  0.4321,  0.8052,\n",
      "           0.3167],\n",
      "         [ 1.2139,  0.8981,  0.4054,  0.5356,  0.7989,  1.0758,  0.8781,\n",
      "           0.2116]]], grad_fn=<AddBackward0>)\n",
      "normed x: torch.Size([1, 3, 8])\n",
      "tensor([[[ 0.9873,  0.8975,  0.3876, -0.2819,  1.7138,  1.4486,  0.1929,\n",
      "           0.9578],\n",
      "         [ 0.7118,  0.8701,  1.0132, -0.4778,  1.8871,  0.6191,  1.1537,\n",
      "           0.4538],\n",
      "         [ 1.4854,  1.0990,  0.4961,  0.6554,  0.9776,  1.3164,  1.0745,\n",
      "           0.2589]]], grad_fn=<MulBackward0>)\n",
      "dim: 8\n",
      "skip: 0\n",
      "spliced scale: torch.Size([8])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 3, 8])\n",
      "tensor([[[ 0.9873,  0.8975,  0.3876, -0.2819,  1.7138,  1.4486,  0.1929,\n",
      "           0.9578],\n",
      "         [ 0.7118,  0.8701,  1.0132, -0.4778,  1.8871,  0.6191,  1.1537,\n",
      "           0.4538],\n",
      "         [ 1.4854,  1.0990,  0.4961,  0.6554,  0.9776,  1.3164,  1.0745,\n",
      "           0.2589]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "---------- MLP Input: torch.Tensor ------------\n",
      "------------- MLP.forwardTensor() ------------\n",
      "x: torch.Size([1, 3, 8])\n",
      "tensor([[[ 0.9873,  0.8975,  0.3876, -0.2819,  1.7138,  1.4486,  0.1929,\n",
      "           0.9578],\n",
      "         [ 0.7118,  0.8701,  1.0132, -0.4778,  1.8871,  0.6191,  1.1537,\n",
      "           0.4538],\n",
      "         [ 1.4854,  1.0990,  0.4961,  0.6554,  0.9776,  1.3164,  1.0745,\n",
      "           0.2589]]], grad_fn=<MulBackward0>)\n",
      "d_dim: 8\n",
      "d_skip: 0\n",
      "i_dim: 32\n",
      "i_skip: 0\n",
      "Wgate: torch.Size([8, 32])\n",
      "Parameter containing:\n",
      "tensor([[-0.2253, -0.0942, -0.1739,  0.1223, -0.3122, -0.0472, -0.3177,  0.2355,\n",
      "          0.1633, -0.3099, -0.1567,  0.0595,  0.0594, -0.2019, -0.2907,  0.0277,\n",
      "         -0.1077, -0.3026,  0.0199,  0.2187,  0.2138,  0.0391, -0.1834,  0.0290,\n",
      "         -0.2029, -0.0681, -0.3417, -0.1087,  0.1296,  0.1340, -0.3009,  0.2096],\n",
      "        [-0.1029, -0.0506, -0.0559, -0.3362, -0.1875, -0.1746, -0.2482, -0.1368,\n",
      "          0.1157,  0.1092, -0.1877, -0.3398,  0.2589,  0.1504,  0.1069, -0.1226,\n",
      "          0.3397, -0.0733, -0.1991,  0.2203, -0.0422, -0.3144,  0.3479,  0.3499,\n",
      "          0.3111,  0.0425, -0.1421, -0.2463,  0.0717,  0.2154,  0.3065,  0.1737],\n",
      "        [-0.3137,  0.1846,  0.2406,  0.1422, -0.0681, -0.0235,  0.2002,  0.2524,\n",
      "          0.0433, -0.0196, -0.2003, -0.1746,  0.3029, -0.0426,  0.3489, -0.1367,\n",
      "         -0.0828, -0.2655, -0.2109, -0.0431, -0.1461, -0.0946,  0.0888, -0.3361,\n",
      "          0.1122, -0.3369, -0.0771, -0.1618, -0.1782, -0.3305,  0.2334,  0.3270],\n",
      "        [ 0.1425, -0.3161, -0.2632,  0.3119, -0.2202, -0.2759,  0.2894,  0.0915,\n",
      "         -0.2918, -0.2535,  0.1432, -0.1125, -0.0448,  0.1906,  0.0717, -0.2443,\n",
      "         -0.0999, -0.3385,  0.0890,  0.1166, -0.1593,  0.1142,  0.2858, -0.1247,\n",
      "          0.2186, -0.1527,  0.0072,  0.1098,  0.2037,  0.2873,  0.0333,  0.2475],\n",
      "        [ 0.2482,  0.1420,  0.2411, -0.0312,  0.3487,  0.0235,  0.2866, -0.2140,\n",
      "         -0.1934, -0.1181,  0.1216, -0.0273,  0.2027, -0.1037, -0.2801, -0.0608,\n",
      "          0.1280, -0.1106, -0.2391, -0.1774,  0.3342, -0.0157, -0.1711,  0.3055,\n",
      "          0.0192, -0.1092, -0.0897, -0.1382, -0.3260,  0.2727, -0.1133, -0.1444],\n",
      "        [-0.0749,  0.2898,  0.1283,  0.2650,  0.0150, -0.0325,  0.0828, -0.1081,\n",
      "          0.2195,  0.1805,  0.2983, -0.1887, -0.0721, -0.0884,  0.3524, -0.0493,\n",
      "          0.0056,  0.1943, -0.1520,  0.0914,  0.1661,  0.2625,  0.0276,  0.2007,\n",
      "         -0.2136,  0.0038,  0.0404, -0.3031, -0.1449, -0.0124,  0.0944,  0.2827],\n",
      "        [-0.3255, -0.1543,  0.3073,  0.1153, -0.1858,  0.0160,  0.2381,  0.1282,\n",
      "          0.2882,  0.2682,  0.3404,  0.2716,  0.1817,  0.2988,  0.0991,  0.0495,\n",
      "          0.0053,  0.0730,  0.1288,  0.0947,  0.3234, -0.1249,  0.3368,  0.0893,\n",
      "          0.0203, -0.1674, -0.1204, -0.3327,  0.0792, -0.1504, -0.0432,  0.2653],\n",
      "        [-0.0954, -0.0387, -0.3197, -0.2788,  0.0892, -0.1148,  0.3169, -0.0280,\n",
      "          0.2408,  0.0759, -0.2557,  0.3145,  0.0125, -0.1799, -0.0451, -0.2109,\n",
      "          0.1505,  0.2687, -0.3414, -0.0593, -0.1065, -0.1653, -0.2685, -0.3471,\n",
      "         -0.3362,  0.2929, -0.0271, -0.1437,  0.2422, -0.1064,  0.2796,  0.1572]],\n",
      "       requires_grad=True)\n",
      "Wgate spliced: torch.Size([8, 32])\n",
      "tensor([[-0.2253, -0.0942, -0.1739,  0.1223, -0.3122, -0.0472, -0.3177,  0.2355,\n",
      "          0.1633, -0.3099, -0.1567,  0.0595,  0.0594, -0.2019, -0.2907,  0.0277,\n",
      "         -0.1077, -0.3026,  0.0199,  0.2187,  0.2138,  0.0391, -0.1834,  0.0290,\n",
      "         -0.2029, -0.0681, -0.3417, -0.1087,  0.1296,  0.1340, -0.3009,  0.2096],\n",
      "        [-0.1029, -0.0506, -0.0559, -0.3362, -0.1875, -0.1746, -0.2482, -0.1368,\n",
      "          0.1157,  0.1092, -0.1877, -0.3398,  0.2589,  0.1504,  0.1069, -0.1226,\n",
      "          0.3397, -0.0733, -0.1991,  0.2203, -0.0422, -0.3144,  0.3479,  0.3499,\n",
      "          0.3111,  0.0425, -0.1421, -0.2463,  0.0717,  0.2154,  0.3065,  0.1737],\n",
      "        [-0.3137,  0.1846,  0.2406,  0.1422, -0.0681, -0.0235,  0.2002,  0.2524,\n",
      "          0.0433, -0.0196, -0.2003, -0.1746,  0.3029, -0.0426,  0.3489, -0.1367,\n",
      "         -0.0828, -0.2655, -0.2109, -0.0431, -0.1461, -0.0946,  0.0888, -0.3361,\n",
      "          0.1122, -0.3369, -0.0771, -0.1618, -0.1782, -0.3305,  0.2334,  0.3270],\n",
      "        [ 0.1425, -0.3161, -0.2632,  0.3119, -0.2202, -0.2759,  0.2894,  0.0915,\n",
      "         -0.2918, -0.2535,  0.1432, -0.1125, -0.0448,  0.1906,  0.0717, -0.2443,\n",
      "         -0.0999, -0.3385,  0.0890,  0.1166, -0.1593,  0.1142,  0.2858, -0.1247,\n",
      "          0.2186, -0.1527,  0.0072,  0.1098,  0.2037,  0.2873,  0.0333,  0.2475],\n",
      "        [ 0.2482,  0.1420,  0.2411, -0.0312,  0.3487,  0.0235,  0.2866, -0.2140,\n",
      "         -0.1934, -0.1181,  0.1216, -0.0273,  0.2027, -0.1037, -0.2801, -0.0608,\n",
      "          0.1280, -0.1106, -0.2391, -0.1774,  0.3342, -0.0157, -0.1711,  0.3055,\n",
      "          0.0192, -0.1092, -0.0897, -0.1382, -0.3260,  0.2727, -0.1133, -0.1444],\n",
      "        [-0.0749,  0.2898,  0.1283,  0.2650,  0.0150, -0.0325,  0.0828, -0.1081,\n",
      "          0.2195,  0.1805,  0.2983, -0.1887, -0.0721, -0.0884,  0.3524, -0.0493,\n",
      "          0.0056,  0.1943, -0.1520,  0.0914,  0.1661,  0.2625,  0.0276,  0.2007,\n",
      "         -0.2136,  0.0038,  0.0404, -0.3031, -0.1449, -0.0124,  0.0944,  0.2827],\n",
      "        [-0.3255, -0.1543,  0.3073,  0.1153, -0.1858,  0.0160,  0.2381,  0.1282,\n",
      "          0.2882,  0.2682,  0.3404,  0.2716,  0.1817,  0.2988,  0.0991,  0.0495,\n",
      "          0.0053,  0.0730,  0.1288,  0.0947,  0.3234, -0.1249,  0.3368,  0.0893,\n",
      "          0.0203, -0.1674, -0.1204, -0.3327,  0.0792, -0.1504, -0.0432,  0.2653],\n",
      "        [-0.0954, -0.0387, -0.3197, -0.2788,  0.0892, -0.1148,  0.3169, -0.0280,\n",
      "          0.2408,  0.0759, -0.2557,  0.3145,  0.0125, -0.1799, -0.0451, -0.2109,\n",
      "          0.1505,  0.2687, -0.3414, -0.0593, -0.1065, -0.1653, -0.2685, -0.3471,\n",
      "         -0.3362,  0.2929, -0.0271, -0.1437,  0.2422, -0.1064,  0.2796,  0.1572]],\n",
      "       grad_fn=<AliasBackward0>)\n",
      "Bgate: torch.Size([32])\n",
      "Parameter containing:\n",
      "tensor([ 0.1340,  0.0426, -0.0416, -0.0498,  0.1808, -0.3497, -0.0203, -0.0232,\n",
      "         0.2109, -0.2864, -0.0186, -0.1914, -0.1441, -0.1883,  0.0004,  0.2480,\n",
      "        -0.1039, -0.0139,  0.3289, -0.3526,  0.0490, -0.1870, -0.3404,  0.2461,\n",
      "        -0.0849,  0.0947, -0.0388,  0.1960, -0.3376,  0.0430, -0.3474, -0.1823],\n",
      "       requires_grad=True)\n",
      "Bgate spliced: torch.Size([32])\n",
      "tensor([ 0.1340,  0.0426, -0.0416, -0.0498,  0.1808, -0.3497, -0.0203, -0.0232,\n",
      "         0.2109, -0.2864, -0.0186, -0.1914, -0.1441, -0.1883,  0.0004,  0.2480,\n",
      "        -0.1039, -0.0139,  0.3289, -0.3526,  0.0490, -0.1870, -0.3404,  0.2461,\n",
      "        -0.0849,  0.0947, -0.0388,  0.1960, -0.3376,  0.0430, -0.3474, -0.1823],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xgate: torch.Size([1, 3, 32])\n",
      "tensor([[[-0.1799,  0.6611,  0.2560, -0.1780,  0.4089, -0.5980,  0.3999,\n",
      "          -0.3669,  0.8477, -0.2469,  0.0016, -0.4402,  0.5668, -0.7434,\n",
      "          -0.0692, -0.1868,  0.4635, -0.0225, -0.8692, -0.1986,  0.9841,\n",
      "          -0.3286, -0.7007,  0.9926, -0.6187,  0.0447, -0.6801, -1.1038,\n",
      "          -0.7930,  0.4778, -0.0863,  0.6013],\n",
      "         [-0.4987,  0.5213,  0.8993, -0.1486,  0.3251, -0.4366,  0.6129,\n",
      "          -0.0982,  0.8239, -0.0779,  0.1261, -0.2802,  1.0049, -0.3228,\n",
      "          -0.0107, -0.0445,  0.3983, -0.2826, -0.6381, -0.3004,  1.1508,\n",
      "          -0.6694, -0.2538,  0.9364, -0.1745, -0.4491, -0.7827, -1.2096,\n",
      "          -0.9644,  0.1386, -0.1526,  0.6110],\n",
      "         [-0.6065,  0.0759,  0.2374,  0.4075, -0.4830, -0.8365,  0.2511,\n",
      "           0.1405,  0.8829, -0.3726,  0.3480, -0.5387,  0.6513, -0.1625,\n",
      "           0.1911, -0.1992,  0.1801, -0.6018, -0.2905,  0.3027,  1.0085,\n",
      "          -0.2934,  0.1625,  0.9940, -0.1731, -0.4327, -0.9070, -1.1733,\n",
      "          -0.3828,  0.5640, -0.2804,  1.2010]]], grad_fn=<AddBackward0>)\n",
      "GeLU'ed Xgate: torch.Size([1, 3, 32])\n",
      "tensor([[[-7.7110e-02,  4.9300e-01,  1.5386e-01, -7.6426e-02,  2.6934e-01,\n",
      "          -1.6440e-01,  2.6205e-01, -1.3093e-01,  6.7964e-01, -9.9388e-02,\n",
      "           8.0484e-04, -1.4522e-01,  4.0505e-01, -1.6996e-01, -3.2669e-02,\n",
      "          -7.9555e-02,  3.1446e-01, -1.1047e-02, -1.6721e-01, -8.3665e-02,\n",
      "           8.2418e-01, -1.2199e-01, -1.6939e-01,  8.3334e-01, -1.6584e-01,\n",
      "           2.3143e-02, -1.6881e-01, -1.4883e-01, -1.6961e-01,  3.2667e-01,\n",
      "          -4.0167e-02,  4.3666e-01],\n",
      "         [-1.5410e-01,  3.6437e-01,  7.3359e-01, -6.5511e-02,  2.0395e-01,\n",
      "          -1.4460e-01,  4.4742e-01, -4.5268e-02,  6.5496e-01, -3.6541e-02,\n",
      "           6.9388e-02, -1.0919e-01,  8.4666e-01, -1.2053e-01, -5.3032e-03,\n",
      "          -2.1453e-02,  2.6084e-01, -1.0986e-01, -1.6699e-01, -1.1473e-01,\n",
      "           1.0070e+00, -1.6844e-01, -1.0147e-01,  7.7294e-01, -7.5166e-02,\n",
      "          -1.4671e-01, -1.6977e-01, -1.3695e-01, -1.6146e-01,  7.6959e-02,\n",
      "          -6.7052e-02,  4.4569e-01],\n",
      "         [-1.6502e-01,  4.0246e-02,  1.4101e-01,  2.6817e-01, -1.5193e-01,\n",
      "          -1.6850e-01,  1.5042e-01,  7.8095e-02,  7.1630e-01, -1.3217e-01,\n",
      "           2.2136e-01, -1.5894e-01,  4.8365e-01, -7.0774e-02,  1.1006e-01,\n",
      "          -8.3863e-02,  1.0294e-01, -1.6468e-01, -1.1206e-01,  1.8738e-01,\n",
      "           8.5055e-01, -1.1283e-01,  9.1731e-02,  8.3485e-01, -7.4645e-02,\n",
      "          -1.4392e-01, -1.6526e-01, -1.4119e-01, -1.3434e-01,  4.0249e-01,\n",
      "          -1.0925e-01,  1.0631e+00]]], grad_fn=<GeluBackward0>)\n",
      "Wup: torch.Size([8, 32])\n",
      "Parameter containing:\n",
      "tensor([[ 0.2694,  0.2090,  0.2224,  0.0314, -0.0881, -0.3446, -0.3520, -0.3037,\n",
      "         -0.0974,  0.2631, -0.2654,  0.1230,  0.0608, -0.2541,  0.2293, -0.0355,\n",
      "          0.1259, -0.3394,  0.0881, -0.3230,  0.3431, -0.1350, -0.3247,  0.1908,\n",
      "          0.1178,  0.2885, -0.2282,  0.3478,  0.0131,  0.3076,  0.3356, -0.0739],\n",
      "        [-0.1364, -0.1351,  0.0415, -0.2112, -0.1075,  0.1884, -0.1701, -0.1844,\n",
      "         -0.0726,  0.2485, -0.3356,  0.0664,  0.0762,  0.0902,  0.3420, -0.0746,\n",
      "         -0.2891,  0.0713,  0.2431, -0.0975,  0.2750,  0.1843, -0.0998,  0.2408,\n",
      "         -0.0088,  0.1778,  0.1630,  0.1856, -0.0011,  0.0808, -0.2354, -0.2323],\n",
      "        [ 0.3393, -0.2568, -0.2447,  0.0434,  0.2395,  0.2249, -0.1244, -0.0266,\n",
      "          0.0458, -0.0124, -0.0905,  0.1546,  0.2968,  0.3111, -0.3450, -0.2358,\n",
      "         -0.1691,  0.0774, -0.0644,  0.2968, -0.2243,  0.1071,  0.3067, -0.0365,\n",
      "          0.0812,  0.2711, -0.0832,  0.1553,  0.3444, -0.1518,  0.0353,  0.0339],\n",
      "        [ 0.2659, -0.3129, -0.1467, -0.0811, -0.2241,  0.0067,  0.3485,  0.3012,\n",
      "          0.0315,  0.2751, -0.0993, -0.1917,  0.3388, -0.3426,  0.0144,  0.2667,\n",
      "          0.0268, -0.1934, -0.2730,  0.0007, -0.1126,  0.2761, -0.1201, -0.2516,\n",
      "          0.1440, -0.2378,  0.0451,  0.1775, -0.2541, -0.2220,  0.2895,  0.2816],\n",
      "        [-0.0885,  0.2126, -0.0575,  0.1008,  0.0083, -0.3009, -0.3247, -0.0037,\n",
      "          0.1443,  0.2243, -0.1833, -0.1049,  0.2278,  0.1890,  0.2420, -0.1615,\n",
      "          0.0973, -0.2231, -0.2922,  0.0540, -0.1200, -0.0504, -0.1222, -0.2243,\n",
      "          0.1517,  0.3157, -0.3429,  0.1561, -0.0987, -0.2596, -0.0195,  0.2445],\n",
      "        [ 0.1952,  0.2877, -0.0972, -0.2198,  0.0062, -0.2613,  0.1507, -0.2280,\n",
      "          0.2732, -0.1391,  0.0169,  0.3409, -0.2260,  0.0110,  0.0010, -0.0854,\n",
      "         -0.0017, -0.1985, -0.2076,  0.3235, -0.0708,  0.2334, -0.2418,  0.2454,\n",
      "         -0.0364,  0.1629,  0.1777,  0.0096, -0.0326, -0.1905, -0.0195, -0.2707],\n",
      "        [ 0.0398,  0.2032,  0.2466, -0.0374,  0.3399, -0.1091,  0.0731,  0.3247,\n",
      "          0.2770,  0.1173,  0.0937,  0.0460,  0.1209, -0.0688,  0.2715, -0.2920,\n",
      "         -0.2797, -0.2882, -0.2972, -0.2037,  0.1482,  0.1054,  0.1429, -0.0405,\n",
      "          0.1746, -0.1669,  0.2211,  0.1831,  0.0261, -0.3459,  0.0688, -0.1664],\n",
      "        [ 0.1560, -0.0571,  0.0605, -0.1945,  0.1294, -0.0635, -0.2500, -0.0588,\n",
      "          0.0429, -0.2159,  0.0710,  0.1439,  0.1511, -0.0391,  0.1689,  0.2573,\n",
      "          0.3257,  0.3120, -0.0642, -0.3137,  0.2976, -0.2592, -0.2000,  0.2657,\n",
      "         -0.2475,  0.2569,  0.3219, -0.1574,  0.2960, -0.0120, -0.1554,  0.3425]],\n",
      "       requires_grad=True)\n",
      "Wup spliced: torch.Size([8, 32])\n",
      "tensor([[ 0.2694,  0.2090,  0.2224,  0.0314, -0.0881, -0.3446, -0.3520, -0.3037,\n",
      "         -0.0974,  0.2631, -0.2654,  0.1230,  0.0608, -0.2541,  0.2293, -0.0355,\n",
      "          0.1259, -0.3394,  0.0881, -0.3230,  0.3431, -0.1350, -0.3247,  0.1908,\n",
      "          0.1178,  0.2885, -0.2282,  0.3478,  0.0131,  0.3076,  0.3356, -0.0739],\n",
      "        [-0.1364, -0.1351,  0.0415, -0.2112, -0.1075,  0.1884, -0.1701, -0.1844,\n",
      "         -0.0726,  0.2485, -0.3356,  0.0664,  0.0762,  0.0902,  0.3420, -0.0746,\n",
      "         -0.2891,  0.0713,  0.2431, -0.0975,  0.2750,  0.1843, -0.0998,  0.2408,\n",
      "         -0.0088,  0.1778,  0.1630,  0.1856, -0.0011,  0.0808, -0.2354, -0.2323],\n",
      "        [ 0.3393, -0.2568, -0.2447,  0.0434,  0.2395,  0.2249, -0.1244, -0.0266,\n",
      "          0.0458, -0.0124, -0.0905,  0.1546,  0.2968,  0.3111, -0.3450, -0.2358,\n",
      "         -0.1691,  0.0774, -0.0644,  0.2968, -0.2243,  0.1071,  0.3067, -0.0365,\n",
      "          0.0812,  0.2711, -0.0832,  0.1553,  0.3444, -0.1518,  0.0353,  0.0339],\n",
      "        [ 0.2659, -0.3129, -0.1467, -0.0811, -0.2241,  0.0067,  0.3485,  0.3012,\n",
      "          0.0315,  0.2751, -0.0993, -0.1917,  0.3388, -0.3426,  0.0144,  0.2667,\n",
      "          0.0268, -0.1934, -0.2730,  0.0007, -0.1126,  0.2761, -0.1201, -0.2516,\n",
      "          0.1440, -0.2378,  0.0451,  0.1775, -0.2541, -0.2220,  0.2895,  0.2816],\n",
      "        [-0.0885,  0.2126, -0.0575,  0.1008,  0.0083, -0.3009, -0.3247, -0.0037,\n",
      "          0.1443,  0.2243, -0.1833, -0.1049,  0.2278,  0.1890,  0.2420, -0.1615,\n",
      "          0.0973, -0.2231, -0.2922,  0.0540, -0.1200, -0.0504, -0.1222, -0.2243,\n",
      "          0.1517,  0.3157, -0.3429,  0.1561, -0.0987, -0.2596, -0.0195,  0.2445],\n",
      "        [ 0.1952,  0.2877, -0.0972, -0.2198,  0.0062, -0.2613,  0.1507, -0.2280,\n",
      "          0.2732, -0.1391,  0.0169,  0.3409, -0.2260,  0.0110,  0.0010, -0.0854,\n",
      "         -0.0017, -0.1985, -0.2076,  0.3235, -0.0708,  0.2334, -0.2418,  0.2454,\n",
      "         -0.0364,  0.1629,  0.1777,  0.0096, -0.0326, -0.1905, -0.0195, -0.2707],\n",
      "        [ 0.0398,  0.2032,  0.2466, -0.0374,  0.3399, -0.1091,  0.0731,  0.3247,\n",
      "          0.2770,  0.1173,  0.0937,  0.0460,  0.1209, -0.0688,  0.2715, -0.2920,\n",
      "         -0.2797, -0.2882, -0.2972, -0.2037,  0.1482,  0.1054,  0.1429, -0.0405,\n",
      "          0.1746, -0.1669,  0.2211,  0.1831,  0.0261, -0.3459,  0.0688, -0.1664],\n",
      "        [ 0.1560, -0.0571,  0.0605, -0.1945,  0.1294, -0.0635, -0.2500, -0.0588,\n",
      "          0.0429, -0.2159,  0.0710,  0.1439,  0.1511, -0.0391,  0.1689,  0.2573,\n",
      "          0.3257,  0.3120, -0.0642, -0.3137,  0.2976, -0.2592, -0.2000,  0.2657,\n",
      "         -0.2475,  0.2569,  0.3219, -0.1574,  0.2960, -0.0120, -0.1554,  0.3425]],\n",
      "       grad_fn=<AliasBackward0>)\n",
      "Bup: torch.Size([32])\n",
      "Parameter containing:\n",
      "tensor([-0.2800,  0.1111, -0.3534, -0.0763, -0.3493, -0.2513,  0.0744, -0.0964,\n",
      "         0.3045, -0.1034,  0.1192, -0.0205,  0.1498,  0.1536,  0.1143, -0.1144,\n",
      "         0.1250,  0.1437,  0.2615, -0.2756,  0.0379, -0.1557,  0.3279, -0.1350,\n",
      "        -0.1293, -0.2682,  0.0448, -0.1766, -0.0850,  0.1643, -0.1031,  0.1994],\n",
      "       requires_grad=True)\n",
      "Bup spliced: torch.Size([32])\n",
      "tensor([-0.2800,  0.1111, -0.3534, -0.0763, -0.3493, -0.2513,  0.0744, -0.0964,\n",
      "         0.3045, -0.1034,  0.1192, -0.0205,  0.1498,  0.1536,  0.1143, -0.1144,\n",
      "         0.1250,  0.1437,  0.2615, -0.2756,  0.0379, -0.1557,  0.3279, -0.1350,\n",
      "        -0.1293, -0.2682,  0.0448, -0.1766, -0.0850,  0.1643, -0.1031,  0.1994],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xup: torch.Size([1, 3, 32])\n",
      "tensor([[[ 0.2084,  0.9505, -0.2839, -0.5342, -0.1640, -1.3132, -1.1357,\n",
      "          -0.9873,  0.8894,  0.2960, -0.6549,  0.7352,  0.5288,  0.4901,\n",
      "           1.1403, -0.5934,  0.3392, -0.4697, -0.3016, -0.3457,  0.5736,\n",
      "          -0.1360, -0.6532,  0.5439, -0.0261,  1.3393, -0.0584,  0.5095,\n",
      "           0.2041, -0.2549, -0.2481,  0.1746],\n",
      "         [ 0.0805,  0.8194, -0.1936, -0.2322,  0.3146, -0.9922, -1.1653,\n",
      "          -0.4442,  0.9837,  0.5308, -0.6012,  0.5045,  0.8963,  0.7966,\n",
      "           1.0659, -1.1488, -0.2132, -0.6000, -0.4511, -0.3651,  0.3836,\n",
      "          -0.0614,  0.0719,  0.0959,  0.3133,  1.1009, -0.2175,  0.7454,\n",
      "           0.3516, -0.6066, -0.2116,  0.1017],\n",
      "         [ 0.5666,  0.7308, -0.0985, -0.5746, -0.2113, -1.2119, -0.5739,\n",
      "          -0.5361,  0.9327,  0.8412, -0.7919,  0.6191,  0.7874, -0.0796,\n",
      "           1.2425, -0.7089, -0.1951, -1.0789, -0.4462, -0.5362,  0.6904,\n",
      "           0.3846, -0.5267,  0.3590,  0.3946,  0.7448,  0.0929,  1.0586,\n",
      "          -0.0973, -0.3901,  0.3330, -0.1717]]], grad_fn=<AddBackward0>)\n",
      "Xfuse: torch.Size([1, 3, 32])\n",
      "tensor([[[-1.6071e-02,  4.6859e-01, -4.3687e-02,  4.0826e-02, -4.4182e-02,\n",
      "           2.1589e-01, -2.9762e-01,  1.2927e-01,  6.0447e-01, -2.9417e-02,\n",
      "          -5.2709e-04, -1.0677e-01,  2.1420e-01, -8.3294e-02, -3.7253e-02,\n",
      "           4.7208e-02,  1.0668e-01,  5.1882e-03,  5.0437e-02,  2.8922e-02,\n",
      "           4.7274e-01,  1.6587e-02,  1.1065e-01,  4.5324e-01,  4.3229e-03,\n",
      "           3.0995e-02,  9.8549e-03, -7.5831e-02, -3.4618e-02, -8.3275e-02,\n",
      "           9.9665e-03,  7.6227e-02],\n",
      "         [-1.2405e-02,  2.9858e-01, -1.4203e-01,  1.5212e-02,  6.4155e-02,\n",
      "           1.4348e-01, -5.2137e-01,  2.0107e-02,  6.4429e-01, -1.9395e-02,\n",
      "          -4.1716e-02, -5.5081e-02,  7.5886e-01, -9.6009e-02, -5.6527e-03,\n",
      "           2.4644e-02, -5.5601e-02,  6.5914e-02,  7.5323e-02,  4.1887e-02,\n",
      "           3.8629e-01,  1.0339e-02, -7.2971e-03,  7.4131e-02, -2.3546e-02,\n",
      "          -1.6151e-01,  3.6921e-02, -1.0208e-01, -5.6774e-02, -4.6686e-02,\n",
      "           1.4185e-02,  4.5312e-02],\n",
      "         [-9.3500e-02,  2.9410e-02, -1.3887e-02, -1.5410e-01,  3.2104e-02,\n",
      "           2.0421e-01, -8.6324e-02, -4.1870e-02,  6.6810e-01, -1.1119e-01,\n",
      "          -1.7529e-01, -9.8408e-02,  3.8081e-01,  5.6355e-03,  1.3675e-01,\n",
      "           5.9449e-02, -2.0081e-02,  1.7767e-01,  5.0004e-02, -1.0048e-01,\n",
      "           5.8723e-01, -4.3395e-02, -4.8312e-02,  2.9968e-01, -2.9457e-02,\n",
      "          -1.0719e-01, -1.5352e-02, -1.4946e-01,  1.3071e-02, -1.5702e-01,\n",
      "          -3.6378e-02, -1.8258e-01]]], grad_fn=<MulBackward0>)\n",
      "Wdown: torch.Size([32, 8])\n",
      "Parameter containing:\n",
      "tensor([[-0.0662, -0.0497,  0.1282, -0.0099, -0.1497,  0.1398,  0.1153, -0.0922],\n",
      "        [ 0.0940, -0.0708,  0.1204, -0.1536, -0.0525, -0.1485, -0.1747, -0.0760],\n",
      "        [-0.1579,  0.0872,  0.0195, -0.0521, -0.0528, -0.0915,  0.1351,  0.0485],\n",
      "        [ 0.0658, -0.0819, -0.0161, -0.0461, -0.0429,  0.0349, -0.0231, -0.1246],\n",
      "        [-0.0992,  0.1070,  0.0316, -0.0387, -0.1449,  0.0994, -0.0363,  0.0629],\n",
      "        [-0.0536,  0.0069,  0.0168, -0.0977, -0.1715, -0.1344, -0.1539,  0.0376],\n",
      "        [ 0.1571,  0.0231, -0.1748, -0.1292, -0.1407, -0.0998, -0.1521,  0.1191],\n",
      "        [ 0.1286,  0.1376, -0.1437,  0.1287, -0.0011, -0.1668, -0.0563, -0.1460],\n",
      "        [ 0.0951, -0.1202, -0.1513,  0.0141, -0.0386, -0.0350, -0.1101, -0.0481],\n",
      "        [ 0.0047, -0.0418,  0.1132, -0.0011,  0.0886,  0.1767, -0.0993, -0.0304],\n",
      "        [-0.0293, -0.0440, -0.0297,  0.1146, -0.0591, -0.0140, -0.0022, -0.1385],\n",
      "        [-0.0737, -0.1578, -0.0077,  0.1029, -0.0559,  0.0974,  0.0693, -0.0283],\n",
      "        [ 0.1645, -0.1357, -0.0935, -0.0086, -0.1506, -0.0398, -0.1063,  0.0367],\n",
      "        [-0.0172,  0.0597, -0.1303,  0.0593, -0.0218, -0.0397,  0.0715,  0.1233],\n",
      "        [ 0.1127, -0.1511,  0.0769, -0.0806, -0.0504,  0.0899, -0.1059,  0.0973],\n",
      "        [ 0.0157, -0.0281,  0.1047,  0.1501,  0.0402,  0.0765,  0.0445,  0.0918],\n",
      "        [-0.0127,  0.1472,  0.1426, -0.1400, -0.0564, -0.0698, -0.1071, -0.0536],\n",
      "        [ 0.0982,  0.0451, -0.0436, -0.1566, -0.0580, -0.0278, -0.1449,  0.0116],\n",
      "        [-0.1053, -0.0511, -0.0181, -0.0130,  0.1741, -0.1080,  0.1316,  0.1488],\n",
      "        [ 0.0755, -0.0631, -0.0551,  0.0769, -0.0762, -0.1259,  0.1186,  0.0197],\n",
      "        [ 0.0382, -0.1560, -0.1126, -0.1519,  0.1590,  0.0226,  0.1442, -0.0837],\n",
      "        [ 0.0325,  0.1690,  0.1348,  0.1244, -0.1351, -0.0658, -0.0567,  0.0945],\n",
      "        [-0.0321,  0.1019, -0.1525,  0.0979,  0.0724, -0.0328, -0.1059,  0.1522],\n",
      "        [-0.0505,  0.1071,  0.1407,  0.0656, -0.0321, -0.0523, -0.1015, -0.1255],\n",
      "        [ 0.1605, -0.1755,  0.0314,  0.0049,  0.1765,  0.1446,  0.0336,  0.1236],\n",
      "        [-0.0847, -0.1607, -0.1091, -0.0645,  0.0983,  0.1437, -0.1164,  0.0785],\n",
      "        [ 0.1346, -0.0749,  0.1166, -0.0364,  0.0900,  0.1451, -0.0801, -0.0740],\n",
      "        [-0.0752, -0.1551,  0.1561,  0.0287, -0.1579, -0.0300, -0.0980,  0.0445],\n",
      "        [-0.0379,  0.0519,  0.1398, -0.0204,  0.1170,  0.0857, -0.0250, -0.0831],\n",
      "        [-0.0334, -0.1752, -0.0639,  0.0265, -0.0708, -0.0991,  0.0746,  0.0497],\n",
      "        [-0.0145, -0.0233,  0.1339, -0.1685,  0.0031, -0.0939,  0.0791, -0.0504],\n",
      "        [-0.0268, -0.0262, -0.1623,  0.0086,  0.0611,  0.0728,  0.0775, -0.1734]],\n",
      "       requires_grad=True)\n",
      "Wdown spliced: torch.Size([32, 8])\n",
      "tensor([[-0.0662, -0.0497,  0.1282, -0.0099, -0.1497,  0.1398,  0.1153, -0.0922],\n",
      "        [ 0.0940, -0.0708,  0.1204, -0.1536, -0.0525, -0.1485, -0.1747, -0.0760],\n",
      "        [-0.1579,  0.0872,  0.0195, -0.0521, -0.0528, -0.0915,  0.1351,  0.0485],\n",
      "        [ 0.0658, -0.0819, -0.0161, -0.0461, -0.0429,  0.0349, -0.0231, -0.1246],\n",
      "        [-0.0992,  0.1070,  0.0316, -0.0387, -0.1449,  0.0994, -0.0363,  0.0629],\n",
      "        [-0.0536,  0.0069,  0.0168, -0.0977, -0.1715, -0.1344, -0.1539,  0.0376],\n",
      "        [ 0.1571,  0.0231, -0.1748, -0.1292, -0.1407, -0.0998, -0.1521,  0.1191],\n",
      "        [ 0.1286,  0.1376, -0.1437,  0.1287, -0.0011, -0.1668, -0.0563, -0.1460],\n",
      "        [ 0.0951, -0.1202, -0.1513,  0.0141, -0.0386, -0.0350, -0.1101, -0.0481],\n",
      "        [ 0.0047, -0.0418,  0.1132, -0.0011,  0.0886,  0.1767, -0.0993, -0.0304],\n",
      "        [-0.0293, -0.0440, -0.0297,  0.1146, -0.0591, -0.0140, -0.0022, -0.1385],\n",
      "        [-0.0737, -0.1578, -0.0077,  0.1029, -0.0559,  0.0974,  0.0693, -0.0283],\n",
      "        [ 0.1645, -0.1357, -0.0935, -0.0086, -0.1506, -0.0398, -0.1063,  0.0367],\n",
      "        [-0.0172,  0.0597, -0.1303,  0.0593, -0.0218, -0.0397,  0.0715,  0.1233],\n",
      "        [ 0.1127, -0.1511,  0.0769, -0.0806, -0.0504,  0.0899, -0.1059,  0.0973],\n",
      "        [ 0.0157, -0.0281,  0.1047,  0.1501,  0.0402,  0.0765,  0.0445,  0.0918],\n",
      "        [-0.0127,  0.1472,  0.1426, -0.1400, -0.0564, -0.0698, -0.1071, -0.0536],\n",
      "        [ 0.0982,  0.0451, -0.0436, -0.1566, -0.0580, -0.0278, -0.1449,  0.0116],\n",
      "        [-0.1053, -0.0511, -0.0181, -0.0130,  0.1741, -0.1080,  0.1316,  0.1488],\n",
      "        [ 0.0755, -0.0631, -0.0551,  0.0769, -0.0762, -0.1259,  0.1186,  0.0197],\n",
      "        [ 0.0382, -0.1560, -0.1126, -0.1519,  0.1590,  0.0226,  0.1442, -0.0837],\n",
      "        [ 0.0325,  0.1690,  0.1348,  0.1244, -0.1351, -0.0658, -0.0567,  0.0945],\n",
      "        [-0.0321,  0.1019, -0.1525,  0.0979,  0.0724, -0.0328, -0.1059,  0.1522],\n",
      "        [-0.0505,  0.1071,  0.1407,  0.0656, -0.0321, -0.0523, -0.1015, -0.1255],\n",
      "        [ 0.1605, -0.1755,  0.0314,  0.0049,  0.1765,  0.1446,  0.0336,  0.1236],\n",
      "        [-0.0847, -0.1607, -0.1091, -0.0645,  0.0983,  0.1437, -0.1164,  0.0785],\n",
      "        [ 0.1346, -0.0749,  0.1166, -0.0364,  0.0900,  0.1451, -0.0801, -0.0740],\n",
      "        [-0.0752, -0.1551,  0.1561,  0.0287, -0.1579, -0.0300, -0.0980,  0.0445],\n",
      "        [-0.0379,  0.0519,  0.1398, -0.0204,  0.1170,  0.0857, -0.0250, -0.0831],\n",
      "        [-0.0334, -0.1752, -0.0639,  0.0265, -0.0708, -0.0991,  0.0746,  0.0497],\n",
      "        [-0.0145, -0.0233,  0.1339, -0.1685,  0.0031, -0.0939,  0.0791, -0.0504],\n",
      "        [-0.0268, -0.0262, -0.1623,  0.0086,  0.0611,  0.0728,  0.0775, -0.1734]],\n",
      "       grad_fn=<AliasBackward0>)\n",
      "Bdown: torch.Size([8])\n",
      "Parameter containing:\n",
      "tensor([-0.1200, -0.0551,  0.0086,  0.0844,  0.0168, -0.0180,  0.0819,  0.0846],\n",
      "       requires_grad=True)\n",
      "Bdown spliced: torch.Size([8])\n",
      "tensor([-0.1200, -0.0551,  0.0086,  0.0844,  0.0168, -0.0180,  0.0819,  0.0846],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "outputs: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.0090, -0.1550, -0.0201, -0.0009,  0.0496, -0.1671, -0.0840,\n",
      "          -0.1242],\n",
      "         [ 0.0705, -0.2815, -0.0720,  0.0397, -0.0174, -0.0939, -0.0087,\n",
      "          -0.0617],\n",
      "         [ 0.0382, -0.1525, -0.1056, -0.0624,  0.0286, -0.1017, -0.0391,\n",
      "           0.0642]]], grad_fn=<AddBackward0>)\n",
      "------------- END MLP.forwardTensor() ------------\n",
      "layer's final residual state:\n",
      "tensor([[[ 0.6164,  0.4135,  0.2254, -0.1795,  1.1352,  0.7506,  0.0381,\n",
      "           0.4825],\n",
      "         [ 0.5673,  0.3258,  0.6352, -0.2937,  1.2997,  0.3382,  0.7965,\n",
      "           0.2551],\n",
      "         [ 1.2522,  0.7456,  0.2998,  0.4732,  0.8275,  0.9740,  0.8390,\n",
      "           0.2758]]], grad_fn=<AddBackward0>)\n",
      "----------------- END Layer.forwardTensor() --------------------\n",
      "y: torch.Size([1, 3, 8])\n",
      "tensor([[[ 0.6164,  0.4135,  0.2254, -0.1795,  1.1352,  0.7506,  0.0381,\n",
      "           0.4825],\n",
      "         [ 0.5673,  0.3258,  0.6352, -0.2937,  1.2997,  0.3382,  0.7965,\n",
      "           0.2551],\n",
      "         [ 1.2522,  0.7456,  0.2998,  0.4732,  0.8275,  0.9740,  0.8390,\n",
      "           0.2758]]], grad_fn=<AddBackward0>)\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the first sub-model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[0.4631, 0.4438, 0.3332, 0.7122],\n",
      "         [0.7312, 0.7130, 0.0594, 0.6392],\n",
      "         [0.6697, 0.5330, 0.5990, 0.0573]]])\n",
      "---------- Layer Input: torch.Tensor ------------\n",
      "----------------- Layer.forwardTensor() --------------------\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[0.4631, 0.4438, 0.3332, 0.7122],\n",
      "         [0.7312, 0.7130, 0.0594, 0.6392],\n",
      "         [0.6697, 0.5330, 0.5990, 0.0573]]])\n",
      "normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[0.9127, 0.8746, 0.6568, 1.4038],\n",
      "         [1.2123, 1.1822, 0.0984, 1.0597],\n",
      "         [1.2801, 1.0189, 1.1450, 0.1096]]])\n",
      "dim: 4\n",
      "skip: 0\n",
      "spliced scale: torch.Size([4])\n",
      "tensor([0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[0.9127, 0.8746, 0.6568, 1.4038],\n",
      "         [1.2123, 1.1822, 0.0984, 1.0597],\n",
      "         [1.2801, 1.0189, 1.1450, 0.1096]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "---------- Attention Input: torch.Tensor ------------\n",
      "----------------- MultiQueryAttention.forwardTensor() --------------------\n",
      "x shape: torch.Size([1, 3, 4])\n",
      "d_skip: 0\n",
      "models_in_this_level: 2\n",
      "h_dim: 2\n",
      "h_skip: 0\n",
      "self.Wqkv: torch.Size([8, 16])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0943,  0.0984,  0.1721, -0.2680, -0.1800,  0.1334, -0.0952, -0.1357,\n",
      "          0.1428,  0.0701, -0.1481, -0.0889,  0.2956,  0.3404,  0.2485,  0.0994],\n",
      "        [-0.0437, -0.0026, -0.3460,  0.2749, -0.2080, -0.0480,  0.0819, -0.1937,\n",
      "         -0.2697, -0.1477, -0.0126, -0.3187, -0.3133,  0.3372,  0.2664, -0.3299],\n",
      "        [ 0.3141, -0.2665, -0.2688,  0.3377,  0.0845, -0.0520,  0.3183,  0.1263,\n",
      "          0.1113, -0.1897, -0.1114, -0.1643, -0.0568, -0.0780, -0.0442, -0.3249],\n",
      "        [-0.0469,  0.2926,  0.1064,  0.0832, -0.2169, -0.3217, -0.2960, -0.2437,\n",
      "         -0.1881, -0.3368, -0.0630,  0.3261, -0.0982,  0.0958, -0.3081, -0.2083],\n",
      "        [ 0.1181, -0.0140, -0.1870, -0.1090, -0.1538, -0.3151,  0.0417,  0.1949,\n",
      "         -0.1996, -0.3047,  0.2557, -0.1691,  0.0115,  0.0761,  0.2231, -0.0160],\n",
      "        [ 0.3463, -0.2736, -0.0374,  0.2403,  0.2242, -0.1357, -0.0907,  0.0466,\n",
      "         -0.0280,  0.1888,  0.0920,  0.2207,  0.1851,  0.3017, -0.1728, -0.2112],\n",
      "        [ 0.2182, -0.2725, -0.3533,  0.1209, -0.2105, -0.0207,  0.1867, -0.2353,\n",
      "          0.0241, -0.2491, -0.0828, -0.3165, -0.2340,  0.1322, -0.0081, -0.1823],\n",
      "        [-0.0278,  0.2067, -0.1717, -0.0381,  0.3105,  0.2171, -0.0332, -0.0620,\n",
      "          0.3409,  0.3099, -0.0681,  0.1786, -0.0221, -0.3371,  0.1868, -0.3250]],\n",
      "       requires_grad=True)\n",
      "Wq: torch.Size([8, 8])\n",
      "tensor([[ 0.0943,  0.0984,  0.1721, -0.2680, -0.1800,  0.1334, -0.0952, -0.1357],\n",
      "        [-0.0437, -0.0026, -0.3460,  0.2749, -0.2080, -0.0480,  0.0819, -0.1937],\n",
      "        [ 0.3141, -0.2665, -0.2688,  0.3377,  0.0845, -0.0520,  0.3183,  0.1263],\n",
      "        [-0.0469,  0.2926,  0.1064,  0.0832, -0.2169, -0.3217, -0.2960, -0.2437],\n",
      "        [ 0.1181, -0.0140, -0.1870, -0.1090, -0.1538, -0.3151,  0.0417,  0.1949],\n",
      "        [ 0.3463, -0.2736, -0.0374,  0.2403,  0.2242, -0.1357, -0.0907,  0.0466],\n",
      "        [ 0.2182, -0.2725, -0.3533,  0.1209, -0.2105, -0.0207,  0.1867, -0.2353],\n",
      "        [-0.0278,  0.2067, -0.1717, -0.0381,  0.3105,  0.2171, -0.0332, -0.0620]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wk: torch.Size([8, 4])\n",
      "tensor([[ 0.1428,  0.0701, -0.1481, -0.0889],\n",
      "        [-0.2697, -0.1477, -0.0126, -0.3187],\n",
      "        [ 0.1113, -0.1897, -0.1114, -0.1643],\n",
      "        [-0.1881, -0.3368, -0.0630,  0.3261],\n",
      "        [-0.1996, -0.3047,  0.2557, -0.1691],\n",
      "        [-0.0280,  0.1888,  0.0920,  0.2207],\n",
      "        [ 0.0241, -0.2491, -0.0828, -0.3165],\n",
      "        [ 0.3409,  0.3099, -0.0681,  0.1786]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wv: torch.Size([8, 4])\n",
      "tensor([[ 0.2956,  0.3404,  0.2485,  0.0994],\n",
      "        [-0.3133,  0.3372,  0.2664, -0.3299],\n",
      "        [-0.0568, -0.0780, -0.0442, -0.3249],\n",
      "        [-0.0982,  0.0958, -0.3081, -0.2083],\n",
      "        [ 0.0115,  0.0761,  0.2231, -0.0160],\n",
      "        [ 0.1851,  0.3017, -0.1728, -0.2112],\n",
      "        [-0.2340,  0.1322, -0.0081, -0.1823],\n",
      "        [-0.0221, -0.3371,  0.1868, -0.3250]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wq spliced: torch.Size([4, 4])\n",
      "tensor([[ 0.0943,  0.0984, -0.1800,  0.1334],\n",
      "        [-0.0437, -0.0026, -0.2080, -0.0480],\n",
      "        [ 0.3141, -0.2665,  0.0845, -0.0520],\n",
      "        [-0.0469,  0.2926, -0.2169, -0.3217]], grad_fn=<CatBackward0>)\n",
      "Wk spliced: torch.Size([4, 2])\n",
      "tensor([[ 0.1428,  0.0701],\n",
      "        [-0.2697, -0.1477],\n",
      "        [ 0.1113, -0.1897],\n",
      "        [-0.1881, -0.3368]], grad_fn=<CatBackward0>)\n",
      "Wv spliced: torch.Size([4, 2])\n",
      "tensor([[ 0.2956,  0.3404],\n",
      "        [-0.3133,  0.3372],\n",
      "        [-0.0568, -0.0780],\n",
      "        [-0.0982,  0.0958]], grad_fn=<CatBackward0>)\n",
      "Wqkv_spliced: torch.Size([4, 8])\n",
      "tensor([[ 0.0943,  0.0984, -0.1800,  0.1334,  0.1428,  0.0701,  0.2956,  0.3404],\n",
      "        [-0.0437, -0.0026, -0.2080, -0.0480, -0.2697, -0.1477, -0.3133,  0.3372],\n",
      "        [ 0.3141, -0.2665,  0.0845, -0.0520,  0.1113, -0.1897, -0.0568, -0.0780],\n",
      "        [-0.0469,  0.2926, -0.2169, -0.3217, -0.1881, -0.3368, -0.0982,  0.0958]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "xqkv: torch.Size([1, 3, 8])\n",
      "tensor([[[ 0.1883,  0.3232, -0.5952, -0.4060, -0.2966, -0.6626, -0.1794,\n",
      "           0.6888],\n",
      "         [ 0.0439,  0.4001, -0.6857, -0.2410, -0.3342, -0.4652, -0.1217,\n",
      "           0.9052],\n",
      "         [ 0.4307, -0.1497, -0.3694,  0.0270,  0.0147, -0.3150, -0.0166,\n",
      "           0.7005]]], grad_fn=<UnsafeViewBackward0>)\n",
      "xq: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.1883,  0.3232, -0.5952, -0.4060],\n",
      "         [ 0.0439,  0.4001, -0.6857, -0.2410],\n",
      "         [ 0.4307, -0.1497, -0.3694,  0.0270]]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "xk: torch.Size([1, 3, 2])\n",
      "tensor([[[-0.2966, -0.6626],\n",
      "         [-0.3342, -0.4652],\n",
      "         [ 0.0147, -0.3150]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xv: torch.Size([1, 3, 2])\n",
      "tensor([[[-0.1794,  0.6888],\n",
      "         [-0.1217,  0.9052],\n",
      "         [-0.0166,  0.7005]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xq reshaped: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[ 0.1883,  0.3232],\n",
      "          [-0.5952, -0.4060]],\n",
      "\n",
      "         [[ 0.0439,  0.4001],\n",
      "          [-0.6857, -0.2410]],\n",
      "\n",
      "         [[ 0.4307, -0.1497],\n",
      "          [-0.3694,  0.0270]]]], grad_fn=<ViewBackward0>)\n",
      "xk reshaped: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[-0.2966, -0.6626]],\n",
      "\n",
      "         [[-0.3342, -0.4652]],\n",
      "\n",
      "         [[ 0.0147, -0.3150]]]], grad_fn=<ViewBackward0>)\n",
      "xv reshaped: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[-0.1794,  0.6888]],\n",
      "\n",
      "         [[-0.1217,  0.9052]],\n",
      "\n",
      "         [[-0.0166,  0.7005]]]], grad_fn=<ViewBackward0>)\n",
      "rotated xq: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[ 0.1883,  0.3232],\n",
      "          [-0.5952, -0.4060]],\n",
      "\n",
      "         [[-0.3129,  0.2531],\n",
      "          [-0.1676, -0.7072]],\n",
      "\n",
      "         [[-0.0431,  0.4539],\n",
      "          [ 0.1291, -0.3471]]]], grad_fn=<TransposeBackward0>)\n",
      "rotated xk: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[-0.2966, -0.6626]],\n",
      "\n",
      "         [[ 0.2109, -0.5326]],\n",
      "\n",
      "         [[ 0.2803,  0.1445]]]], grad_fn=<TransposeBackward0>)\n",
      "repeat_interleaved xk: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[-0.2966, -0.6626],\n",
      "          [-0.2966, -0.6626]],\n",
      "\n",
      "         [[ 0.2109, -0.5326],\n",
      "          [ 0.2109, -0.5326]],\n",
      "\n",
      "         [[ 0.2803,  0.1445],\n",
      "          [ 0.2803,  0.1445]]]], grad_fn=<ViewBackward0>)\n",
      "repeat_interleaved xv: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[-0.1794,  0.6888],\n",
      "          [-0.1794,  0.6888]],\n",
      "\n",
      "         [[-0.1217,  0.9052],\n",
      "          [-0.1217,  0.9052]],\n",
      "\n",
      "         [[-0.0166,  0.7005],\n",
      "          [-0.0166,  0.7005]]]], grad_fn=<ViewBackward0>)\n",
      "transposed xq: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[ 0.1883,  0.3232],\n",
      "          [-0.3129,  0.2531],\n",
      "          [-0.0431,  0.4539]],\n",
      "\n",
      "         [[-0.5952, -0.4060],\n",
      "          [-0.1676, -0.7072],\n",
      "          [ 0.1291, -0.3471]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xk: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[-0.2966, -0.6626],\n",
      "          [ 0.2109, -0.5326],\n",
      "          [ 0.2803,  0.1445]],\n",
      "\n",
      "         [[-0.2966, -0.6626],\n",
      "          [ 0.2109, -0.5326],\n",
      "          [ 0.2803,  0.1445]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xv: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[-0.1794,  0.6888],\n",
      "          [-0.1217,  0.9052],\n",
      "          [-0.0166,  0.7005]],\n",
      "\n",
      "         [[-0.1794,  0.6888],\n",
      "          [-0.1217,  0.9052],\n",
      "          [-0.0166,  0.7005]]]], grad_fn=<TransposeBackward0>)\n",
      "scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[-0.1909, -0.0936,  0.0703],\n",
      "          [-0.0529, -0.1420, -0.0362],\n",
      "          [-0.2036, -0.1774,  0.0378]],\n",
      "\n",
      "         [[ 0.3150,  0.0641, -0.1594],\n",
      "          [ 0.3665,  0.2413, -0.1055],\n",
      "          [ 0.1355,  0.1500, -0.0099]]]], grad_fn=<MulBackward0>)\n",
      "mask: torch.Size([1, 1, 3, 3])\n",
      "tensor([[[[ 0.0000e+00, -2.3820e+38, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]]]])\n",
      "masked scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[-1.9094e-01, -2.3820e+38, -2.3820e+38],\n",
      "          [-5.2942e-02, -1.4197e-01, -2.3820e+38],\n",
      "          [-2.0365e-01, -1.7737e-01,  3.7837e-02]],\n",
      "\n",
      "         [[ 3.1503e-01, -2.3820e+38, -2.3820e+38],\n",
      "          [ 3.6649e-01,  2.4132e-01, -2.3820e+38],\n",
      "          [ 1.3553e-01,  1.4997e-01, -9.8633e-03]]]], grad_fn=<AddBackward0>)\n",
      "softmaxed scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [0.5222, 0.4778, 0.0000],\n",
      "          [0.3031, 0.3111, 0.3858]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.5313, 0.4687, 0.0000],\n",
      "          [0.3473, 0.3524, 0.3003]]]], grad_fn=<SoftmaxBackward0>)\n",
      "attention: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[-0.1794,  0.6888],\n",
      "          [-0.1519,  0.7922],\n",
      "          [-0.0987,  0.7606]],\n",
      "\n",
      "         [[-0.1794,  0.6888],\n",
      "          [-0.1524,  0.7902],\n",
      "          [-0.1102,  0.7686]]]], grad_fn=<UnsafeViewBackward0>)\n",
      "reshaped attention: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.1794,  0.6888, -0.1794,  0.6888],\n",
      "         [-0.1519,  0.7922, -0.1524,  0.7902],\n",
      "         [-0.0987,  0.7606, -0.1102,  0.7686]]], grad_fn=<ViewBackward0>)\n",
      "self.Wo: torch.Size([8, 8])\n",
      "Parameter containing:\n",
      "tensor([[ 0.2230, -0.1857,  0.3520,  0.1651,  0.0284,  0.1897, -0.2598, -0.0584],\n",
      "        [ 0.1927,  0.1348, -0.0726, -0.2868, -0.0214, -0.1102, -0.1854, -0.3324],\n",
      "        [-0.1281, -0.1910,  0.2500, -0.1509, -0.0712, -0.2954,  0.2343,  0.0442],\n",
      "        [ 0.1348,  0.0159, -0.0595,  0.2097, -0.1257,  0.2610, -0.1585,  0.1628],\n",
      "        [-0.2551,  0.2873, -0.2347,  0.3300, -0.0301, -0.2882,  0.1180, -0.0209],\n",
      "        [ 0.1979, -0.2413,  0.2265,  0.1313, -0.1660, -0.1114,  0.3264,  0.0338],\n",
      "        [ 0.0733,  0.2523, -0.1506, -0.1665, -0.1176, -0.3462, -0.0907, -0.2066],\n",
      "        [ 0.3044,  0.0854, -0.1253, -0.0488,  0.2085,  0.2700, -0.0108, -0.3240]],\n",
      "       requires_grad=True)\n",
      "spliced Wo: torch.Size([4, 4])\n",
      "tensor([[ 0.2230, -0.1857,  0.3520,  0.1651],\n",
      "        [ 0.1927,  0.1348, -0.0726, -0.2868],\n",
      "        [-0.2551,  0.2873, -0.2347,  0.3300],\n",
      "        [ 0.1979, -0.2413,  0.2265,  0.1313]], grad_fn=<CatBackward0>)\n",
      "projected output: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.2748, -0.0916,  0.0850, -0.1959],\n",
      "         [ 0.3140, -0.0995,  0.1038, -0.1988],\n",
      "         [ 0.3048, -0.0962,  0.1100, -0.1699]]], grad_fn=<UnsafeViewBackward0>)\n",
      "----------------- END MultiQueryAttention.forwardTensor() --------------------\n",
      "x in layer after MQA & resid connection and before MLP:\n",
      "tensor([[[ 0.7379,  0.3522,  0.4183,  0.5163],\n",
      "         [ 1.0453,  0.6136,  0.1632,  0.4404],\n",
      "         [ 0.9745,  0.4368,  0.7090, -0.1126]]], grad_fn=<AddBackward0>)\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.7379,  0.3522,  0.4183,  0.5163],\n",
      "         [ 1.0453,  0.6136,  0.1632,  0.4404],\n",
      "         [ 0.9745,  0.4368,  0.7090, -0.1126]]], grad_fn=<AddBackward0>)\n",
      "normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[ 1.4008,  0.6686,  0.7939,  0.9801],\n",
      "         [ 1.6083,  0.9441,  0.2511,  0.6776],\n",
      "         [ 1.5146,  0.6789,  1.1021, -0.1750]]], grad_fn=<MulBackward0>)\n",
      "dim: 4\n",
      "skip: 0\n",
      "spliced scale: torch.Size([4])\n",
      "tensor([0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[ 1.4008,  0.6686,  0.7939,  0.9801],\n",
      "         [ 1.6083,  0.9441,  0.2511,  0.6776],\n",
      "         [ 1.5146,  0.6789,  1.1021, -0.1750]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "---------- MLP Input: torch.Tensor ------------\n",
      "------------- MLP.forwardTensor() ------------\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[ 1.4008,  0.6686,  0.7939,  0.9801],\n",
      "         [ 1.6083,  0.9441,  0.2511,  0.6776],\n",
      "         [ 1.5146,  0.6789,  1.1021, -0.1750]]], grad_fn=<MulBackward0>)\n",
      "d_dim: 4\n",
      "d_skip: 0\n",
      "i_dim: 16\n",
      "i_skip: 0\n",
      "Wgate: torch.Size([8, 32])\n",
      "Parameter containing:\n",
      "tensor([[ 2.7696e-01,  4.4119e-02, -2.3632e-02,  2.0816e-01,  2.9269e-01,\n",
      "          1.9164e-02,  3.6683e-02, -4.3430e-02,  1.3773e-02, -2.4971e-01,\n",
      "         -2.5365e-01, -4.7275e-02,  1.8978e-01, -3.4244e-01, -6.0466e-03,\n",
      "          8.8459e-02,  3.3298e-02, -6.9899e-02, -5.1214e-02, -3.1807e-01,\n",
      "          3.4446e-01, -3.0637e-02, -2.2211e-01, -1.7561e-01, -2.8003e-01,\n",
      "         -1.6241e-01,  2.3924e-01, -3.3911e-01,  3.0746e-01, -1.1257e-01,\n",
      "         -1.5524e-01,  1.1162e-01],\n",
      "        [ 2.8190e-01, -3.2874e-01,  3.1624e-01,  1.0040e-02, -3.6685e-02,\n",
      "         -2.5660e-01,  3.5687e-02, -2.2955e-01,  7.7329e-02, -6.1736e-02,\n",
      "          6.2376e-02, -1.2587e-01,  9.8117e-02, -6.8643e-03,  5.2350e-03,\n",
      "          1.7382e-01, -2.8216e-01,  2.0011e-01, -3.4540e-01, -2.5383e-01,\n",
      "          2.7614e-01,  1.1159e-01,  2.1756e-01,  1.6015e-02, -1.7874e-01,\n",
      "         -1.3673e-01,  2.7171e-02,  2.0584e-01,  1.2468e-01,  1.7590e-02,\n",
      "         -2.3941e-02, -2.0743e-02],\n",
      "        [-8.3554e-02, -1.9984e-01,  1.5161e-01,  2.9542e-02, -2.5973e-01,\n",
      "         -3.0074e-01, -3.4169e-01,  2.6323e-01,  3.1818e-01, -1.0318e-03,\n",
      "          2.2501e-01,  2.0711e-02,  3.5152e-01, -2.9705e-01, -2.5692e-01,\n",
      "         -2.5367e-01, -1.1250e-01,  1.0593e-01,  8.4633e-02,  1.9955e-01,\n",
      "         -7.2729e-02, -1.4594e-01,  3.0376e-02,  1.7884e-01,  1.3993e-01,\n",
      "         -3.0178e-01, -3.0030e-01, -1.1422e-01, -1.0721e-01,  2.0666e-02,\n",
      "          1.4545e-01, -7.3817e-02],\n",
      "        [-5.7779e-02,  2.4396e-01, -3.3920e-01, -1.6233e-01,  2.3869e-01,\n",
      "          2.0971e-01,  1.6318e-01,  2.6317e-01,  8.0743e-02, -3.0797e-02,\n",
      "          3.8437e-02,  1.8785e-01,  7.0092e-02,  1.9727e-01, -1.3265e-01,\n",
      "          2.8248e-02,  2.8722e-01,  9.4228e-02, -5.6396e-02, -9.7493e-02,\n",
      "         -1.5724e-01, -1.6790e-01,  5.7549e-02,  2.7555e-01,  3.4311e-01,\n",
      "          1.5882e-01,  2.4249e-01,  2.4166e-01, -8.1371e-03, -4.8073e-02,\n",
      "          2.1756e-01,  2.6253e-01],\n",
      "        [ 6.6082e-02, -2.4466e-01,  6.1540e-02,  6.5228e-02,  7.3098e-02,\n",
      "         -1.9688e-01, -2.1233e-01,  1.2814e-01, -3.3319e-01, -2.2086e-01,\n",
      "         -2.3090e-01, -2.9455e-02, -2.3795e-01,  6.5451e-02,  2.5302e-01,\n",
      "         -1.1421e-01,  9.0178e-02, -3.4134e-01, -2.7901e-01,  1.4506e-01,\n",
      "          1.4550e-01,  2.8043e-01, -1.0987e-01, -3.4562e-01,  7.4493e-02,\n",
      "          3.3718e-01,  1.8062e-01, -2.8831e-01,  1.8259e-01,  3.2526e-01,\n",
      "         -3.0041e-01, -3.2374e-01],\n",
      "        [-6.8354e-02, -6.5391e-02,  3.4912e-01,  2.3638e-01,  6.8518e-02,\n",
      "         -3.1568e-01, -3.1246e-01,  7.4730e-02, -1.9271e-01, -2.3584e-02,\n",
      "          2.3010e-01, -1.8246e-02,  3.2064e-01,  2.2149e-02,  3.5268e-01,\n",
      "          1.9923e-01,  2.9953e-01, -2.4401e-01, -4.4420e-02, -1.8791e-01,\n",
      "         -3.2687e-01, -1.6431e-01,  2.8816e-01, -2.2224e-01, -1.2125e-01,\n",
      "          2.4239e-01,  1.9470e-01, -1.7252e-01,  3.0633e-02,  3.3852e-01,\n",
      "         -3.2387e-01, -2.3031e-01],\n",
      "        [ 7.9245e-02, -2.0524e-02,  1.0334e-01,  1.7748e-01,  2.0014e-01,\n",
      "         -1.8616e-01,  1.0528e-01,  2.8613e-02, -1.9418e-01, -1.3303e-01,\n",
      "          8.6996e-02, -2.2053e-01,  7.3208e-02, -1.4067e-01,  6.0644e-02,\n",
      "          1.7516e-01, -3.2873e-02, -3.1335e-01, -1.7506e-02,  2.2085e-01,\n",
      "         -1.5737e-01,  2.6141e-01, -7.1143e-03, -1.9138e-01, -2.0407e-01,\n",
      "         -2.9138e-01, -2.9534e-01, -3.0966e-02,  1.9783e-02,  2.6241e-01,\n",
      "          2.9568e-01, -2.2307e-04],\n",
      "        [-6.7545e-02,  1.0348e-01,  2.5018e-01, -2.1576e-01,  2.1435e-01,\n",
      "          8.7583e-02, -1.0067e-01, -4.2804e-02,  3.3235e-01,  3.1753e-01,\n",
      "         -1.1145e-01, -2.2867e-01, -1.5618e-01,  3.2845e-01,  2.5913e-02,\n",
      "         -3.2012e-01, -1.7407e-01, -2.2907e-01,  7.6543e-02, -3.2207e-01,\n",
      "         -2.5375e-01, -9.9934e-02, -1.7271e-01,  3.2003e-01, -1.0196e-01,\n",
      "          3.4143e-01,  2.0842e-01,  3.5346e-02, -2.5052e-01, -2.2200e-01,\n",
      "          9.1169e-02,  1.8392e-01]], requires_grad=True)\n",
      "Wgate spliced: torch.Size([4, 16])\n",
      "tensor([[ 0.2770,  0.0441, -0.0236,  0.2082,  0.2927,  0.0192,  0.0367, -0.0434,\n",
      "          0.0138, -0.2497, -0.2537, -0.0473,  0.1898, -0.3424, -0.0060,  0.0885],\n",
      "        [ 0.2819, -0.3287,  0.3162,  0.0100, -0.0367, -0.2566,  0.0357, -0.2296,\n",
      "          0.0773, -0.0617,  0.0624, -0.1259,  0.0981, -0.0069,  0.0052,  0.1738],\n",
      "        [-0.0836, -0.1998,  0.1516,  0.0295, -0.2597, -0.3007, -0.3417,  0.2632,\n",
      "          0.3182, -0.0010,  0.2250,  0.0207,  0.3515, -0.2970, -0.2569, -0.2537],\n",
      "        [-0.0578,  0.2440, -0.3392, -0.1623,  0.2387,  0.2097,  0.1632,  0.2632,\n",
      "          0.0807, -0.0308,  0.0384,  0.1879,  0.0701,  0.1973, -0.1327,  0.0282]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Bgate: torch.Size([32])\n",
      "Parameter containing:\n",
      "tensor([ 0.2146, -0.2670,  0.1984, -0.0742, -0.2408,  0.0686,  0.2205,  0.2083,\n",
      "         0.3255,  0.0210,  0.1917, -0.1590, -0.0152, -0.1553, -0.0103, -0.1725,\n",
      "        -0.3532, -0.2155, -0.3048, -0.0443, -0.1526,  0.2215, -0.0605,  0.2883,\n",
      "        -0.2464, -0.1612, -0.2733,  0.2628, -0.0747,  0.1755,  0.0299, -0.3258],\n",
      "       requires_grad=True)\n",
      "Bgate spliced: torch.Size([16])\n",
      "tensor([ 0.2146, -0.2670,  0.1984, -0.0742, -0.2408,  0.0686,  0.2205,  0.2083,\n",
      "         0.3255,  0.0210,  0.1917, -0.1590, -0.0152, -0.1553, -0.0103, -0.1725],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xgate: torch.Size([1, 3, 16])\n",
      "tensor([[[ 0.6680, -0.3445,  0.1647,  0.0884,  0.1724, -0.1094,  0.1844,\n",
      "           0.4609,  0.7282, -0.4011,  0.0944, -0.1088,  0.6640, -0.6820,\n",
      "          -0.3493, -0.1061],\n",
      "         [ 0.8660, -0.3912,  0.2672,  0.1674,  0.2919, -0.0763,  0.3380,\n",
      "           0.1662,  0.5553, -0.4600, -0.0749, -0.2214,  0.5184, -0.6534,\n",
      "          -0.1695,  0.0893],\n",
      "         [ 0.7435, -0.6862,  0.6037,  0.3088, -0.1504, -0.4448, -0.1048,\n",
      "           0.2308,  0.7354, -0.3949,  0.0911, -0.3261,  0.7140, -1.0405,\n",
      "          -0.2759, -0.2050]]], grad_fn=<AddBackward0>)\n",
      "GeLU'ed Xgate: torch.Size([1, 3, 16])\n",
      "tensor([[[ 0.4997, -0.1258,  0.0931,  0.0473,  0.0980, -0.0499,  0.1057,\n",
      "           0.3123,  0.5584, -0.1380,  0.0507, -0.0497,  0.4958, -0.1689,\n",
      "          -0.1269, -0.0486],\n",
      "         [ 0.6987, -0.1361,  0.1617,  0.0948,  0.1794, -0.0358,  0.2137,\n",
      "           0.0941,  0.3946, -0.1485, -0.0352, -0.0913,  0.3618, -0.1678,\n",
      "          -0.0733,  0.0478],\n",
      "         [ 0.5735, -0.1690,  0.4389,  0.1918, -0.0662, -0.1460, -0.0480,\n",
      "           0.1364,  0.5655, -0.1368,  0.0488, -0.1214,  0.5444, -0.1551,\n",
      "          -0.1080, -0.0859]]], grad_fn=<GeluBackward0>)\n",
      "Wup: torch.Size([8, 32])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0021, -0.1046,  0.1817, -0.0516,  0.0193,  0.0600, -0.0075, -0.1344,\n",
      "         -0.0286, -0.1644,  0.1269, -0.0955,  0.3523, -0.0235,  0.0861, -0.0074,\n",
      "          0.2408,  0.0535, -0.3045,  0.1206,  0.1398, -0.0948, -0.2301, -0.3268,\n",
      "          0.3289,  0.0191, -0.1122,  0.2339, -0.1324,  0.2810, -0.2562,  0.1368],\n",
      "        [ 0.2951, -0.0752, -0.3063,  0.0138,  0.2637, -0.3246,  0.1927, -0.0232,\n",
      "         -0.0732, -0.3022, -0.1025, -0.1623,  0.3245, -0.0196, -0.3018,  0.3118,\n",
      "          0.0272, -0.0570, -0.1402, -0.2682, -0.2442, -0.3467, -0.1708, -0.2771,\n",
      "          0.0431, -0.3041,  0.2787, -0.2813,  0.3203, -0.0231, -0.0604,  0.1721],\n",
      "        [-0.0448,  0.2554,  0.1358,  0.2768, -0.0091,  0.2906, -0.2163, -0.2868,\n",
      "          0.1679, -0.0158, -0.0328,  0.0704, -0.2616,  0.0945,  0.3430,  0.3355,\n",
      "         -0.3456, -0.3253, -0.1721, -0.0508,  0.2064,  0.2841, -0.1301,  0.1717,\n",
      "         -0.2925, -0.0750, -0.2901, -0.0972, -0.1391, -0.0813, -0.2779, -0.0292],\n",
      "        [-0.0876,  0.0051, -0.2380, -0.0245, -0.3333,  0.0636,  0.0732,  0.1649,\n",
      "         -0.2813, -0.0807, -0.2072, -0.0534, -0.0387, -0.1205,  0.1881, -0.2462,\n",
      "         -0.0189,  0.3517,  0.3153, -0.1116, -0.3284, -0.1903,  0.1285,  0.0584,\n",
      "         -0.1383, -0.1909,  0.2390, -0.3015, -0.0025,  0.0838,  0.3152,  0.0292],\n",
      "        [-0.1278, -0.0509,  0.0753, -0.1185,  0.2015,  0.0344, -0.2904, -0.2126,\n",
      "          0.3092, -0.0234,  0.2858,  0.1018,  0.0700,  0.3181, -0.0697, -0.1937,\n",
      "          0.1092, -0.2342, -0.1897,  0.0071, -0.2887,  0.0980,  0.3046,  0.1311,\n",
      "         -0.1741, -0.0501,  0.1944,  0.0524,  0.0033, -0.2669,  0.2403, -0.0793],\n",
      "        [-0.1439, -0.1908, -0.1193, -0.3009, -0.0303, -0.3356,  0.2311, -0.0965,\n",
      "          0.0337, -0.0190, -0.0337,  0.1605, -0.0188,  0.2980,  0.2662, -0.3276,\n",
      "         -0.1984,  0.0278, -0.0983, -0.2530,  0.2723, -0.1311,  0.0187,  0.0117,\n",
      "          0.0294, -0.0216,  0.2812,  0.3086, -0.0656,  0.1818,  0.1877,  0.0530],\n",
      "        [ 0.1595, -0.0583,  0.1740, -0.2696,  0.2035,  0.0832, -0.3327,  0.0464,\n",
      "         -0.2065,  0.0928,  0.2197, -0.0637,  0.3240,  0.1166,  0.0849,  0.1735,\n",
      "         -0.3223,  0.1503,  0.1325,  0.1845,  0.2463,  0.2347,  0.2019,  0.3238,\n",
      "         -0.0025,  0.0020, -0.0801, -0.1887,  0.2771, -0.0410,  0.2758, -0.1157],\n",
      "        [ 0.1686, -0.0343, -0.2964,  0.2292, -0.0889,  0.0496, -0.1099,  0.0443,\n",
      "         -0.2176,  0.0409, -0.2988,  0.0489, -0.1662,  0.2171, -0.2294, -0.2152,\n",
      "         -0.3295,  0.1492, -0.2923, -0.0260,  0.0732,  0.1180,  0.0124, -0.3084,\n",
      "          0.0835,  0.1517,  0.2558, -0.2343, -0.1899,  0.3362, -0.1467, -0.2081]],\n",
      "       requires_grad=True)\n",
      "Wup spliced: torch.Size([4, 16])\n",
      "tensor([[ 0.0021, -0.1046,  0.1817, -0.0516,  0.0193,  0.0600, -0.0075, -0.1344,\n",
      "         -0.0286, -0.1644,  0.1269, -0.0955,  0.3523, -0.0235,  0.0861, -0.0074],\n",
      "        [ 0.2951, -0.0752, -0.3063,  0.0138,  0.2637, -0.3246,  0.1927, -0.0232,\n",
      "         -0.0732, -0.3022, -0.1025, -0.1623,  0.3245, -0.0196, -0.3018,  0.3118],\n",
      "        [-0.0448,  0.2554,  0.1358,  0.2768, -0.0091,  0.2906, -0.2163, -0.2868,\n",
      "          0.1679, -0.0158, -0.0328,  0.0704, -0.2616,  0.0945,  0.3430,  0.3355],\n",
      "        [-0.0876,  0.0051, -0.2380, -0.0245, -0.3333,  0.0636,  0.0732,  0.1649,\n",
      "         -0.2813, -0.0807, -0.2072, -0.0534, -0.0387, -0.1205,  0.1881, -0.2462]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Bup: torch.Size([32])\n",
      "Parameter containing:\n",
      "tensor([-0.2727, -0.1305,  0.3092, -0.2606,  0.2041, -0.2500,  0.1340,  0.1879,\n",
      "        -0.0259, -0.0267, -0.2939,  0.2898,  0.1879,  0.2356,  0.0805,  0.2208,\n",
      "        -0.3364,  0.0372, -0.3519,  0.1030, -0.3401,  0.1854, -0.0025, -0.3101,\n",
      "        -0.3136, -0.0217, -0.0798, -0.0020,  0.3074, -0.1309, -0.1273,  0.0306],\n",
      "       requires_grad=True)\n",
      "Bup spliced: torch.Size([16])\n",
      "tensor([-0.2727, -0.1305,  0.3092, -0.2606,  0.2041, -0.2500,  0.1340,  0.1879,\n",
      "        -0.0259, -0.0267, -0.2939,  0.2898,  0.1879,  0.2356,  0.0805,  0.2208],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xup: torch.Size([1, 3, 16])\n",
      "tensor([[[-0.1939, -0.1195,  0.2335, -0.1279,  0.0735, -0.0898,  0.1524,\n",
      "          -0.0820, -0.2574, -0.5506, -0.4138,  0.0511,  0.6527,  0.1466,\n",
      "           0.4560,  0.4440],\n",
      "         [-0.0613, -0.3021,  0.1851, -0.2777,  0.2559, -0.3438,  0.2992,\n",
      "          -0.0105, -0.2895, -0.6350, -0.3352, -0.0355,  0.9690,  0.1214,\n",
      "           0.1477,  0.4207],\n",
      "         [-0.1032, -0.0594,  0.5677, -0.0201,  0.4606, -0.0703,  0.0023,\n",
      "          -0.3764,  0.1152, -0.4841, -0.1712,  0.1219,  0.6603,  0.3119,\n",
      "           0.3512,  0.8341]]], grad_fn=<AddBackward0>)\n",
      "Xfuse: torch.Size([1, 3, 16])\n",
      "tensor([[[-9.6862e-02,  1.5039e-02,  2.1739e-02, -6.0506e-03,  7.2037e-03,\n",
      "           4.4860e-03,  1.6112e-02, -2.5603e-02, -1.4372e-01,  7.6013e-02,\n",
      "          -2.0997e-02, -2.5381e-03,  3.2365e-01, -2.4752e-02, -5.7891e-02,\n",
      "          -2.1566e-02],\n",
      "         [-4.2830e-02,  4.1114e-02,  2.9942e-02, -2.6339e-02,  4.5915e-02,\n",
      "           1.2315e-02,  6.3952e-02, -9.8429e-04, -1.1425e-01,  9.4282e-02,\n",
      "           1.1799e-02,  3.2410e-03,  3.5059e-01, -2.0374e-02, -1.0831e-02,\n",
      "           2.0116e-02],\n",
      "         [-5.9181e-02,  1.0037e-02,  2.4919e-01, -3.8466e-03, -3.0488e-02,\n",
      "           1.0259e-02, -1.1188e-04, -5.1352e-02,  6.5151e-02,  6.6235e-02,\n",
      "          -8.3636e-03, -1.4793e-02,  3.5944e-01, -4.8379e-02, -3.7909e-02,\n",
      "          -7.1623e-02]]], grad_fn=<MulBackward0>)\n",
      "Wdown: torch.Size([32, 8])\n",
      "Parameter containing:\n",
      "tensor([[-0.1416,  0.1635,  0.0652,  0.0065,  0.0774,  0.1437, -0.1448,  0.1509],\n",
      "        [-0.0977,  0.1709, -0.0774, -0.0781, -0.1184, -0.1117,  0.1766,  0.1497],\n",
      "        [ 0.1116, -0.0374,  0.0344,  0.1375, -0.0414, -0.1583, -0.0810,  0.1291],\n",
      "        [-0.1740, -0.0532, -0.1557,  0.0540, -0.0258,  0.0535, -0.0921,  0.0311],\n",
      "        [-0.0443,  0.0382,  0.1719, -0.1403, -0.0549,  0.1301,  0.0280, -0.0404],\n",
      "        [ 0.1099, -0.0275, -0.0688, -0.0526,  0.0880, -0.0161, -0.1417, -0.0140],\n",
      "        [-0.0683,  0.1125,  0.0087, -0.1625,  0.0661, -0.0958,  0.1342, -0.0077],\n",
      "        [ 0.0312,  0.0685,  0.0634, -0.0289,  0.1377,  0.0399,  0.1647,  0.1216],\n",
      "        [ 0.1041, -0.0446, -0.0406,  0.1694, -0.0538,  0.0758, -0.0148,  0.1536],\n",
      "        [ 0.0580,  0.0526,  0.0939,  0.0452,  0.0162, -0.0416, -0.1432, -0.1521],\n",
      "        [ 0.0349, -0.0354,  0.1109, -0.1479, -0.0762, -0.0124, -0.1299,  0.0613],\n",
      "        [ 0.1091,  0.1134,  0.0034,  0.0666,  0.1535,  0.0654, -0.0814, -0.1199],\n",
      "        [-0.1521, -0.0221,  0.1660, -0.1310,  0.1395,  0.0313,  0.1124, -0.0992],\n",
      "        [ 0.0242, -0.0988,  0.0378, -0.1087, -0.1568, -0.1468, -0.1573,  0.0532],\n",
      "        [-0.0495,  0.0809,  0.0499, -0.1107,  0.1614,  0.1322,  0.1089,  0.0912],\n",
      "        [-0.0257,  0.0723, -0.1577,  0.0132,  0.1717, -0.1475, -0.0848,  0.1743],\n",
      "        [-0.1391, -0.0706,  0.0881,  0.0898, -0.0542, -0.0510,  0.1619, -0.0047],\n",
      "        [-0.0383, -0.1081,  0.1379, -0.0152, -0.0846, -0.0450, -0.0235,  0.0784],\n",
      "        [ 0.1708, -0.1260,  0.0533, -0.1561,  0.1098,  0.0590,  0.0939,  0.1070],\n",
      "        [ 0.0559,  0.1042,  0.1308, -0.0085, -0.0242,  0.1431,  0.0423, -0.1011],\n",
      "        [ 0.1376,  0.1428, -0.0577,  0.0919, -0.0173,  0.0626, -0.0168,  0.1649],\n",
      "        [ 0.0933, -0.1746, -0.0429, -0.1735,  0.0142,  0.0360,  0.0450, -0.0874],\n",
      "        [ 0.1490,  0.1235,  0.1578, -0.1719,  0.0985, -0.0367, -0.0152,  0.0330],\n",
      "        [-0.1557,  0.0212,  0.0014,  0.1424, -0.0899, -0.1017, -0.0888,  0.1616],\n",
      "        [ 0.1557, -0.1532,  0.0074, -0.0922,  0.1579, -0.1049, -0.1762,  0.1100],\n",
      "        [ 0.0190, -0.1558,  0.1612, -0.1515,  0.0971, -0.1725, -0.0897, -0.1486],\n",
      "        [ 0.0417, -0.0091, -0.1571,  0.0408,  0.0752, -0.0605, -0.1193,  0.1113],\n",
      "        [-0.0574,  0.1678, -0.0077,  0.1439, -0.0400,  0.0047,  0.0225,  0.0770],\n",
      "        [ 0.1393,  0.0636, -0.0425,  0.0662,  0.0463, -0.0487,  0.0937, -0.0558],\n",
      "        [ 0.1167, -0.1688, -0.0530, -0.0713,  0.1112, -0.1106, -0.1487,  0.0743],\n",
      "        [-0.1189, -0.0051,  0.0517,  0.1383,  0.1255, -0.0311, -0.1170, -0.0355],\n",
      "        [-0.0642, -0.0106,  0.1669,  0.0511,  0.0708, -0.0757, -0.0570, -0.0116]],\n",
      "       requires_grad=True)\n",
      "Wdown spliced: torch.Size([16, 4])\n",
      "tensor([[-0.1416,  0.1635,  0.0652,  0.0065],\n",
      "        [-0.0977,  0.1709, -0.0774, -0.0781],\n",
      "        [ 0.1116, -0.0374,  0.0344,  0.1375],\n",
      "        [-0.1740, -0.0532, -0.1557,  0.0540],\n",
      "        [-0.0443,  0.0382,  0.1719, -0.1403],\n",
      "        [ 0.1099, -0.0275, -0.0688, -0.0526],\n",
      "        [-0.0683,  0.1125,  0.0087, -0.1625],\n",
      "        [ 0.0312,  0.0685,  0.0634, -0.0289],\n",
      "        [ 0.1041, -0.0446, -0.0406,  0.1694],\n",
      "        [ 0.0580,  0.0526,  0.0939,  0.0452],\n",
      "        [ 0.0349, -0.0354,  0.1109, -0.1479],\n",
      "        [ 0.1091,  0.1134,  0.0034,  0.0666],\n",
      "        [-0.1521, -0.0221,  0.1660, -0.1310],\n",
      "        [ 0.0242, -0.0988,  0.0378, -0.1087],\n",
      "        [-0.0495,  0.0809,  0.0499, -0.1107],\n",
      "        [-0.0257,  0.0723, -0.1577,  0.0132]], grad_fn=<SliceBackward0>)\n",
      "Bdown: torch.Size([8])\n",
      "Parameter containing:\n",
      "tensor([-0.0799, -0.0316,  0.1597, -0.1266, -0.0430, -0.0583,  0.1753,  0.1656],\n",
      "       requires_grad=True)\n",
      "Bdown spliced: torch.Size([4])\n",
      "tensor([-0.0799, -0.0316,  0.1597, -0.1266], grad_fn=<SliceBackward0>)\n",
      "outputs: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.1239, -0.0452,  0.2173, -0.1804],\n",
      "         [-0.1345, -0.0179,  0.2349, -0.2037],\n",
      "         [-0.0866, -0.0659,  0.2248, -0.1128]]], grad_fn=<AddBackward0>)\n",
      "------------- END MLP.forwardTensor() ------------\n",
      "layer's final residual state:\n",
      "tensor([[[ 0.6140,  0.3070,  0.6355,  0.3359],\n",
      "         [ 0.9108,  0.5957,  0.3981,  0.2366],\n",
      "         [ 0.8879,  0.3709,  0.9339, -0.2254]]], grad_fn=<AddBackward0>)\n",
      "----------------- END Layer.forwardTensor() --------------------\n",
      "y: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.6140,  0.3070,  0.6355,  0.3359],\n",
      "         [ 0.9108,  0.5957,  0.3981,  0.2366],\n",
      "         [ 0.8879,  0.3709,  0.9339, -0.2254]]], grad_fn=<AddBackward0>)\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the second sub-model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[0.5547, 0.1986, 0.1366, 0.5718],\n",
      "         [0.0822, 0.9497, 0.9029, 0.8696],\n",
      "         [0.0334, 0.3715, 0.1783, 0.6699]]])\n",
      "---------- Layer Input: torch.Tensor ------------\n",
      "----------------- Layer.forwardTensor() --------------------\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[0.5547, 0.1986, 0.1366, 0.5718],\n",
      "         [0.0822, 0.9497, 0.9029, 0.8696],\n",
      "         [0.0334, 0.3715, 0.1783, 0.6699]]])\n",
      "normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[1.3329, 0.4772, 0.3283, 1.3740],\n",
      "         [0.1043, 1.2061, 1.1466, 1.1044],\n",
      "         [0.0848, 0.9439, 0.4529, 1.7020]]])\n",
      "dim: 4\n",
      "skip: 4\n",
      "spliced scale: torch.Size([4])\n",
      "tensor([0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[1.3329, 0.4772, 0.3283, 1.3740],\n",
      "         [0.1043, 1.2061, 1.1466, 1.1044],\n",
      "         [0.0848, 0.9439, 0.4529, 1.7020]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "---------- Attention Input: torch.Tensor ------------\n",
      "----------------- MultiQueryAttention.forwardTensor() --------------------\n",
      "x shape: torch.Size([1, 3, 4])\n",
      "d_skip: 4\n",
      "models_in_this_level: 2\n",
      "h_dim: 2\n",
      "h_skip: 2\n",
      "self.Wqkv: torch.Size([8, 16])\n",
      "Parameter containing:\n",
      "tensor([[-0.2795, -0.1959, -0.0577, -0.3372,  0.3515,  0.1040,  0.0581, -0.1344,\n",
      "          0.2202, -0.2689, -0.2042,  0.0192, -0.0123,  0.0247,  0.1658,  0.0396],\n",
      "        [-0.0167,  0.2210,  0.1171, -0.3024,  0.1205, -0.0933, -0.2408, -0.0716,\n",
      "         -0.3044,  0.1289, -0.2500,  0.1857, -0.2718,  0.2858, -0.2783,  0.3311],\n",
      "        [ 0.3288,  0.1904,  0.2037, -0.1837, -0.0693, -0.2727,  0.1358, -0.3424,\n",
      "          0.2774, -0.0815, -0.1885, -0.2175, -0.1789,  0.2166,  0.0592,  0.0259],\n",
      "        [ 0.1927,  0.3458, -0.2680,  0.1348,  0.2199,  0.2324, -0.0811,  0.0221,\n",
      "          0.2725,  0.0543, -0.1913, -0.1547, -0.1372,  0.0405,  0.1637,  0.1964],\n",
      "        [-0.3250,  0.0694, -0.2578, -0.1762,  0.3375,  0.2496, -0.3482, -0.1832,\n",
      "         -0.2154, -0.1529, -0.2016,  0.0534,  0.1694, -0.1703,  0.0114, -0.0081],\n",
      "        [-0.1063, -0.2405,  0.0740, -0.3430, -0.0927,  0.0179, -0.0667,  0.3236,\n",
      "         -0.0174,  0.2923, -0.2366,  0.0560,  0.0746,  0.3073, -0.2393,  0.0559],\n",
      "        [-0.3385,  0.2803,  0.1343,  0.1673,  0.0041,  0.1136,  0.1144, -0.0798,\n",
      "         -0.3208,  0.1621,  0.2544, -0.1202, -0.3067,  0.3233,  0.1938, -0.0959],\n",
      "        [ 0.1249, -0.1772, -0.1219,  0.1273,  0.3222,  0.3003,  0.1611,  0.0218,\n",
      "          0.2918,  0.0201,  0.1494,  0.3410,  0.2852, -0.1394,  0.0333, -0.0864]],\n",
      "       requires_grad=True)\n",
      "Wq: torch.Size([8, 8])\n",
      "tensor([[-0.2795, -0.1959, -0.0577, -0.3372,  0.3515,  0.1040,  0.0581, -0.1344],\n",
      "        [-0.0167,  0.2210,  0.1171, -0.3024,  0.1205, -0.0933, -0.2408, -0.0716],\n",
      "        [ 0.3288,  0.1904,  0.2037, -0.1837, -0.0693, -0.2727,  0.1358, -0.3424],\n",
      "        [ 0.1927,  0.3458, -0.2680,  0.1348,  0.2199,  0.2324, -0.0811,  0.0221],\n",
      "        [-0.3250,  0.0694, -0.2578, -0.1762,  0.3375,  0.2496, -0.3482, -0.1832],\n",
      "        [-0.1063, -0.2405,  0.0740, -0.3430, -0.0927,  0.0179, -0.0667,  0.3236],\n",
      "        [-0.3385,  0.2803,  0.1343,  0.1673,  0.0041,  0.1136,  0.1144, -0.0798],\n",
      "        [ 0.1249, -0.1772, -0.1219,  0.1273,  0.3222,  0.3003,  0.1611,  0.0218]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wk: torch.Size([8, 4])\n",
      "tensor([[ 0.2202, -0.2689, -0.2042,  0.0192],\n",
      "        [-0.3044,  0.1289, -0.2500,  0.1857],\n",
      "        [ 0.2774, -0.0815, -0.1885, -0.2175],\n",
      "        [ 0.2725,  0.0543, -0.1913, -0.1547],\n",
      "        [-0.2154, -0.1529, -0.2016,  0.0534],\n",
      "        [-0.0174,  0.2923, -0.2366,  0.0560],\n",
      "        [-0.3208,  0.1621,  0.2544, -0.1202],\n",
      "        [ 0.2918,  0.0201,  0.1494,  0.3410]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wv: torch.Size([8, 4])\n",
      "tensor([[-0.0123,  0.0247,  0.1658,  0.0396],\n",
      "        [-0.2718,  0.2858, -0.2783,  0.3311],\n",
      "        [-0.1789,  0.2166,  0.0592,  0.0259],\n",
      "        [-0.1372,  0.0405,  0.1637,  0.1964],\n",
      "        [ 0.1694, -0.1703,  0.0114, -0.0081],\n",
      "        [ 0.0746,  0.3073, -0.2393,  0.0559],\n",
      "        [-0.3067,  0.3233,  0.1938, -0.0959],\n",
      "        [ 0.2852, -0.1394,  0.0333, -0.0864]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wq spliced: torch.Size([4, 4])\n",
      "tensor([[-0.2578, -0.1762, -0.3482, -0.1832],\n",
      "        [ 0.0740, -0.3430, -0.0667,  0.3236],\n",
      "        [ 0.1343,  0.1673,  0.1144, -0.0798],\n",
      "        [-0.1219,  0.1273,  0.1611,  0.0218]], grad_fn=<CatBackward0>)\n",
      "Wk spliced: torch.Size([4, 2])\n",
      "tensor([[-0.2016,  0.0534],\n",
      "        [-0.2366,  0.0560],\n",
      "        [ 0.2544, -0.1202],\n",
      "        [ 0.1494,  0.3410]], grad_fn=<CatBackward0>)\n",
      "Wv spliced: torch.Size([4, 2])\n",
      "tensor([[ 0.0114, -0.0081],\n",
      "        [-0.2393,  0.0559],\n",
      "        [ 0.1938, -0.0959],\n",
      "        [ 0.0333, -0.0864]], grad_fn=<CatBackward0>)\n",
      "Wqkv_spliced: torch.Size([4, 8])\n",
      "tensor([[-0.2578, -0.1762, -0.3482, -0.1832, -0.2016,  0.0534,  0.0114, -0.0081],\n",
      "        [ 0.0740, -0.3430, -0.0667,  0.3236, -0.2366,  0.0560, -0.2393,  0.0559],\n",
      "        [ 0.1343,  0.1673,  0.1144, -0.0798,  0.2544, -0.1202,  0.1938, -0.0959],\n",
      "        [-0.1219,  0.1273,  0.1611,  0.0218,  0.1494,  0.3410,  0.0333, -0.0864]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "xqkv: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.4317, -0.1687, -0.2370, -0.0861, -0.0928,  0.5269,  0.0104,\n",
      "          -0.1344],\n",
      "         [ 0.0817, -0.0997,  0.1924,  0.3036,  0.1503,  0.3118, -0.0284,\n",
      "          -0.1389],\n",
      "         [-0.0986, -0.0463,  0.2336,  0.2908,  0.1291,  0.5832, -0.0805,\n",
      "          -0.1385]]], grad_fn=<UnsafeViewBackward0>)\n",
      "xq: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.4317, -0.1687, -0.2370, -0.0861],\n",
      "         [ 0.0817, -0.0997,  0.1924,  0.3036],\n",
      "         [-0.0986, -0.0463,  0.2336,  0.2908]]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "xk: torch.Size([1, 3, 2])\n",
      "tensor([[[-0.0928,  0.5269],\n",
      "         [ 0.1503,  0.3118],\n",
      "         [ 0.1291,  0.5832]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xv: torch.Size([1, 3, 2])\n",
      "tensor([[[ 0.0104, -0.1344],\n",
      "         [-0.0284, -0.1389],\n",
      "         [-0.0805, -0.1385]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xq reshaped: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[-0.4317, -0.1687],\n",
      "          [-0.2370, -0.0861]],\n",
      "\n",
      "         [[ 0.0817, -0.0997],\n",
      "          [ 0.1924,  0.3036]],\n",
      "\n",
      "         [[-0.0986, -0.0463],\n",
      "          [ 0.2336,  0.2908]]]], grad_fn=<ViewBackward0>)\n",
      "xk reshaped: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[-0.0928,  0.5269]],\n",
      "\n",
      "         [[ 0.1503,  0.3118]],\n",
      "\n",
      "         [[ 0.1291,  0.5832]]]], grad_fn=<ViewBackward0>)\n",
      "xv reshaped: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[ 0.0104, -0.1344]],\n",
      "\n",
      "         [[-0.0284, -0.1389]],\n",
      "\n",
      "         [[-0.0805, -0.1385]]]], grad_fn=<ViewBackward0>)\n",
      "rotated xq: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[-0.4317, -0.1687],\n",
      "          [-0.2370, -0.0861]],\n",
      "\n",
      "         [[ 0.1280,  0.0149],\n",
      "          [-0.1516,  0.3259]],\n",
      "\n",
      "         [[ 0.0832, -0.0704],\n",
      "          [-0.3616,  0.0913]]]], grad_fn=<TransposeBackward0>)\n",
      "rotated xk: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[-0.0928,  0.5269]],\n",
      "\n",
      "         [[-0.1812,  0.2949]],\n",
      "\n",
      "         [[-0.5840, -0.1253]]]], grad_fn=<TransposeBackward0>)\n",
      "repeat_interleaved xk: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[-0.0928,  0.5269],\n",
      "          [-0.0928,  0.5269]],\n",
      "\n",
      "         [[-0.1812,  0.2949],\n",
      "          [-0.1812,  0.2949]],\n",
      "\n",
      "         [[-0.5840, -0.1253],\n",
      "          [-0.5840, -0.1253]]]], grad_fn=<ViewBackward0>)\n",
      "repeat_interleaved xv: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[ 0.0104, -0.1344],\n",
      "          [ 0.0104, -0.1344]],\n",
      "\n",
      "         [[-0.0284, -0.1389],\n",
      "          [-0.0284, -0.1389]],\n",
      "\n",
      "         [[-0.0805, -0.1385],\n",
      "          [-0.0805, -0.1385]]]], grad_fn=<ViewBackward0>)\n",
      "transposed xq: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[-0.4317, -0.1687],\n",
      "          [ 0.1280,  0.0149],\n",
      "          [ 0.0832, -0.0704]],\n",
      "\n",
      "         [[-0.2370, -0.0861],\n",
      "          [-0.1516,  0.3259],\n",
      "          [-0.3616,  0.0913]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xk: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[-0.0928,  0.5269],\n",
      "          [-0.1812,  0.2949],\n",
      "          [-0.5840, -0.1253]],\n",
      "\n",
      "         [[-0.0928,  0.5269],\n",
      "          [-0.1812,  0.2949],\n",
      "          [-0.5840, -0.1253]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xv: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[ 0.0104, -0.1344],\n",
      "          [-0.0284, -0.1389],\n",
      "          [-0.0805, -0.1385]],\n",
      "\n",
      "         [[ 0.0104, -0.1344],\n",
      "          [-0.0284, -0.1389],\n",
      "          [-0.0805, -0.1385]]]], grad_fn=<TransposeBackward0>)\n",
      "scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[-0.0345,  0.0201,  0.1932],\n",
      "          [-0.0028, -0.0133, -0.0542],\n",
      "          [-0.0317, -0.0253, -0.0281]],\n",
      "\n",
      "         [[-0.0165,  0.0124,  0.1055],\n",
      "          [ 0.1314,  0.0874,  0.0337],\n",
      "          [ 0.0578,  0.0654,  0.1412]]]], grad_fn=<MulBackward0>)\n",
      "mask: torch.Size([1, 1, 3, 3])\n",
      "tensor([[[[ 0.0000e+00, -2.3820e+38, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]]]])\n",
      "masked scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[-3.4527e-02, -2.3820e+38, -2.3820e+38],\n",
      "          [-2.8442e-03, -1.3287e-02, -2.3820e+38],\n",
      "          [-3.1690e-02, -2.5338e-02, -2.8112e-02]],\n",
      "\n",
      "         [[-1.6522e-02, -2.3820e+38, -2.3820e+38],\n",
      "          [ 1.3138e-01,  8.7387e-02, -2.3820e+38],\n",
      "          [ 5.7764e-02,  6.5372e-02,  1.4125e-01]]]], grad_fn=<AddBackward0>)\n",
      "softmaxed scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [0.5026, 0.4974, 0.0000],\n",
      "          [0.3322, 0.3343, 0.3334]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.5110, 0.4890, 0.0000],\n",
      "          [0.3231, 0.3256, 0.3513]]]], grad_fn=<SoftmaxBackward0>)\n",
      "attention: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[ 0.0104, -0.1344],\n",
      "          [-0.0089, -0.1366],\n",
      "          [-0.0329, -0.1373]],\n",
      "\n",
      "         [[ 0.0104, -0.1344],\n",
      "          [-0.0086, -0.1366],\n",
      "          [-0.0342, -0.1373]]]], grad_fn=<UnsafeViewBackward0>)\n",
      "reshaped attention: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.0104, -0.1344,  0.0104, -0.1344],\n",
      "         [-0.0089, -0.1366, -0.0086, -0.1366],\n",
      "         [-0.0329, -0.1373, -0.0342, -0.1373]]], grad_fn=<ViewBackward0>)\n",
      "self.Wo: torch.Size([8, 8])\n",
      "Parameter containing:\n",
      "tensor([[ 0.1495,  0.0336, -0.0273,  0.2754,  0.3318,  0.0045, -0.3371, -0.3035],\n",
      "        [-0.1053,  0.3432,  0.2328, -0.1015,  0.2373,  0.1693,  0.3349,  0.1904],\n",
      "        [-0.1138,  0.1994, -0.2799,  0.2186, -0.0599, -0.3217, -0.2027, -0.3199],\n",
      "        [-0.3253,  0.2455,  0.1900, -0.1089,  0.3096,  0.1647, -0.0330, -0.0647],\n",
      "        [-0.2556,  0.0661, -0.1449,  0.0507, -0.0772,  0.3468,  0.1446,  0.1222],\n",
      "        [ 0.2727,  0.2987, -0.1218,  0.3358, -0.1733, -0.0069, -0.2356, -0.0779],\n",
      "        [ 0.1813, -0.1557, -0.2543,  0.0080, -0.2965, -0.1474,  0.1068, -0.1449],\n",
      "        [ 0.3324, -0.0392,  0.0235,  0.0787, -0.2675,  0.1911,  0.0174, -0.2690]],\n",
      "       requires_grad=True)\n",
      "spliced Wo: torch.Size([4, 4])\n",
      "tensor([[-0.0599, -0.3217, -0.2027, -0.3199],\n",
      "        [ 0.3096,  0.1647, -0.0330, -0.0647],\n",
      "        [-0.2965, -0.1474,  0.1068, -0.1449],\n",
      "        [-0.2675,  0.1911,  0.0174, -0.2690]], grad_fn=<CatBackward0>)\n",
      "projected output: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.0094, -0.0527,  0.0011,  0.0400],\n",
      "         [-0.0027, -0.0445,  0.0030,  0.0497],\n",
      "         [ 0.0063, -0.0332,  0.0051,  0.0613]]], grad_fn=<UnsafeViewBackward0>)\n",
      "----------------- END MultiQueryAttention.forwardTensor() --------------------\n",
      "x in layer after MQA & resid connection and before MLP:\n",
      "tensor([[[0.5454, 0.1459, 0.1377, 0.6119],\n",
      "         [0.0795, 0.9052, 0.9059, 0.9193],\n",
      "         [0.0397, 0.3383, 0.1834, 0.7312]]], grad_fn=<AddBackward0>)\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[0.5454, 0.1459, 0.1377, 0.6119],\n",
      "         [0.0795, 0.9052, 0.9059, 0.9193],\n",
      "         [0.0397, 0.3383, 0.1834, 0.7312]]], grad_fn=<AddBackward0>)\n",
      "normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[1.2926, 0.3459, 0.3265, 1.4502],\n",
      "         [0.1007, 1.1469, 1.1478, 1.1648],\n",
      "         [0.0960, 0.8179, 0.4434, 1.7678]]], grad_fn=<MulBackward0>)\n",
      "dim: 4\n",
      "skip: 4\n",
      "spliced scale: torch.Size([4])\n",
      "tensor([0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[1.2926, 0.3459, 0.3265, 1.4502],\n",
      "         [0.1007, 1.1469, 1.1478, 1.1648],\n",
      "         [0.0960, 0.8179, 0.4434, 1.7678]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "---------- MLP Input: torch.Tensor ------------\n",
      "------------- MLP.forwardTensor() ------------\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[1.2926, 0.3459, 0.3265, 1.4502],\n",
      "         [0.1007, 1.1469, 1.1478, 1.1648],\n",
      "         [0.0960, 0.8179, 0.4434, 1.7678]]], grad_fn=<MulBackward0>)\n",
      "d_dim: 4\n",
      "d_skip: 4\n",
      "i_dim: 16\n",
      "i_skip: 16\n",
      "Wgate: torch.Size([8, 32])\n",
      "Parameter containing:\n",
      "tensor([[-0.1712,  0.0362,  0.2685,  0.0212,  0.2970,  0.0827,  0.2403,  0.1483,\n",
      "         -0.2216, -0.0260,  0.1847, -0.0429,  0.1844,  0.0748,  0.0067,  0.1346,\n",
      "         -0.1674, -0.0864, -0.2825, -0.2131, -0.0867,  0.1415,  0.2791,  0.0600,\n",
      "          0.1796,  0.0021, -0.2319,  0.3120,  0.2564, -0.1925,  0.2477, -0.0341],\n",
      "        [-0.0949, -0.1479, -0.3011, -0.2218,  0.3300,  0.2162,  0.2880,  0.1049,\n",
      "         -0.2606,  0.3487,  0.3453,  0.0698,  0.1575, -0.2133,  0.2667,  0.1235,\n",
      "          0.2873,  0.3316, -0.3487,  0.2266,  0.0391,  0.2298, -0.0233,  0.2986,\n",
      "         -0.2293,  0.2721, -0.0872, -0.2926,  0.2716,  0.3419,  0.0179,  0.1776],\n",
      "        [-0.3096,  0.2372, -0.1694,  0.2074,  0.0206, -0.2965, -0.1705,  0.2543,\n",
      "          0.2095,  0.1390,  0.2930, -0.0659, -0.1727, -0.0833, -0.0914, -0.1759,\n",
      "          0.2872,  0.2310,  0.2871, -0.1879, -0.0722,  0.2071, -0.2280,  0.1873,\n",
      "          0.2306, -0.3415, -0.2543, -0.1485, -0.0134,  0.1373,  0.1801,  0.2642],\n",
      "        [-0.2640, -0.1393,  0.3175,  0.0804, -0.2453, -0.1518, -0.2173,  0.0804,\n",
      "         -0.1442, -0.1160, -0.0112,  0.3402, -0.1097, -0.1703,  0.1195,  0.3063,\n",
      "          0.3114, -0.1937,  0.1526,  0.2471,  0.0008, -0.2090, -0.2319,  0.0368,\n",
      "          0.1574,  0.1774, -0.0007,  0.2288,  0.0822, -0.2814,  0.2052,  0.0733],\n",
      "        [ 0.0957, -0.1565,  0.0696,  0.3302,  0.2298, -0.2780,  0.0634,  0.2989,\n",
      "          0.1345, -0.0937, -0.0607,  0.0846, -0.2216, -0.2559,  0.1893, -0.0655,\n",
      "          0.1555, -0.0319, -0.3181,  0.2859,  0.0236,  0.0826, -0.1238,  0.0907,\n",
      "         -0.2622,  0.2240,  0.2674,  0.0809,  0.2085, -0.3086, -0.0117,  0.2030],\n",
      "        [-0.0538,  0.2864,  0.1046, -0.0278, -0.1697, -0.3011,  0.3268, -0.2656,\n",
      "         -0.1165,  0.0364, -0.3192,  0.1998,  0.0874, -0.0166, -0.0707, -0.2241,\n",
      "         -0.2666,  0.0095,  0.3387, -0.2342, -0.0437, -0.2427, -0.0549,  0.1748,\n",
      "         -0.2777,  0.2894,  0.1893, -0.0489, -0.2260, -0.3382,  0.1153,  0.1167],\n",
      "        [ 0.3132, -0.1494, -0.1066, -0.0877, -0.0740,  0.0823,  0.1156,  0.1317,\n",
      "         -0.0592,  0.0239,  0.3100, -0.2426, -0.2555,  0.2782,  0.2920, -0.3133,\n",
      "         -0.1357,  0.1552, -0.3221,  0.1103, -0.2241,  0.1420,  0.2184, -0.0428,\n",
      "          0.1677, -0.2026,  0.3392,  0.1325, -0.0684, -0.1941, -0.1457,  0.2381],\n",
      "        [ 0.1051, -0.2891, -0.0496, -0.2891,  0.1769,  0.0790,  0.1233, -0.3081,\n",
      "         -0.2035,  0.2435,  0.0919, -0.0381, -0.0226,  0.2137, -0.0013,  0.1808,\n",
      "          0.2861, -0.0134, -0.0746, -0.1822, -0.3078,  0.0941,  0.3484,  0.1230,\n",
      "          0.2197,  0.2430, -0.3529, -0.1401, -0.1059, -0.2397,  0.2627,  0.0719]],\n",
      "       requires_grad=True)\n",
      "Wgate spliced: torch.Size([4, 16])\n",
      "tensor([[ 0.1555, -0.0319, -0.3181,  0.2859,  0.0236,  0.0826, -0.1238,  0.0907,\n",
      "         -0.2622,  0.2240,  0.2674,  0.0809,  0.2085, -0.3086, -0.0117,  0.2030],\n",
      "        [-0.2666,  0.0095,  0.3387, -0.2342, -0.0437, -0.2427, -0.0549,  0.1748,\n",
      "         -0.2777,  0.2894,  0.1893, -0.0489, -0.2260, -0.3382,  0.1153,  0.1167],\n",
      "        [-0.1357,  0.1552, -0.3221,  0.1103, -0.2241,  0.1420,  0.2184, -0.0428,\n",
      "          0.1677, -0.2026,  0.3392,  0.1325, -0.0684, -0.1941, -0.1457,  0.2381],\n",
      "        [ 0.2861, -0.0134, -0.0746, -0.1822, -0.3078,  0.0941,  0.3484,  0.1230,\n",
      "          0.2197,  0.2430, -0.3529, -0.1401, -0.1059, -0.2397,  0.2627,  0.0719]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Bgate: torch.Size([32])\n",
      "Parameter containing:\n",
      "tensor([ 0.1901, -0.0993,  0.2912,  0.2741, -0.2587,  0.0835,  0.3282, -0.2119,\n",
      "        -0.0719, -0.1567, -0.3464,  0.2397, -0.2258, -0.2170, -0.2597, -0.1387,\n",
      "        -0.1190,  0.0231,  0.0961, -0.2485,  0.1674,  0.2209,  0.0934, -0.1873,\n",
      "         0.3145,  0.0601, -0.1522, -0.2679, -0.0829, -0.2464, -0.3532,  0.1567],\n",
      "       requires_grad=True)\n",
      "Bgate spliced: torch.Size([16])\n",
      "tensor([-0.1190,  0.0231,  0.0961, -0.2485,  0.1674,  0.2209,  0.0934, -0.1873,\n",
      "         0.3145,  0.0601, -0.1522, -0.2679, -0.0829, -0.2464, -0.3532,  0.1567],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xgate: torch.Size([1, 3, 16])\n",
      "tensor([[[ 0.3603,  0.0164, -0.4112, -0.1881, -0.3367,  0.4265,  0.4909,\n",
      "           0.1547,  0.2529,  0.7361, -0.1421, -0.3402, -0.0675, -1.1732,\n",
      "           0.0050,  0.6414],\n",
      "         [-0.2317,  0.1933, -0.0041, -0.5740, -0.4961,  0.2235,  0.6745,\n",
      "           0.1164,  0.4180,  0.4651,  0.0701, -0.3270, -0.5229, -1.1674,\n",
      "          -0.0833,  0.6680],\n",
      "         [ 0.1234,  0.0730,  0.0679, -0.6858, -0.5096,  0.2597,  0.7493,\n",
      "           0.1628,  0.5249,  0.6581, -0.4452, -0.4891, -0.4652, -1.0625,\n",
      "           0.1398,  0.5043]]], grad_fn=<AddBackward0>)\n",
      "GeLU'ed Xgate: torch.Size([1, 3, 16])\n",
      "tensor([[[ 0.2309,  0.0083, -0.1400, -0.0800, -0.1240,  0.2837,  0.3378,\n",
      "           0.0869,  0.1517,  0.5661, -0.0630, -0.1248, -0.0319, -0.1412,\n",
      "           0.0025,  0.4742],\n",
      "         [-0.0946,  0.1115, -0.0020, -0.1624, -0.1537,  0.1315,  0.5058,\n",
      "           0.0636,  0.2767,  0.3159,  0.0370, -0.1216, -0.1571, -0.1419,\n",
      "          -0.0389,  0.4996],\n",
      "         [ 0.0677,  0.0386,  0.0358, -0.1690, -0.1555,  0.1565,  0.5793,\n",
      "           0.0919,  0.3675,  0.4901, -0.1461, -0.1528, -0.1493, -0.1530,\n",
      "           0.0777,  0.3495]]], grad_fn=<GeluBackward0>)\n",
      "Wup: torch.Size([8, 32])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0268,  0.2920, -0.1318,  0.2792, -0.2422, -0.1329,  0.0137,  0.3038,\n",
      "         -0.0630, -0.1378, -0.1249,  0.1764, -0.2337, -0.1456, -0.2696, -0.3236,\n",
      "          0.2206,  0.2434,  0.1670, -0.2196,  0.2458, -0.2051, -0.0078, -0.1489,\n",
      "          0.1243,  0.2190,  0.2708,  0.1128,  0.1080, -0.1787,  0.2409, -0.2559],\n",
      "        [-0.1157, -0.0387,  0.3377, -0.0976, -0.3353, -0.0268, -0.3394,  0.1140,\n",
      "         -0.2785, -0.2823,  0.0133,  0.0321, -0.1985,  0.1899, -0.2794, -0.2885,\n",
      "         -0.0566,  0.2240, -0.3250,  0.0711, -0.1741, -0.2567,  0.2031, -0.1600,\n",
      "          0.0013, -0.2023, -0.3176, -0.0830,  0.0930,  0.0006,  0.2568,  0.0935],\n",
      "        [ 0.2206, -0.1426, -0.0313,  0.2701, -0.2374, -0.1265, -0.3009,  0.1944,\n",
      "         -0.1489, -0.1974, -0.1729,  0.2036, -0.2576,  0.2773, -0.1522, -0.2090,\n",
      "         -0.0228,  0.3033, -0.2258, -0.0199,  0.1763, -0.3232, -0.0802, -0.1439,\n",
      "          0.0363,  0.0169, -0.2768, -0.3209, -0.0886,  0.2632,  0.0900,  0.2658],\n",
      "        [ 0.2142,  0.0301,  0.1310, -0.2199,  0.0737,  0.1692,  0.2331, -0.1438,\n",
      "          0.0868, -0.2813,  0.1128,  0.2285,  0.0121,  0.2658,  0.0241,  0.0775,\n",
      "         -0.0448, -0.3430, -0.2467,  0.1803,  0.1606, -0.1126,  0.0440, -0.0988,\n",
      "         -0.1158,  0.1621, -0.3395,  0.3291,  0.1983,  0.2865,  0.0913,  0.1180],\n",
      "        [ 0.3456, -0.2113, -0.0703,  0.1868,  0.0631,  0.3280,  0.0049, -0.1273,\n",
      "          0.2151, -0.2605,  0.0408,  0.2169,  0.1804, -0.1579, -0.0602,  0.0223,\n",
      "         -0.0321, -0.1922,  0.1666, -0.1290, -0.0873,  0.1126, -0.2831, -0.3262,\n",
      "          0.2222, -0.1025,  0.1793,  0.0447,  0.0818,  0.3532, -0.1969, -0.1674],\n",
      "        [ 0.2789,  0.2317, -0.1835, -0.2653,  0.3470, -0.0715, -0.1296, -0.0693,\n",
      "         -0.0136, -0.2091, -0.1143,  0.0326,  0.3440, -0.3530,  0.2238,  0.2619,\n",
      "         -0.2475,  0.3140, -0.1574, -0.3004,  0.0164,  0.0448,  0.0383, -0.1356,\n",
      "         -0.1036,  0.0626,  0.1360,  0.0313,  0.0224,  0.2718,  0.2952, -0.0090],\n",
      "        [-0.2549, -0.0635, -0.2328, -0.2210, -0.1177,  0.1228, -0.0705, -0.1101,\n",
      "         -0.2002,  0.1781,  0.3490,  0.1073,  0.0612,  0.0871, -0.0584, -0.1931,\n",
      "         -0.1130, -0.1739, -0.1016, -0.2508, -0.1755, -0.1859, -0.2575,  0.1326,\n",
      "          0.3413, -0.2647,  0.2159, -0.1947, -0.1044,  0.1775, -0.0478,  0.2331],\n",
      "        [-0.1757,  0.3104,  0.0320, -0.1652,  0.2390, -0.2291,  0.1567, -0.1867,\n",
      "         -0.0236,  0.2079,  0.1813,  0.1892, -0.0353,  0.0294, -0.1266,  0.3187,\n",
      "         -0.2651,  0.0514,  0.0008,  0.1024, -0.0861, -0.1158,  0.0874,  0.3265,\n",
      "         -0.3153,  0.1903,  0.0746, -0.1074,  0.2422, -0.3080, -0.0438,  0.2794]],\n",
      "       requires_grad=True)\n",
      "Wup spliced: torch.Size([4, 16])\n",
      "tensor([[-0.0321, -0.1922,  0.1666, -0.1290, -0.0873,  0.1126, -0.2831, -0.3262,\n",
      "          0.2222, -0.1025,  0.1793,  0.0447,  0.0818,  0.3532, -0.1969, -0.1674],\n",
      "        [-0.2475,  0.3140, -0.1574, -0.3004,  0.0164,  0.0448,  0.0383, -0.1356,\n",
      "         -0.1036,  0.0626,  0.1360,  0.0313,  0.0224,  0.2718,  0.2952, -0.0090],\n",
      "        [-0.1130, -0.1739, -0.1016, -0.2508, -0.1755, -0.1859, -0.2575,  0.1326,\n",
      "          0.3413, -0.2647,  0.2159, -0.1947, -0.1044,  0.1775, -0.0478,  0.2331],\n",
      "        [-0.2651,  0.0514,  0.0008,  0.1024, -0.0861, -0.1158,  0.0874,  0.3265,\n",
      "         -0.3153,  0.1903,  0.0746, -0.1074,  0.2422, -0.3080, -0.0438,  0.2794]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Bup: torch.Size([32])\n",
      "Parameter containing:\n",
      "tensor([-0.1263,  0.3174, -0.3065,  0.2315,  0.3079, -0.1040,  0.1689, -0.2727,\n",
      "        -0.0686,  0.0261,  0.2942,  0.1003, -0.0185,  0.0841,  0.2196, -0.0217,\n",
      "         0.1145, -0.0702, -0.1810, -0.2465,  0.0124, -0.0137, -0.1668, -0.0974,\n",
      "         0.1849,  0.3159, -0.0158, -0.2216,  0.2333,  0.1893,  0.2655,  0.1137],\n",
      "       requires_grad=True)\n",
      "Bup spliced: torch.Size([16])\n",
      "tensor([ 0.1145, -0.0702, -0.1810, -0.2465,  0.0124, -0.0137, -0.1668, -0.0974,\n",
      "         0.1849,  0.3159, -0.0158, -0.2216,  0.2333,  0.1893,  0.2655,  0.1137],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xup: torch.Size([1, 3, 16])\n",
      "tensor([[[-0.4338, -0.1922, -0.0521, -0.4505, -0.2770, -0.0813, -0.4768,\n",
      "          -0.0492,  0.0905,  0.3947,  0.4417, -0.3724,  0.6640,  0.3511,\n",
      "           0.0340,  0.3754],\n",
      "         [-0.6110,  0.1308, -0.4605, -0.7727, -0.2793, -0.2993, -0.3452,\n",
      "           0.2467,  0.1129,  0.2953,  0.4929, -0.5299,  0.4296,  0.3815,\n",
      "           0.4783,  0.6795],\n",
      "         [-0.6097,  0.1819, -0.3374, -0.4348, -0.2126, -0.2534, -0.1224,\n",
      "           0.3963, -0.2846,  0.5763,  0.3402, -0.4680,  0.6414, -0.0203,\n",
      "           0.3894,  0.6875]]], grad_fn=<AddBackward0>)\n",
      "Xfuse: torch.Size([1, 3, 16])\n",
      "tensor([[[-1.0016e-01, -1.5961e-03,  7.2965e-03,  3.6054e-02,  3.4336e-02,\n",
      "          -2.3064e-02, -1.6109e-01, -4.2705e-03,  1.3722e-02,  2.2344e-01,\n",
      "          -2.7845e-02,  4.6475e-02, -2.1192e-02, -4.9571e-02,  8.5352e-05,\n",
      "           1.7803e-01],\n",
      "         [ 5.7810e-02,  1.4581e-02,  9.2945e-04,  1.2550e-01,  4.2946e-02,\n",
      "          -3.9350e-02, -1.7464e-01,  1.5682e-02,  3.1246e-02,  9.3275e-02,\n",
      "           1.8252e-02,  6.4437e-02, -6.7510e-02, -5.4127e-02, -1.8599e-02,\n",
      "           3.3947e-01],\n",
      "         [-4.1300e-02,  7.0245e-03, -1.2084e-02,  7.3479e-02,  3.3064e-02,\n",
      "          -3.9652e-02, -7.0921e-02,  3.6425e-02, -1.0459e-01,  2.8247e-01,\n",
      "          -4.9696e-02,  7.1510e-02, -9.5744e-02,  3.1119e-03,  3.0256e-02,\n",
      "           2.4026e-01]]], grad_fn=<MulBackward0>)\n",
      "Wdown: torch.Size([32, 8])\n",
      "Parameter containing:\n",
      "tensor([[-0.1241,  0.0783,  0.0730,  0.0259, -0.1381,  0.0747, -0.1715,  0.1537],\n",
      "        [-0.1596, -0.1684, -0.0292, -0.0412,  0.0923,  0.1547,  0.0334,  0.1289],\n",
      "        [ 0.1213,  0.1457, -0.0210,  0.1240,  0.0637,  0.0740,  0.0162, -0.0739],\n",
      "        [-0.0862, -0.0014, -0.0264,  0.1342, -0.1002,  0.0110,  0.1300, -0.0151],\n",
      "        [-0.0040,  0.0095,  0.0276,  0.1669,  0.0372,  0.0579,  0.1159,  0.1258],\n",
      "        [ 0.1371,  0.1380,  0.0287,  0.0697, -0.0781,  0.1642, -0.1018, -0.1430],\n",
      "        [ 0.0996, -0.1285,  0.1319, -0.0708, -0.0266, -0.1723, -0.1559,  0.0321],\n",
      "        [ 0.1390,  0.1571,  0.1646, -0.0278,  0.0746, -0.1574, -0.1281,  0.0912],\n",
      "        [-0.1040,  0.0916,  0.0430, -0.0957, -0.0408, -0.1658, -0.1142,  0.0173],\n",
      "        [-0.0722,  0.0830,  0.0118, -0.0619, -0.1046,  0.1278, -0.0762,  0.0279],\n",
      "        [-0.0272,  0.0816,  0.1482, -0.0292,  0.1253,  0.0815, -0.0249, -0.0581],\n",
      "        [-0.0029,  0.0515, -0.0707,  0.0132, -0.1068, -0.0613,  0.1656,  0.1624],\n",
      "        [ 0.0510,  0.1066, -0.0719,  0.0784,  0.1125,  0.0504,  0.0547, -0.1580],\n",
      "        [ 0.0778,  0.0588, -0.1735,  0.0080, -0.0231, -0.1132,  0.0483,  0.0880],\n",
      "        [-0.0895,  0.1129, -0.1432,  0.0801, -0.0021,  0.0963, -0.1706, -0.1499],\n",
      "        [ 0.1003, -0.1261,  0.0942,  0.0259, -0.0838, -0.0652,  0.1394, -0.0694],\n",
      "        [ 0.0823, -0.0817,  0.0455,  0.0296,  0.1454, -0.0793,  0.1062,  0.0904],\n",
      "        [-0.0927,  0.0954,  0.1379,  0.0037,  0.1186, -0.0879, -0.1434, -0.1696],\n",
      "        [ 0.0445,  0.1270, -0.0009,  0.0845,  0.0462,  0.1026,  0.0362, -0.1679],\n",
      "        [-0.1232,  0.0885, -0.1641,  0.1179, -0.1339, -0.0635,  0.0254,  0.1558],\n",
      "        [-0.1700,  0.1163,  0.0696, -0.0375,  0.0400,  0.0781, -0.1068,  0.1074],\n",
      "        [ 0.0985, -0.0615,  0.1425,  0.1567, -0.1377,  0.0810, -0.1679, -0.1053],\n",
      "        [-0.0532,  0.1071, -0.0154,  0.0856, -0.1131,  0.0551, -0.1367,  0.0397],\n",
      "        [-0.0508, -0.0443, -0.1044,  0.0951,  0.1628, -0.1078,  0.1052, -0.1248],\n",
      "        [-0.1044, -0.1298, -0.1095, -0.1548,  0.0819, -0.1494, -0.0378,  0.0422],\n",
      "        [ 0.1118,  0.0006, -0.1601, -0.1347, -0.0039,  0.0746,  0.0424,  0.1572],\n",
      "        [ 0.0751,  0.1399, -0.1155, -0.0073, -0.0762,  0.0377, -0.0870,  0.0093],\n",
      "        [ 0.0435, -0.1709, -0.1514, -0.1676, -0.1712, -0.1218, -0.0800, -0.1674],\n",
      "        [-0.0874,  0.0180,  0.1243, -0.1090,  0.0453,  0.0630, -0.0956,  0.1718],\n",
      "        [ 0.1588, -0.1196, -0.1668, -0.0507, -0.1614,  0.0299,  0.1525,  0.0559],\n",
      "        [ 0.1050, -0.1639, -0.0307,  0.0893, -0.0188,  0.1412,  0.0198, -0.0413],\n",
      "        [-0.0911, -0.1290, -0.0829,  0.1627,  0.0872, -0.1347,  0.0544,  0.1462]],\n",
      "       requires_grad=True)\n",
      "Wdown spliced: torch.Size([16, 4])\n",
      "tensor([[ 0.1454, -0.0793,  0.1062,  0.0904],\n",
      "        [ 0.1186, -0.0879, -0.1434, -0.1696],\n",
      "        [ 0.0462,  0.1026,  0.0362, -0.1679],\n",
      "        [-0.1339, -0.0635,  0.0254,  0.1558],\n",
      "        [ 0.0400,  0.0781, -0.1068,  0.1074],\n",
      "        [-0.1377,  0.0810, -0.1679, -0.1053],\n",
      "        [-0.1131,  0.0551, -0.1367,  0.0397],\n",
      "        [ 0.1628, -0.1078,  0.1052, -0.1248],\n",
      "        [ 0.0819, -0.1494, -0.0378,  0.0422],\n",
      "        [-0.0039,  0.0746,  0.0424,  0.1572],\n",
      "        [-0.0762,  0.0377, -0.0870,  0.0093],\n",
      "        [-0.1712, -0.1218, -0.0800, -0.1674],\n",
      "        [ 0.0453,  0.0630, -0.0956,  0.1718],\n",
      "        [-0.1614,  0.0299,  0.1525,  0.0559],\n",
      "        [-0.0188,  0.1412,  0.0198, -0.0413],\n",
      "        [ 0.0872, -0.1347,  0.0544,  0.1462]], grad_fn=<SliceBackward0>)\n",
      "Bdown: torch.Size([8])\n",
      "Parameter containing:\n",
      "tensor([ 0.1082,  0.0287, -0.0995,  0.0940,  0.0914, -0.0069, -0.1760, -0.0538],\n",
      "       requires_grad=True)\n",
      "Bdown spliced: torch.Size([4])\n",
      "tensor([ 0.0914, -0.0069, -0.1760, -0.0538], grad_fn=<SliceBackward0>)\n",
      "outputs: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.1112, -0.0268, -0.1517, -0.0106],\n",
      "         [ 0.1396, -0.0908, -0.1289,  0.0094],\n",
      "         [ 0.0940, -0.0265, -0.1255,  0.0003]]], grad_fn=<AddBackward0>)\n",
      "------------- END MLP.forwardTensor() ------------\n",
      "layer's final residual state:\n",
      "tensor([[[ 0.6566,  0.1191, -0.0139,  0.6012],\n",
      "         [ 0.2191,  0.8144,  0.7770,  0.9287],\n",
      "         [ 0.1337,  0.3118,  0.0579,  0.7315]]], grad_fn=<AddBackward0>)\n",
      "----------------- END Layer.forwardTensor() --------------------\n",
      "y: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.6566,  0.1191, -0.0139,  0.6012],\n",
      "         [ 0.2191,  0.8144,  0.7770,  0.9287],\n",
      "         [ 0.1337,  0.3118,  0.0579,  0.7315]]], grad_fn=<AddBackward0>)\n",
      "---------- RESET CONFIG --------\n",
      "model_count:  [1, 2, 4]\n",
      "model_dim_list:  [128, 64, 32]\n",
      "head_dim_list:  [32, 16, 8]\n"
     ]
    }
   ],
   "source": [
    "# Testing our Layer's forwardTensor()\n",
    "verbose = True\n",
    "\n",
    "print(\"--------- Micro Hyperparameters -------\")\n",
    "hold1, hold2, hold3, hold4 = config.hidden_size, config.num_attention_heads, config.head_dim, config.max_position_embeddings\n",
    "config.hidden_size = 8\n",
    "config.num_attention_heads = 2\n",
    "config.head_dim = 4\n",
    "config.max_position_embeddings = 3\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "print(\"head_dim_list: \", config.head_dim_list)\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the big model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,3,config.hidden_size)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "layer = Layer(config)\n",
    "y = layer(x)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the first sub-model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,3,config.hidden_size//config.split)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "layer = Layer(config)\n",
    "y = layer(x)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the second sub-model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,3,config.hidden_size//config.split)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "layer = Layer(config)\n",
    "y = layer(x, model=1)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "verbose = False\n",
    "print(\"---------- RESET CONFIG --------\")\n",
    "config.hidden_size = hold1\n",
    "config.num_attention_heads = hold2\n",
    "config.head_dim = hold3\n",
    "config.max_position_embeddings = hold4\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "print(\"head_dim_list: \", config.head_dim_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a2e1e7b-d356-4215-a5ec-1b18c21cd38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Micro Hyperparameters -------\n",
      "model_count:  [1, 2]\n",
      "model_dim_list:  [8, 4]\n",
      "head_dim_list:  [4, 2]\n",
      "x: ((tensor([[[-1.7914, -0.7665, -0.2937, -0.1975, -1.4712, -0.0993, -0.5784,\n",
      "           0.4662],\n",
      "         [ 0.0025,  1.0751,  1.0316,  0.9245,  0.5560, -1.1830,  0.2358,\n",
      "          -0.4465],\n",
      "         [-0.5631,  0.4334,  1.9844, -0.6209,  0.3073, -0.8403, -0.2487,\n",
      "           0.1071]]]),), (tensor([[[ 0.2024,  0.0779,  0.1512, -0.4145],\n",
      "         [-0.5984,  1.4727,  0.0435, -0.8533],\n",
      "         [-2.6348,  1.5094, -1.7780, -1.2693]]]), tensor([[[ 0.5754,  0.3787, -0.9614,  2.2824],\n",
      "         [ 1.4094,  0.2831,  1.7523,  0.6584],\n",
      "         [-0.7426,  0.0394,  0.3453, -0.3844]]])))\n",
      "---------- Layer Input: Tuple ------------\n",
      "------------- Layer.forwardTuple() ------------\n",
      "x:\n",
      "((tensor([[[-1.7914, -0.7665, -0.2937, -0.1975, -1.4712, -0.0993, -0.5784,\n",
      "           0.4662],\n",
      "         [ 0.0025,  1.0751,  1.0316,  0.9245,  0.5560, -1.1830,  0.2358,\n",
      "          -0.4465],\n",
      "         [-0.5631,  0.4334,  1.9844, -0.6209,  0.3073, -0.8403, -0.2487,\n",
      "           0.1071]]]),), (tensor([[[ 0.2024,  0.0779,  0.1512, -0.4145],\n",
      "         [-0.5984,  1.4727,  0.0435, -0.8533],\n",
      "         [-2.6348,  1.5094, -1.7780, -1.2693]]]), tensor([[[ 0.5754,  0.3787, -0.9614,  2.2824],\n",
      "         [ 1.4094,  0.2831,  1.7523,  0.6584],\n",
      "         [-0.7426,  0.0394,  0.3453, -0.3844]]])))\n",
      "input_len: 3\n",
      "num_levels: 2\n",
      "models_per_level: [1, 2]\n",
      "Level 0 from range(2)\n",
      "Model 0 from range(1)\n",
      "----------------- Layer.forwardTensor() --------------------\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 3, 8])\n",
      "tensor([[[-1.7914, -0.7665, -0.2937, -0.1975, -1.4712, -0.0993, -0.5784,\n",
      "           0.4662],\n",
      "         [ 0.0025,  1.0751,  1.0316,  0.9245,  0.5560, -1.1830,  0.2358,\n",
      "          -0.4465],\n",
      "         [-0.5631,  0.4334,  1.9844, -0.6209,  0.3073, -0.8403, -0.2487,\n",
      "           0.1071]]])\n",
      "normed x: torch.Size([1, 3, 8])\n",
      "tensor([[[-1.9651, -0.8409, -0.3222, -0.2167, -1.6139, -0.1089, -0.6345,\n",
      "           0.5114],\n",
      "         [ 0.0031,  1.3548,  1.2999,  1.1649,  0.7006, -1.4907,  0.2972,\n",
      "          -0.5626],\n",
      "         [-0.6670,  0.5133,  2.3505, -0.7354,  0.3640, -0.9953, -0.2945,\n",
      "           0.1269]]])\n",
      "dim: 8\n",
      "skip: 0\n",
      "spliced scale: torch.Size([8])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 3, 8])\n",
      "tensor([[[-1.9651, -0.8409, -0.3222, -0.2167, -1.6139, -0.1089, -0.6345,\n",
      "           0.5114],\n",
      "         [ 0.0031,  1.3548,  1.2999,  1.1649,  0.7006, -1.4907,  0.2972,\n",
      "          -0.5626],\n",
      "         [-0.6670,  0.5133,  2.3505, -0.7354,  0.3640, -0.9953, -0.2945,\n",
      "           0.1269]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "---------- Attention Input: torch.Tensor ------------\n",
      "----------------- MultiQueryAttention.forwardTensor() --------------------\n",
      "x shape: torch.Size([1, 3, 8])\n",
      "d_skip: 0\n",
      "models_in_this_level: 1\n",
      "h_dim: 4\n",
      "h_skip: 0\n",
      "self.Wqkv: torch.Size([8, 16])\n",
      "Parameter containing:\n",
      "tensor([[-0.0340, -0.1815, -0.3210, -0.1459, -0.1058,  0.0699, -0.0545, -0.3136,\n",
      "         -0.1625, -0.3407,  0.0713,  0.1895,  0.1236, -0.0918,  0.1145,  0.0372],\n",
      "        [-0.0765,  0.2838, -0.1542,  0.3024, -0.2310,  0.1960,  0.3432, -0.1336,\n",
      "         -0.0016,  0.1019,  0.3275,  0.0040, -0.2962, -0.2300,  0.1264,  0.3188],\n",
      "        [ 0.2791, -0.0656,  0.0931, -0.1173, -0.0828,  0.0369, -0.2055, -0.0018,\n",
      "          0.0459, -0.0719,  0.1220,  0.1993, -0.0510,  0.2554,  0.2919, -0.1963],\n",
      "        [-0.0580,  0.0674,  0.1863, -0.2996, -0.1225, -0.2918,  0.3034, -0.0289,\n",
      "          0.3401,  0.2546, -0.2533,  0.0773, -0.2015, -0.2837,  0.0371, -0.1208],\n",
      "        [-0.1807,  0.2407, -0.0230, -0.0681,  0.1628, -0.2487,  0.1491,  0.1595,\n",
      "         -0.2048, -0.0425, -0.1204,  0.0531,  0.0862, -0.2871, -0.2986, -0.1081],\n",
      "        [ 0.0699,  0.1571, -0.2550,  0.1524, -0.0307,  0.0327, -0.1545,  0.0394,\n",
      "          0.1334, -0.2649,  0.0729, -0.2974,  0.2341, -0.1605, -0.1469,  0.2894],\n",
      "        [-0.2351,  0.2850, -0.0287, -0.3331,  0.3337,  0.0153,  0.0692,  0.1914,\n",
      "          0.1876, -0.3101, -0.1262, -0.3145,  0.2511, -0.2142,  0.2297, -0.1992],\n",
      "        [-0.3401,  0.1138, -0.3317, -0.0407, -0.1315,  0.2288,  0.2154,  0.3395,\n",
      "          0.1770,  0.0745,  0.1152,  0.0304, -0.0858,  0.0956,  0.2461, -0.0038]],\n",
      "       requires_grad=True)\n",
      "Wq: torch.Size([8, 8])\n",
      "tensor([[-0.0340, -0.1815, -0.3210, -0.1459, -0.1058,  0.0699, -0.0545, -0.3136],\n",
      "        [-0.0765,  0.2838, -0.1542,  0.3024, -0.2310,  0.1960,  0.3432, -0.1336],\n",
      "        [ 0.2791, -0.0656,  0.0931, -0.1173, -0.0828,  0.0369, -0.2055, -0.0018],\n",
      "        [-0.0580,  0.0674,  0.1863, -0.2996, -0.1225, -0.2918,  0.3034, -0.0289],\n",
      "        [-0.1807,  0.2407, -0.0230, -0.0681,  0.1628, -0.2487,  0.1491,  0.1595],\n",
      "        [ 0.0699,  0.1571, -0.2550,  0.1524, -0.0307,  0.0327, -0.1545,  0.0394],\n",
      "        [-0.2351,  0.2850, -0.0287, -0.3331,  0.3337,  0.0153,  0.0692,  0.1914],\n",
      "        [-0.3401,  0.1138, -0.3317, -0.0407, -0.1315,  0.2288,  0.2154,  0.3395]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wk: torch.Size([8, 4])\n",
      "tensor([[-0.1625, -0.3407,  0.0713,  0.1895],\n",
      "        [-0.0016,  0.1019,  0.3275,  0.0040],\n",
      "        [ 0.0459, -0.0719,  0.1220,  0.1993],\n",
      "        [ 0.3401,  0.2546, -0.2533,  0.0773],\n",
      "        [-0.2048, -0.0425, -0.1204,  0.0531],\n",
      "        [ 0.1334, -0.2649,  0.0729, -0.2974],\n",
      "        [ 0.1876, -0.3101, -0.1262, -0.3145],\n",
      "        [ 0.1770,  0.0745,  0.1152,  0.0304]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wv: torch.Size([8, 4])\n",
      "tensor([[ 0.1236, -0.0918,  0.1145,  0.0372],\n",
      "        [-0.2962, -0.2300,  0.1264,  0.3188],\n",
      "        [-0.0510,  0.2554,  0.2919, -0.1963],\n",
      "        [-0.2015, -0.2837,  0.0371, -0.1208],\n",
      "        [ 0.0862, -0.2871, -0.2986, -0.1081],\n",
      "        [ 0.2341, -0.1605, -0.1469,  0.2894],\n",
      "        [ 0.2511, -0.2142,  0.2297, -0.1992],\n",
      "        [-0.0858,  0.0956,  0.2461, -0.0038]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wq spliced: torch.Size([8, 8])\n",
      "tensor([[-0.0340, -0.1815, -0.3210, -0.1459, -0.1058,  0.0699, -0.0545, -0.3136],\n",
      "        [-0.0765,  0.2838, -0.1542,  0.3024, -0.2310,  0.1960,  0.3432, -0.1336],\n",
      "        [ 0.2791, -0.0656,  0.0931, -0.1173, -0.0828,  0.0369, -0.2055, -0.0018],\n",
      "        [-0.0580,  0.0674,  0.1863, -0.2996, -0.1225, -0.2918,  0.3034, -0.0289],\n",
      "        [-0.1807,  0.2407, -0.0230, -0.0681,  0.1628, -0.2487,  0.1491,  0.1595],\n",
      "        [ 0.0699,  0.1571, -0.2550,  0.1524, -0.0307,  0.0327, -0.1545,  0.0394],\n",
      "        [-0.2351,  0.2850, -0.0287, -0.3331,  0.3337,  0.0153,  0.0692,  0.1914],\n",
      "        [-0.3401,  0.1138, -0.3317, -0.0407, -0.1315,  0.2288,  0.2154,  0.3395]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Wk spliced: torch.Size([8, 4])\n",
      "tensor([[-0.1625, -0.3407,  0.0713,  0.1895],\n",
      "        [-0.0016,  0.1019,  0.3275,  0.0040],\n",
      "        [ 0.0459, -0.0719,  0.1220,  0.1993],\n",
      "        [ 0.3401,  0.2546, -0.2533,  0.0773],\n",
      "        [-0.2048, -0.0425, -0.1204,  0.0531],\n",
      "        [ 0.1334, -0.2649,  0.0729, -0.2974],\n",
      "        [ 0.1876, -0.3101, -0.1262, -0.3145],\n",
      "        [ 0.1770,  0.0745,  0.1152,  0.0304]], grad_fn=<CatBackward0>)\n",
      "Wv spliced: torch.Size([8, 4])\n",
      "tensor([[ 0.1236, -0.0918,  0.1145,  0.0372],\n",
      "        [-0.2962, -0.2300,  0.1264,  0.3188],\n",
      "        [-0.0510,  0.2554,  0.2919, -0.1963],\n",
      "        [-0.2015, -0.2837,  0.0371, -0.1208],\n",
      "        [ 0.0862, -0.2871, -0.2986, -0.1081],\n",
      "        [ 0.2341, -0.1605, -0.1469,  0.2894],\n",
      "        [ 0.2511, -0.2142,  0.2297, -0.1992],\n",
      "        [-0.0858,  0.0956,  0.2461, -0.0038]], grad_fn=<CatBackward0>)\n",
      "Wqkv_spliced: torch.Size([8, 16])\n",
      "tensor([[-0.0340, -0.1815, -0.3210, -0.1459, -0.1058,  0.0699, -0.0545, -0.3136,\n",
      "         -0.1625, -0.3407,  0.0713,  0.1895,  0.1236, -0.0918,  0.1145,  0.0372],\n",
      "        [-0.0765,  0.2838, -0.1542,  0.3024, -0.2310,  0.1960,  0.3432, -0.1336,\n",
      "         -0.0016,  0.1019,  0.3275,  0.0040, -0.2962, -0.2300,  0.1264,  0.3188],\n",
      "        [ 0.2791, -0.0656,  0.0931, -0.1173, -0.0828,  0.0369, -0.2055, -0.0018,\n",
      "          0.0459, -0.0719,  0.1220,  0.1993, -0.0510,  0.2554,  0.2919, -0.1963],\n",
      "        [-0.0580,  0.0674,  0.1863, -0.2996, -0.1225, -0.2918,  0.3034, -0.0289,\n",
      "          0.3401,  0.2546, -0.2533,  0.0773, -0.2015, -0.2837,  0.0371, -0.1208],\n",
      "        [-0.1807,  0.2407, -0.0230, -0.0681,  0.1628, -0.2487,  0.1491,  0.1595,\n",
      "         -0.2048, -0.0425, -0.1204,  0.0531,  0.0862, -0.2871, -0.2986, -0.1081],\n",
      "        [ 0.0699,  0.1571, -0.2550,  0.1524, -0.0307,  0.0327, -0.1545,  0.0394,\n",
      "          0.1334, -0.2649,  0.0729, -0.2974,  0.2341, -0.1605, -0.1469,  0.2894],\n",
      "        [-0.2351,  0.2850, -0.0287, -0.3331,  0.3337,  0.0153,  0.0692,  0.1914,\n",
      "          0.1876, -0.3101, -0.1262, -0.3145,  0.2511, -0.2142,  0.2297, -0.1992],\n",
      "        [-0.3401,  0.1138, -0.3317, -0.0407, -0.1315,  0.2288,  0.2154,  0.3395,\n",
      "          0.1770,  0.0745,  0.1152,  0.0304, -0.0858,  0.0956,  0.2461, -0.0038]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "xqkv: torch.Size([1, 3, 16])\n",
      "tensor([[[ 0.3132, -0.4036,  0.6035,  0.4190, -0.0829,  0.2542, -0.3386,\n",
      "           0.5260,  0.5196,  0.8840, -0.0745, -0.2950, -0.3015,  1.0187,\n",
      "           0.0446,  0.0157],\n",
      "         [ 0.0820,  0.3323,  0.6703, -0.4433, -0.2308, -0.3732,  0.7852,\n",
      "          -0.2990,  0.0670,  0.5711,  0.0120,  0.7251, -0.8675, -0.3897,\n",
      "           0.5339, -0.5281],\n",
      "         [ 0.5728, -0.0754,  0.4285,  0.1137, -0.1778,  0.2567, -0.2787,\n",
      "           0.1632, -0.2749,  0.2723,  0.5289,  0.6992, -0.4925,  0.8825,\n",
      "           0.6485, -0.5028]]], grad_fn=<UnsafeViewBackward0>)\n",
      "xq: torch.Size([1, 3, 8])\n",
      "tensor([[[ 0.3132, -0.4036,  0.6035,  0.4190, -0.0829,  0.2542, -0.3386,\n",
      "           0.5260],\n",
      "         [ 0.0820,  0.3323,  0.6703, -0.4433, -0.2308, -0.3732,  0.7852,\n",
      "          -0.2990],\n",
      "         [ 0.5728, -0.0754,  0.4285,  0.1137, -0.1778,  0.2567, -0.2787,\n",
      "           0.1632]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xk: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.5196,  0.8840, -0.0745, -0.2950],\n",
      "         [ 0.0670,  0.5711,  0.0120,  0.7251],\n",
      "         [-0.2749,  0.2723,  0.5289,  0.6992]]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "xv: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.3015,  1.0187,  0.0446,  0.0157],\n",
      "         [-0.8675, -0.3897,  0.5339, -0.5281],\n",
      "         [-0.4925,  0.8825,  0.6485, -0.5028]]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "xq reshaped: torch.Size([1, 3, 2, 4])\n",
      "tensor([[[[ 0.3132, -0.4036,  0.6035,  0.4190],\n",
      "          [-0.0829,  0.2542, -0.3386,  0.5260]],\n",
      "\n",
      "         [[ 0.0820,  0.3323,  0.6703, -0.4433],\n",
      "          [-0.2308, -0.3732,  0.7852, -0.2990]],\n",
      "\n",
      "         [[ 0.5728, -0.0754,  0.4285,  0.1137],\n",
      "          [-0.1778,  0.2567, -0.2787,  0.1632]]]], grad_fn=<ViewBackward0>)\n",
      "xk reshaped: torch.Size([1, 3, 1, 4])\n",
      "tensor([[[[ 0.5196,  0.8840, -0.0745, -0.2950]],\n",
      "\n",
      "         [[ 0.0670,  0.5711,  0.0120,  0.7251]],\n",
      "\n",
      "         [[-0.2749,  0.2723,  0.5289,  0.6992]]]], grad_fn=<ViewBackward0>)\n",
      "xv reshaped: torch.Size([1, 3, 1, 4])\n",
      "tensor([[[[-0.3015,  1.0187,  0.0446,  0.0157]],\n",
      "\n",
      "         [[-0.8675, -0.3897,  0.5339, -0.5281]],\n",
      "\n",
      "         [[-0.4925,  0.8825,  0.6485, -0.5028]]]], grad_fn=<ViewBackward0>)\n",
      "rotated xq: torch.Size([1, 3, 2, 4])\n",
      "tensor([[[[ 0.3132, -0.4036,  0.6035,  0.4190],\n",
      "          [-0.0829,  0.2542, -0.3386,  0.5260]],\n",
      "\n",
      "         [[-0.5197,  0.3749,  0.4311, -0.4079],\n",
      "          [-0.7854, -0.3415,  0.2301, -0.3348]],\n",
      "\n",
      "         [[-0.6280, -0.0964,  0.3426,  0.0964],\n",
      "          [ 0.3274,  0.2191, -0.0457,  0.2110]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "rotated xk: torch.Size([1, 3, 1, 4])\n",
      "tensor([[[[ 0.5196,  0.8840, -0.0745, -0.2950]],\n",
      "\n",
      "         [[ 0.0261,  0.4958,  0.0628,  0.7785]],\n",
      "\n",
      "         [[-0.3665,  0.1279, -0.4700,  0.7394]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "repeat_interleaved xk: torch.Size([1, 3, 2, 4])\n",
      "tensor([[[[ 0.5196,  0.8840, -0.0745, -0.2950],\n",
      "          [ 0.5196,  0.8840, -0.0745, -0.2950]],\n",
      "\n",
      "         [[ 0.0261,  0.4958,  0.0628,  0.7785],\n",
      "          [ 0.0261,  0.4958,  0.0628,  0.7785]],\n",
      "\n",
      "         [[-0.3665,  0.1279, -0.4700,  0.7394],\n",
      "          [-0.3665,  0.1279, -0.4700,  0.7394]]]], grad_fn=<ViewBackward0>)\n",
      "repeat_interleaved xv: torch.Size([1, 3, 2, 4])\n",
      "tensor([[[[-0.3015,  1.0187,  0.0446,  0.0157],\n",
      "          [-0.3015,  1.0187,  0.0446,  0.0157]],\n",
      "\n",
      "         [[-0.8675, -0.3897,  0.5339, -0.5281],\n",
      "          [-0.8675, -0.3897,  0.5339, -0.5281]],\n",
      "\n",
      "         [[-0.4925,  0.8825,  0.6485, -0.5028],\n",
      "          [-0.4925,  0.8825,  0.6485, -0.5028]]]], grad_fn=<ViewBackward0>)\n",
      "transposed xq: torch.Size([1, 2, 3, 4])\n",
      "tensor([[[[ 0.3132, -0.4036,  0.6035,  0.4190],\n",
      "          [-0.5197,  0.3749,  0.4311, -0.4079],\n",
      "          [-0.6280, -0.0964,  0.3426,  0.0964]],\n",
      "\n",
      "         [[-0.0829,  0.2542, -0.3386,  0.5260],\n",
      "          [-0.7854, -0.3415,  0.2301, -0.3348],\n",
      "          [ 0.3274,  0.2191, -0.0457,  0.2110]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "transposed xk: torch.Size([1, 2, 3, 4])\n",
      "tensor([[[[ 0.5196,  0.8840, -0.0745, -0.2950],\n",
      "          [ 0.0261,  0.4958,  0.0628,  0.7785],\n",
      "          [-0.3665,  0.1279, -0.4700,  0.7394]],\n",
      "\n",
      "         [[ 0.5196,  0.8840, -0.0745, -0.2950],\n",
      "          [ 0.0261,  0.4958,  0.0628,  0.7785],\n",
      "          [-0.3665,  0.1279, -0.4700,  0.7394]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "transposed xv: torch.Size([1, 2, 3, 4])\n",
      "tensor([[[[-0.3015,  1.0187,  0.0446,  0.0157],\n",
      "          [-0.8675, -0.3897,  0.5339, -0.5281],\n",
      "          [-0.4925,  0.8825,  0.6485, -0.5028]],\n",
      "\n",
      "         [[-0.3015,  1.0187,  0.0446,  0.0157],\n",
      "          [-0.8675, -0.3897,  0.5339, -0.5281],\n",
      "          [-0.4925,  0.8825,  0.6485, -0.5028]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[-0.1813,  0.0861, -0.0701],\n",
      "          [ 0.0748, -0.0591, -0.1329],\n",
      "          [-0.2328,  0.0162,  0.0641]],\n",
      "\n",
      "         [[ 0.0259,  0.2560,  0.3055],\n",
      "          [-0.3142, -0.2180, -0.0558],\n",
      "          [ 0.1525,  0.1393,  0.0428]]]], grad_fn=<MulBackward0>)\n",
      "mask: torch.Size([1, 1, 3, 3])\n",
      "tensor([[[[ 0.0000e+00, -2.3820e+38, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]]]])\n",
      "masked scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[-1.8132e-01, -2.3820e+38, -2.3820e+38],\n",
      "          [ 7.4795e-02, -5.9063e-02, -2.3820e+38],\n",
      "          [-2.3275e-01,  1.6208e-02,  6.4052e-02]],\n",
      "\n",
      "         [[ 2.5852e-02, -2.3820e+38, -2.3820e+38],\n",
      "          [-3.1419e-01, -2.1800e-01, -2.3820e+38],\n",
      "          [ 1.5250e-01,  1.3927e-01,  4.2754e-02]]]], grad_fn=<AddBackward0>)\n",
      "softmaxed scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [0.5334, 0.4666, 0.0000],\n",
      "          [0.2756, 0.3535, 0.3709]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.4760, 0.5240, 0.0000],\n",
      "          [0.3469, 0.3423, 0.3108]]]], grad_fn=<SoftmaxBackward0>)\n",
      "attention: torch.Size([1, 2, 3, 4])\n",
      "tensor([[[[-0.3015,  1.0187,  0.0446,  0.0157],\n",
      "          [-0.5656,  0.3616,  0.2729, -0.2380],\n",
      "          [-0.5724,  0.4703,  0.4415, -0.3688]],\n",
      "\n",
      "         [[-0.3015,  1.0187,  0.0446,  0.0157],\n",
      "          [-0.5981,  0.2806,  0.3010, -0.2692],\n",
      "          [-0.5546,  0.4942,  0.3998, -0.3316]]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "reshaped attention: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.3015,  1.0187,  0.0446,  0.0157, -0.3015,  1.0187,  0.0446,\n",
      "           0.0157],\n",
      "         [-0.5656,  0.3616,  0.2729, -0.2380, -0.5981,  0.2806,  0.3010,\n",
      "          -0.2692],\n",
      "         [-0.5724,  0.4703,  0.4415, -0.3688, -0.5546,  0.4942,  0.3998,\n",
      "          -0.3316]]], grad_fn=<ViewBackward0>)\n",
      "self.Wo: torch.Size([8, 8])\n",
      "Parameter containing:\n",
      "tensor([[ 0.2720, -0.0599,  0.1861, -0.1372, -0.1935, -0.1548, -0.3042,  0.1437],\n",
      "        [ 0.0298,  0.1093, -0.0187, -0.1633, -0.1128,  0.1514,  0.1242,  0.2416],\n",
      "        [ 0.1782,  0.0222,  0.2303,  0.3073, -0.2227, -0.0706,  0.3020, -0.2865],\n",
      "        [-0.2260,  0.2797, -0.3507,  0.0173, -0.1894,  0.3031,  0.2639, -0.2744],\n",
      "        [ 0.0999, -0.0104,  0.0846,  0.0732,  0.2059, -0.3449, -0.1057, -0.1274],\n",
      "        [ 0.2284, -0.1917, -0.3070, -0.3047, -0.1733, -0.1838,  0.3154,  0.0262],\n",
      "        [-0.1188, -0.0889, -0.0819,  0.0254, -0.0091,  0.0690,  0.0810,  0.0455],\n",
      "        [-0.1355,  0.1819, -0.1108, -0.2114,  0.1157,  0.0880,  0.0930,  0.1926]],\n",
      "       requires_grad=True)\n",
      "spliced Wo: torch.Size([8, 8])\n",
      "tensor([[ 0.2720, -0.0599,  0.1861, -0.1372, -0.1935, -0.1548, -0.3042,  0.1437],\n",
      "        [ 0.0298,  0.1093, -0.0187, -0.1633, -0.1128,  0.1514,  0.1242,  0.2416],\n",
      "        [ 0.1782,  0.0222,  0.2303,  0.3073, -0.2227, -0.0706,  0.3020, -0.2865],\n",
      "        [-0.2260,  0.2797, -0.3507,  0.0173, -0.1894,  0.3031,  0.2639, -0.2744],\n",
      "        [ 0.0999, -0.0104,  0.0846,  0.0732,  0.2059, -0.3449, -0.1057, -0.1274],\n",
      "        [ 0.2284, -0.1917, -0.3070, -0.3047, -0.1733, -0.1838,  0.3154,  0.0262],\n",
      "        [-0.1188, -0.0889, -0.0819,  0.0254, -0.0091,  0.0690,  0.0810,  0.0455],\n",
      "        [-0.1355,  0.1819, -0.1108, -0.2114,  0.1157,  0.0880,  0.0930,  0.1926]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "projected output: torch.Size([1, 3, 8])\n",
      "tensor([[[ 0.1478, -0.0585, -0.4140, -0.4457, -0.3067,  0.1237,  0.5940,\n",
      "           0.2559],\n",
      "         [-0.0356, -0.1104, -0.0972,  0.0336, -0.1527,  0.2026,  0.3877,\n",
      "           0.0386],\n",
      "         [ 0.0752, -0.1925, -0.0789,  0.0201, -0.2126,  0.1156,  0.4846,\n",
      "           0.0440]]], grad_fn=<UnsafeViewBackward0>)\n",
      "----------------- END MultiQueryAttention.forwardTensor() --------------------\n",
      "x in layer after MQA & resid connection and before MLP:\n",
      "tensor([[[-1.6435, -0.8250, -0.7077, -0.6432, -1.7779,  0.0244,  0.0156,\n",
      "           0.7220],\n",
      "         [-0.0331,  0.9647,  0.9344,  0.9580,  0.4032, -0.9804,  0.6235,\n",
      "          -0.4079],\n",
      "         [-0.4879,  0.2409,  1.9055, -0.6008,  0.0947, -0.7247,  0.2360,\n",
      "           0.1511]]], grad_fn=<AddBackward0>)\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 3, 8])\n",
      "tensor([[[-1.6435, -0.8250, -0.7077, -0.6432, -1.7779,  0.0244,  0.0156,\n",
      "           0.7220],\n",
      "         [-0.0331,  0.9647,  0.9344,  0.9580,  0.4032, -0.9804,  0.6235,\n",
      "          -0.4079],\n",
      "         [-0.4879,  0.2409,  1.9055, -0.6008,  0.0947, -0.7247,  0.2360,\n",
      "           0.1511]]], grad_fn=<AddBackward0>)\n",
      "normed x: torch.Size([1, 3, 8])\n",
      "tensor([[[-1.6456, -0.8260, -0.7086, -0.6441, -1.7802,  0.0244,  0.0156,\n",
      "           0.7230],\n",
      "         [-0.0447,  1.3006,  1.2597,  1.2916,  0.5436, -1.3217,  0.8406,\n",
      "          -0.5499],\n",
      "         [-0.6234,  0.3078,  2.4346, -0.7676,  0.1210, -0.9259,  0.3015,\n",
      "           0.1931]]], grad_fn=<MulBackward0>)\n",
      "dim: 8\n",
      "skip: 0\n",
      "spliced scale: torch.Size([8])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 3, 8])\n",
      "tensor([[[-1.6456, -0.8260, -0.7086, -0.6441, -1.7802,  0.0244,  0.0156,\n",
      "           0.7230],\n",
      "         [-0.0447,  1.3006,  1.2597,  1.2916,  0.5436, -1.3217,  0.8406,\n",
      "          -0.5499],\n",
      "         [-0.6234,  0.3078,  2.4346, -0.7676,  0.1210, -0.9259,  0.3015,\n",
      "           0.1931]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "---------- MLP Input: torch.Tensor ------------\n",
      "------------- MLP.forwardTensor() ------------\n",
      "x: torch.Size([1, 3, 8])\n",
      "tensor([[[-1.6456, -0.8260, -0.7086, -0.6441, -1.7802,  0.0244,  0.0156,\n",
      "           0.7230],\n",
      "         [-0.0447,  1.3006,  1.2597,  1.2916,  0.5436, -1.3217,  0.8406,\n",
      "          -0.5499],\n",
      "         [-0.6234,  0.3078,  2.4346, -0.7676,  0.1210, -0.9259,  0.3015,\n",
      "           0.1931]]], grad_fn=<MulBackward0>)\n",
      "d_dim: 8\n",
      "d_skip: 0\n",
      "i_dim: 32\n",
      "i_skip: 0\n",
      "Wgate: torch.Size([8, 32])\n",
      "Parameter containing:\n",
      "tensor([[ 0.2630,  0.3304, -0.2631, -0.3178,  0.3219, -0.1386,  0.0899, -0.1480,\n",
      "          0.3116,  0.2974, -0.1163,  0.0538, -0.2158,  0.2218,  0.2058,  0.3158,\n",
      "          0.0195, -0.0748,  0.2612,  0.0902,  0.2668, -0.1171,  0.0344, -0.2618,\n",
      "         -0.2624,  0.0140, -0.1836, -0.2710,  0.1948,  0.1456,  0.0496,  0.1199],\n",
      "        [ 0.1676,  0.1079, -0.1379, -0.1800, -0.1866,  0.0735,  0.0232,  0.3478,\n",
      "         -0.0545,  0.0847,  0.0208, -0.3432,  0.0005, -0.0354,  0.1616, -0.1833,\n",
      "         -0.2717,  0.1721,  0.3339, -0.1524, -0.3199,  0.1380,  0.3210, -0.0216,\n",
      "          0.2402, -0.0941,  0.1791, -0.1290,  0.2034, -0.2613, -0.2594, -0.0470],\n",
      "        [ 0.1839,  0.2218, -0.1851, -0.1301,  0.0134,  0.0530, -0.0466, -0.1877,\n",
      "          0.1577,  0.0715,  0.0227,  0.0646,  0.3183,  0.0467,  0.0097,  0.2110,\n",
      "          0.1942, -0.0720, -0.3510,  0.2467,  0.3202, -0.3234, -0.2969,  0.1870,\n",
      "         -0.1467,  0.2700,  0.3276,  0.3361,  0.2249, -0.1557, -0.2221, -0.1109],\n",
      "        [ 0.0458,  0.3373,  0.2697, -0.2697, -0.3212, -0.0153,  0.1222,  0.1895,\n",
      "          0.1133,  0.1937, -0.0664,  0.0580, -0.1034, -0.0585, -0.0800,  0.2740,\n",
      "         -0.1762, -0.2514, -0.1972,  0.0756,  0.0571, -0.1100, -0.0780,  0.0126,\n",
      "          0.2733, -0.2975,  0.3090,  0.1289, -0.1540,  0.3491, -0.1759,  0.1693],\n",
      "        [-0.0699, -0.2775,  0.0744,  0.3324, -0.1086, -0.3070,  0.3282,  0.0535,\n",
      "          0.2561,  0.2565,  0.1168,  0.2214, -0.2430, -0.0757, -0.0059,  0.1686,\n",
      "         -0.0024,  0.0463,  0.0686, -0.1675,  0.3514,  0.1367,  0.2145,  0.3221,\n",
      "          0.3521, -0.1050, -0.1648, -0.1585,  0.1677,  0.3048, -0.0717,  0.0128],\n",
      "        [-0.2278,  0.2873,  0.0899, -0.0237, -0.2272, -0.2384,  0.3030, -0.2309,\n",
      "          0.0950, -0.1990,  0.1762,  0.1141,  0.3087, -0.0611,  0.1287, -0.0067,\n",
      "         -0.2072, -0.3240, -0.2671,  0.3352, -0.2212, -0.2166, -0.2868, -0.0535,\n",
      "          0.0012, -0.0098, -0.2042, -0.0029, -0.2062,  0.0910, -0.1916, -0.0311],\n",
      "        [ 0.0901,  0.1729,  0.2315,  0.1777, -0.1764, -0.0368,  0.1807, -0.0849,\n",
      "          0.1730, -0.1213, -0.2346,  0.1807,  0.0504,  0.1159, -0.3311, -0.2011,\n",
      "          0.0062,  0.2386,  0.1295, -0.2162, -0.1954, -0.1578,  0.0050, -0.1642,\n",
      "         -0.2161, -0.2246,  0.2946, -0.1803, -0.0570, -0.2793,  0.0114, -0.2245],\n",
      "        [ 0.0996,  0.1585,  0.1973,  0.3181,  0.0441,  0.0943,  0.3289,  0.0041,\n",
      "          0.1049,  0.1627, -0.0094,  0.1471, -0.1573,  0.2798,  0.1198,  0.1096,\n",
      "          0.1820,  0.2727, -0.1306,  0.0610, -0.1315, -0.3494,  0.0193,  0.1307,\n",
      "          0.2901,  0.0608, -0.0100, -0.1351, -0.0377, -0.1324,  0.0643, -0.0645]],\n",
      "       requires_grad=True)\n",
      "Wgate spliced: torch.Size([8, 32])\n",
      "tensor([[ 0.2630,  0.3304, -0.2631, -0.3178,  0.3219, -0.1386,  0.0899, -0.1480,\n",
      "          0.3116,  0.2974, -0.1163,  0.0538, -0.2158,  0.2218,  0.2058,  0.3158,\n",
      "          0.0195, -0.0748,  0.2612,  0.0902,  0.2668, -0.1171,  0.0344, -0.2618,\n",
      "         -0.2624,  0.0140, -0.1836, -0.2710,  0.1948,  0.1456,  0.0496,  0.1199],\n",
      "        [ 0.1676,  0.1079, -0.1379, -0.1800, -0.1866,  0.0735,  0.0232,  0.3478,\n",
      "         -0.0545,  0.0847,  0.0208, -0.3432,  0.0005, -0.0354,  0.1616, -0.1833,\n",
      "         -0.2717,  0.1721,  0.3339, -0.1524, -0.3199,  0.1380,  0.3210, -0.0216,\n",
      "          0.2402, -0.0941,  0.1791, -0.1290,  0.2034, -0.2613, -0.2594, -0.0470],\n",
      "        [ 0.1839,  0.2218, -0.1851, -0.1301,  0.0134,  0.0530, -0.0466, -0.1877,\n",
      "          0.1577,  0.0715,  0.0227,  0.0646,  0.3183,  0.0467,  0.0097,  0.2110,\n",
      "          0.1942, -0.0720, -0.3510,  0.2467,  0.3202, -0.3234, -0.2969,  0.1870,\n",
      "         -0.1467,  0.2700,  0.3276,  0.3361,  0.2249, -0.1557, -0.2221, -0.1109],\n",
      "        [ 0.0458,  0.3373,  0.2697, -0.2697, -0.3212, -0.0153,  0.1222,  0.1895,\n",
      "          0.1133,  0.1937, -0.0664,  0.0580, -0.1034, -0.0585, -0.0800,  0.2740,\n",
      "         -0.1762, -0.2514, -0.1972,  0.0756,  0.0571, -0.1100, -0.0780,  0.0126,\n",
      "          0.2733, -0.2975,  0.3090,  0.1289, -0.1540,  0.3491, -0.1759,  0.1693],\n",
      "        [-0.0699, -0.2775,  0.0744,  0.3324, -0.1086, -0.3070,  0.3282,  0.0535,\n",
      "          0.2561,  0.2565,  0.1168,  0.2214, -0.2430, -0.0757, -0.0059,  0.1686,\n",
      "         -0.0024,  0.0463,  0.0686, -0.1675,  0.3514,  0.1367,  0.2145,  0.3221,\n",
      "          0.3521, -0.1050, -0.1648, -0.1585,  0.1677,  0.3048, -0.0717,  0.0128],\n",
      "        [-0.2278,  0.2873,  0.0899, -0.0237, -0.2272, -0.2384,  0.3030, -0.2309,\n",
      "          0.0950, -0.1990,  0.1762,  0.1141,  0.3087, -0.0611,  0.1287, -0.0067,\n",
      "         -0.2072, -0.3240, -0.2671,  0.3352, -0.2212, -0.2166, -0.2868, -0.0535,\n",
      "          0.0012, -0.0098, -0.2042, -0.0029, -0.2062,  0.0910, -0.1916, -0.0311],\n",
      "        [ 0.0901,  0.1729,  0.2315,  0.1777, -0.1764, -0.0368,  0.1807, -0.0849,\n",
      "          0.1730, -0.1213, -0.2346,  0.1807,  0.0504,  0.1159, -0.3311, -0.2011,\n",
      "          0.0062,  0.2386,  0.1295, -0.2162, -0.1954, -0.1578,  0.0050, -0.1642,\n",
      "         -0.2161, -0.2246,  0.2946, -0.1803, -0.0570, -0.2793,  0.0114, -0.2245],\n",
      "        [ 0.0996,  0.1585,  0.1973,  0.3181,  0.0441,  0.0943,  0.3289,  0.0041,\n",
      "          0.1049,  0.1627, -0.0094,  0.1471, -0.1573,  0.2798,  0.1198,  0.1096,\n",
      "          0.1820,  0.2727, -0.1306,  0.0610, -0.1315, -0.3494,  0.0193,  0.1307,\n",
      "          0.2901,  0.0608, -0.0100, -0.1351, -0.0377, -0.1324,  0.0643, -0.0645]],\n",
      "       grad_fn=<AliasBackward0>)\n",
      "Bgate: torch.Size([32])\n",
      "Parameter containing:\n",
      "tensor([ 0.2499,  0.1922, -0.2573, -0.2518, -0.1869, -0.0425,  0.2350,  0.1774,\n",
      "         0.0343, -0.0149,  0.0916, -0.0965,  0.0010, -0.1320,  0.1573, -0.1944,\n",
      "         0.1862,  0.2137, -0.0462, -0.2737, -0.2999, -0.0852,  0.1353,  0.2650,\n",
      "        -0.0265, -0.0306, -0.0561, -0.0669,  0.1266,  0.0516, -0.1485,  0.0746],\n",
      "       requires_grad=True)\n",
      "Bgate spliced: torch.Size([32])\n",
      "tensor([ 0.2499,  0.1922, -0.2573, -0.2518, -0.1869, -0.0425,  0.2350,  0.1774,\n",
      "         0.0343, -0.0149,  0.0916, -0.0965,  0.0010, -0.1320,  0.1573, -0.1944,\n",
      "         0.1862,  0.2137, -0.0462, -0.2737, -0.2999, -0.0852,  0.1353,  0.2650,\n",
      "        -0.0265, -0.0306, -0.0561, -0.0669,  0.1266,  0.0516, -0.1485,  0.0746],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xgate: torch.Size([1, 3, 32])\n",
      "tensor([[[-0.2889, -0.1967,  0.2631,  0.3262, -0.1483,  0.7054, -0.3141,\n",
      "           0.0454, -0.9932, -1.0954,  0.0783, -0.2669,  0.5239, -0.1259,\n",
      "          -0.1751, -1.1129,  0.4852,  0.5182, -0.5972, -0.1725, -1.4673,\n",
      "          -0.2102, -0.3006,  0.0903, -0.2855,  0.2515, -0.0472,  0.3459,\n",
      "          -0.7539, -0.7273,  0.4245, -0.1880],\n",
      "         [ 1.0311,  0.5604, -0.3021, -0.7974, -0.7733,  0.1716,  0.1093,\n",
      "           0.9053,  0.3960,  0.6333, -0.2947, -0.3484, -0.1325, -0.2214,\n",
      "          -0.2505,  0.0438,  0.0267,  0.5293,  0.2507, -0.8169,  0.1406,\n",
      "          -0.0298,  0.5659,  0.5364,  0.3145, -0.4643,  1.4301,  0.2077,\n",
      "           0.8033, -0.1565, -0.8066, -0.0180],\n",
      "         [ 0.7990,  0.0836, -0.7598, -0.0417, -0.0133,  0.3979, -0.1440,\n",
      "          -0.0303,  0.1359,  0.0613,  0.0551, -0.1189,  0.6597,  0.0139,\n",
      "          -0.0329, -0.1572,  0.9269,  0.7611, -0.5400, -0.2183,  0.3340,\n",
      "          -0.5706, -0.1534,  0.9313, -0.3236,  0.7576,  0.9298,  0.6848,\n",
      "           0.9203, -0.9237, -0.4805, -0.4644]]], grad_fn=<AddBackward0>)\n",
      "GeLU'ed Xgate: torch.Size([1, 3, 32])\n",
      "tensor([[[-0.1116, -0.0830,  0.1588,  0.2048, -0.0654,  0.5359, -0.1183,\n",
      "           0.0235, -0.1592, -0.1497,  0.0416, -0.1054,  0.3667, -0.0566,\n",
      "          -0.0754, -0.1479,  0.3329,  0.3616, -0.1643, -0.0745, -0.1044,\n",
      "          -0.0876, -0.1148,  0.0484, -0.1107,  0.1507, -0.0227,  0.2198,\n",
      "          -0.1700, -0.1698,  0.2820, -0.0800],\n",
      "         [ 0.8751,  0.3992, -0.1152, -0.1695, -0.1699,  0.0975,  0.0594,\n",
      "           0.7400,  0.2590,  0.4665, -0.1132, -0.1267, -0.0593, -0.0913,\n",
      "          -0.1005,  0.0227,  0.0136,  0.3715,  0.1501, -0.1691,  0.0781,\n",
      "          -0.0146,  0.4042,  0.3777,  0.1960, -0.1491,  1.3209,  0.1209,\n",
      "           0.6339, -0.0685, -0.1693, -0.0089],\n",
      "         [ 0.6295,  0.0446, -0.1700, -0.0201, -0.0066,  0.2605, -0.0637,\n",
      "          -0.0148,  0.0753,  0.0322,  0.0288, -0.0538,  0.4917,  0.0070,\n",
      "          -0.0160, -0.0688,  0.7628,  0.5911, -0.1591, -0.0903,  0.2107,\n",
      "          -0.1621, -0.0674,  0.7675, -0.1207,  0.5876,  0.7659,  0.5158,\n",
      "           0.7559, -0.1643, -0.1516, -0.1492]]], grad_fn=<GeluBackward0>)\n",
      "Wup: torch.Size([8, 32])\n",
      "Parameter containing:\n",
      "tensor([[ 0.1461, -0.1881, -0.0114, -0.2953,  0.1766, -0.0054,  0.0200,  0.2929,\n",
      "          0.2036,  0.1445,  0.0007, -0.2298,  0.0682, -0.1226,  0.2082,  0.2925,\n",
      "          0.0774, -0.0253, -0.1358, -0.1838,  0.3420, -0.2564, -0.0681,  0.2453,\n",
      "         -0.2752,  0.0909, -0.2523, -0.3247,  0.1653, -0.1776,  0.2798, -0.2458],\n",
      "        [-0.3247, -0.2901,  0.2185,  0.0110, -0.1257, -0.0880, -0.2688,  0.0008,\n",
      "         -0.0161, -0.2435, -0.1607,  0.3109, -0.0768, -0.1983,  0.2455,  0.1018,\n",
      "          0.1656,  0.2817, -0.2373,  0.1910,  0.1478,  0.1714,  0.1657, -0.3000,\n",
      "         -0.1971,  0.2098, -0.2455,  0.1353, -0.3296, -0.0129, -0.0986,  0.0954],\n",
      "        [ 0.1068, -0.2128,  0.1991,  0.1008,  0.2752,  0.2930,  0.2256, -0.0079,\n",
      "          0.2821,  0.3137,  0.3496, -0.0613, -0.1468, -0.0054,  0.0866,  0.2523,\n",
      "         -0.1940, -0.2411, -0.2022,  0.2475,  0.0258,  0.0016,  0.1201, -0.3108,\n",
      "          0.0214,  0.0367, -0.1037,  0.2520,  0.1717, -0.0658,  0.2890, -0.1868],\n",
      "        [ 0.0058, -0.1731,  0.1676,  0.0108, -0.2819, -0.1985,  0.2458, -0.1946,\n",
      "         -0.1752,  0.0611,  0.3184,  0.2923, -0.0479,  0.2621,  0.2844,  0.2188,\n",
      "         -0.2579,  0.3230, -0.1489,  0.0944,  0.3340,  0.2295, -0.1343,  0.2445,\n",
      "          0.0340, -0.0537, -0.1785,  0.2965,  0.1392,  0.1501, -0.3371,  0.1051],\n",
      "        [-0.2644,  0.0722,  0.3389, -0.3096, -0.0578, -0.0960,  0.1849, -0.0896,\n",
      "          0.2314, -0.1201,  0.1368,  0.3479, -0.1061,  0.1036,  0.1223,  0.1666,\n",
      "         -0.3237, -0.3464,  0.0718, -0.2749,  0.2485,  0.1218,  0.3492,  0.0366,\n",
      "          0.2144,  0.3232,  0.0853,  0.2870, -0.2610, -0.1287,  0.2955,  0.1155],\n",
      "        [ 0.2456, -0.0144,  0.0137,  0.1239, -0.2818, -0.0874,  0.1061, -0.1882,\n",
      "          0.3299, -0.2652,  0.1321, -0.2266, -0.3398,  0.2878, -0.1701, -0.0402,\n",
      "          0.1522, -0.2065, -0.0613,  0.2694,  0.3201,  0.2417,  0.2183, -0.0752,\n",
      "         -0.1656, -0.2163, -0.1730,  0.0922,  0.0674,  0.1748,  0.0196,  0.2470],\n",
      "        [-0.3207, -0.1177, -0.2640, -0.0134, -0.3183, -0.1480, -0.1676, -0.3459,\n",
      "         -0.1791, -0.2782,  0.2909, -0.0460,  0.1268, -0.0369, -0.2830, -0.1581,\n",
      "         -0.0776, -0.1935, -0.1141, -0.2595,  0.1222,  0.1648, -0.1104,  0.3182,\n",
      "         -0.2715, -0.0320, -0.1157, -0.0379,  0.1773, -0.1688, -0.3005, -0.0800],\n",
      "        [ 0.1153, -0.3059, -0.0326, -0.2650, -0.1102, -0.2051,  0.3459, -0.0013,\n",
      "         -0.1888, -0.3529, -0.0028, -0.3385, -0.3253, -0.2511, -0.0515,  0.1634,\n",
      "          0.2073,  0.1018,  0.2722, -0.3286,  0.1437,  0.3030,  0.2420,  0.2834,\n",
      "          0.2969,  0.0161,  0.1945, -0.2914,  0.1334,  0.0514,  0.2667,  0.1051]],\n",
      "       requires_grad=True)\n",
      "Wup spliced: torch.Size([8, 32])\n",
      "tensor([[ 0.1461, -0.1881, -0.0114, -0.2953,  0.1766, -0.0054,  0.0200,  0.2929,\n",
      "          0.2036,  0.1445,  0.0007, -0.2298,  0.0682, -0.1226,  0.2082,  0.2925,\n",
      "          0.0774, -0.0253, -0.1358, -0.1838,  0.3420, -0.2564, -0.0681,  0.2453,\n",
      "         -0.2752,  0.0909, -0.2523, -0.3247,  0.1653, -0.1776,  0.2798, -0.2458],\n",
      "        [-0.3247, -0.2901,  0.2185,  0.0110, -0.1257, -0.0880, -0.2688,  0.0008,\n",
      "         -0.0161, -0.2435, -0.1607,  0.3109, -0.0768, -0.1983,  0.2455,  0.1018,\n",
      "          0.1656,  0.2817, -0.2373,  0.1910,  0.1478,  0.1714,  0.1657, -0.3000,\n",
      "         -0.1971,  0.2098, -0.2455,  0.1353, -0.3296, -0.0129, -0.0986,  0.0954],\n",
      "        [ 0.1068, -0.2128,  0.1991,  0.1008,  0.2752,  0.2930,  0.2256, -0.0079,\n",
      "          0.2821,  0.3137,  0.3496, -0.0613, -0.1468, -0.0054,  0.0866,  0.2523,\n",
      "         -0.1940, -0.2411, -0.2022,  0.2475,  0.0258,  0.0016,  0.1201, -0.3108,\n",
      "          0.0214,  0.0367, -0.1037,  0.2520,  0.1717, -0.0658,  0.2890, -0.1868],\n",
      "        [ 0.0058, -0.1731,  0.1676,  0.0108, -0.2819, -0.1985,  0.2458, -0.1946,\n",
      "         -0.1752,  0.0611,  0.3184,  0.2923, -0.0479,  0.2621,  0.2844,  0.2188,\n",
      "         -0.2579,  0.3230, -0.1489,  0.0944,  0.3340,  0.2295, -0.1343,  0.2445,\n",
      "          0.0340, -0.0537, -0.1785,  0.2965,  0.1392,  0.1501, -0.3371,  0.1051],\n",
      "        [-0.2644,  0.0722,  0.3389, -0.3096, -0.0578, -0.0960,  0.1849, -0.0896,\n",
      "          0.2314, -0.1201,  0.1368,  0.3479, -0.1061,  0.1036,  0.1223,  0.1666,\n",
      "         -0.3237, -0.3464,  0.0718, -0.2749,  0.2485,  0.1218,  0.3492,  0.0366,\n",
      "          0.2144,  0.3232,  0.0853,  0.2870, -0.2610, -0.1287,  0.2955,  0.1155],\n",
      "        [ 0.2456, -0.0144,  0.0137,  0.1239, -0.2818, -0.0874,  0.1061, -0.1882,\n",
      "          0.3299, -0.2652,  0.1321, -0.2266, -0.3398,  0.2878, -0.1701, -0.0402,\n",
      "          0.1522, -0.2065, -0.0613,  0.2694,  0.3201,  0.2417,  0.2183, -0.0752,\n",
      "         -0.1656, -0.2163, -0.1730,  0.0922,  0.0674,  0.1748,  0.0196,  0.2470],\n",
      "        [-0.3207, -0.1177, -0.2640, -0.0134, -0.3183, -0.1480, -0.1676, -0.3459,\n",
      "         -0.1791, -0.2782,  0.2909, -0.0460,  0.1268, -0.0369, -0.2830, -0.1581,\n",
      "         -0.0776, -0.1935, -0.1141, -0.2595,  0.1222,  0.1648, -0.1104,  0.3182,\n",
      "         -0.2715, -0.0320, -0.1157, -0.0379,  0.1773, -0.1688, -0.3005, -0.0800],\n",
      "        [ 0.1153, -0.3059, -0.0326, -0.2650, -0.1102, -0.2051,  0.3459, -0.0013,\n",
      "         -0.1888, -0.3529, -0.0028, -0.3385, -0.3253, -0.2511, -0.0515,  0.1634,\n",
      "          0.2073,  0.1018,  0.2722, -0.3286,  0.1437,  0.3030,  0.2420,  0.2834,\n",
      "          0.2969,  0.0161,  0.1945, -0.2914,  0.1334,  0.0514,  0.2667,  0.1051]],\n",
      "       grad_fn=<AliasBackward0>)\n",
      "Bup: torch.Size([32])\n",
      "Parameter containing:\n",
      "tensor([-0.2515,  0.2189, -0.0972,  0.1164, -0.2267, -0.2862,  0.3457, -0.0580,\n",
      "        -0.0643,  0.0126,  0.1822, -0.0607,  0.1557, -0.0858,  0.3095, -0.0990,\n",
      "         0.2946, -0.2724,  0.1024, -0.3385,  0.2050,  0.0544, -0.3525,  0.0763,\n",
      "         0.3103, -0.2710, -0.1986, -0.2860,  0.0696,  0.0840,  0.0303, -0.1089],\n",
      "       requires_grad=True)\n",
      "Bup spliced: torch.Size([32])\n",
      "tensor([-0.2515,  0.2189, -0.0972,  0.1164, -0.2267, -0.2862,  0.3457, -0.0580,\n",
      "        -0.0643,  0.0126,  0.1822, -0.0607,  0.1557, -0.0858,  0.3095, -0.0990,\n",
      "         0.2946, -0.2724,  0.1024, -0.3385,  0.2050,  0.0544, -0.3525,  0.0763,\n",
      "         0.3103, -0.2710, -0.1986, -0.2860,  0.0696,  0.0840,  0.0303, -0.1089],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xup: torch.Size([1, 3, 32])\n",
      "tensor([[[ 0.2520,  0.6785, -1.1386,  0.8773, -0.4157, -0.2662,  0.1376,\n",
      "          -0.2613, -1.0164, -0.3379, -0.3768, -0.9545,  0.1890, -0.2447,\n",
      "          -0.7440, -1.1660,  1.0626,  0.1816,  0.8266, -0.1757, -1.0421,\n",
      "           0.1965, -0.8189,  0.1262,  0.7136, -1.1549,  0.5907, -0.9530,\n",
      "           0.4239,  0.6047, -0.6737,  0.1566],\n",
      "         [-1.3396, -0.5141,  0.6168,  0.0872, -0.2814, -0.2359,  0.2259,\n",
      "          -0.4213, -0.3221,  0.4089,  0.9708,  1.2904,  0.4831, -0.2233,\n",
      "           1.1776,  0.5419, -0.6273,  0.0744, -0.7729, -0.1911,  0.5816,\n",
      "           0.3058, -0.4808, -0.1698,  0.0811,  0.4005, -0.7971,  0.7673,\n",
      "          -0.1257, -0.2850, -0.4463, -0.4621],\n",
      "         [-0.5206, -0.2107,  0.2757,  0.3337,  0.6476,  0.5410,  0.5513,\n",
      "          -0.0514,  0.2574,  0.6433,  0.7205, -0.0226,  0.0462, -0.5982,\n",
      "           0.3247,  0.2378, -0.1405, -0.8944, -0.1805, -0.0593, -0.3579,\n",
      "          -0.0060, -0.0101, -0.8887,  0.6020,  0.1004, -0.0592,  0.2257,\n",
      "           0.1616, -0.3030,  0.7664, -0.6802]]], grad_fn=<AddBackward0>)\n",
      "Xfuse: torch.Size([1, 3, 32])\n",
      "tensor([[[-2.8124e-02, -5.6316e-02, -1.8085e-01,  1.7964e-01,  2.7187e-02,\n",
      "          -1.4268e-01, -1.6282e-02, -6.1496e-03,  1.6183e-01,  5.0586e-02,\n",
      "          -1.5662e-02,  1.0056e-01,  6.9304e-02,  1.3859e-02,  5.6092e-02,\n",
      "           1.7243e-01,  3.5379e-01,  6.5674e-02, -1.3585e-01,  1.3080e-02,\n",
      "           1.0878e-01, -1.7216e-02,  9.4005e-02,  6.1065e-03, -7.8976e-02,\n",
      "          -1.7409e-01, -1.3416e-02, -2.0944e-01, -7.2048e-02, -1.0270e-01,\n",
      "          -1.8998e-01, -1.2524e-02],\n",
      "         [-1.1723e+00, -2.0526e-01, -7.1052e-02, -1.4791e-02,  4.7798e-02,\n",
      "          -2.2997e-02,  1.3422e-02, -3.1179e-01, -8.3414e-02,  1.9077e-01,\n",
      "          -1.0990e-01, -1.6353e-01, -2.8625e-02,  2.0390e-02, -1.1830e-01,\n",
      "           1.2288e-02, -8.5453e-03,  2.7649e-02, -1.1604e-01,  3.2310e-02,\n",
      "           4.5445e-02, -4.4528e-03, -1.9435e-01, -6.4137e-02,  1.5898e-02,\n",
      "          -5.9738e-02, -1.0529e+00,  9.2775e-02, -7.9667e-02,  1.9528e-02,\n",
      "           7.5583e-02,  4.1061e-03],\n",
      "         [-3.2773e-01, -9.3980e-03, -4.6849e-02, -6.7213e-03, -4.2637e-03,\n",
      "           1.4095e-01, -3.5142e-02,  7.5994e-04,  1.9379e-02,  2.0695e-02,\n",
      "           2.0718e-02,  1.2172e-03,  2.2727e-02, -4.1909e-03, -5.2034e-03,\n",
      "          -1.6355e-02, -1.0714e-01, -5.2871e-01,  2.8713e-02,  5.3501e-03,\n",
      "          -7.5404e-02,  9.8037e-04,  6.8050e-04, -6.8210e-01, -7.2689e-02,\n",
      "           5.9027e-02, -4.5354e-02,  1.1641e-01,  1.2211e-01,  4.9766e-02,\n",
      "          -1.1616e-01,  1.0146e-01]]], grad_fn=<MulBackward0>)\n",
      "Wdown: torch.Size([32, 8])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0182, -0.0335,  0.1160,  0.0016,  0.1674,  0.0666,  0.0672, -0.0870],\n",
      "        [-0.1036, -0.1544,  0.0656,  0.0593,  0.0246,  0.1329, -0.0141,  0.1280],\n",
      "        [ 0.0924, -0.0610, -0.0202, -0.1488, -0.1268, -0.1293,  0.1564,  0.0257],\n",
      "        [-0.0521, -0.0812,  0.0997, -0.1479, -0.1325, -0.0349,  0.0567,  0.1022],\n",
      "        [-0.1485, -0.1232,  0.0503, -0.1232,  0.1660, -0.0445, -0.0446,  0.0510],\n",
      "        [-0.1061, -0.0046, -0.0265,  0.1611, -0.0745,  0.0744, -0.1586,  0.0883],\n",
      "        [ 0.1349, -0.0969,  0.0187,  0.0635,  0.0267,  0.1135, -0.1686,  0.1749],\n",
      "        [-0.0706,  0.0030,  0.0220, -0.1169, -0.1222,  0.0809, -0.0947,  0.1453],\n",
      "        [-0.0279, -0.1280, -0.0225, -0.1762,  0.0917,  0.1144, -0.0192,  0.0034],\n",
      "        [-0.0751,  0.0913,  0.0093,  0.0964, -0.1255, -0.1656, -0.0056, -0.0406],\n",
      "        [ 0.1591,  0.0859,  0.0305,  0.0196, -0.1654,  0.0772,  0.1611,  0.1560],\n",
      "        [-0.0746, -0.0580,  0.0504, -0.1644, -0.0701,  0.0872, -0.1648,  0.1040],\n",
      "        [ 0.1301,  0.0403,  0.0711, -0.1759,  0.0784, -0.1548, -0.0742, -0.0850],\n",
      "        [-0.1415, -0.0116, -0.0338,  0.1498, -0.0192,  0.1108,  0.1595,  0.0837],\n",
      "        [-0.1039, -0.0155,  0.0275, -0.1285,  0.0839,  0.1181,  0.0983,  0.0754],\n",
      "        [-0.1239,  0.0488, -0.1654, -0.0906,  0.0470, -0.0620, -0.1542, -0.1062],\n",
      "        [-0.0375,  0.0230, -0.0777,  0.1664,  0.0647,  0.1694,  0.0817,  0.1513],\n",
      "        [ 0.0760,  0.1130, -0.0421,  0.0471, -0.1388,  0.0021,  0.1197,  0.0242],\n",
      "        [ 0.0810,  0.1652,  0.0490,  0.1459, -0.0303,  0.0509, -0.0201,  0.1173],\n",
      "        [ 0.1563, -0.0756, -0.0584,  0.0121, -0.0436, -0.1635,  0.1264,  0.1484],\n",
      "        [-0.0628,  0.1063, -0.1746,  0.1726, -0.1372,  0.0788,  0.1273, -0.0905],\n",
      "        [ 0.1667,  0.1260,  0.1609, -0.1081, -0.1310,  0.0144, -0.1354, -0.0869],\n",
      "        [ 0.1608,  0.0912, -0.1313, -0.1481,  0.0570, -0.0335,  0.1258, -0.0184],\n",
      "        [-0.1426, -0.0447,  0.1231,  0.1384,  0.0974, -0.1639, -0.1728,  0.0261],\n",
      "        [-0.0215, -0.0968,  0.1192, -0.0503, -0.0652, -0.1343, -0.0023, -0.0068],\n",
      "        [-0.0486, -0.0081,  0.1722, -0.0421, -0.1066, -0.1005,  0.1318,  0.1001],\n",
      "        [-0.0495, -0.1124, -0.0681,  0.0858,  0.1235,  0.0307,  0.1351, -0.1294],\n",
      "        [ 0.0583,  0.0458,  0.0871, -0.0338, -0.0507, -0.0004, -0.0430,  0.0936],\n",
      "        [-0.0916,  0.0825,  0.1493,  0.0817, -0.0655, -0.0552, -0.0992,  0.0182],\n",
      "        [ 0.1523, -0.0392,  0.1635, -0.0732, -0.1632, -0.1637,  0.0139,  0.1081],\n",
      "        [-0.1676, -0.1737, -0.0526, -0.0201,  0.0717,  0.1610,  0.0641,  0.0570],\n",
      "        [-0.1431, -0.0946,  0.1332,  0.0950,  0.0392,  0.0311, -0.1628, -0.1136]],\n",
      "       requires_grad=True)\n",
      "Wdown spliced: torch.Size([32, 8])\n",
      "tensor([[ 0.0182, -0.0335,  0.1160,  0.0016,  0.1674,  0.0666,  0.0672, -0.0870],\n",
      "        [-0.1036, -0.1544,  0.0656,  0.0593,  0.0246,  0.1329, -0.0141,  0.1280],\n",
      "        [ 0.0924, -0.0610, -0.0202, -0.1488, -0.1268, -0.1293,  0.1564,  0.0257],\n",
      "        [-0.0521, -0.0812,  0.0997, -0.1479, -0.1325, -0.0349,  0.0567,  0.1022],\n",
      "        [-0.1485, -0.1232,  0.0503, -0.1232,  0.1660, -0.0445, -0.0446,  0.0510],\n",
      "        [-0.1061, -0.0046, -0.0265,  0.1611, -0.0745,  0.0744, -0.1586,  0.0883],\n",
      "        [ 0.1349, -0.0969,  0.0187,  0.0635,  0.0267,  0.1135, -0.1686,  0.1749],\n",
      "        [-0.0706,  0.0030,  0.0220, -0.1169, -0.1222,  0.0809, -0.0947,  0.1453],\n",
      "        [-0.0279, -0.1280, -0.0225, -0.1762,  0.0917,  0.1144, -0.0192,  0.0034],\n",
      "        [-0.0751,  0.0913,  0.0093,  0.0964, -0.1255, -0.1656, -0.0056, -0.0406],\n",
      "        [ 0.1591,  0.0859,  0.0305,  0.0196, -0.1654,  0.0772,  0.1611,  0.1560],\n",
      "        [-0.0746, -0.0580,  0.0504, -0.1644, -0.0701,  0.0872, -0.1648,  0.1040],\n",
      "        [ 0.1301,  0.0403,  0.0711, -0.1759,  0.0784, -0.1548, -0.0742, -0.0850],\n",
      "        [-0.1415, -0.0116, -0.0338,  0.1498, -0.0192,  0.1108,  0.1595,  0.0837],\n",
      "        [-0.1039, -0.0155,  0.0275, -0.1285,  0.0839,  0.1181,  0.0983,  0.0754],\n",
      "        [-0.1239,  0.0488, -0.1654, -0.0906,  0.0470, -0.0620, -0.1542, -0.1062],\n",
      "        [-0.0375,  0.0230, -0.0777,  0.1664,  0.0647,  0.1694,  0.0817,  0.1513],\n",
      "        [ 0.0760,  0.1130, -0.0421,  0.0471, -0.1388,  0.0021,  0.1197,  0.0242],\n",
      "        [ 0.0810,  0.1652,  0.0490,  0.1459, -0.0303,  0.0509, -0.0201,  0.1173],\n",
      "        [ 0.1563, -0.0756, -0.0584,  0.0121, -0.0436, -0.1635,  0.1264,  0.1484],\n",
      "        [-0.0628,  0.1063, -0.1746,  0.1726, -0.1372,  0.0788,  0.1273, -0.0905],\n",
      "        [ 0.1667,  0.1260,  0.1609, -0.1081, -0.1310,  0.0144, -0.1354, -0.0869],\n",
      "        [ 0.1608,  0.0912, -0.1313, -0.1481,  0.0570, -0.0335,  0.1258, -0.0184],\n",
      "        [-0.1426, -0.0447,  0.1231,  0.1384,  0.0974, -0.1639, -0.1728,  0.0261],\n",
      "        [-0.0215, -0.0968,  0.1192, -0.0503, -0.0652, -0.1343, -0.0023, -0.0068],\n",
      "        [-0.0486, -0.0081,  0.1722, -0.0421, -0.1066, -0.1005,  0.1318,  0.1001],\n",
      "        [-0.0495, -0.1124, -0.0681,  0.0858,  0.1235,  0.0307,  0.1351, -0.1294],\n",
      "        [ 0.0583,  0.0458,  0.0871, -0.0338, -0.0507, -0.0004, -0.0430,  0.0936],\n",
      "        [-0.0916,  0.0825,  0.1493,  0.0817, -0.0655, -0.0552, -0.0992,  0.0182],\n",
      "        [ 0.1523, -0.0392,  0.1635, -0.0732, -0.1632, -0.1637,  0.0139,  0.1081],\n",
      "        [-0.1676, -0.1737, -0.0526, -0.0201,  0.0717,  0.1610,  0.0641,  0.0570],\n",
      "        [-0.1431, -0.0946,  0.1332,  0.0950,  0.0392,  0.0311, -0.1628, -0.1136]],\n",
      "       grad_fn=<AliasBackward0>)\n",
      "Bdown: torch.Size([8])\n",
      "Parameter containing:\n",
      "tensor([-0.0420, -0.1305,  0.1604, -0.0753, -0.0015, -0.0793, -0.0961,  0.0816],\n",
      "       requires_grad=True)\n",
      "Bdown spliced: torch.Size([8])\n",
      "tensor([-0.0420, -0.1305,  0.1604, -0.0753, -0.0015, -0.0793, -0.0961,  0.0816],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "outputs: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.0813, -0.0956,  0.0117, -0.1070,  0.0796, -0.0085, -0.0889,\n",
      "           0.0379],\n",
      "         [-0.0123,  0.0385,  0.0516, -0.0581, -0.3047, -0.2815, -0.2933,\n",
      "           0.1953],\n",
      "         [ 0.0155, -0.1096,  0.1439, -0.1856, -0.0901, -0.0334, -0.1420,\n",
      "           0.0916]]], grad_fn=<AddBackward0>)\n",
      "------------- END MLP.forwardTensor() ------------\n",
      "layer's final residual state:\n",
      "tensor([[[-1.7248, -0.9206, -0.6961, -0.7503, -1.6982,  0.0159, -0.0733,\n",
      "           0.7600],\n",
      "         [-0.0454,  1.0032,  0.9860,  0.9000,  0.0985, -1.2619,  0.3302,\n",
      "          -0.2126],\n",
      "         [-0.4724,  0.1314,  2.0494, -0.7864,  0.0046, -0.7580,  0.0940,\n",
      "           0.2428]]], grad_fn=<AddBackward0>)\n",
      "----------------- END Layer.forwardTensor() --------------------\n",
      "forwardTensor() output: torch.Size([1, 3, 8])\n",
      "tensor([[[-1.7248, -0.9206, -0.6961, -0.7503, -1.6982,  0.0159, -0.0733,\n",
      "           0.7600],\n",
      "         [-0.0454,  1.0032,  0.9860,  0.9000,  0.0985, -1.2619,  0.3302,\n",
      "          -0.2126],\n",
      "         [-0.4724,  0.1314,  2.0494, -0.7864,  0.0046, -0.7580,  0.0940,\n",
      "           0.2428]]], grad_fn=<AddBackward0>)\n",
      "Level 1 from range(2)\n",
      "Model 0 from range(2)\n",
      "----------------- Layer.forwardTensor() --------------------\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.2024,  0.0779,  0.1512, -0.4145],\n",
      "         [-0.5984,  1.4727,  0.0435, -0.8533],\n",
      "         [-2.6348,  1.5094, -1.7780, -1.2693]]])\n",
      "normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.8232,  0.3171,  0.6152, -1.6862],\n",
      "         [-0.6632,  1.6320,  0.0482, -0.9457],\n",
      "         [-1.4087,  0.8070, -0.9506, -0.6786]]])\n",
      "dim: 4\n",
      "skip: 0\n",
      "spliced scale: torch.Size([4])\n",
      "tensor([0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.8232,  0.3171,  0.6152, -1.6862],\n",
      "         [-0.6632,  1.6320,  0.0482, -0.9457],\n",
      "         [-1.4087,  0.8070, -0.9506, -0.6786]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "---------- Attention Input: torch.Tensor ------------\n",
      "----------------- MultiQueryAttention.forwardTensor() --------------------\n",
      "x shape: torch.Size([1, 3, 4])\n",
      "d_skip: 0\n",
      "models_in_this_level: 2\n",
      "h_dim: 2\n",
      "h_skip: 0\n",
      "self.Wqkv: torch.Size([8, 16])\n",
      "Parameter containing:\n",
      "tensor([[-0.0340, -0.1815, -0.3210, -0.1459, -0.1058,  0.0699, -0.0545, -0.3136,\n",
      "         -0.1625, -0.3407,  0.0713,  0.1895,  0.1236, -0.0918,  0.1145,  0.0372],\n",
      "        [-0.0765,  0.2838, -0.1542,  0.3024, -0.2310,  0.1960,  0.3432, -0.1336,\n",
      "         -0.0016,  0.1019,  0.3275,  0.0040, -0.2962, -0.2300,  0.1264,  0.3188],\n",
      "        [ 0.2791, -0.0656,  0.0931, -0.1173, -0.0828,  0.0369, -0.2055, -0.0018,\n",
      "          0.0459, -0.0719,  0.1220,  0.1993, -0.0510,  0.2554,  0.2919, -0.1963],\n",
      "        [-0.0580,  0.0674,  0.1863, -0.2996, -0.1225, -0.2918,  0.3034, -0.0289,\n",
      "          0.3401,  0.2546, -0.2533,  0.0773, -0.2015, -0.2837,  0.0371, -0.1208],\n",
      "        [-0.1807,  0.2407, -0.0230, -0.0681,  0.1628, -0.2487,  0.1491,  0.1595,\n",
      "         -0.2048, -0.0425, -0.1204,  0.0531,  0.0862, -0.2871, -0.2986, -0.1081],\n",
      "        [ 0.0699,  0.1571, -0.2550,  0.1524, -0.0307,  0.0327, -0.1545,  0.0394,\n",
      "          0.1334, -0.2649,  0.0729, -0.2974,  0.2341, -0.1605, -0.1469,  0.2894],\n",
      "        [-0.2351,  0.2850, -0.0287, -0.3331,  0.3337,  0.0153,  0.0692,  0.1914,\n",
      "          0.1876, -0.3101, -0.1262, -0.3145,  0.2511, -0.2142,  0.2297, -0.1992],\n",
      "        [-0.3401,  0.1138, -0.3317, -0.0407, -0.1315,  0.2288,  0.2154,  0.3395,\n",
      "          0.1770,  0.0745,  0.1152,  0.0304, -0.0858,  0.0956,  0.2461, -0.0038]],\n",
      "       requires_grad=True)\n",
      "Wq: torch.Size([8, 8])\n",
      "tensor([[-0.0340, -0.1815, -0.3210, -0.1459, -0.1058,  0.0699, -0.0545, -0.3136],\n",
      "        [-0.0765,  0.2838, -0.1542,  0.3024, -0.2310,  0.1960,  0.3432, -0.1336],\n",
      "        [ 0.2791, -0.0656,  0.0931, -0.1173, -0.0828,  0.0369, -0.2055, -0.0018],\n",
      "        [-0.0580,  0.0674,  0.1863, -0.2996, -0.1225, -0.2918,  0.3034, -0.0289],\n",
      "        [-0.1807,  0.2407, -0.0230, -0.0681,  0.1628, -0.2487,  0.1491,  0.1595],\n",
      "        [ 0.0699,  0.1571, -0.2550,  0.1524, -0.0307,  0.0327, -0.1545,  0.0394],\n",
      "        [-0.2351,  0.2850, -0.0287, -0.3331,  0.3337,  0.0153,  0.0692,  0.1914],\n",
      "        [-0.3401,  0.1138, -0.3317, -0.0407, -0.1315,  0.2288,  0.2154,  0.3395]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wk: torch.Size([8, 4])\n",
      "tensor([[-0.1625, -0.3407,  0.0713,  0.1895],\n",
      "        [-0.0016,  0.1019,  0.3275,  0.0040],\n",
      "        [ 0.0459, -0.0719,  0.1220,  0.1993],\n",
      "        [ 0.3401,  0.2546, -0.2533,  0.0773],\n",
      "        [-0.2048, -0.0425, -0.1204,  0.0531],\n",
      "        [ 0.1334, -0.2649,  0.0729, -0.2974],\n",
      "        [ 0.1876, -0.3101, -0.1262, -0.3145],\n",
      "        [ 0.1770,  0.0745,  0.1152,  0.0304]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wv: torch.Size([8, 4])\n",
      "tensor([[ 0.1236, -0.0918,  0.1145,  0.0372],\n",
      "        [-0.2962, -0.2300,  0.1264,  0.3188],\n",
      "        [-0.0510,  0.2554,  0.2919, -0.1963],\n",
      "        [-0.2015, -0.2837,  0.0371, -0.1208],\n",
      "        [ 0.0862, -0.2871, -0.2986, -0.1081],\n",
      "        [ 0.2341, -0.1605, -0.1469,  0.2894],\n",
      "        [ 0.2511, -0.2142,  0.2297, -0.1992],\n",
      "        [-0.0858,  0.0956,  0.2461, -0.0038]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wq spliced: torch.Size([4, 4])\n",
      "tensor([[-0.0340, -0.1815, -0.1058,  0.0699],\n",
      "        [-0.0765,  0.2838, -0.2310,  0.1960],\n",
      "        [ 0.2791, -0.0656, -0.0828,  0.0369],\n",
      "        [-0.0580,  0.0674, -0.1225, -0.2918]], grad_fn=<CatBackward0>)\n",
      "Wk spliced: torch.Size([4, 2])\n",
      "tensor([[-0.1625, -0.3407],\n",
      "        [-0.0016,  0.1019],\n",
      "        [ 0.0459, -0.0719],\n",
      "        [ 0.3401,  0.2546]], grad_fn=<CatBackward0>)\n",
      "Wv spliced: torch.Size([4, 2])\n",
      "tensor([[ 0.1236, -0.0918],\n",
      "        [-0.2962, -0.2300],\n",
      "        [-0.0510,  0.2554],\n",
      "        [-0.2015, -0.2837]], grad_fn=<CatBackward0>)\n",
      "Wqkv_spliced: torch.Size([4, 8])\n",
      "tensor([[-0.0340, -0.1815, -0.1058,  0.0699, -0.1625, -0.3407,  0.1236, -0.0918],\n",
      "        [-0.0765,  0.2838, -0.2310,  0.1960, -0.0016,  0.1019, -0.2962, -0.2300],\n",
      "        [ 0.2791, -0.0656, -0.0828,  0.0369,  0.0459, -0.0719, -0.0510,  0.2554],\n",
      "        [-0.0580,  0.0674, -0.1225, -0.2918,  0.3401,  0.2546, -0.2015, -0.2837]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "xqkv: torch.Size([1, 3, 8])\n",
      "tensor([[[ 0.2173, -0.2135, -0.0048,  0.6344, -0.6795, -0.7216,  0.3162,\n",
      "           0.4870],\n",
      "         [-0.0340,  0.5166, -0.1950,  0.5513, -0.2143,  0.1480, -0.3772,\n",
      "          -0.0339],\n",
      "         [-0.2398,  0.5013,  0.1245,  0.2227, -0.0468,  0.4577, -0.2279,\n",
      "          -0.1065]]], grad_fn=<UnsafeViewBackward0>)\n",
      "xq: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.2173, -0.2135, -0.0048,  0.6344],\n",
      "         [-0.0340,  0.5166, -0.1950,  0.5513],\n",
      "         [-0.2398,  0.5013,  0.1245,  0.2227]]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "xk: torch.Size([1, 3, 2])\n",
      "tensor([[[-0.6795, -0.7216],\n",
      "         [-0.2143,  0.1480],\n",
      "         [-0.0468,  0.4577]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xv: torch.Size([1, 3, 2])\n",
      "tensor([[[ 0.3162,  0.4870],\n",
      "         [-0.3772, -0.0339],\n",
      "         [-0.2279, -0.1065]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xq reshaped: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[ 0.2173, -0.2135],\n",
      "          [-0.0048,  0.6344]],\n",
      "\n",
      "         [[-0.0340,  0.5166],\n",
      "          [-0.1950,  0.5513]],\n",
      "\n",
      "         [[-0.2398,  0.5013],\n",
      "          [ 0.1245,  0.2227]]]], grad_fn=<ViewBackward0>)\n",
      "xk reshaped: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[-0.6795, -0.7216]],\n",
      "\n",
      "         [[-0.2143,  0.1480]],\n",
      "\n",
      "         [[-0.0468,  0.4577]]]], grad_fn=<ViewBackward0>)\n",
      "xv reshaped: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[ 0.3162,  0.4870]],\n",
      "\n",
      "         [[-0.3772, -0.0339]],\n",
      "\n",
      "         [[-0.2279, -0.1065]]]], grad_fn=<ViewBackward0>)\n",
      "rotated xq: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[ 0.2173, -0.2135],\n",
      "          [-0.0048,  0.6344]],\n",
      "\n",
      "         [[-0.4531,  0.2505],\n",
      "          [-0.5692,  0.1337]],\n",
      "\n",
      "         [[-0.3560, -0.4266],\n",
      "          [-0.2543,  0.0206]]]], grad_fn=<TransposeBackward0>)\n",
      "rotated xk: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[-0.6795, -0.7216]],\n",
      "\n",
      "         [[-0.2403, -0.1003]],\n",
      "\n",
      "         [[-0.3967, -0.2330]]]], grad_fn=<TransposeBackward0>)\n",
      "repeat_interleaved xk: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[-0.6795, -0.7216],\n",
      "          [-0.6795, -0.7216]],\n",
      "\n",
      "         [[-0.2403, -0.1003],\n",
      "          [-0.2403, -0.1003]],\n",
      "\n",
      "         [[-0.3967, -0.2330],\n",
      "          [-0.3967, -0.2330]]]], grad_fn=<ViewBackward0>)\n",
      "repeat_interleaved xv: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[ 0.3162,  0.4870],\n",
      "          [ 0.3162,  0.4870]],\n",
      "\n",
      "         [[-0.3772, -0.0339],\n",
      "          [-0.3772, -0.0339]],\n",
      "\n",
      "         [[-0.2279, -0.1065],\n",
      "          [-0.2279, -0.1065]]]], grad_fn=<ViewBackward0>)\n",
      "transposed xq: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[ 0.2173, -0.2135],\n",
      "          [-0.4531,  0.2505],\n",
      "          [-0.3560, -0.4266]],\n",
      "\n",
      "         [[-0.0048,  0.6344],\n",
      "          [-0.5692,  0.1337],\n",
      "          [-0.2543,  0.0206]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xk: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[-0.6795, -0.7216],\n",
      "          [-0.2403, -0.1003],\n",
      "          [-0.3967, -0.2330]],\n",
      "\n",
      "         [[-0.6795, -0.7216],\n",
      "          [-0.2403, -0.1003],\n",
      "          [-0.3967, -0.2330]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xv: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[ 0.3162,  0.4870],\n",
      "          [-0.3772, -0.0339],\n",
      "          [-0.2279, -0.1065]],\n",
      "\n",
      "         [[ 0.3162,  0.4870],\n",
      "          [-0.3772, -0.0339],\n",
      "          [-0.2279, -0.1065]]]], grad_fn=<TransposeBackward0>)\n",
      "scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[ 0.0045, -0.0218, -0.0258],\n",
      "          [ 0.0899,  0.0592,  0.0858],\n",
      "          [ 0.3888,  0.0908,  0.1702]],\n",
      "\n",
      "         [[-0.3214, -0.0442, -0.1032],\n",
      "          [ 0.2053,  0.0872,  0.1376],\n",
      "          [ 0.1117,  0.0418,  0.0679]]]], grad_fn=<MulBackward0>)\n",
      "mask: torch.Size([1, 1, 3, 3])\n",
      "tensor([[[[ 0.0000e+00, -2.3820e+38, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]]]])\n",
      "masked scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[ 4.5439e-03, -2.3820e+38, -2.3820e+38],\n",
      "          [ 8.9880e-02,  5.9223e-02, -2.3820e+38],\n",
      "          [ 3.8876e-01,  9.0762e-02,  1.7017e-01]],\n",
      "\n",
      "         [[-3.2138e-01, -2.3820e+38, -2.3820e+38],\n",
      "          [ 2.0525e-01,  8.7243e-02, -2.3820e+38],\n",
      "          [ 1.1168e-01,  4.1751e-02,  6.7937e-02]]]], grad_fn=<AddBackward0>)\n",
      "softmaxed scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [0.5077, 0.4923, 0.0000],\n",
      "          [0.3928, 0.2916, 0.3157]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.5295, 0.4705, 0.0000],\n",
      "          [0.3461, 0.3227, 0.3313]]]], grad_fn=<SoftmaxBackward0>)\n",
      "attention: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[ 0.3162,  0.4870],\n",
      "          [-0.0252,  0.2305],\n",
      "          [-0.0577,  0.1478]],\n",
      "\n",
      "         [[ 0.3162,  0.4870],\n",
      "          [-0.0101,  0.2419],\n",
      "          [-0.0878,  0.1223]]]], grad_fn=<UnsafeViewBackward0>)\n",
      "reshaped attention: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.3162,  0.4870,  0.3162,  0.4870],\n",
      "         [-0.0252,  0.2305, -0.0101,  0.2419],\n",
      "         [-0.0577,  0.1478, -0.0878,  0.1223]]], grad_fn=<ViewBackward0>)\n",
      "self.Wo: torch.Size([8, 8])\n",
      "Parameter containing:\n",
      "tensor([[ 0.2720, -0.0599,  0.1861, -0.1372, -0.1935, -0.1548, -0.3042,  0.1437],\n",
      "        [ 0.0298,  0.1093, -0.0187, -0.1633, -0.1128,  0.1514,  0.1242,  0.2416],\n",
      "        [ 0.1782,  0.0222,  0.2303,  0.3073, -0.2227, -0.0706,  0.3020, -0.2865],\n",
      "        [-0.2260,  0.2797, -0.3507,  0.0173, -0.1894,  0.3031,  0.2639, -0.2744],\n",
      "        [ 0.0999, -0.0104,  0.0846,  0.0732,  0.2059, -0.3449, -0.1057, -0.1274],\n",
      "        [ 0.2284, -0.1917, -0.3070, -0.3047, -0.1733, -0.1838,  0.3154,  0.0262],\n",
      "        [-0.1188, -0.0889, -0.0819,  0.0254, -0.0091,  0.0690,  0.0810,  0.0455],\n",
      "        [-0.1355,  0.1819, -0.1108, -0.2114,  0.1157,  0.0880,  0.0930,  0.1926]],\n",
      "       requires_grad=True)\n",
      "spliced Wo: torch.Size([4, 4])\n",
      "tensor([[ 0.2720, -0.0599,  0.1861, -0.1372],\n",
      "        [ 0.0298,  0.1093, -0.0187, -0.1633],\n",
      "        [ 0.0999, -0.0104,  0.0846,  0.0732],\n",
      "        [ 0.2284, -0.1917, -0.3070, -0.3047]], grad_fn=<CatBackward0>)\n",
      "projected output: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.2433, -0.0623, -0.0730, -0.2482],\n",
      "         [ 0.0543, -0.0196, -0.0841, -0.1086],\n",
      "         [ 0.0079, -0.0029, -0.0585, -0.0599]]], grad_fn=<UnsafeViewBackward0>)\n",
      "----------------- END MultiQueryAttention.forwardTensor() --------------------\n",
      "x in layer after MQA & resid connection and before MLP:\n",
      "tensor([[[ 0.4457,  0.0156,  0.0782, -0.6626],\n",
      "         [-0.5442,  1.4531, -0.0406, -0.9620],\n",
      "         [-2.6269,  1.5065, -1.8365, -1.3292]]], grad_fn=<AddBackward0>)\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.4457,  0.0156,  0.0782, -0.6626],\n",
      "         [-0.5442,  1.4531, -0.0406, -0.9620],\n",
      "         [-2.6269,  1.5065, -1.8365, -1.3292]]], grad_fn=<AddBackward0>)\n",
      "normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[ 1.1107,  0.0388,  0.1949, -1.6513],\n",
      "         [-0.5960,  1.5915, -0.0445, -1.0536],\n",
      "         [-1.3889,  0.7965, -0.9710, -0.7028]]], grad_fn=<MulBackward0>)\n",
      "dim: 4\n",
      "skip: 0\n",
      "spliced scale: torch.Size([4])\n",
      "tensor([0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[ 1.1107,  0.0388,  0.1949, -1.6513],\n",
      "         [-0.5960,  1.5915, -0.0445, -1.0536],\n",
      "         [-1.3889,  0.7965, -0.9710, -0.7028]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "---------- MLP Input: torch.Tensor ------------\n",
      "------------- MLP.forwardTensor() ------------\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[ 1.1107,  0.0388,  0.1949, -1.6513],\n",
      "         [-0.5960,  1.5915, -0.0445, -1.0536],\n",
      "         [-1.3889,  0.7965, -0.9710, -0.7028]]], grad_fn=<MulBackward0>)\n",
      "d_dim: 4\n",
      "d_skip: 0\n",
      "i_dim: 16\n",
      "i_skip: 0\n",
      "Wgate: torch.Size([8, 32])\n",
      "Parameter containing:\n",
      "tensor([[ 0.2630,  0.3304, -0.2631, -0.3178,  0.3219, -0.1386,  0.0899, -0.1480,\n",
      "          0.3116,  0.2974, -0.1163,  0.0538, -0.2158,  0.2218,  0.2058,  0.3158,\n",
      "          0.0195, -0.0748,  0.2612,  0.0902,  0.2668, -0.1171,  0.0344, -0.2618,\n",
      "         -0.2624,  0.0140, -0.1836, -0.2710,  0.1948,  0.1456,  0.0496,  0.1199],\n",
      "        [ 0.1676,  0.1079, -0.1379, -0.1800, -0.1866,  0.0735,  0.0232,  0.3478,\n",
      "         -0.0545,  0.0847,  0.0208, -0.3432,  0.0005, -0.0354,  0.1616, -0.1833,\n",
      "         -0.2717,  0.1721,  0.3339, -0.1524, -0.3199,  0.1380,  0.3210, -0.0216,\n",
      "          0.2402, -0.0941,  0.1791, -0.1290,  0.2034, -0.2613, -0.2594, -0.0470],\n",
      "        [ 0.1839,  0.2218, -0.1851, -0.1301,  0.0134,  0.0530, -0.0466, -0.1877,\n",
      "          0.1577,  0.0715,  0.0227,  0.0646,  0.3183,  0.0467,  0.0097,  0.2110,\n",
      "          0.1942, -0.0720, -0.3510,  0.2467,  0.3202, -0.3234, -0.2969,  0.1870,\n",
      "         -0.1467,  0.2700,  0.3276,  0.3361,  0.2249, -0.1557, -0.2221, -0.1109],\n",
      "        [ 0.0458,  0.3373,  0.2697, -0.2697, -0.3212, -0.0153,  0.1222,  0.1895,\n",
      "          0.1133,  0.1937, -0.0664,  0.0580, -0.1034, -0.0585, -0.0800,  0.2740,\n",
      "         -0.1762, -0.2514, -0.1972,  0.0756,  0.0571, -0.1100, -0.0780,  0.0126,\n",
      "          0.2733, -0.2975,  0.3090,  0.1289, -0.1540,  0.3491, -0.1759,  0.1693],\n",
      "        [-0.0699, -0.2775,  0.0744,  0.3324, -0.1086, -0.3070,  0.3282,  0.0535,\n",
      "          0.2561,  0.2565,  0.1168,  0.2214, -0.2430, -0.0757, -0.0059,  0.1686,\n",
      "         -0.0024,  0.0463,  0.0686, -0.1675,  0.3514,  0.1367,  0.2145,  0.3221,\n",
      "          0.3521, -0.1050, -0.1648, -0.1585,  0.1677,  0.3048, -0.0717,  0.0128],\n",
      "        [-0.2278,  0.2873,  0.0899, -0.0237, -0.2272, -0.2384,  0.3030, -0.2309,\n",
      "          0.0950, -0.1990,  0.1762,  0.1141,  0.3087, -0.0611,  0.1287, -0.0067,\n",
      "         -0.2072, -0.3240, -0.2671,  0.3352, -0.2212, -0.2166, -0.2868, -0.0535,\n",
      "          0.0012, -0.0098, -0.2042, -0.0029, -0.2062,  0.0910, -0.1916, -0.0311],\n",
      "        [ 0.0901,  0.1729,  0.2315,  0.1777, -0.1764, -0.0368,  0.1807, -0.0849,\n",
      "          0.1730, -0.1213, -0.2346,  0.1807,  0.0504,  0.1159, -0.3311, -0.2011,\n",
      "          0.0062,  0.2386,  0.1295, -0.2162, -0.1954, -0.1578,  0.0050, -0.1642,\n",
      "         -0.2161, -0.2246,  0.2946, -0.1803, -0.0570, -0.2793,  0.0114, -0.2245],\n",
      "        [ 0.0996,  0.1585,  0.1973,  0.3181,  0.0441,  0.0943,  0.3289,  0.0041,\n",
      "          0.1049,  0.1627, -0.0094,  0.1471, -0.1573,  0.2798,  0.1198,  0.1096,\n",
      "          0.1820,  0.2727, -0.1306,  0.0610, -0.1315, -0.3494,  0.0193,  0.1307,\n",
      "          0.2901,  0.0608, -0.0100, -0.1351, -0.0377, -0.1324,  0.0643, -0.0645]],\n",
      "       requires_grad=True)\n",
      "Wgate spliced: torch.Size([4, 16])\n",
      "tensor([[ 0.2630,  0.3304, -0.2631, -0.3178,  0.3219, -0.1386,  0.0899, -0.1480,\n",
      "          0.3116,  0.2974, -0.1163,  0.0538, -0.2158,  0.2218,  0.2058,  0.3158],\n",
      "        [ 0.1676,  0.1079, -0.1379, -0.1800, -0.1866,  0.0735,  0.0232,  0.3478,\n",
      "         -0.0545,  0.0847,  0.0208, -0.3432,  0.0005, -0.0354,  0.1616, -0.1833],\n",
      "        [ 0.1839,  0.2218, -0.1851, -0.1301,  0.0134,  0.0530, -0.0466, -0.1877,\n",
      "          0.1577,  0.0715,  0.0227,  0.0646,  0.3183,  0.0467,  0.0097,  0.2110],\n",
      "        [ 0.0458,  0.3373,  0.2697, -0.2697, -0.3212, -0.0153,  0.1222,  0.1895,\n",
      "          0.1133,  0.1937, -0.0664,  0.0580, -0.1034, -0.0585, -0.0800,  0.2740]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Bgate: torch.Size([32])\n",
      "Parameter containing:\n",
      "tensor([ 0.2499,  0.1922, -0.2573, -0.2518, -0.1869, -0.0425,  0.2350,  0.1774,\n",
      "         0.0343, -0.0149,  0.0916, -0.0965,  0.0010, -0.1320,  0.1573, -0.1944,\n",
      "         0.1862,  0.2137, -0.0462, -0.2737, -0.2999, -0.0852,  0.1353,  0.2650,\n",
      "        -0.0265, -0.0306, -0.0561, -0.0669,  0.1266,  0.0516, -0.1485,  0.0746],\n",
      "       requires_grad=True)\n",
      "Bgate spliced: torch.Size([16])\n",
      "tensor([ 0.2499,  0.1922, -0.2573, -0.2518, -0.1869, -0.0425,  0.2350,  0.1774,\n",
      "         0.0343, -0.0149,  0.0916, -0.0965,  0.0010, -0.1320,  0.1573, -0.1944],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xgate: torch.Size([1, 3, 16])\n",
      "tensor([[[ 0.5087,  0.0495, -1.0363, -0.1918,  0.6965, -0.1580,  0.1249,\n",
      "          -0.3230,  0.2220,  0.0128,  0.0773, -0.1333, -0.0058,  0.2186,\n",
      "           0.5262, -0.2620],\n",
      "         [ 0.3034, -0.1983, -0.5959, -0.0590, -0.3379,  0.1709,  0.0917,\n",
      "           0.6279, -0.3644, -0.2646,  0.2629, -0.7386,  0.2252, -0.2610,\n",
      "           0.3757, -0.9724],\n",
      "         [-0.1926, -0.6332, -0.0116,  0.3620, -0.5699,  0.1678,  0.0880,\n",
      "           0.7091, -0.6745, -0.5660,  0.2942, -0.5479,  0.0647, -0.4725,\n",
      "           0.0470, -1.1765]]], grad_fn=<AddBackward0>)\n",
      "GeLU'ed Xgate: torch.Size([1, 3, 16])\n",
      "tensor([[[ 0.3533,  0.0257, -0.1555, -0.0813,  0.5272, -0.0691,  0.0686,\n",
      "          -0.1206,  0.1305,  0.0065,  0.0410, -0.0596, -0.0029,  0.1282,\n",
      "           0.3686, -0.1039],\n",
      "         [ 0.1879, -0.0836, -0.1642, -0.0281, -0.1242,  0.0970,  0.0492,\n",
      "           0.4615, -0.1304, -0.1047,  0.1587, -0.1699,  0.1327, -0.1036,\n",
      "           0.2429, -0.1609],\n",
      "         [-0.0816, -0.1667, -0.0057,  0.2322, -0.1621,  0.0951,  0.0471,\n",
      "           0.5396, -0.1686, -0.1617,  0.1812, -0.1599,  0.0340, -0.1504,\n",
      "           0.0244, -0.1408]]], grad_fn=<GeluBackward0>)\n",
      "Wup: torch.Size([8, 32])\n",
      "Parameter containing:\n",
      "tensor([[ 0.1461, -0.1881, -0.0114, -0.2953,  0.1766, -0.0054,  0.0200,  0.2929,\n",
      "          0.2036,  0.1445,  0.0007, -0.2298,  0.0682, -0.1226,  0.2082,  0.2925,\n",
      "          0.0774, -0.0253, -0.1358, -0.1838,  0.3420, -0.2564, -0.0681,  0.2453,\n",
      "         -0.2752,  0.0909, -0.2523, -0.3247,  0.1653, -0.1776,  0.2798, -0.2458],\n",
      "        [-0.3247, -0.2901,  0.2185,  0.0110, -0.1257, -0.0880, -0.2688,  0.0008,\n",
      "         -0.0161, -0.2435, -0.1607,  0.3109, -0.0768, -0.1983,  0.2455,  0.1018,\n",
      "          0.1656,  0.2817, -0.2373,  0.1910,  0.1478,  0.1714,  0.1657, -0.3000,\n",
      "         -0.1971,  0.2098, -0.2455,  0.1353, -0.3296, -0.0129, -0.0986,  0.0954],\n",
      "        [ 0.1068, -0.2128,  0.1991,  0.1008,  0.2752,  0.2930,  0.2256, -0.0079,\n",
      "          0.2821,  0.3137,  0.3496, -0.0613, -0.1468, -0.0054,  0.0866,  0.2523,\n",
      "         -0.1940, -0.2411, -0.2022,  0.2475,  0.0258,  0.0016,  0.1201, -0.3108,\n",
      "          0.0214,  0.0367, -0.1037,  0.2520,  0.1717, -0.0658,  0.2890, -0.1868],\n",
      "        [ 0.0058, -0.1731,  0.1676,  0.0108, -0.2819, -0.1985,  0.2458, -0.1946,\n",
      "         -0.1752,  0.0611,  0.3184,  0.2923, -0.0479,  0.2621,  0.2844,  0.2188,\n",
      "         -0.2579,  0.3230, -0.1489,  0.0944,  0.3340,  0.2295, -0.1343,  0.2445,\n",
      "          0.0340, -0.0537, -0.1785,  0.2965,  0.1392,  0.1501, -0.3371,  0.1051],\n",
      "        [-0.2644,  0.0722,  0.3389, -0.3096, -0.0578, -0.0960,  0.1849, -0.0896,\n",
      "          0.2314, -0.1201,  0.1368,  0.3479, -0.1061,  0.1036,  0.1223,  0.1666,\n",
      "         -0.3237, -0.3464,  0.0718, -0.2749,  0.2485,  0.1218,  0.3492,  0.0366,\n",
      "          0.2144,  0.3232,  0.0853,  0.2870, -0.2610, -0.1287,  0.2955,  0.1155],\n",
      "        [ 0.2456, -0.0144,  0.0137,  0.1239, -0.2818, -0.0874,  0.1061, -0.1882,\n",
      "          0.3299, -0.2652,  0.1321, -0.2266, -0.3398,  0.2878, -0.1701, -0.0402,\n",
      "          0.1522, -0.2065, -0.0613,  0.2694,  0.3201,  0.2417,  0.2183, -0.0752,\n",
      "         -0.1656, -0.2163, -0.1730,  0.0922,  0.0674,  0.1748,  0.0196,  0.2470],\n",
      "        [-0.3207, -0.1177, -0.2640, -0.0134, -0.3183, -0.1480, -0.1676, -0.3459,\n",
      "         -0.1791, -0.2782,  0.2909, -0.0460,  0.1268, -0.0369, -0.2830, -0.1581,\n",
      "         -0.0776, -0.1935, -0.1141, -0.2595,  0.1222,  0.1648, -0.1104,  0.3182,\n",
      "         -0.2715, -0.0320, -0.1157, -0.0379,  0.1773, -0.1688, -0.3005, -0.0800],\n",
      "        [ 0.1153, -0.3059, -0.0326, -0.2650, -0.1102, -0.2051,  0.3459, -0.0013,\n",
      "         -0.1888, -0.3529, -0.0028, -0.3385, -0.3253, -0.2511, -0.0515,  0.1634,\n",
      "          0.2073,  0.1018,  0.2722, -0.3286,  0.1437,  0.3030,  0.2420,  0.2834,\n",
      "          0.2969,  0.0161,  0.1945, -0.2914,  0.1334,  0.0514,  0.2667,  0.1051]],\n",
      "       requires_grad=True)\n",
      "Wup spliced: torch.Size([4, 16])\n",
      "tensor([[ 0.1461, -0.1881, -0.0114, -0.2953,  0.1766, -0.0054,  0.0200,  0.2929,\n",
      "          0.2036,  0.1445,  0.0007, -0.2298,  0.0682, -0.1226,  0.2082,  0.2925],\n",
      "        [-0.3247, -0.2901,  0.2185,  0.0110, -0.1257, -0.0880, -0.2688,  0.0008,\n",
      "         -0.0161, -0.2435, -0.1607,  0.3109, -0.0768, -0.1983,  0.2455,  0.1018],\n",
      "        [ 0.1068, -0.2128,  0.1991,  0.1008,  0.2752,  0.2930,  0.2256, -0.0079,\n",
      "          0.2821,  0.3137,  0.3496, -0.0613, -0.1468, -0.0054,  0.0866,  0.2523],\n",
      "        [ 0.0058, -0.1731,  0.1676,  0.0108, -0.2819, -0.1985,  0.2458, -0.1946,\n",
      "         -0.1752,  0.0611,  0.3184,  0.2923, -0.0479,  0.2621,  0.2844,  0.2188]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Bup: torch.Size([32])\n",
      "Parameter containing:\n",
      "tensor([-0.2515,  0.2189, -0.0972,  0.1164, -0.2267, -0.2862,  0.3457, -0.0580,\n",
      "        -0.0643,  0.0126,  0.1822, -0.0607,  0.1557, -0.0858,  0.3095, -0.0990,\n",
      "         0.2946, -0.2724,  0.1024, -0.3385,  0.2050,  0.0544, -0.3525,  0.0763,\n",
      "         0.3103, -0.2710, -0.1986, -0.2860,  0.0696,  0.0840,  0.0303, -0.1089],\n",
      "       requires_grad=True)\n",
      "Bup spliced: torch.Size([16])\n",
      "tensor([-0.2515,  0.2189, -0.0972,  0.1164, -0.2267, -0.2862,  0.3457, -0.0580,\n",
      "        -0.0643,  0.0126,  0.1822, -0.0607,  0.1557, -0.0858,  0.3095, -0.0990],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xup: torch.Size([1, 3, 16])\n",
      "tensor([[[-0.0905,  0.2432, -0.3393, -0.2093,  0.4837,  0.0893, -0.0045,\n",
      "           0.5872,  0.5056,  0.1239, -0.2809, -0.7986,  0.2789, -0.6635,\n",
      "           0.0976, -0.0823],\n",
      "         [-0.8661,  0.0611,  0.0719,  0.2941, -0.2472, -0.2269, -0.3630,\n",
      "          -0.0259, -0.0392, -0.5394, -0.4250,  0.2657,  0.0499, -0.6041,\n",
      "           0.2726, -0.3530],\n",
      "         [-0.8208,  0.5773, -0.2185,  0.4299, -0.6412, -0.4937, -0.2879,\n",
      "          -0.3197, -0.5107, -0.7296, -0.5100,  0.3601,  0.1760, -0.2523,\n",
      "          -0.0681, -0.8229]]], grad_fn=<AddBackward0>)\n",
      "Xfuse: torch.Size([1, 3, 16])\n",
      "tensor([[[-0.0320,  0.0063,  0.0528,  0.0170,  0.2550, -0.0062, -0.0003,\n",
      "          -0.0708,  0.0660,  0.0008, -0.0115,  0.0476, -0.0008, -0.0851,\n",
      "           0.0360,  0.0085],\n",
      "         [-0.1627, -0.0051, -0.0118, -0.0083,  0.0307, -0.0220, -0.0179,\n",
      "          -0.0120,  0.0051,  0.0565, -0.0674, -0.0451,  0.0066,  0.0626,\n",
      "           0.0662,  0.0568],\n",
      "         [ 0.0670, -0.0962,  0.0013,  0.0998,  0.1039, -0.0469, -0.0136,\n",
      "          -0.1725,  0.0861,  0.1180, -0.0924, -0.0576,  0.0060,  0.0379,\n",
      "          -0.0017,  0.1159]]], grad_fn=<MulBackward0>)\n",
      "Wdown: torch.Size([32, 8])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0182, -0.0335,  0.1160,  0.0016,  0.1674,  0.0666,  0.0672, -0.0870],\n",
      "        [-0.1036, -0.1544,  0.0656,  0.0593,  0.0246,  0.1329, -0.0141,  0.1280],\n",
      "        [ 0.0924, -0.0610, -0.0202, -0.1488, -0.1268, -0.1293,  0.1564,  0.0257],\n",
      "        [-0.0521, -0.0812,  0.0997, -0.1479, -0.1325, -0.0349,  0.0567,  0.1022],\n",
      "        [-0.1485, -0.1232,  0.0503, -0.1232,  0.1660, -0.0445, -0.0446,  0.0510],\n",
      "        [-0.1061, -0.0046, -0.0265,  0.1611, -0.0745,  0.0744, -0.1586,  0.0883],\n",
      "        [ 0.1349, -0.0969,  0.0187,  0.0635,  0.0267,  0.1135, -0.1686,  0.1749],\n",
      "        [-0.0706,  0.0030,  0.0220, -0.1169, -0.1222,  0.0809, -0.0947,  0.1453],\n",
      "        [-0.0279, -0.1280, -0.0225, -0.1762,  0.0917,  0.1144, -0.0192,  0.0034],\n",
      "        [-0.0751,  0.0913,  0.0093,  0.0964, -0.1255, -0.1656, -0.0056, -0.0406],\n",
      "        [ 0.1591,  0.0859,  0.0305,  0.0196, -0.1654,  0.0772,  0.1611,  0.1560],\n",
      "        [-0.0746, -0.0580,  0.0504, -0.1644, -0.0701,  0.0872, -0.1648,  0.1040],\n",
      "        [ 0.1301,  0.0403,  0.0711, -0.1759,  0.0784, -0.1548, -0.0742, -0.0850],\n",
      "        [-0.1415, -0.0116, -0.0338,  0.1498, -0.0192,  0.1108,  0.1595,  0.0837],\n",
      "        [-0.1039, -0.0155,  0.0275, -0.1285,  0.0839,  0.1181,  0.0983,  0.0754],\n",
      "        [-0.1239,  0.0488, -0.1654, -0.0906,  0.0470, -0.0620, -0.1542, -0.1062],\n",
      "        [-0.0375,  0.0230, -0.0777,  0.1664,  0.0647,  0.1694,  0.0817,  0.1513],\n",
      "        [ 0.0760,  0.1130, -0.0421,  0.0471, -0.1388,  0.0021,  0.1197,  0.0242],\n",
      "        [ 0.0810,  0.1652,  0.0490,  0.1459, -0.0303,  0.0509, -0.0201,  0.1173],\n",
      "        [ 0.1563, -0.0756, -0.0584,  0.0121, -0.0436, -0.1635,  0.1264,  0.1484],\n",
      "        [-0.0628,  0.1063, -0.1746,  0.1726, -0.1372,  0.0788,  0.1273, -0.0905],\n",
      "        [ 0.1667,  0.1260,  0.1609, -0.1081, -0.1310,  0.0144, -0.1354, -0.0869],\n",
      "        [ 0.1608,  0.0912, -0.1313, -0.1481,  0.0570, -0.0335,  0.1258, -0.0184],\n",
      "        [-0.1426, -0.0447,  0.1231,  0.1384,  0.0974, -0.1639, -0.1728,  0.0261],\n",
      "        [-0.0215, -0.0968,  0.1192, -0.0503, -0.0652, -0.1343, -0.0023, -0.0068],\n",
      "        [-0.0486, -0.0081,  0.1722, -0.0421, -0.1066, -0.1005,  0.1318,  0.1001],\n",
      "        [-0.0495, -0.1124, -0.0681,  0.0858,  0.1235,  0.0307,  0.1351, -0.1294],\n",
      "        [ 0.0583,  0.0458,  0.0871, -0.0338, -0.0507, -0.0004, -0.0430,  0.0936],\n",
      "        [-0.0916,  0.0825,  0.1493,  0.0817, -0.0655, -0.0552, -0.0992,  0.0182],\n",
      "        [ 0.1523, -0.0392,  0.1635, -0.0732, -0.1632, -0.1637,  0.0139,  0.1081],\n",
      "        [-0.1676, -0.1737, -0.0526, -0.0201,  0.0717,  0.1610,  0.0641,  0.0570],\n",
      "        [-0.1431, -0.0946,  0.1332,  0.0950,  0.0392,  0.0311, -0.1628, -0.1136]],\n",
      "       requires_grad=True)\n",
      "Wdown spliced: torch.Size([16, 4])\n",
      "tensor([[ 0.0182, -0.0335,  0.1160,  0.0016],\n",
      "        [-0.1036, -0.1544,  0.0656,  0.0593],\n",
      "        [ 0.0924, -0.0610, -0.0202, -0.1488],\n",
      "        [-0.0521, -0.0812,  0.0997, -0.1479],\n",
      "        [-0.1485, -0.1232,  0.0503, -0.1232],\n",
      "        [-0.1061, -0.0046, -0.0265,  0.1611],\n",
      "        [ 0.1349, -0.0969,  0.0187,  0.0635],\n",
      "        [-0.0706,  0.0030,  0.0220, -0.1169],\n",
      "        [-0.0279, -0.1280, -0.0225, -0.1762],\n",
      "        [-0.0751,  0.0913,  0.0093,  0.0964],\n",
      "        [ 0.1591,  0.0859,  0.0305,  0.0196],\n",
      "        [-0.0746, -0.0580,  0.0504, -0.1644],\n",
      "        [ 0.1301,  0.0403,  0.0711, -0.1759],\n",
      "        [-0.1415, -0.0116, -0.0338,  0.1498],\n",
      "        [-0.1039, -0.0155,  0.0275, -0.1285],\n",
      "        [-0.1239,  0.0488, -0.1654, -0.0906]], grad_fn=<SliceBackward0>)\n",
      "Bdown: torch.Size([8])\n",
      "Parameter containing:\n",
      "tensor([-0.0420, -0.1305,  0.1604, -0.0753, -0.0015, -0.0793, -0.0961,  0.0816],\n",
      "       requires_grad=True)\n",
      "Bdown spliced: torch.Size([4])\n",
      "tensor([-0.0420, -0.1305,  0.1604, -0.0753], grad_fn=<SliceBackward0>)\n",
      "outputs: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.0716, -0.1779,  0.1722, -0.1471],\n",
      "         [-0.0825, -0.1223,  0.1290, -0.0747],\n",
      "         [-0.0764, -0.1372,  0.1476, -0.0987]]], grad_fn=<AddBackward0>)\n",
      "------------- END MLP.forwardTensor() ------------\n",
      "layer's final residual state:\n",
      "tensor([[[ 0.3741, -0.1623,  0.2504, -0.8097],\n",
      "         [-0.6267,  1.3308,  0.0884, -1.0367],\n",
      "         [-2.7033,  1.3693, -1.6889, -1.4279]]], grad_fn=<AddBackward0>)\n",
      "----------------- END Layer.forwardTensor() --------------------\n",
      "forwardTensor() output: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.3741, -0.1623,  0.2504, -0.8097],\n",
      "         [-0.6267,  1.3308,  0.0884, -1.0367],\n",
      "         [-2.7033,  1.3693, -1.6889, -1.4279]]], grad_fn=<AddBackward0>)\n",
      "Model 1 from range(2)\n",
      "----------------- Layer.forwardTensor() --------------------\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.5754,  0.3787, -0.9614,  2.2824],\n",
      "         [ 1.4094,  0.2831,  1.7523,  0.6584],\n",
      "         [-0.7426,  0.0394,  0.3453, -0.3844]]])\n",
      "normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.4476,  0.2947, -0.7480,  1.7757],\n",
      "         [ 1.1943,  0.2399,  1.4849,  0.5579],\n",
      "         [-1.6401,  0.0871,  0.7626, -0.8491]]])\n",
      "dim: 4\n",
      "skip: 4\n",
      "spliced scale: torch.Size([4])\n",
      "tensor([0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.4476,  0.2947, -0.7480,  1.7757],\n",
      "         [ 1.1943,  0.2399,  1.4849,  0.5579],\n",
      "         [-1.6401,  0.0871,  0.7626, -0.8491]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "---------- Attention Input: torch.Tensor ------------\n",
      "----------------- MultiQueryAttention.forwardTensor() --------------------\n",
      "x shape: torch.Size([1, 3, 4])\n",
      "d_skip: 4\n",
      "models_in_this_level: 2\n",
      "h_dim: 2\n",
      "h_skip: 2\n",
      "self.Wqkv: torch.Size([8, 16])\n",
      "Parameter containing:\n",
      "tensor([[-0.0340, -0.1815, -0.3210, -0.1459, -0.1058,  0.0699, -0.0545, -0.3136,\n",
      "         -0.1625, -0.3407,  0.0713,  0.1895,  0.1236, -0.0918,  0.1145,  0.0372],\n",
      "        [-0.0765,  0.2838, -0.1542,  0.3024, -0.2310,  0.1960,  0.3432, -0.1336,\n",
      "         -0.0016,  0.1019,  0.3275,  0.0040, -0.2962, -0.2300,  0.1264,  0.3188],\n",
      "        [ 0.2791, -0.0656,  0.0931, -0.1173, -0.0828,  0.0369, -0.2055, -0.0018,\n",
      "          0.0459, -0.0719,  0.1220,  0.1993, -0.0510,  0.2554,  0.2919, -0.1963],\n",
      "        [-0.0580,  0.0674,  0.1863, -0.2996, -0.1225, -0.2918,  0.3034, -0.0289,\n",
      "          0.3401,  0.2546, -0.2533,  0.0773, -0.2015, -0.2837,  0.0371, -0.1208],\n",
      "        [-0.1807,  0.2407, -0.0230, -0.0681,  0.1628, -0.2487,  0.1491,  0.1595,\n",
      "         -0.2048, -0.0425, -0.1204,  0.0531,  0.0862, -0.2871, -0.2986, -0.1081],\n",
      "        [ 0.0699,  0.1571, -0.2550,  0.1524, -0.0307,  0.0327, -0.1545,  0.0394,\n",
      "          0.1334, -0.2649,  0.0729, -0.2974,  0.2341, -0.1605, -0.1469,  0.2894],\n",
      "        [-0.2351,  0.2850, -0.0287, -0.3331,  0.3337,  0.0153,  0.0692,  0.1914,\n",
      "          0.1876, -0.3101, -0.1262, -0.3145,  0.2511, -0.2142,  0.2297, -0.1992],\n",
      "        [-0.3401,  0.1138, -0.3317, -0.0407, -0.1315,  0.2288,  0.2154,  0.3395,\n",
      "          0.1770,  0.0745,  0.1152,  0.0304, -0.0858,  0.0956,  0.2461, -0.0038]],\n",
      "       requires_grad=True)\n",
      "Wq: torch.Size([8, 8])\n",
      "tensor([[-0.0340, -0.1815, -0.3210, -0.1459, -0.1058,  0.0699, -0.0545, -0.3136],\n",
      "        [-0.0765,  0.2838, -0.1542,  0.3024, -0.2310,  0.1960,  0.3432, -0.1336],\n",
      "        [ 0.2791, -0.0656,  0.0931, -0.1173, -0.0828,  0.0369, -0.2055, -0.0018],\n",
      "        [-0.0580,  0.0674,  0.1863, -0.2996, -0.1225, -0.2918,  0.3034, -0.0289],\n",
      "        [-0.1807,  0.2407, -0.0230, -0.0681,  0.1628, -0.2487,  0.1491,  0.1595],\n",
      "        [ 0.0699,  0.1571, -0.2550,  0.1524, -0.0307,  0.0327, -0.1545,  0.0394],\n",
      "        [-0.2351,  0.2850, -0.0287, -0.3331,  0.3337,  0.0153,  0.0692,  0.1914],\n",
      "        [-0.3401,  0.1138, -0.3317, -0.0407, -0.1315,  0.2288,  0.2154,  0.3395]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wk: torch.Size([8, 4])\n",
      "tensor([[-0.1625, -0.3407,  0.0713,  0.1895],\n",
      "        [-0.0016,  0.1019,  0.3275,  0.0040],\n",
      "        [ 0.0459, -0.0719,  0.1220,  0.1993],\n",
      "        [ 0.3401,  0.2546, -0.2533,  0.0773],\n",
      "        [-0.2048, -0.0425, -0.1204,  0.0531],\n",
      "        [ 0.1334, -0.2649,  0.0729, -0.2974],\n",
      "        [ 0.1876, -0.3101, -0.1262, -0.3145],\n",
      "        [ 0.1770,  0.0745,  0.1152,  0.0304]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wv: torch.Size([8, 4])\n",
      "tensor([[ 0.1236, -0.0918,  0.1145,  0.0372],\n",
      "        [-0.2962, -0.2300,  0.1264,  0.3188],\n",
      "        [-0.0510,  0.2554,  0.2919, -0.1963],\n",
      "        [-0.2015, -0.2837,  0.0371, -0.1208],\n",
      "        [ 0.0862, -0.2871, -0.2986, -0.1081],\n",
      "        [ 0.2341, -0.1605, -0.1469,  0.2894],\n",
      "        [ 0.2511, -0.2142,  0.2297, -0.1992],\n",
      "        [-0.0858,  0.0956,  0.2461, -0.0038]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wq spliced: torch.Size([4, 4])\n",
      "tensor([[-0.0230, -0.0681,  0.1491,  0.1595],\n",
      "        [-0.2550,  0.1524, -0.1545,  0.0394],\n",
      "        [-0.0287, -0.3331,  0.0692,  0.1914],\n",
      "        [-0.3317, -0.0407,  0.2154,  0.3395]], grad_fn=<CatBackward0>)\n",
      "Wk spliced: torch.Size([4, 2])\n",
      "tensor([[-0.1204,  0.0531],\n",
      "        [ 0.0729, -0.2974],\n",
      "        [-0.1262, -0.3145],\n",
      "        [ 0.1152,  0.0304]], grad_fn=<CatBackward0>)\n",
      "Wv spliced: torch.Size([4, 2])\n",
      "tensor([[-0.2986, -0.1081],\n",
      "        [-0.1469,  0.2894],\n",
      "        [ 0.2297, -0.1992],\n",
      "        [ 0.2461, -0.0038]], grad_fn=<CatBackward0>)\n",
      "Wqkv_spliced: torch.Size([4, 8])\n",
      "tensor([[-0.0230, -0.0681,  0.1491,  0.1595, -0.1204,  0.0531, -0.2986, -0.1081],\n",
      "        [-0.2550,  0.1524, -0.1545,  0.0394,  0.0729, -0.2974, -0.1469,  0.2894],\n",
      "        [-0.0287, -0.3331,  0.0692,  0.1914, -0.1262, -0.3145,  0.2297, -0.1992],\n",
      "        [-0.3317, -0.0407,  0.2154,  0.3395,  0.1152,  0.0304,  0.2461, -0.0038]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "xqkv: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.6530,  0.1914,  0.3519,  0.5427,  0.2665,  0.2254,  0.0882,\n",
      "           0.1792],\n",
      "         [-0.3163, -0.5621,  0.3640,  0.6735, -0.2495, -0.4579,  0.0865,\n",
      "          -0.3576],\n",
      "         [ 0.2753, -0.0946, -0.3881, -0.4005,  0.0098, -0.3788,  0.4431,\n",
      "           0.0538]]], grad_fn=<UnsafeViewBackward0>)\n",
      "xq: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.6530,  0.1914,  0.3519,  0.5427],\n",
      "         [-0.3163, -0.5621,  0.3640,  0.6735],\n",
      "         [ 0.2753, -0.0946, -0.3881, -0.4005]]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "xk: torch.Size([1, 3, 2])\n",
      "tensor([[[ 0.2665,  0.2254],\n",
      "         [-0.2495, -0.4579],\n",
      "         [ 0.0098, -0.3788]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xv: torch.Size([1, 3, 2])\n",
      "tensor([[[ 0.0882,  0.1792],\n",
      "         [ 0.0865, -0.3576],\n",
      "         [ 0.4431,  0.0538]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xq reshaped: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[-0.6530,  0.1914],\n",
      "          [ 0.3519,  0.5427]],\n",
      "\n",
      "         [[-0.3163, -0.5621],\n",
      "          [ 0.3640,  0.6735]],\n",
      "\n",
      "         [[ 0.2753, -0.0946],\n",
      "          [-0.3881, -0.4005]]]], grad_fn=<ViewBackward0>)\n",
      "xk reshaped: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[ 0.2665,  0.2254]],\n",
      "\n",
      "         [[-0.2495, -0.4579]],\n",
      "\n",
      "         [[ 0.0098, -0.3788]]]], grad_fn=<ViewBackward0>)\n",
      "xv reshaped: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[ 0.0882,  0.1792]],\n",
      "\n",
      "         [[ 0.0865, -0.3576]],\n",
      "\n",
      "         [[ 0.4431,  0.0538]]]], grad_fn=<ViewBackward0>)\n",
      "rotated xq: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[-0.6530,  0.1914],\n",
      "          [ 0.3519,  0.5427]],\n",
      "\n",
      "         [[ 0.3021, -0.5699],\n",
      "          [-0.3700,  0.6702]],\n",
      "\n",
      "         [[-0.0285,  0.2897],\n",
      "          [ 0.5257, -0.1863]]]], grad_fn=<TransposeBackward0>)\n",
      "rotated xk: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[ 0.2665,  0.2254]],\n",
      "\n",
      "         [[ 0.2505, -0.4573]],\n",
      "\n",
      "         [[ 0.3403,  0.1665]]]], grad_fn=<TransposeBackward0>)\n",
      "repeat_interleaved xk: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[ 0.2665,  0.2254],\n",
      "          [ 0.2665,  0.2254]],\n",
      "\n",
      "         [[ 0.2505, -0.4573],\n",
      "          [ 0.2505, -0.4573]],\n",
      "\n",
      "         [[ 0.3403,  0.1665],\n",
      "          [ 0.3403,  0.1665]]]], grad_fn=<ViewBackward0>)\n",
      "repeat_interleaved xv: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[ 0.0882,  0.1792],\n",
      "          [ 0.0882,  0.1792]],\n",
      "\n",
      "         [[ 0.0865, -0.3576],\n",
      "          [ 0.0865, -0.3576]],\n",
      "\n",
      "         [[ 0.4431,  0.0538],\n",
      "          [ 0.4431,  0.0538]]]], grad_fn=<ViewBackward0>)\n",
      "transposed xq: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[-0.6530,  0.1914],\n",
      "          [ 0.3021, -0.5699],\n",
      "          [-0.0285,  0.2897]],\n",
      "\n",
      "         [[ 0.3519,  0.5427],\n",
      "          [-0.3700,  0.6702],\n",
      "          [ 0.5257, -0.1863]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xk: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[ 0.2665,  0.2254],\n",
      "          [ 0.2505, -0.4573],\n",
      "          [ 0.3403,  0.1665]],\n",
      "\n",
      "         [[ 0.2665,  0.2254],\n",
      "          [ 0.2505, -0.4573],\n",
      "          [ 0.3403,  0.1665]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xv: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[ 0.0882,  0.1792],\n",
      "          [ 0.0865, -0.3576],\n",
      "          [ 0.4431,  0.0538]],\n",
      "\n",
      "         [[ 0.0882,  0.1792],\n",
      "          [ 0.0865, -0.3576],\n",
      "          [ 0.4431,  0.0538]]]], grad_fn=<TransposeBackward0>)\n",
      "scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[-0.0926, -0.1776, -0.1346],\n",
      "          [-0.0339,  0.2378,  0.0056],\n",
      "          [ 0.0408, -0.0987,  0.0273]],\n",
      "\n",
      "         [[ 0.1528, -0.1132,  0.1486],\n",
      "          [ 0.0371, -0.2823, -0.0101],\n",
      "          [ 0.0694,  0.1533,  0.1046]]]], grad_fn=<MulBackward0>)\n",
      "mask: torch.Size([1, 1, 3, 3])\n",
      "tensor([[[[ 0.0000e+00, -2.3820e+38, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]]]])\n",
      "masked scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[-9.2555e-02, -2.3820e+38, -2.3820e+38],\n",
      "          [-3.3920e-02,  2.3780e-01, -2.3820e+38],\n",
      "          [ 4.0810e-02, -9.8731e-02,  2.7257e-02]],\n",
      "\n",
      "         [[ 1.5283e-01, -2.3820e+38, -2.3820e+38],\n",
      "          [ 3.7107e-02, -2.8227e-01, -2.3820e+38],\n",
      "          [ 6.9377e-02,  1.5334e-01,  1.0457e-01]]]], grad_fn=<AddBackward0>)\n",
      "softmaxed scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [0.4325, 0.5675, 0.0000],\n",
      "          [0.3501, 0.3045, 0.3454]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.5792, 0.4208, 0.0000],\n",
      "          [0.3202, 0.3482, 0.3316]]]], grad_fn=<SoftmaxBackward0>)\n",
      "attention: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[ 0.0882,  0.1792],\n",
      "          [ 0.0873, -0.1255],\n",
      "          [ 0.2103, -0.0276]],\n",
      "\n",
      "         [[ 0.0882,  0.1792],\n",
      "          [ 0.0875, -0.0467],\n",
      "          [ 0.2053, -0.0493]]]], grad_fn=<UnsafeViewBackward0>)\n",
      "reshaped attention: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.0882,  0.1792,  0.0882,  0.1792],\n",
      "         [ 0.0873, -0.1255,  0.0875, -0.0467],\n",
      "         [ 0.2103, -0.0276,  0.2053, -0.0493]]], grad_fn=<ViewBackward0>)\n",
      "self.Wo: torch.Size([8, 8])\n",
      "Parameter containing:\n",
      "tensor([[ 0.2720, -0.0599,  0.1861, -0.1372, -0.1935, -0.1548, -0.3042,  0.1437],\n",
      "        [ 0.0298,  0.1093, -0.0187, -0.1633, -0.1128,  0.1514,  0.1242,  0.2416],\n",
      "        [ 0.1782,  0.0222,  0.2303,  0.3073, -0.2227, -0.0706,  0.3020, -0.2865],\n",
      "        [-0.2260,  0.2797, -0.3507,  0.0173, -0.1894,  0.3031,  0.2639, -0.2744],\n",
      "        [ 0.0999, -0.0104,  0.0846,  0.0732,  0.2059, -0.3449, -0.1057, -0.1274],\n",
      "        [ 0.2284, -0.1917, -0.3070, -0.3047, -0.1733, -0.1838,  0.3154,  0.0262],\n",
      "        [-0.1188, -0.0889, -0.0819,  0.0254, -0.0091,  0.0690,  0.0810,  0.0455],\n",
      "        [-0.1355,  0.1819, -0.1108, -0.2114,  0.1157,  0.0880,  0.0930,  0.1926]],\n",
      "       requires_grad=True)\n",
      "spliced Wo: torch.Size([4, 4])\n",
      "tensor([[-0.2227, -0.0706,  0.3020, -0.2865],\n",
      "        [-0.1894,  0.3031,  0.2639, -0.2744],\n",
      "        [-0.0091,  0.0690,  0.0810,  0.0455],\n",
      "        [ 0.1157,  0.0880,  0.0930,  0.1926]], grad_fn=<CatBackward0>)\n",
      "projected output: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.0336,  0.0699,  0.0977, -0.0359],\n",
      "         [-0.0019, -0.0423, -0.0040,  0.0044],\n",
      "         [-0.0492, -0.0134,  0.0683, -0.0528]]], grad_fn=<UnsafeViewBackward0>)\n",
      "----------------- END MultiQueryAttention.forwardTensor() --------------------\n",
      "x in layer after MQA & resid connection and before MLP:\n",
      "tensor([[[ 0.5417,  0.4486, -0.8637,  2.2465],\n",
      "         [ 1.4076,  0.2409,  1.7483,  0.6628],\n",
      "         [-0.7918,  0.0260,  0.4136, -0.4373]]], grad_fn=<AddBackward0>)\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.5417,  0.4486, -0.8637,  2.2465],\n",
      "         [ 1.4076,  0.2409,  1.7483,  0.6628],\n",
      "         [-0.7918,  0.0260,  0.4136, -0.4373]]], grad_fn=<AddBackward0>)\n",
      "normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.4321,  0.3578, -0.6889,  1.7918],\n",
      "         [ 1.1966,  0.2048,  1.4862,  0.5634],\n",
      "         [-1.5916,  0.0524,  0.8314, -0.8790]]], grad_fn=<MulBackward0>)\n",
      "dim: 4\n",
      "skip: 4\n",
      "spliced scale: torch.Size([4])\n",
      "tensor([0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.4321,  0.3578, -0.6889,  1.7918],\n",
      "         [ 1.1966,  0.2048,  1.4862,  0.5634],\n",
      "         [-1.5916,  0.0524,  0.8314, -0.8790]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "---------- MLP Input: torch.Tensor ------------\n",
      "------------- MLP.forwardTensor() ------------\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.4321,  0.3578, -0.6889,  1.7918],\n",
      "         [ 1.1966,  0.2048,  1.4862,  0.5634],\n",
      "         [-1.5916,  0.0524,  0.8314, -0.8790]]], grad_fn=<MulBackward0>)\n",
      "d_dim: 4\n",
      "d_skip: 4\n",
      "i_dim: 16\n",
      "i_skip: 16\n",
      "Wgate: torch.Size([8, 32])\n",
      "Parameter containing:\n",
      "tensor([[ 0.2630,  0.3304, -0.2631, -0.3178,  0.3219, -0.1386,  0.0899, -0.1480,\n",
      "          0.3116,  0.2974, -0.1163,  0.0538, -0.2158,  0.2218,  0.2058,  0.3158,\n",
      "          0.0195, -0.0748,  0.2612,  0.0902,  0.2668, -0.1171,  0.0344, -0.2618,\n",
      "         -0.2624,  0.0140, -0.1836, -0.2710,  0.1948,  0.1456,  0.0496,  0.1199],\n",
      "        [ 0.1676,  0.1079, -0.1379, -0.1800, -0.1866,  0.0735,  0.0232,  0.3478,\n",
      "         -0.0545,  0.0847,  0.0208, -0.3432,  0.0005, -0.0354,  0.1616, -0.1833,\n",
      "         -0.2717,  0.1721,  0.3339, -0.1524, -0.3199,  0.1380,  0.3210, -0.0216,\n",
      "          0.2402, -0.0941,  0.1791, -0.1290,  0.2034, -0.2613, -0.2594, -0.0470],\n",
      "        [ 0.1839,  0.2218, -0.1851, -0.1301,  0.0134,  0.0530, -0.0466, -0.1877,\n",
      "          0.1577,  0.0715,  0.0227,  0.0646,  0.3183,  0.0467,  0.0097,  0.2110,\n",
      "          0.1942, -0.0720, -0.3510,  0.2467,  0.3202, -0.3234, -0.2969,  0.1870,\n",
      "         -0.1467,  0.2700,  0.3276,  0.3361,  0.2249, -0.1557, -0.2221, -0.1109],\n",
      "        [ 0.0458,  0.3373,  0.2697, -0.2697, -0.3212, -0.0153,  0.1222,  0.1895,\n",
      "          0.1133,  0.1937, -0.0664,  0.0580, -0.1034, -0.0585, -0.0800,  0.2740,\n",
      "         -0.1762, -0.2514, -0.1972,  0.0756,  0.0571, -0.1100, -0.0780,  0.0126,\n",
      "          0.2733, -0.2975,  0.3090,  0.1289, -0.1540,  0.3491, -0.1759,  0.1693],\n",
      "        [-0.0699, -0.2775,  0.0744,  0.3324, -0.1086, -0.3070,  0.3282,  0.0535,\n",
      "          0.2561,  0.2565,  0.1168,  0.2214, -0.2430, -0.0757, -0.0059,  0.1686,\n",
      "         -0.0024,  0.0463,  0.0686, -0.1675,  0.3514,  0.1367,  0.2145,  0.3221,\n",
      "          0.3521, -0.1050, -0.1648, -0.1585,  0.1677,  0.3048, -0.0717,  0.0128],\n",
      "        [-0.2278,  0.2873,  0.0899, -0.0237, -0.2272, -0.2384,  0.3030, -0.2309,\n",
      "          0.0950, -0.1990,  0.1762,  0.1141,  0.3087, -0.0611,  0.1287, -0.0067,\n",
      "         -0.2072, -0.3240, -0.2671,  0.3352, -0.2212, -0.2166, -0.2868, -0.0535,\n",
      "          0.0012, -0.0098, -0.2042, -0.0029, -0.2062,  0.0910, -0.1916, -0.0311],\n",
      "        [ 0.0901,  0.1729,  0.2315,  0.1777, -0.1764, -0.0368,  0.1807, -0.0849,\n",
      "          0.1730, -0.1213, -0.2346,  0.1807,  0.0504,  0.1159, -0.3311, -0.2011,\n",
      "          0.0062,  0.2386,  0.1295, -0.2162, -0.1954, -0.1578,  0.0050, -0.1642,\n",
      "         -0.2161, -0.2246,  0.2946, -0.1803, -0.0570, -0.2793,  0.0114, -0.2245],\n",
      "        [ 0.0996,  0.1585,  0.1973,  0.3181,  0.0441,  0.0943,  0.3289,  0.0041,\n",
      "          0.1049,  0.1627, -0.0094,  0.1471, -0.1573,  0.2798,  0.1198,  0.1096,\n",
      "          0.1820,  0.2727, -0.1306,  0.0610, -0.1315, -0.3494,  0.0193,  0.1307,\n",
      "          0.2901,  0.0608, -0.0100, -0.1351, -0.0377, -0.1324,  0.0643, -0.0645]],\n",
      "       requires_grad=True)\n",
      "Wgate spliced: torch.Size([4, 16])\n",
      "tensor([[-0.0024,  0.0463,  0.0686, -0.1675,  0.3514,  0.1367,  0.2145,  0.3221,\n",
      "          0.3521, -0.1050, -0.1648, -0.1585,  0.1677,  0.3048, -0.0717,  0.0128],\n",
      "        [-0.2072, -0.3240, -0.2671,  0.3352, -0.2212, -0.2166, -0.2868, -0.0535,\n",
      "          0.0012, -0.0098, -0.2042, -0.0029, -0.2062,  0.0910, -0.1916, -0.0311],\n",
      "        [ 0.0062,  0.2386,  0.1295, -0.2162, -0.1954, -0.1578,  0.0050, -0.1642,\n",
      "         -0.2161, -0.2246,  0.2946, -0.1803, -0.0570, -0.2793,  0.0114, -0.2245],\n",
      "        [ 0.1820,  0.2727, -0.1306,  0.0610, -0.1315, -0.3494,  0.0193,  0.1307,\n",
      "          0.2901,  0.0608, -0.0100, -0.1351, -0.0377, -0.1324,  0.0643, -0.0645]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Bgate: torch.Size([32])\n",
      "Parameter containing:\n",
      "tensor([ 0.2499,  0.1922, -0.2573, -0.2518, -0.1869, -0.0425,  0.2350,  0.1774,\n",
      "         0.0343, -0.0149,  0.0916, -0.0965,  0.0010, -0.1320,  0.1573, -0.1944,\n",
      "         0.1862,  0.2137, -0.0462, -0.2737, -0.2999, -0.0852,  0.1353,  0.2650,\n",
      "        -0.0265, -0.0306, -0.0561, -0.0669,  0.1266,  0.0516, -0.1485,  0.0746],\n",
      "       requires_grad=True)\n",
      "Bgate spliced: torch.Size([16])\n",
      "tensor([ 0.1862,  0.2137, -0.0462, -0.2737, -0.2999, -0.0852,  0.1353,  0.2650,\n",
      "        -0.0265, -0.0306, -0.0561, -0.0669,  0.1266,  0.0516, -0.1485,  0.0746],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xgate: torch.Size([1, 3, 16])\n",
      "tensor([[[ 0.4329,  0.4420, -0.4354,  0.0322, -0.3283, -0.6211,  0.1565,\n",
      "           0.7324,  0.7947,  0.1841, -0.4211, -0.2543,  0.0971,  0.1710,\n",
      "          -0.1407,  0.1080],\n",
      "         [ 0.2527,  0.7109,  0.1001, -0.6925, -0.2893, -0.3973,  0.3515,\n",
      "           0.4692,  0.2373, -0.4579,  0.1372, -0.6012,  0.1792, -0.0548,\n",
      "          -0.2204, -0.2865],\n",
      "         [ 0.0243,  0.0817,  0.0531, -0.2229, -0.9176, -0.1382, -0.2338,\n",
      "          -0.5019, -1.0214, -0.1041,  0.4492,  0.1541, -0.1654, -0.5446,\n",
      "          -0.0915, -0.0772]]], grad_fn=<AddBackward0>)\n",
      "GeLU'ed Xgate: torch.Size([1, 3, 16])\n",
      "tensor([[[ 0.2889,  0.2965, -0.1444,  0.0165, -0.1219, -0.1660,  0.0880,\n",
      "           0.5625,  0.6251,  0.1055, -0.1418, -0.1016,  0.0523,  0.0971,\n",
      "          -0.0625,  0.0587],\n",
      "         [ 0.1516,  0.5413,  0.0541, -0.1692, -0.1117, -0.1373,  0.2241,\n",
      "           0.3193,  0.1409, -0.1481,  0.0761, -0.1646,  0.1023, -0.0262,\n",
      "          -0.0910, -0.1109],\n",
      "         [ 0.0124,  0.0435,  0.0277, -0.0918, -0.1646, -0.0615, -0.0953,\n",
      "          -0.1545, -0.1568, -0.0477,  0.3025,  0.0865, -0.0718, -0.1596,\n",
      "          -0.0424, -0.0362]]], grad_fn=<GeluBackward0>)\n",
      "Wup: torch.Size([8, 32])\n",
      "Parameter containing:\n",
      "tensor([[ 0.1461, -0.1881, -0.0114, -0.2953,  0.1766, -0.0054,  0.0200,  0.2929,\n",
      "          0.2036,  0.1445,  0.0007, -0.2298,  0.0682, -0.1226,  0.2082,  0.2925,\n",
      "          0.0774, -0.0253, -0.1358, -0.1838,  0.3420, -0.2564, -0.0681,  0.2453,\n",
      "         -0.2752,  0.0909, -0.2523, -0.3247,  0.1653, -0.1776,  0.2798, -0.2458],\n",
      "        [-0.3247, -0.2901,  0.2185,  0.0110, -0.1257, -0.0880, -0.2688,  0.0008,\n",
      "         -0.0161, -0.2435, -0.1607,  0.3109, -0.0768, -0.1983,  0.2455,  0.1018,\n",
      "          0.1656,  0.2817, -0.2373,  0.1910,  0.1478,  0.1714,  0.1657, -0.3000,\n",
      "         -0.1971,  0.2098, -0.2455,  0.1353, -0.3296, -0.0129, -0.0986,  0.0954],\n",
      "        [ 0.1068, -0.2128,  0.1991,  0.1008,  0.2752,  0.2930,  0.2256, -0.0079,\n",
      "          0.2821,  0.3137,  0.3496, -0.0613, -0.1468, -0.0054,  0.0866,  0.2523,\n",
      "         -0.1940, -0.2411, -0.2022,  0.2475,  0.0258,  0.0016,  0.1201, -0.3108,\n",
      "          0.0214,  0.0367, -0.1037,  0.2520,  0.1717, -0.0658,  0.2890, -0.1868],\n",
      "        [ 0.0058, -0.1731,  0.1676,  0.0108, -0.2819, -0.1985,  0.2458, -0.1946,\n",
      "         -0.1752,  0.0611,  0.3184,  0.2923, -0.0479,  0.2621,  0.2844,  0.2188,\n",
      "         -0.2579,  0.3230, -0.1489,  0.0944,  0.3340,  0.2295, -0.1343,  0.2445,\n",
      "          0.0340, -0.0537, -0.1785,  0.2965,  0.1392,  0.1501, -0.3371,  0.1051],\n",
      "        [-0.2644,  0.0722,  0.3389, -0.3096, -0.0578, -0.0960,  0.1849, -0.0896,\n",
      "          0.2314, -0.1201,  0.1368,  0.3479, -0.1061,  0.1036,  0.1223,  0.1666,\n",
      "         -0.3237, -0.3464,  0.0718, -0.2749,  0.2485,  0.1218,  0.3492,  0.0366,\n",
      "          0.2144,  0.3232,  0.0853,  0.2870, -0.2610, -0.1287,  0.2955,  0.1155],\n",
      "        [ 0.2456, -0.0144,  0.0137,  0.1239, -0.2818, -0.0874,  0.1061, -0.1882,\n",
      "          0.3299, -0.2652,  0.1321, -0.2266, -0.3398,  0.2878, -0.1701, -0.0402,\n",
      "          0.1522, -0.2065, -0.0613,  0.2694,  0.3201,  0.2417,  0.2183, -0.0752,\n",
      "         -0.1656, -0.2163, -0.1730,  0.0922,  0.0674,  0.1748,  0.0196,  0.2470],\n",
      "        [-0.3207, -0.1177, -0.2640, -0.0134, -0.3183, -0.1480, -0.1676, -0.3459,\n",
      "         -0.1791, -0.2782,  0.2909, -0.0460,  0.1268, -0.0369, -0.2830, -0.1581,\n",
      "         -0.0776, -0.1935, -0.1141, -0.2595,  0.1222,  0.1648, -0.1104,  0.3182,\n",
      "         -0.2715, -0.0320, -0.1157, -0.0379,  0.1773, -0.1688, -0.3005, -0.0800],\n",
      "        [ 0.1153, -0.3059, -0.0326, -0.2650, -0.1102, -0.2051,  0.3459, -0.0013,\n",
      "         -0.1888, -0.3529, -0.0028, -0.3385, -0.3253, -0.2511, -0.0515,  0.1634,\n",
      "          0.2073,  0.1018,  0.2722, -0.3286,  0.1437,  0.3030,  0.2420,  0.2834,\n",
      "          0.2969,  0.0161,  0.1945, -0.2914,  0.1334,  0.0514,  0.2667,  0.1051]],\n",
      "       requires_grad=True)\n",
      "Wup spliced: torch.Size([4, 16])\n",
      "tensor([[-0.3237, -0.3464,  0.0718, -0.2749,  0.2485,  0.1218,  0.3492,  0.0366,\n",
      "          0.2144,  0.3232,  0.0853,  0.2870, -0.2610, -0.1287,  0.2955,  0.1155],\n",
      "        [ 0.1522, -0.2065, -0.0613,  0.2694,  0.3201,  0.2417,  0.2183, -0.0752,\n",
      "         -0.1656, -0.2163, -0.1730,  0.0922,  0.0674,  0.1748,  0.0196,  0.2470],\n",
      "        [-0.0776, -0.1935, -0.1141, -0.2595,  0.1222,  0.1648, -0.1104,  0.3182,\n",
      "         -0.2715, -0.0320, -0.1157, -0.0379,  0.1773, -0.1688, -0.3005, -0.0800],\n",
      "        [ 0.2073,  0.1018,  0.2722, -0.3286,  0.1437,  0.3030,  0.2420,  0.2834,\n",
      "          0.2969,  0.0161,  0.1945, -0.2914,  0.1334,  0.0514,  0.2667,  0.1051]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Bup: torch.Size([32])\n",
      "Parameter containing:\n",
      "tensor([-0.2515,  0.2189, -0.0972,  0.1164, -0.2267, -0.2862,  0.3457, -0.0580,\n",
      "        -0.0643,  0.0126,  0.1822, -0.0607,  0.1557, -0.0858,  0.3095, -0.0990,\n",
      "         0.2946, -0.2724,  0.1024, -0.3385,  0.2050,  0.0544, -0.3525,  0.0763,\n",
      "         0.3103, -0.2710, -0.1986, -0.2860,  0.0696,  0.0840,  0.0303, -0.1089],\n",
      "       requires_grad=True)\n",
      "Bup spliced: torch.Size([16])\n",
      "tensor([ 0.2946, -0.2724,  0.1024, -0.3385,  0.2050,  0.0544, -0.3525,  0.0763,\n",
      "         0.3103, -0.2710, -0.1986, -0.2860,  0.0696,  0.0840,  0.0303, -0.1089],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xup: torch.Size([1, 3, 16])\n",
      "tensor([[[ 0.6341, -0.1803,  0.6778, -0.7709,  0.6002,  0.6229,  0.3861,\n",
      "           0.3539,  1.0627, -0.1579,  0.2046, -0.6250,  0.0979,  0.2994,\n",
      "           0.8499,  0.2728],\n",
      "         [-0.0601, -0.9594,  0.1595, -1.1831,  0.8305,  0.6654,  0.0822,\n",
      "           0.7373,  0.2967,  0.0330, -0.1943, -0.1442,  0.1097, -0.2560,\n",
      "           0.0916,  0.0202],\n",
      "         [ 0.5711,  0.0178, -0.3493,  0.1862, -0.1984, -0.2562, -1.2014,\n",
      "           0.0295, -0.5263, -0.8375, -0.6107, -0.5134,  0.5186,  0.1124,\n",
      "          -0.9232, -0.4387]]], grad_fn=<AddBackward0>)\n",
      "Xfuse: torch.Size([1, 3, 16])\n",
      "tensor([[[ 0.1832, -0.0534, -0.0979, -0.0127, -0.0732, -0.1034,  0.0340,\n",
      "           0.1991,  0.6643, -0.0167, -0.0290,  0.0635,  0.0051,  0.0291,\n",
      "          -0.0531,  0.0160],\n",
      "         [-0.0091, -0.5194,  0.0086,  0.2002, -0.0928, -0.0914,  0.0184,\n",
      "           0.2354,  0.0418, -0.0049, -0.0148,  0.0237,  0.0112,  0.0067,\n",
      "          -0.0083, -0.0022],\n",
      "         [ 0.0071,  0.0008, -0.0097, -0.0171,  0.0327,  0.0158,  0.1145,\n",
      "          -0.0046,  0.0825,  0.0400, -0.1847, -0.0444, -0.0373, -0.0179,\n",
      "           0.0391,  0.0159]]], grad_fn=<MulBackward0>)\n",
      "Wdown: torch.Size([32, 8])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0182, -0.0335,  0.1160,  0.0016,  0.1674,  0.0666,  0.0672, -0.0870],\n",
      "        [-0.1036, -0.1544,  0.0656,  0.0593,  0.0246,  0.1329, -0.0141,  0.1280],\n",
      "        [ 0.0924, -0.0610, -0.0202, -0.1488, -0.1268, -0.1293,  0.1564,  0.0257],\n",
      "        [-0.0521, -0.0812,  0.0997, -0.1479, -0.1325, -0.0349,  0.0567,  0.1022],\n",
      "        [-0.1485, -0.1232,  0.0503, -0.1232,  0.1660, -0.0445, -0.0446,  0.0510],\n",
      "        [-0.1061, -0.0046, -0.0265,  0.1611, -0.0745,  0.0744, -0.1586,  0.0883],\n",
      "        [ 0.1349, -0.0969,  0.0187,  0.0635,  0.0267,  0.1135, -0.1686,  0.1749],\n",
      "        [-0.0706,  0.0030,  0.0220, -0.1169, -0.1222,  0.0809, -0.0947,  0.1453],\n",
      "        [-0.0279, -0.1280, -0.0225, -0.1762,  0.0917,  0.1144, -0.0192,  0.0034],\n",
      "        [-0.0751,  0.0913,  0.0093,  0.0964, -0.1255, -0.1656, -0.0056, -0.0406],\n",
      "        [ 0.1591,  0.0859,  0.0305,  0.0196, -0.1654,  0.0772,  0.1611,  0.1560],\n",
      "        [-0.0746, -0.0580,  0.0504, -0.1644, -0.0701,  0.0872, -0.1648,  0.1040],\n",
      "        [ 0.1301,  0.0403,  0.0711, -0.1759,  0.0784, -0.1548, -0.0742, -0.0850],\n",
      "        [-0.1415, -0.0116, -0.0338,  0.1498, -0.0192,  0.1108,  0.1595,  0.0837],\n",
      "        [-0.1039, -0.0155,  0.0275, -0.1285,  0.0839,  0.1181,  0.0983,  0.0754],\n",
      "        [-0.1239,  0.0488, -0.1654, -0.0906,  0.0470, -0.0620, -0.1542, -0.1062],\n",
      "        [-0.0375,  0.0230, -0.0777,  0.1664,  0.0647,  0.1694,  0.0817,  0.1513],\n",
      "        [ 0.0760,  0.1130, -0.0421,  0.0471, -0.1388,  0.0021,  0.1197,  0.0242],\n",
      "        [ 0.0810,  0.1652,  0.0490,  0.1459, -0.0303,  0.0509, -0.0201,  0.1173],\n",
      "        [ 0.1563, -0.0756, -0.0584,  0.0121, -0.0436, -0.1635,  0.1264,  0.1484],\n",
      "        [-0.0628,  0.1063, -0.1746,  0.1726, -0.1372,  0.0788,  0.1273, -0.0905],\n",
      "        [ 0.1667,  0.1260,  0.1609, -0.1081, -0.1310,  0.0144, -0.1354, -0.0869],\n",
      "        [ 0.1608,  0.0912, -0.1313, -0.1481,  0.0570, -0.0335,  0.1258, -0.0184],\n",
      "        [-0.1426, -0.0447,  0.1231,  0.1384,  0.0974, -0.1639, -0.1728,  0.0261],\n",
      "        [-0.0215, -0.0968,  0.1192, -0.0503, -0.0652, -0.1343, -0.0023, -0.0068],\n",
      "        [-0.0486, -0.0081,  0.1722, -0.0421, -0.1066, -0.1005,  0.1318,  0.1001],\n",
      "        [-0.0495, -0.1124, -0.0681,  0.0858,  0.1235,  0.0307,  0.1351, -0.1294],\n",
      "        [ 0.0583,  0.0458,  0.0871, -0.0338, -0.0507, -0.0004, -0.0430,  0.0936],\n",
      "        [-0.0916,  0.0825,  0.1493,  0.0817, -0.0655, -0.0552, -0.0992,  0.0182],\n",
      "        [ 0.1523, -0.0392,  0.1635, -0.0732, -0.1632, -0.1637,  0.0139,  0.1081],\n",
      "        [-0.1676, -0.1737, -0.0526, -0.0201,  0.0717,  0.1610,  0.0641,  0.0570],\n",
      "        [-0.1431, -0.0946,  0.1332,  0.0950,  0.0392,  0.0311, -0.1628, -0.1136]],\n",
      "       requires_grad=True)\n",
      "Wdown spliced: torch.Size([16, 4])\n",
      "tensor([[ 0.0647,  0.1694,  0.0817,  0.1513],\n",
      "        [-0.1388,  0.0021,  0.1197,  0.0242],\n",
      "        [-0.0303,  0.0509, -0.0201,  0.1173],\n",
      "        [-0.0436, -0.1635,  0.1264,  0.1484],\n",
      "        [-0.1372,  0.0788,  0.1273, -0.0905],\n",
      "        [-0.1310,  0.0144, -0.1354, -0.0869],\n",
      "        [ 0.0570, -0.0335,  0.1258, -0.0184],\n",
      "        [ 0.0974, -0.1639, -0.1728,  0.0261],\n",
      "        [-0.0652, -0.1343, -0.0023, -0.0068],\n",
      "        [-0.1066, -0.1005,  0.1318,  0.1001],\n",
      "        [ 0.1235,  0.0307,  0.1351, -0.1294],\n",
      "        [-0.0507, -0.0004, -0.0430,  0.0936],\n",
      "        [-0.0655, -0.0552, -0.0992,  0.0182],\n",
      "        [-0.1632, -0.1637,  0.0139,  0.1081],\n",
      "        [ 0.0717,  0.1610,  0.0641,  0.0570],\n",
      "        [ 0.0392,  0.0311, -0.1628, -0.1136]], grad_fn=<SliceBackward0>)\n",
      "Bdown: torch.Size([8])\n",
      "Parameter containing:\n",
      "tensor([-0.0420, -0.1305,  0.1604, -0.0753, -0.0015, -0.0793, -0.0961,  0.0816],\n",
      "       requires_grad=True)\n",
      "Bdown spliced: torch.Size([4])\n",
      "tensor([-0.0015, -0.0793, -0.0961,  0.0816], grad_fn=<SliceBackward0>)\n",
      "outputs: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.0095, -0.1938, -0.1290,  0.1168],\n",
      "         [ 0.1019, -0.1708, -0.1766,  0.1246],\n",
      "         [-0.0220, -0.0851, -0.0948,  0.0935]]], grad_fn=<AddBackward0>)\n",
      "------------- END MLP.forwardTensor() ------------\n",
      "layer's final residual state:\n",
      "tensor([[[ 0.5513,  0.2548, -0.9928,  2.3633],\n",
      "         [ 1.5095,  0.0701,  1.5717,  0.7874],\n",
      "         [-0.8138, -0.0590,  0.3188, -0.3438]]], grad_fn=<AddBackward0>)\n",
      "----------------- END Layer.forwardTensor() --------------------\n",
      "forwardTensor() output: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.5513,  0.2548, -0.9928,  2.3633],\n",
      "         [ 1.5095,  0.0701,  1.5717,  0.7874],\n",
      "         [-0.8138, -0.0590,  0.3188, -0.3438]]], grad_fn=<AddBackward0>)\n",
      "final output: ((tensor([[[-1.7248, -0.9206, -0.6961, -0.7503, -1.6982,  0.0159, -0.0733,\n",
      "           0.7600],\n",
      "         [-0.0454,  1.0032,  0.9860,  0.9000,  0.0985, -1.2619,  0.3302,\n",
      "          -0.2126],\n",
      "         [-0.4724,  0.1314,  2.0494, -0.7864,  0.0046, -0.7580,  0.0940,\n",
      "           0.2428]]], grad_fn=<AddBackward0>),), (tensor([[[ 0.3741, -0.1623,  0.2504, -0.8097],\n",
      "         [-0.6267,  1.3308,  0.0884, -1.0367],\n",
      "         [-2.7033,  1.3693, -1.6889, -1.4279]]], grad_fn=<AddBackward0>), tensor([[[ 0.5513,  0.2548, -0.9928,  2.3633],\n",
      "         [ 1.5095,  0.0701,  1.5717,  0.7874],\n",
      "         [-0.8138, -0.0590,  0.3188, -0.3438]]], grad_fn=<AddBackward0>)))\n",
      "------------- END Layer.forwardTuple() ------------\n",
      "out: ((tensor([[[-1.7248, -0.9206, -0.6961, -0.7503, -1.6982,  0.0159, -0.0733,\n",
      "           0.7600],\n",
      "         [-0.0454,  1.0032,  0.9860,  0.9000,  0.0985, -1.2619,  0.3302,\n",
      "          -0.2126],\n",
      "         [-0.4724,  0.1314,  2.0494, -0.7864,  0.0046, -0.7580,  0.0940,\n",
      "           0.2428]]], grad_fn=<AddBackward0>),), (tensor([[[ 0.3741, -0.1623,  0.2504, -0.8097],\n",
      "         [-0.6267,  1.3308,  0.0884, -1.0367],\n",
      "         [-2.7033,  1.3693, -1.6889, -1.4279]]], grad_fn=<AddBackward0>), tensor([[[ 0.5513,  0.2548, -0.9928,  2.3633],\n",
      "         [ 1.5095,  0.0701,  1.5717,  0.7874],\n",
      "         [-0.8138, -0.0590,  0.3188, -0.3438]]], grad_fn=<AddBackward0>)))\n",
      "---------- RESET CONFIG --------\n",
      "model_count:  [1, 2, 4]\n",
      "model_dim_list:  [128, 64, 32]\n",
      "head_dim_list:  [32, 16, 8]\n"
     ]
    }
   ],
   "source": [
    "# Testing our Layer's forwardTuple()\n",
    "verbose = True\n",
    "\n",
    "print(\"--------- Micro Hyperparameters -------\")\n",
    "hold1, hold2, hold3, hold4, hold5 = config.hidden_size, config.num_attention_heads, config.head_dim, config.levels, config.max_position_embeddings\n",
    "config.hidden_size = 8\n",
    "config.num_attention_heads = 2\n",
    "config.head_dim = 4\n",
    "config.levels = 2\n",
    "config.max_position_embeddings = 3\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "print(\"head_dim_list: \", config.head_dim_list)\n",
    "\n",
    "layer = Layer(config)\n",
    "# we need to make sure to send in a tuple of the expected size. above we set hidden_size=4 and levels=2\n",
    "x = ((torch.randn((1,3,8)),),(torch.randn((1,3,4)),torch.randn((1,3,4))))\n",
    "print(f\"x: {x}\")\n",
    "out = layer(x)\n",
    "print(f\"out: {out}\")\n",
    "\n",
    "verbose = False\n",
    "print(\"---------- RESET CONFIG --------\")\n",
    "config.hidden_size = hold1\n",
    "config.num_attention_heads = hold2\n",
    "config.head_dim = hold3\n",
    "config.levels = hold4\n",
    "config.max_position_embeddings = hold5\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "print(\"head_dim_list: \", config.head_dim_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7934c0d0-d213-46c6-b75f-39fba2edea81",
   "metadata": {},
   "source": [
    "# Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2c19d5f-94eb-4347-8922-c88fa3c20735",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputLayer(nn.Module):\n",
    "    def __init__(self, embedding: torch.Tensor, config: Config):\n",
    "        super().__init__()\n",
    "        self.embedding = embedding\n",
    "        self.v = config.vocab_size\n",
    "        self.model_dim_list = config.model_dim_list\n",
    "\n",
    "        # applies RMSNorm to the embedding matrix\n",
    "        self.embedding_norm = RMSNorm(config.hidden_size,\n",
    "                                      eps = config.rms_norm_eps)\n",
    "        \n",
    "        # Applies RMSNorm to the model's final residual state before we use the embedding matrix to get logits\n",
    "        self.final_norm = RMSNorm(config.hidden_size,\n",
    "                                  eps = config.rms_norm_eps)\n",
    "\n",
    "    def forwardTensor(self, x, model=0):\n",
    "        global verbose\n",
    "        if verbose: \n",
    "            print(\"------------- OutputLayer.forwardTensor() ------------\")\n",
    "            print(f\"x: {x.shape}\\n{x}\")\n",
    "\n",
    "        # setting up our splicing logic\n",
    "        d_i = x.shape[-1]\n",
    "        skip = model * d_i\n",
    "        if verbose:\n",
    "            print(f\"d_i: {d_i}\")\n",
    "            print(f\"skip: {skip}\")\n",
    "            print(f\"embedding: {self.embedding.shape}\\n{self.embedding}\")\n",
    "\n",
    "        # splice out our embedding matrix according to what model we're using\n",
    "        sliced_embed = self.embedding[:,skip:skip + d_i]\n",
    "        if verbose: print(f\"sliced_embed: {sliced_embed.shape}\\n{sliced_embed}\")\n",
    "\n",
    "        # normalize our sliced embedding matrix\n",
    "        normed_sliced_embed = self.embedding_norm(sliced_embed)\n",
    "        if verbose: print(f\"normed & sliced embedding: {normed_sliced_embed.shape}\\n{normed_sliced_embed}\")\n",
    "\n",
    "        # normalize the residual state before the final linear layer\n",
    "        x = self.final_norm(x, model)\n",
    "        if verbose: print(f\"normed x: {x.shape}\\n{x}\")\n",
    "\n",
    "        # calculating the final output logits of the model\n",
    "        logits = x @ normed_sliced_embed.t()\n",
    "        if verbose: \n",
    "            print(f\"final logits: {logits.shape}\\n{logits}\")\n",
    "            print(\"------------- END OutputLayer.forwardTensor() ------------\")\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def forwardTuple(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the final embedding classification layer during training.\n",
    "\n",
    "        Parameters:\n",
    "            x (Tuple[Tuple[Tensor]]): \n",
    "                The input tuple of tuples of tensors \n",
    "                first tuple is of length config.levels and second layer of tuples have lengths of config.model_count\n",
    "                tensors are shape (batch size, sequence length, hidden dimension) where hidden dimension changes by which model was used\n",
    "\n",
    "        Returns:\n",
    "            output (Tuple[Tuple[Tensor]]): \n",
    "                The output tuple of tuples of tensors after applying the final embedding classification\n",
    "        \"\"\"\n",
    "        global verbose\n",
    "        if verbose: \n",
    "            print(\"------------- OutputLayer.forwardTuple() ------------\")\n",
    "            print(f\"x:\\n{x}\")\n",
    "            \n",
    "        # forwardTuple() should only be used during training, so we assert input_len == max_position_embeddings\n",
    "        assert type(x) == tuple\n",
    "        input_len = x[0][0].shape[1]\n",
    "        if verbose: print(f\"input_len: {input_len}\")\n",
    "        assert input_len == config.max_position_embeddings\n",
    "\n",
    "        # we could define these from the config but this way the method is more flexible to testing\n",
    "        num_levels = len(x)\n",
    "        models_per_level = [len(x[i]) for i in range(num_levels)]\n",
    "        if verbose: \n",
    "            print(f\"num_levels: {num_levels}\")\n",
    "            print(f\"models_per_level: {models_per_level}\")\n",
    "\n",
    "        # the loop that iterates over levels, aka the different potential sizes of models\n",
    "        out = ()\n",
    "        for i in range(num_levels):\n",
    "            if verbose: print(f\"Level {i} from range({num_levels})\")\n",
    "\n",
    "            # now for the loop that iterates over models in this level\n",
    "            out_lvl = ()\n",
    "            for j in range(models_per_level[i]):\n",
    "                if verbose: print(f\"Model {j} from range({models_per_level[i]})\")\n",
    "\n",
    "                output = self.forwardTensor(x[i][j], model = j)\n",
    "                if verbose: print(f\"forwardTensor() output: {output.shape}\\n{output}\")\n",
    "                \n",
    "                out_lvl += (output,)\n",
    "            \n",
    "            out += (out_lvl,)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"final output: {out}\")\n",
    "            print(\"------------- END Layer.forwardTuple() ------------\")\n",
    "        \n",
    "        return out\n",
    "        \n",
    "    def forward(self, x, model=0):\n",
    "        train = True if type(x) == tuple else False\n",
    "        if verbose: print(f\"---------- Layer Input: {'Tuple' if train else 'torch.Tensor'} ------------\")\n",
    "        return self.forwardTuple(x) if train else self.forwardTensor(x, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "242486f7-e76a-47dd-8325-94fba4d0d990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Micro Hyperparameters -------\n",
      "model_count:  [1, 2, 4]\n",
      "model_dim_list:  [4, 2, 1]\n",
      "embedding: torch.Size([5, 4])\n",
      "tensor([[ 1.4780, -0.0921, -0.7166, -0.7181],\n",
      "        [-0.4840,  0.4295,  0.4929,  0.5351],\n",
      "        [-0.6410,  0.1184,  0.1937, -0.4385],\n",
      "        [-0.2204,  0.3830, -0.4141, -1.4633],\n",
      "        [ 0.5593,  0.6539, -0.7188, -1.0490]])\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the big model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[0.2562, 0.2100, 0.6033, 0.1476],\n",
      "         [0.6056, 0.1158, 0.5613, 0.4368],\n",
      "         [0.6224, 0.3627, 0.0193, 0.4497]]])\n",
      "---------- Layer Input: torch.Tensor ------------\n",
      "------------- OutputLayer.forwardTensor() ------------\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[0.2562, 0.2100, 0.6033, 0.1476],\n",
      "         [0.6056, 0.1158, 0.5613, 0.4368],\n",
      "         [0.6224, 0.3627, 0.0193, 0.4497]]])\n",
      "d_i: 4\n",
      "skip: 0\n",
      "embedding: torch.Size([5, 4])\n",
      "tensor([[ 1.4780, -0.0921, -0.7166, -0.7181],\n",
      "        [-0.4840,  0.4295,  0.4929,  0.5351],\n",
      "        [-0.6410,  0.1184,  0.1937, -0.4385],\n",
      "        [-0.2204,  0.3830, -0.4141, -1.4633],\n",
      "        [ 0.5593,  0.6539, -0.7188, -1.0490]])\n",
      "sliced_embed: torch.Size([5, 4])\n",
      "tensor([[ 1.4780, -0.0921, -0.7166, -0.7181],\n",
      "        [-0.4840,  0.4295,  0.4929,  0.5351],\n",
      "        [-0.6410,  0.1184,  0.1937, -0.4385],\n",
      "        [-0.2204,  0.3830, -0.4141, -1.4633],\n",
      "        [ 0.5593,  0.6539, -0.7188, -1.0490]])\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([5, 4])\n",
      "tensor([[ 1.4780, -0.0921, -0.7166, -0.7181],\n",
      "        [-0.4840,  0.4295,  0.4929,  0.5351],\n",
      "        [-0.6410,  0.1184,  0.1937, -0.4385],\n",
      "        [-0.2204,  0.3830, -0.4141, -1.4633],\n",
      "        [ 0.5593,  0.6539, -0.7188, -1.0490]])\n",
      "normed x: torch.Size([5, 4])\n",
      "tensor([[ 1.6468, -0.1026, -0.7984, -0.8001],\n",
      "        [-0.9942,  0.8822,  1.0125,  1.0991],\n",
      "        [-1.5844,  0.2927,  0.4788, -1.0838],\n",
      "        [-0.2783,  0.4836, -0.5230, -1.8480],\n",
      "        [ 0.7286,  0.8517, -0.9363, -1.3664]])\n",
      "dim: 4\n",
      "skip: 0\n",
      "spliced scale: torch.Size([4])\n",
      "tensor([0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([5, 4])\n",
      "tensor([[ 1.6468, -0.1026, -0.7984, -0.8001],\n",
      "        [-0.9942,  0.8822,  1.0125,  1.0991],\n",
      "        [-1.5844,  0.2927,  0.4788, -1.0838],\n",
      "        [-0.2783,  0.4836, -0.5230, -1.8480],\n",
      "        [ 0.7286,  0.8517, -0.9363, -1.3664]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "normed & sliced embedding: torch.Size([5, 4])\n",
      "tensor([[ 1.6468, -0.1026, -0.7984, -0.8001],\n",
      "        [-0.9942,  0.8822,  1.0125,  1.0991],\n",
      "        [-1.5844,  0.2927,  0.4788, -1.0838],\n",
      "        [-0.2783,  0.4836, -0.5230, -1.8480],\n",
      "        [ 0.7286,  0.8517, -0.9363, -1.3664]], grad_fn=<MulBackward0>)\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[0.2562, 0.2100, 0.6033, 0.1476],\n",
      "         [0.6056, 0.1158, 0.5613, 0.4368],\n",
      "         [0.6224, 0.3627, 0.0193, 0.4497]]])\n",
      "normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[0.7279, 0.5967, 1.7141, 0.4195],\n",
      "         [1.2868, 0.2460, 1.1926, 0.9281],\n",
      "         [1.4654, 0.8540, 0.0455, 1.0589]]])\n",
      "dim: 4\n",
      "skip: 0\n",
      "spliced scale: torch.Size([4])\n",
      "tensor([0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[0.7279, 0.5967, 1.7141, 0.4195],\n",
      "         [1.2868, 0.2460, 1.1926, 0.9281],\n",
      "         [1.4654, 0.8540, 0.0455, 1.0589]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[0.7279, 0.5967, 1.7141, 0.4195],\n",
      "         [1.2868, 0.2460, 1.1926, 0.9281],\n",
      "         [1.4654, 0.8540, 0.0455, 1.0589]]], grad_fn=<MulBackward0>)\n",
      "final logits: torch.Size([1, 3, 5])\n",
      "tensor([[[-0.5668,  1.9994, -0.6126, -1.5856, -1.1395],\n",
      "         [ 0.3990,  1.1653, -2.4018, -2.5781, -1.2378],\n",
      "         [ 1.4419,  0.5065, -3.1978, -1.9755,  0.3054]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "------------- END OutputLayer.forwardTensor() ------------\n",
      "y: torch.Size([1, 3, 5])\n",
      "tensor([[[-0.5668,  1.9994, -0.6126, -1.5856, -1.1395],\n",
      "         [ 0.3990,  1.1653, -2.4018, -2.5781, -1.2378],\n",
      "         [ 1.4419,  0.5065, -3.1978, -1.9755,  0.3054]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the first sub-model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 3, 2])\n",
      "tensor([[[0.2079, 0.4850],\n",
      "         [0.6401, 0.1940],\n",
      "         [0.6288, 0.8747]]])\n",
      "---------- Layer Input: torch.Tensor ------------\n",
      "------------- OutputLayer.forwardTensor() ------------\n",
      "x: torch.Size([1, 3, 2])\n",
      "tensor([[[0.2079, 0.4850],\n",
      "         [0.6401, 0.1940],\n",
      "         [0.6288, 0.8747]]])\n",
      "d_i: 2\n",
      "skip: 0\n",
      "embedding: torch.Size([5, 4])\n",
      "tensor([[ 1.4780, -0.0921, -0.7166, -0.7181],\n",
      "        [-0.4840,  0.4295,  0.4929,  0.5351],\n",
      "        [-0.6410,  0.1184,  0.1937, -0.4385],\n",
      "        [-0.2204,  0.3830, -0.4141, -1.4633],\n",
      "        [ 0.5593,  0.6539, -0.7188, -1.0490]])\n",
      "sliced_embed: torch.Size([5, 2])\n",
      "tensor([[ 1.4780, -0.0921],\n",
      "        [-0.4840,  0.4295],\n",
      "        [-0.6410,  0.1184],\n",
      "        [-0.2204,  0.3830],\n",
      "        [ 0.5593,  0.6539]])\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([5, 2])\n",
      "tensor([[ 1.4780, -0.0921],\n",
      "        [-0.4840,  0.4295],\n",
      "        [-0.6410,  0.1184],\n",
      "        [-0.2204,  0.3830],\n",
      "        [ 0.5593,  0.6539]])\n",
      "normed x: torch.Size([5, 2])\n",
      "tensor([[ 1.4115, -0.0879],\n",
      "        [-1.0578,  0.9387],\n",
      "        [-1.3907,  0.2569],\n",
      "        [-0.7054,  1.2257],\n",
      "        [ 0.9193,  1.0747]])\n",
      "dim: 2\n",
      "skip: 0\n",
      "spliced scale: torch.Size([2])\n",
      "tensor([0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([5, 2])\n",
      "tensor([[ 1.4115, -0.0879],\n",
      "        [-1.0578,  0.9387],\n",
      "        [-1.3907,  0.2569],\n",
      "        [-0.7054,  1.2257],\n",
      "        [ 0.9193,  1.0747]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "normed & sliced embedding: torch.Size([5, 2])\n",
      "tensor([[ 1.4115, -0.0879],\n",
      "        [-1.0578,  0.9387],\n",
      "        [-1.3907,  0.2569],\n",
      "        [-0.7054,  1.2257],\n",
      "        [ 0.9193,  1.0747]], grad_fn=<MulBackward0>)\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 3, 2])\n",
      "tensor([[[0.2079, 0.4850],\n",
      "         [0.6401, 0.1940],\n",
      "         [0.6288, 0.8747]]])\n",
      "normed x: torch.Size([1, 3, 2])\n",
      "tensor([[[0.5573, 1.2998],\n",
      "         [1.3534, 0.4103],\n",
      "         [0.8255, 1.1483]]])\n",
      "dim: 2\n",
      "skip: 0\n",
      "spliced scale: torch.Size([2])\n",
      "tensor([0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 3, 2])\n",
      "tensor([[[0.5573, 1.2998],\n",
      "         [1.3534, 0.4103],\n",
      "         [0.8255, 1.1483]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "normed x: torch.Size([1, 3, 2])\n",
      "tensor([[[0.5573, 1.2998],\n",
      "         [1.3534, 0.4103],\n",
      "         [0.8255, 1.1483]]], grad_fn=<MulBackward0>)\n",
      "final logits: torch.Size([1, 3, 5])\n",
      "tensor([[[ 0.6723,  0.6306, -0.4411,  1.2001,  1.9091],\n",
      "         [ 1.8742, -1.0465, -1.7768, -0.4518,  1.6850],\n",
      "         [ 1.0642,  0.2047, -0.8530,  0.8252,  1.9929]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "------------- END OutputLayer.forwardTensor() ------------\n",
      "y: torch.Size([1, 3, 5])\n",
      "tensor([[[ 0.6723,  0.6306, -0.4411,  1.2001,  1.9091],\n",
      "         [ 1.8742, -1.0465, -1.7768, -0.4518,  1.6850],\n",
      "         [ 1.0642,  0.2047, -0.8530,  0.8252,  1.9929]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the second sub-model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 3, 2])\n",
      "tensor([[[0.7675, 0.7719],\n",
      "         [0.2240, 0.1584],\n",
      "         [0.0984, 0.7101]]])\n",
      "---------- Layer Input: torch.Tensor ------------\n",
      "------------- OutputLayer.forwardTensor() ------------\n",
      "x: torch.Size([1, 3, 2])\n",
      "tensor([[[0.7675, 0.7719],\n",
      "         [0.2240, 0.1584],\n",
      "         [0.0984, 0.7101]]])\n",
      "d_i: 2\n",
      "skip: 2\n",
      "embedding: torch.Size([5, 4])\n",
      "tensor([[ 1.4780, -0.0921, -0.7166, -0.7181],\n",
      "        [-0.4840,  0.4295,  0.4929,  0.5351],\n",
      "        [-0.6410,  0.1184,  0.1937, -0.4385],\n",
      "        [-0.2204,  0.3830, -0.4141, -1.4633],\n",
      "        [ 0.5593,  0.6539, -0.7188, -1.0490]])\n",
      "sliced_embed: torch.Size([5, 2])\n",
      "tensor([[-0.7166, -0.7181],\n",
      "        [ 0.4929,  0.5351],\n",
      "        [ 0.1937, -0.4385],\n",
      "        [-0.4141, -1.4633],\n",
      "        [-0.7188, -1.0490]])\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([5, 2])\n",
      "tensor([[-0.7166, -0.7181],\n",
      "        [ 0.4929,  0.5351],\n",
      "        [ 0.1937, -0.4385],\n",
      "        [-0.4141, -1.4633],\n",
      "        [-0.7188, -1.0490]])\n",
      "normed x: torch.Size([5, 2])\n",
      "tensor([[-0.9989, -1.0011],\n",
      "        [ 0.9582,  1.0402],\n",
      "        [ 0.5714, -1.2936],\n",
      "        [-0.3851, -1.3608],\n",
      "        [-0.7994, -1.1666]])\n",
      "dim: 2\n",
      "skip: 0\n",
      "spliced scale: torch.Size([2])\n",
      "tensor([0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([5, 2])\n",
      "tensor([[-0.9989, -1.0011],\n",
      "        [ 0.9582,  1.0402],\n",
      "        [ 0.5714, -1.2936],\n",
      "        [-0.3851, -1.3608],\n",
      "        [-0.7994, -1.1666]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "normed & sliced embedding: torch.Size([5, 2])\n",
      "tensor([[-0.9989, -1.0011],\n",
      "        [ 0.9582,  1.0402],\n",
      "        [ 0.5714, -1.2936],\n",
      "        [-0.3851, -1.3608],\n",
      "        [-0.7994, -1.1666]], grad_fn=<MulBackward0>)\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 3, 2])\n",
      "tensor([[[0.7675, 0.7719],\n",
      "         [0.2240, 0.1584],\n",
      "         [0.0984, 0.7101]]])\n",
      "normed x: torch.Size([1, 3, 2])\n",
      "tensor([[[0.9972, 1.0028],\n",
      "         [1.1547, 0.8164],\n",
      "         [0.1941, 1.4008]]])\n",
      "dim: 2\n",
      "skip: 2\n",
      "spliced scale: torch.Size([2])\n",
      "tensor([0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 3, 2])\n",
      "tensor([[[0.9972, 1.0028],\n",
      "         [1.1547, 0.8164],\n",
      "         [0.1941, 1.4008]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "normed x: torch.Size([1, 3, 2])\n",
      "tensor([[[0.9972, 1.0028],\n",
      "         [1.1547, 0.8164],\n",
      "         [0.1941, 1.4008]]], grad_fn=<MulBackward0>)\n",
      "final logits: torch.Size([1, 3, 5])\n",
      "tensor([[[-2.0000,  1.9985, -0.7274, -1.7486, -1.9670],\n",
      "         [-1.9708,  1.9556, -0.3962, -1.5556, -1.8755],\n",
      "         [-1.5963,  1.6431, -1.7012, -1.9810, -1.7894]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "------------- END OutputLayer.forwardTensor() ------------\n",
      "y: torch.Size([1, 3, 5])\n",
      "tensor([[[-2.0000,  1.9985, -0.7274, -1.7486, -1.9670],\n",
      "         [-1.9708,  1.9556, -0.3962, -1.5556, -1.8755],\n",
      "         [-1.5963,  1.6431, -1.7012, -1.9810, -1.7894]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "---------- RESET CONFIG --------\n",
      "model_count:  [1, 2, 4]\n",
      "model_dim_list:  [128, 64, 32]\n"
     ]
    }
   ],
   "source": [
    "# Testing our OutputLayer's forwardTensor()\n",
    "verbose = True\n",
    "\n",
    "print(\"--------- Micro Hyperparameters -------\")\n",
    "hold1, hold2 = config.hidden_size, config.vocab_size\n",
    "config.hidden_size = 4\n",
    "config.vocab_size = 5\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "\n",
    "embedding = torch.randn(config.vocab_size, config.hidden_size)\n",
    "print(f\"embedding: {embedding.shape}\\n{embedding}\")\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the big model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,3,config.hidden_size)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "layer = OutputLayer(embedding, config)\n",
    "y = layer(x)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the first sub-model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,3,config.hidden_size//config.split)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "layer = OutputLayer(embedding, config)\n",
    "y = layer(x)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the second sub-model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,3,config.hidden_size//config.split)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "layer = OutputLayer(embedding, config)\n",
    "y = layer(x, model=1)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "verbose = False\n",
    "print(\"---------- RESET CONFIG --------\")\n",
    "config.hidden_size = hold1\n",
    "config.vocab_size = hold2\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b436e615-f928-4e03-9b14-86e916baff38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Micro Hyperparameters -------\n",
      "model_count:  [1, 2]\n",
      "model_dim_list:  [4, 2]\n",
      "embedding: torch.Size([5, 4])\n",
      "tensor([[ 0.9889, -1.2414, -0.2899, -0.0413],\n",
      "        [-1.5586, -0.3241, -0.4515, -1.1664],\n",
      "        [-0.3591, -0.5835, -0.0803, -0.9438],\n",
      "        [ 1.7739,  0.0523,  1.7394, -0.5131],\n",
      "        [-0.3921, -0.8147,  2.1062, -0.8210]])\n",
      "x: ((tensor([[[-0.3976, -0.0663,  1.0279, -1.3364],\n",
      "         [-1.0201,  0.6082, -2.2391,  0.5265],\n",
      "         [ 0.1258, -0.0052,  0.5070, -1.6951]]]),), (tensor([[[ 0.1415,  1.4670],\n",
      "         [-0.9165,  0.4451],\n",
      "         [-0.5364, -0.1246]]]), tensor([[[ 0.8669,  0.1827],\n",
      "         [-1.2019,  0.5401],\n",
      "         [-0.9241,  0.2901]]])))\n",
      "---------- Layer Input: Tuple ------------\n",
      "------------- Layer.forwardTuple() ------------\n",
      "x:\n",
      "((tensor([[[-0.3976, -0.0663,  1.0279, -1.3364],\n",
      "         [-1.0201,  0.6082, -2.2391,  0.5265],\n",
      "         [ 0.1258, -0.0052,  0.5070, -1.6951]]]),), (tensor([[[ 0.1415,  1.4670],\n",
      "         [-0.9165,  0.4451],\n",
      "         [-0.5364, -0.1246]]]), tensor([[[ 0.8669,  0.1827],\n",
      "         [-1.2019,  0.5401],\n",
      "         [-0.9241,  0.2901]]])))\n",
      "input_len: 3\n",
      "num_levels: 2\n",
      "models_per_level: [1, 2]\n",
      "Level 0 from range(2)\n",
      "Model 0 from range(1)\n",
      "----------------- Layer.forwardTensor() --------------------\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.3976, -0.0663,  1.0279, -1.3364],\n",
      "         [-1.0201,  0.6082, -2.2391,  0.5265],\n",
      "         [ 0.1258, -0.0052,  0.5070, -1.6951]]])\n",
      "normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.4588, -0.0765,  1.1859, -1.5419],\n",
      "         [-0.7881,  0.4699, -1.7299,  0.4068],\n",
      "         [ 0.1418, -0.0059,  0.5716, -1.9113]]])\n",
      "dim: 4\n",
      "skip: 0\n",
      "spliced scale: torch.Size([4])\n",
      "tensor([0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.4588, -0.0765,  1.1859, -1.5419],\n",
      "         [-0.7881,  0.4699, -1.7299,  0.4068],\n",
      "         [ 0.1418, -0.0059,  0.5716, -1.9113]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "---------- Attention Input: torch.Tensor ------------\n",
      "----------------- MultiQueryAttention.forwardTensor() --------------------\n",
      "x shape: torch.Size([1, 3, 4])\n",
      "d_skip: 0\n",
      "models_in_this_level: 1\n",
      "h_dim: 32\n",
      "h_skip: 0\n",
      "self.Wqkv: torch.Size([4, 192])\n",
      "Parameter containing:\n",
      "tensor([[ 1.8937e-01, -2.8415e-01, -4.6238e-01, -4.0144e-01, -4.4552e-01,\n",
      "          1.9680e-01,  4.4328e-02,  4.4592e-01, -2.1865e-01,  2.1341e-01,\n",
      "         -3.0257e-03, -4.0493e-01,  1.3867e-01,  2.9379e-01,  3.5597e-01,\n",
      "          4.7860e-01, -3.5840e-01,  8.7989e-02, -2.9445e-01, -9.1477e-02,\n",
      "         -3.5326e-02, -1.1366e-01,  1.4490e-01, -2.6402e-03,  6.1200e-02,\n",
      "          6.7722e-02, -3.2172e-01, -3.8489e-01,  2.5546e-01, -2.6750e-01,\n",
      "         -1.2889e-01, -4.3475e-01,  1.1832e-01,  1.9844e-01,  2.2294e-01,\n",
      "          2.2098e-01,  3.4316e-01,  4.0263e-01,  1.8223e-01, -3.1401e-01,\n",
      "          1.4572e-01,  2.5463e-01,  1.4850e-01,  4.2974e-01,  3.9195e-01,\n",
      "         -4.2508e-01,  1.2758e-01,  4.7005e-01,  2.9028e-01,  2.7730e-01,\n",
      "          2.4163e-01,  4.1844e-01, -3.1530e-01,  3.5747e-01,  2.3997e-02,\n",
      "          2.9750e-01,  4.4569e-01, -3.0182e-01, -1.2491e-01,  2.3174e-01,\n",
      "          4.1144e-01, -1.1239e-01, -3.2242e-01,  2.1355e-01, -2.8637e-01,\n",
      "         -4.6897e-01,  3.3208e-01,  3.4030e-02,  4.2027e-01,  1.1778e-01,\n",
      "         -2.8841e-01,  3.7903e-01, -9.4648e-02, -5.8460e-02, -2.6424e-01,\n",
      "          2.9487e-01,  5.8353e-02, -1.1010e-01, -3.4630e-01,  2.7946e-01,\n",
      "          3.0002e-01,  2.4929e-01, -4.9700e-01, -5.6904e-02, -2.3638e-01,\n",
      "          2.2197e-01, -3.2396e-01,  1.0468e-01, -1.2194e-01,  2.9137e-01,\n",
      "         -2.7048e-01, -2.6034e-02,  4.3632e-01,  2.0930e-01,  1.6822e-01,\n",
      "         -2.1106e-02, -2.5414e-01, -1.1344e-02,  1.7114e-01, -3.4630e-01,\n",
      "          3.0182e-01, -1.7840e-01, -4.4955e-02,  4.7685e-01,  2.3429e-01,\n",
      "         -2.0993e-04, -3.1590e-01,  1.4933e-01, -3.4794e-01,  4.0296e-02,\n",
      "         -5.2708e-02, -3.9691e-01,  3.8710e-02, -7.3864e-02,  3.9559e-01,\n",
      "         -2.1087e-01,  4.0306e-01, -3.9320e-01, -9.2826e-02,  4.8890e-01,\n",
      "          4.7009e-01, -4.8984e-01,  1.8414e-02,  4.3571e-01,  3.3965e-02,\n",
      "         -3.1877e-01,  2.5281e-01, -3.5005e-01, -2.3904e-01,  4.2611e-01,\n",
      "         -2.9801e-01,  8.4876e-02,  4.0464e-01, -1.8968e-01, -3.6074e-01,\n",
      "         -3.8140e-03, -3.8703e-01, -1.0132e-01, -4.1207e-01, -1.3494e-04,\n",
      "          3.9459e-01,  2.9657e-01,  2.0455e-01, -1.1478e-01,  4.8224e-01,\n",
      "         -2.4734e-01, -1.0230e-01,  9.2335e-02,  2.4343e-02, -3.5300e-01,\n",
      "          2.0875e-01,  4.3911e-01, -4.2997e-01, -1.1652e-01,  2.6931e-01,\n",
      "          1.5951e-01,  4.8167e-01, -1.0938e-01,  6.8503e-02,  9.3351e-02,\n",
      "         -3.8608e-01, -4.6630e-03, -3.0631e-01, -3.1452e-01,  3.6388e-01,\n",
      "          3.9371e-01, -4.8803e-01, -2.3385e-01, -2.9818e-01, -3.0848e-01,\n",
      "          2.9630e-01,  7.3386e-02, -1.2276e-02,  1.8542e-01, -4.1924e-02,\n",
      "         -4.0357e-01, -1.0265e-01,  2.8177e-01, -4.9081e-01,  2.1151e-01,\n",
      "          3.6830e-01,  3.0946e-01,  1.5720e-01,  1.4172e-01, -4.2283e-01,\n",
      "         -4.7915e-01,  3.8314e-01, -3.4119e-01, -2.9776e-01, -1.5916e-01,\n",
      "          3.1954e-01,  3.8582e-01],\n",
      "        [ 2.8393e-02, -2.1507e-01,  7.3449e-02, -3.5425e-02,  8.4971e-03,\n",
      "          6.4568e-02, -4.1707e-01,  4.9329e-01,  6.2499e-02, -3.1489e-01,\n",
      "         -2.6528e-01, -2.0642e-01,  2.8555e-01,  2.0611e-01, -2.4231e-01,\n",
      "          1.9236e-01, -2.4110e-01,  3.8176e-01, -3.1023e-01,  5.9767e-02,\n",
      "         -3.1687e-01, -4.0727e-01, -2.7802e-01, -4.0448e-01, -2.9654e-01,\n",
      "          4.2772e-01, -2.7135e-02, -9.2047e-02, -1.7364e-01, -3.5465e-02,\n",
      "          4.5284e-02, -1.1347e-01,  1.1493e-01,  2.4035e-02,  4.2536e-01,\n",
      "         -2.7699e-01, -2.6960e-01,  4.5004e-01,  1.0166e-01,  4.5971e-01,\n",
      "         -4.6756e-01,  2.6671e-01,  6.4181e-02,  3.1845e-01, -4.9001e-01,\n",
      "          1.8032e-01,  1.0662e-01, -4.9253e-01, -8.3386e-02, -3.9306e-01,\n",
      "         -4.4291e-01, -7.9380e-02,  5.9529e-03,  1.9958e-02, -2.0477e-01,\n",
      "         -3.1541e-01, -1.0251e-01, -3.1129e-01, -2.1621e-02, -1.0289e-02,\n",
      "         -4.2008e-01, -4.8188e-02,  3.7436e-01, -1.1315e-01, -4.9122e-01,\n",
      "         -4.0513e-01,  2.0755e-01,  1.1163e-01, -4.9098e-01,  4.1759e-01,\n",
      "          1.6288e-01,  6.8331e-04, -3.7547e-01,  1.7793e-01, -1.0203e-01,\n",
      "         -3.3526e-01,  3.0938e-01,  3.5360e-01, -4.9011e-01, -3.8699e-01,\n",
      "          2.1129e-01, -4.3664e-01, -2.1207e-01, -3.9717e-01,  3.2691e-01,\n",
      "         -1.7923e-01,  3.1414e-01,  4.3294e-01,  4.4856e-01,  4.4316e-01,\n",
      "          2.7930e-01, -3.8357e-01,  3.5021e-01,  3.0501e-01,  3.6156e-01,\n",
      "         -3.5105e-01,  3.5966e-01,  4.6147e-01,  2.8881e-01,  1.0856e-02,\n",
      "          3.1861e-01,  3.0779e-02,  2.5312e-02, -1.1220e-01,  4.2442e-01,\n",
      "          8.0456e-02, -4.1062e-01,  4.2825e-01,  1.9142e-01,  1.2905e-01,\n",
      "         -4.1203e-01, -4.0837e-01,  1.0011e-01,  1.0266e-01,  1.1884e-01,\n",
      "         -2.9486e-01, -1.7023e-01, -3.9583e-01,  4.3081e-01, -4.8520e-01,\n",
      "          8.0888e-03,  1.3026e-02,  3.0812e-02,  3.1113e-01,  5.4653e-02,\n",
      "          3.3968e-01, -2.7318e-02, -1.7378e-01, -1.9805e-01,  1.3532e-02,\n",
      "         -7.3870e-02, -2.5023e-01,  4.2612e-01,  1.9641e-01,  4.4966e-01,\n",
      "          3.5824e-01, -3.8031e-01,  6.2354e-02,  2.4223e-01, -2.8999e-01,\n",
      "         -2.9022e-01,  1.9836e-01,  2.7220e-01,  3.3876e-01,  2.3070e-01,\n",
      "         -3.7776e-01,  3.4646e-01,  3.5032e-01, -2.7519e-01, -3.7461e-01,\n",
      "         -1.4845e-01, -4.0748e-01,  4.1486e-01,  4.7666e-01,  2.3587e-01,\n",
      "          3.7843e-01,  4.7986e-01,  3.1608e-01,  4.0587e-01,  4.5123e-01,\n",
      "          1.6128e-01,  3.4837e-02, -2.4213e-01,  8.3348e-02,  4.0535e-01,\n",
      "         -2.0574e-01,  1.2630e-01, -1.4109e-01, -1.3830e-01, -1.7473e-01,\n",
      "          2.6931e-01,  1.7263e-01, -3.6738e-01, -2.2899e-01,  4.7750e-02,\n",
      "         -3.8553e-01, -3.5645e-01,  1.2255e-01,  2.6394e-01,  1.4087e-01,\n",
      "          4.8568e-01,  3.9760e-01, -4.6300e-01, -4.0858e-01, -1.0156e-02,\n",
      "          2.2234e-01, -1.7810e-02,  2.4384e-01, -1.7978e-01, -4.6589e-01,\n",
      "         -4.8086e-01,  4.0185e-01],\n",
      "        [ 2.4316e-01,  5.1094e-02,  5.5459e-02, -4.2938e-01, -1.7976e-01,\n",
      "          3.1237e-01, -3.2596e-02, -2.8321e-01,  2.7905e-01, -2.7461e-01,\n",
      "          1.0299e-01,  1.8728e-01, -1.6608e-01,  1.3264e-01,  2.7119e-01,\n",
      "          6.4278e-02,  3.1660e-01,  2.2600e-01, -4.8537e-01,  4.2158e-01,\n",
      "          4.8353e-01, -1.8455e-01,  8.0709e-02, -1.7443e-01,  4.0629e-01,\n",
      "          4.3725e-01, -1.5402e-01, -1.8739e-03,  4.4505e-01, -3.8958e-01,\n",
      "          1.8676e-01,  4.1720e-01, -4.8323e-01,  1.4650e-01,  3.3617e-01,\n",
      "          4.6191e-01,  4.8709e-01,  4.2077e-01,  2.7973e-01,  1.4151e-01,\n",
      "          4.8756e-01, -9.2613e-02, -8.1956e-02,  3.6287e-01, -4.3972e-01,\n",
      "         -3.8881e-01, -5.7135e-02,  3.9335e-01,  4.2260e-01, -4.6757e-01,\n",
      "         -2.9373e-01, -3.9142e-01, -4.4682e-01,  3.2180e-01,  3.9332e-01,\n",
      "          5.9391e-02, -4.0696e-01,  4.0756e-01, -3.7403e-01, -3.4418e-01,\n",
      "         -6.7751e-02,  3.9745e-01, -3.6439e-01,  3.8382e-01, -7.6625e-02,\n",
      "         -3.1299e-01,  4.9208e-02, -9.0591e-02,  2.8831e-01, -4.6503e-01,\n",
      "         -7.9162e-02, -1.3593e-01,  2.0823e-01,  1.3763e-01, -2.5408e-01,\n",
      "          1.9507e-01,  3.0950e-01, -4.6143e-01,  4.1417e-02, -3.1686e-02,\n",
      "         -8.1151e-02, -7.1416e-02,  1.4370e-01, -4.2339e-01, -3.9791e-01,\n",
      "          1.2989e-02,  9.9221e-02,  6.6198e-02,  2.0977e-01,  7.8000e-02,\n",
      "         -1.7535e-01,  4.6218e-01, -2.5272e-01, -4.7835e-01,  1.2324e-01,\n",
      "         -2.4548e-01,  1.5080e-01, -5.3550e-02,  1.2076e-01,  3.8934e-01,\n",
      "          3.8205e-01, -4.0994e-01, -1.1160e-01,  2.9541e-01, -3.4564e-01,\n",
      "          2.6261e-01,  5.1345e-02, -3.3273e-02,  3.0493e-01, -7.7605e-02,\n",
      "         -1.5532e-01,  1.4562e-01, -2.4403e-01,  2.2133e-01, -4.4685e-01,\n",
      "         -9.4684e-02,  2.7953e-01, -4.0533e-01,  2.5822e-01,  4.4835e-01,\n",
      "          3.3890e-01,  1.3830e-01, -2.2497e-01,  9.8689e-02,  2.8423e-01,\n",
      "         -2.7537e-01, -2.9027e-02,  3.8509e-01, -1.0216e-01,  8.3434e-02,\n",
      "         -4.5676e-01, -3.2859e-01, -1.8846e-01, -3.8350e-01,  4.8841e-01,\n",
      "          7.9258e-02, -3.2171e-01, -4.3615e-02, -3.8480e-01, -6.7773e-02,\n",
      "         -4.7649e-01, -4.3129e-01, -4.9387e-01,  4.9946e-01,  2.1872e-01,\n",
      "          1.6125e-01,  4.2134e-01,  4.6237e-01, -2.6883e-02, -6.6878e-02,\n",
      "         -3.0271e-01,  4.1654e-01,  3.9426e-01,  6.2786e-03,  3.8204e-01,\n",
      "         -2.3100e-01,  1.6674e-01,  1.9269e-02, -1.5630e-01,  1.3106e-01,\n",
      "          3.4213e-01, -2.8111e-01, -4.6205e-01,  3.2671e-01, -1.4839e-01,\n",
      "         -3.5897e-01, -4.2942e-01,  3.7445e-01,  2.6903e-01, -2.6800e-01,\n",
      "          4.6427e-01, -4.1747e-01,  3.9074e-01, -2.8527e-01,  2.7827e-01,\n",
      "         -1.4761e-02, -7.7906e-02, -2.4888e-01,  4.9965e-01,  3.1182e-01,\n",
      "          4.2581e-01, -3.5637e-02,  2.6706e-01, -2.1018e-01, -4.6920e-01,\n",
      "         -3.0328e-01, -8.8848e-02,  1.3523e-01, -1.2193e-01,  2.9067e-01,\n",
      "          1.4192e-01,  2.9863e-01],\n",
      "        [ 2.1939e-02, -2.1620e-01, -1.3989e-01,  2.0001e-01, -2.6691e-01,\n",
      "         -2.2999e-01,  3.5089e-01,  4.2780e-01, -2.5216e-01,  2.5742e-01,\n",
      "         -4.2682e-01, -2.3400e-01,  2.5624e-01,  5.2858e-02, -4.4735e-02,\n",
      "         -2.7120e-01,  2.7121e-01, -1.1131e-02, -2.0003e-01,  3.6435e-01,\n",
      "         -1.3997e-01, -3.0246e-01,  2.5170e-01, -1.9140e-01, -4.2226e-01,\n",
      "         -2.8699e-01,  4.4266e-02,  6.2823e-02,  1.7323e-01, -8.4048e-02,\n",
      "          2.8983e-02,  7.1588e-02, -4.9993e-01,  4.3573e-01, -1.6429e-01,\n",
      "         -2.9448e-01, -4.0873e-01,  2.7543e-01, -3.6739e-01,  1.9690e-01,\n",
      "         -2.6216e-01,  2.5825e-01, -1.5555e-01, -1.9827e-01,  3.1932e-01,\n",
      "         -2.0889e-01, -2.8329e-01, -1.9644e-01,  7.6186e-02, -4.7227e-01,\n",
      "          1.4629e-01,  3.2163e-01,  4.3594e-01, -2.9314e-01,  2.6589e-01,\n",
      "         -6.6499e-02,  2.4181e-01,  4.1164e-01,  4.9924e-01,  9.3671e-02,\n",
      "         -4.7423e-01, -3.0037e-01, -3.6135e-01, -4.7988e-01,  4.1612e-01,\n",
      "          7.5813e-02, -4.3853e-01, -1.3715e-01,  2.6466e-01, -4.1169e-01,\n",
      "         -2.6106e-01,  2.4077e-01,  2.4765e-01,  2.0474e-01, -4.8767e-01,\n",
      "          3.4312e-01,  3.0089e-01, -2.6358e-01, -2.3430e-01,  3.1284e-01,\n",
      "          4.9953e-01, -3.5684e-02,  1.9416e-01,  4.6613e-01,  4.7486e-01,\n",
      "          7.1292e-02, -1.6498e-01,  2.6788e-01,  3.3629e-01,  4.4775e-01,\n",
      "          4.7669e-01, -4.7820e-01, -4.4081e-01, -4.0668e-01,  2.4537e-01,\n",
      "         -2.8373e-01,  2.6604e-01, -1.5824e-01, -2.7501e-01,  4.6280e-01,\n",
      "          4.3243e-01, -6.9161e-03,  3.4847e-01, -4.3560e-01, -3.1419e-01,\n",
      "         -5.0649e-02, -1.4905e-01, -2.1218e-01,  4.0496e-01,  9.8498e-02,\n",
      "          2.1212e-02, -2.1080e-01,  3.0203e-01, -3.2247e-02, -3.5137e-01,\n",
      "         -4.0825e-01,  3.6829e-01,  7.6268e-02, -4.5142e-01, -7.0428e-02,\n",
      "         -3.8538e-02, -4.0860e-01, -2.3721e-01, -2.6784e-01,  2.6255e-01,\n",
      "          4.1158e-01, -1.2125e-01,  4.8033e-01, -3.1060e-01,  4.8208e-04,\n",
      "         -1.2071e-01,  3.9326e-02,  3.2534e-01,  1.0235e-01,  2.3560e-01,\n",
      "          3.7913e-01, -4.0633e-01, -3.9607e-01,  4.2034e-01,  9.0152e-02,\n",
      "         -3.9707e-01,  2.2478e-01,  4.7630e-01,  8.5905e-02, -3.4227e-01,\n",
      "         -2.6248e-01,  2.0666e-01,  2.5354e-01,  4.9642e-01, -3.3560e-01,\n",
      "         -2.5921e-01, -1.5643e-01,  2.4026e-01,  2.8152e-01,  7.2462e-02,\n",
      "          5.1272e-02,  4.5968e-01,  2.5359e-02, -3.2823e-01, -9.0428e-02,\n",
      "         -4.1559e-01,  1.7903e-01, -1.5719e-01,  3.1318e-01,  1.4970e-01,\n",
      "         -3.7570e-01, -3.2366e-01,  7.3289e-02, -1.7857e-01, -7.6912e-02,\n",
      "         -1.5517e-01, -2.5391e-01, -3.7961e-01, -3.2698e-01,  5.0610e-02,\n",
      "         -2.0522e-01, -5.3520e-02,  7.4136e-02,  4.2519e-01, -2.1097e-01,\n",
      "          1.1299e-01,  4.5611e-01, -4.4404e-02,  3.8389e-01, -2.2574e-01,\n",
      "         -2.3627e-01, -4.0755e-01, -4.2760e-01, -4.0825e-02, -4.1447e-01,\n",
      "          4.5897e-01, -1.1031e-01]], requires_grad=True)\n",
      "Wq: torch.Size([4, 128])\n",
      "tensor([[ 1.8937e-01, -2.8415e-01, -4.6238e-01, -4.0144e-01, -4.4552e-01,\n",
      "          1.9680e-01,  4.4328e-02,  4.4592e-01, -2.1865e-01,  2.1341e-01,\n",
      "         -3.0257e-03, -4.0493e-01,  1.3867e-01,  2.9379e-01,  3.5597e-01,\n",
      "          4.7860e-01, -3.5840e-01,  8.7989e-02, -2.9445e-01, -9.1477e-02,\n",
      "         -3.5326e-02, -1.1366e-01,  1.4490e-01, -2.6402e-03,  6.1200e-02,\n",
      "          6.7722e-02, -3.2172e-01, -3.8489e-01,  2.5546e-01, -2.6750e-01,\n",
      "         -1.2889e-01, -4.3475e-01,  1.1832e-01,  1.9844e-01,  2.2294e-01,\n",
      "          2.2098e-01,  3.4316e-01,  4.0263e-01,  1.8223e-01, -3.1401e-01,\n",
      "          1.4572e-01,  2.5463e-01,  1.4850e-01,  4.2974e-01,  3.9195e-01,\n",
      "         -4.2508e-01,  1.2758e-01,  4.7005e-01,  2.9028e-01,  2.7730e-01,\n",
      "          2.4163e-01,  4.1844e-01, -3.1530e-01,  3.5747e-01,  2.3997e-02,\n",
      "          2.9750e-01,  4.4569e-01, -3.0182e-01, -1.2491e-01,  2.3174e-01,\n",
      "          4.1144e-01, -1.1239e-01, -3.2242e-01,  2.1355e-01, -2.8637e-01,\n",
      "         -4.6897e-01,  3.3208e-01,  3.4030e-02,  4.2027e-01,  1.1778e-01,\n",
      "         -2.8841e-01,  3.7903e-01, -9.4648e-02, -5.8460e-02, -2.6424e-01,\n",
      "          2.9487e-01,  5.8353e-02, -1.1010e-01, -3.4630e-01,  2.7946e-01,\n",
      "          3.0002e-01,  2.4929e-01, -4.9700e-01, -5.6904e-02, -2.3638e-01,\n",
      "          2.2197e-01, -3.2396e-01,  1.0468e-01, -1.2194e-01,  2.9137e-01,\n",
      "         -2.7048e-01, -2.6034e-02,  4.3632e-01,  2.0930e-01,  1.6822e-01,\n",
      "         -2.1106e-02, -2.5414e-01, -1.1344e-02,  1.7114e-01, -3.4630e-01,\n",
      "          3.0182e-01, -1.7840e-01, -4.4955e-02,  4.7685e-01,  2.3429e-01,\n",
      "         -2.0993e-04, -3.1590e-01,  1.4933e-01, -3.4794e-01,  4.0296e-02,\n",
      "         -5.2708e-02, -3.9691e-01,  3.8710e-02, -7.3864e-02,  3.9559e-01,\n",
      "         -2.1087e-01,  4.0306e-01, -3.9320e-01, -9.2826e-02,  4.8890e-01,\n",
      "          4.7009e-01, -4.8984e-01,  1.8414e-02,  4.3571e-01,  3.3965e-02,\n",
      "         -3.1877e-01,  2.5281e-01, -3.5005e-01],\n",
      "        [ 2.8393e-02, -2.1507e-01,  7.3449e-02, -3.5425e-02,  8.4971e-03,\n",
      "          6.4568e-02, -4.1707e-01,  4.9329e-01,  6.2499e-02, -3.1489e-01,\n",
      "         -2.6528e-01, -2.0642e-01,  2.8555e-01,  2.0611e-01, -2.4231e-01,\n",
      "          1.9236e-01, -2.4110e-01,  3.8176e-01, -3.1023e-01,  5.9767e-02,\n",
      "         -3.1687e-01, -4.0727e-01, -2.7802e-01, -4.0448e-01, -2.9654e-01,\n",
      "          4.2772e-01, -2.7135e-02, -9.2047e-02, -1.7364e-01, -3.5465e-02,\n",
      "          4.5284e-02, -1.1347e-01,  1.1493e-01,  2.4035e-02,  4.2536e-01,\n",
      "         -2.7699e-01, -2.6960e-01,  4.5004e-01,  1.0166e-01,  4.5971e-01,\n",
      "         -4.6756e-01,  2.6671e-01,  6.4181e-02,  3.1845e-01, -4.9001e-01,\n",
      "          1.8032e-01,  1.0662e-01, -4.9253e-01, -8.3386e-02, -3.9306e-01,\n",
      "         -4.4291e-01, -7.9380e-02,  5.9529e-03,  1.9958e-02, -2.0477e-01,\n",
      "         -3.1541e-01, -1.0251e-01, -3.1129e-01, -2.1621e-02, -1.0289e-02,\n",
      "         -4.2008e-01, -4.8188e-02,  3.7436e-01, -1.1315e-01, -4.9122e-01,\n",
      "         -4.0513e-01,  2.0755e-01,  1.1163e-01, -4.9098e-01,  4.1759e-01,\n",
      "          1.6288e-01,  6.8331e-04, -3.7547e-01,  1.7793e-01, -1.0203e-01,\n",
      "         -3.3526e-01,  3.0938e-01,  3.5360e-01, -4.9011e-01, -3.8699e-01,\n",
      "          2.1129e-01, -4.3664e-01, -2.1207e-01, -3.9717e-01,  3.2691e-01,\n",
      "         -1.7923e-01,  3.1414e-01,  4.3294e-01,  4.4856e-01,  4.4316e-01,\n",
      "          2.7930e-01, -3.8357e-01,  3.5021e-01,  3.0501e-01,  3.6156e-01,\n",
      "         -3.5105e-01,  3.5966e-01,  4.6147e-01,  2.8881e-01,  1.0856e-02,\n",
      "          3.1861e-01,  3.0779e-02,  2.5312e-02, -1.1220e-01,  4.2442e-01,\n",
      "          8.0456e-02, -4.1062e-01,  4.2825e-01,  1.9142e-01,  1.2905e-01,\n",
      "         -4.1203e-01, -4.0837e-01,  1.0011e-01,  1.0266e-01,  1.1884e-01,\n",
      "         -2.9486e-01, -1.7023e-01, -3.9583e-01,  4.3081e-01, -4.8520e-01,\n",
      "          8.0888e-03,  1.3026e-02,  3.0812e-02,  3.1113e-01,  5.4653e-02,\n",
      "          3.3968e-01, -2.7318e-02, -1.7378e-01],\n",
      "        [ 2.4316e-01,  5.1094e-02,  5.5459e-02, -4.2938e-01, -1.7976e-01,\n",
      "          3.1237e-01, -3.2596e-02, -2.8321e-01,  2.7905e-01, -2.7461e-01,\n",
      "          1.0299e-01,  1.8728e-01, -1.6608e-01,  1.3264e-01,  2.7119e-01,\n",
      "          6.4278e-02,  3.1660e-01,  2.2600e-01, -4.8537e-01,  4.2158e-01,\n",
      "          4.8353e-01, -1.8455e-01,  8.0709e-02, -1.7443e-01,  4.0629e-01,\n",
      "          4.3725e-01, -1.5402e-01, -1.8739e-03,  4.4505e-01, -3.8958e-01,\n",
      "          1.8676e-01,  4.1720e-01, -4.8323e-01,  1.4650e-01,  3.3617e-01,\n",
      "          4.6191e-01,  4.8709e-01,  4.2077e-01,  2.7973e-01,  1.4151e-01,\n",
      "          4.8756e-01, -9.2613e-02, -8.1956e-02,  3.6287e-01, -4.3972e-01,\n",
      "         -3.8881e-01, -5.7135e-02,  3.9335e-01,  4.2260e-01, -4.6757e-01,\n",
      "         -2.9373e-01, -3.9142e-01, -4.4682e-01,  3.2180e-01,  3.9332e-01,\n",
      "          5.9391e-02, -4.0696e-01,  4.0756e-01, -3.7403e-01, -3.4418e-01,\n",
      "         -6.7751e-02,  3.9745e-01, -3.6439e-01,  3.8382e-01, -7.6625e-02,\n",
      "         -3.1299e-01,  4.9208e-02, -9.0591e-02,  2.8831e-01, -4.6503e-01,\n",
      "         -7.9162e-02, -1.3593e-01,  2.0823e-01,  1.3763e-01, -2.5408e-01,\n",
      "          1.9507e-01,  3.0950e-01, -4.6143e-01,  4.1417e-02, -3.1686e-02,\n",
      "         -8.1151e-02, -7.1416e-02,  1.4370e-01, -4.2339e-01, -3.9791e-01,\n",
      "          1.2989e-02,  9.9221e-02,  6.6198e-02,  2.0977e-01,  7.8000e-02,\n",
      "         -1.7535e-01,  4.6218e-01, -2.5272e-01, -4.7835e-01,  1.2324e-01,\n",
      "         -2.4548e-01,  1.5080e-01, -5.3550e-02,  1.2076e-01,  3.8934e-01,\n",
      "          3.8205e-01, -4.0994e-01, -1.1160e-01,  2.9541e-01, -3.4564e-01,\n",
      "          2.6261e-01,  5.1345e-02, -3.3273e-02,  3.0493e-01, -7.7605e-02,\n",
      "         -1.5532e-01,  1.4562e-01, -2.4403e-01,  2.2133e-01, -4.4685e-01,\n",
      "         -9.4684e-02,  2.7953e-01, -4.0533e-01,  2.5822e-01,  4.4835e-01,\n",
      "          3.3890e-01,  1.3830e-01, -2.2497e-01,  9.8689e-02,  2.8423e-01,\n",
      "         -2.7537e-01, -2.9027e-02,  3.8509e-01],\n",
      "        [ 2.1939e-02, -2.1620e-01, -1.3989e-01,  2.0001e-01, -2.6691e-01,\n",
      "         -2.2999e-01,  3.5089e-01,  4.2780e-01, -2.5216e-01,  2.5742e-01,\n",
      "         -4.2682e-01, -2.3400e-01,  2.5624e-01,  5.2858e-02, -4.4735e-02,\n",
      "         -2.7120e-01,  2.7121e-01, -1.1131e-02, -2.0003e-01,  3.6435e-01,\n",
      "         -1.3997e-01, -3.0246e-01,  2.5170e-01, -1.9140e-01, -4.2226e-01,\n",
      "         -2.8699e-01,  4.4266e-02,  6.2823e-02,  1.7323e-01, -8.4048e-02,\n",
      "          2.8983e-02,  7.1588e-02, -4.9993e-01,  4.3573e-01, -1.6429e-01,\n",
      "         -2.9448e-01, -4.0873e-01,  2.7543e-01, -3.6739e-01,  1.9690e-01,\n",
      "         -2.6216e-01,  2.5825e-01, -1.5555e-01, -1.9827e-01,  3.1932e-01,\n",
      "         -2.0889e-01, -2.8329e-01, -1.9644e-01,  7.6186e-02, -4.7227e-01,\n",
      "          1.4629e-01,  3.2163e-01,  4.3594e-01, -2.9314e-01,  2.6589e-01,\n",
      "         -6.6499e-02,  2.4181e-01,  4.1164e-01,  4.9924e-01,  9.3671e-02,\n",
      "         -4.7423e-01, -3.0037e-01, -3.6135e-01, -4.7988e-01,  4.1612e-01,\n",
      "          7.5813e-02, -4.3853e-01, -1.3715e-01,  2.6466e-01, -4.1169e-01,\n",
      "         -2.6106e-01,  2.4077e-01,  2.4765e-01,  2.0474e-01, -4.8767e-01,\n",
      "          3.4312e-01,  3.0089e-01, -2.6358e-01, -2.3430e-01,  3.1284e-01,\n",
      "          4.9953e-01, -3.5684e-02,  1.9416e-01,  4.6613e-01,  4.7486e-01,\n",
      "          7.1292e-02, -1.6498e-01,  2.6788e-01,  3.3629e-01,  4.4775e-01,\n",
      "          4.7669e-01, -4.7820e-01, -4.4081e-01, -4.0668e-01,  2.4537e-01,\n",
      "         -2.8373e-01,  2.6604e-01, -1.5824e-01, -2.7501e-01,  4.6280e-01,\n",
      "          4.3243e-01, -6.9161e-03,  3.4847e-01, -4.3560e-01, -3.1419e-01,\n",
      "         -5.0649e-02, -1.4905e-01, -2.1218e-01,  4.0496e-01,  9.8498e-02,\n",
      "          2.1212e-02, -2.1080e-01,  3.0203e-01, -3.2247e-02, -3.5137e-01,\n",
      "         -4.0825e-01,  3.6829e-01,  7.6268e-02, -4.5142e-01, -7.0428e-02,\n",
      "         -3.8538e-02, -4.0860e-01, -2.3721e-01, -2.6784e-01,  2.6255e-01,\n",
      "          4.1158e-01, -1.2125e-01,  4.8033e-01]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wk: torch.Size([4, 32])\n",
      "tensor([[-2.3904e-01,  4.2611e-01, -2.9801e-01,  8.4876e-02,  4.0464e-01,\n",
      "         -1.8968e-01, -3.6074e-01, -3.8140e-03, -3.8703e-01, -1.0132e-01,\n",
      "         -4.1207e-01, -1.3494e-04,  3.9459e-01,  2.9657e-01,  2.0455e-01,\n",
      "         -1.1478e-01,  4.8224e-01, -2.4734e-01, -1.0230e-01,  9.2335e-02,\n",
      "          2.4343e-02, -3.5300e-01,  2.0875e-01,  4.3911e-01, -4.2997e-01,\n",
      "         -1.1652e-01,  2.6931e-01,  1.5951e-01,  4.8167e-01, -1.0938e-01,\n",
      "          6.8503e-02,  9.3351e-02],\n",
      "        [-1.9805e-01,  1.3532e-02, -7.3870e-02, -2.5023e-01,  4.2612e-01,\n",
      "          1.9641e-01,  4.4966e-01,  3.5824e-01, -3.8031e-01,  6.2354e-02,\n",
      "          2.4223e-01, -2.8999e-01, -2.9022e-01,  1.9836e-01,  2.7220e-01,\n",
      "          3.3876e-01,  2.3070e-01, -3.7776e-01,  3.4646e-01,  3.5032e-01,\n",
      "         -2.7519e-01, -3.7461e-01, -1.4845e-01, -4.0748e-01,  4.1486e-01,\n",
      "          4.7666e-01,  2.3587e-01,  3.7843e-01,  4.7986e-01,  3.1608e-01,\n",
      "          4.0587e-01,  4.5123e-01],\n",
      "        [-1.0216e-01,  8.3434e-02, -4.5676e-01, -3.2859e-01, -1.8846e-01,\n",
      "         -3.8350e-01,  4.8841e-01,  7.9258e-02, -3.2171e-01, -4.3615e-02,\n",
      "         -3.8480e-01, -6.7773e-02, -4.7649e-01, -4.3129e-01, -4.9387e-01,\n",
      "          4.9946e-01,  2.1872e-01,  1.6125e-01,  4.2134e-01,  4.6237e-01,\n",
      "         -2.6883e-02, -6.6878e-02, -3.0271e-01,  4.1654e-01,  3.9426e-01,\n",
      "          6.2786e-03,  3.8204e-01, -2.3100e-01,  1.6674e-01,  1.9269e-02,\n",
      "         -1.5630e-01,  1.3106e-01],\n",
      "        [-3.1060e-01,  4.8208e-04, -1.2071e-01,  3.9326e-02,  3.2534e-01,\n",
      "          1.0235e-01,  2.3560e-01,  3.7913e-01, -4.0633e-01, -3.9607e-01,\n",
      "          4.2034e-01,  9.0152e-02, -3.9707e-01,  2.2478e-01,  4.7630e-01,\n",
      "          8.5905e-02, -3.4227e-01, -2.6248e-01,  2.0666e-01,  2.5354e-01,\n",
      "          4.9642e-01, -3.3560e-01, -2.5921e-01, -1.5643e-01,  2.4026e-01,\n",
      "          2.8152e-01,  7.2462e-02,  5.1272e-02,  4.5968e-01,  2.5359e-02,\n",
      "         -3.2823e-01, -9.0428e-02]], grad_fn=<SplitWithSizesBackward0>)\n",
      "Wv: torch.Size([4, 32])\n",
      "tensor([[-0.3861, -0.0047, -0.3063, -0.3145,  0.3639,  0.3937, -0.4880, -0.2338,\n",
      "         -0.2982, -0.3085,  0.2963,  0.0734, -0.0123,  0.1854, -0.0419, -0.4036,\n",
      "         -0.1026,  0.2818, -0.4908,  0.2115,  0.3683,  0.3095,  0.1572,  0.1417,\n",
      "         -0.4228, -0.4791,  0.3831, -0.3412, -0.2978, -0.1592,  0.3195,  0.3858],\n",
      "        [ 0.1613,  0.0348, -0.2421,  0.0833,  0.4053, -0.2057,  0.1263, -0.1411,\n",
      "         -0.1383, -0.1747,  0.2693,  0.1726, -0.3674, -0.2290,  0.0477, -0.3855,\n",
      "         -0.3565,  0.1225,  0.2639,  0.1409,  0.4857,  0.3976, -0.4630, -0.4086,\n",
      "         -0.0102,  0.2223, -0.0178,  0.2438, -0.1798, -0.4659, -0.4809,  0.4019],\n",
      "        [ 0.3421, -0.2811, -0.4621,  0.3267, -0.1484, -0.3590, -0.4294,  0.3745,\n",
      "          0.2690, -0.2680,  0.4643, -0.4175,  0.3907, -0.2853,  0.2783, -0.0148,\n",
      "         -0.0779, -0.2489,  0.4996,  0.3118,  0.4258, -0.0356,  0.2671, -0.2102,\n",
      "         -0.4692, -0.3033, -0.0888,  0.1352, -0.1219,  0.2907,  0.1419,  0.2986],\n",
      "        [-0.4156,  0.1790, -0.1572,  0.3132,  0.1497, -0.3757, -0.3237,  0.0733,\n",
      "         -0.1786, -0.0769, -0.1552, -0.2539, -0.3796, -0.3270,  0.0506, -0.2052,\n",
      "         -0.0535,  0.0741,  0.4252, -0.2110,  0.1130,  0.4561, -0.0444,  0.3839,\n",
      "         -0.2257, -0.2363, -0.4075, -0.4276, -0.0408, -0.4145,  0.4590, -0.1103]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wq spliced: torch.Size([4, 128])\n",
      "tensor([[ 1.8937e-01, -2.8415e-01, -4.6238e-01, -4.0144e-01, -4.4552e-01,\n",
      "          1.9680e-01,  4.4328e-02,  4.4592e-01, -2.1865e-01,  2.1341e-01,\n",
      "         -3.0257e-03, -4.0493e-01,  1.3867e-01,  2.9379e-01,  3.5597e-01,\n",
      "          4.7860e-01, -3.5840e-01,  8.7989e-02, -2.9445e-01, -9.1477e-02,\n",
      "         -3.5326e-02, -1.1366e-01,  1.4490e-01, -2.6402e-03,  6.1200e-02,\n",
      "          6.7722e-02, -3.2172e-01, -3.8489e-01,  2.5546e-01, -2.6750e-01,\n",
      "         -1.2889e-01, -4.3475e-01,  1.1832e-01,  1.9844e-01,  2.2294e-01,\n",
      "          2.2098e-01,  3.4316e-01,  4.0263e-01,  1.8223e-01, -3.1401e-01,\n",
      "          1.4572e-01,  2.5463e-01,  1.4850e-01,  4.2974e-01,  3.9195e-01,\n",
      "         -4.2508e-01,  1.2758e-01,  4.7005e-01,  2.9028e-01,  2.7730e-01,\n",
      "          2.4163e-01,  4.1844e-01, -3.1530e-01,  3.5747e-01,  2.3997e-02,\n",
      "          2.9750e-01,  4.4569e-01, -3.0182e-01, -1.2491e-01,  2.3174e-01,\n",
      "          4.1144e-01, -1.1239e-01, -3.2242e-01,  2.1355e-01, -2.8637e-01,\n",
      "         -4.6897e-01,  3.3208e-01,  3.4030e-02,  4.2027e-01,  1.1778e-01,\n",
      "         -2.8841e-01,  3.7903e-01, -9.4648e-02, -5.8460e-02, -2.6424e-01,\n",
      "          2.9487e-01,  5.8353e-02, -1.1010e-01, -3.4630e-01,  2.7946e-01,\n",
      "          3.0002e-01,  2.4929e-01, -4.9700e-01, -5.6904e-02, -2.3638e-01,\n",
      "          2.2197e-01, -3.2396e-01,  1.0468e-01, -1.2194e-01,  2.9137e-01,\n",
      "         -2.7048e-01, -2.6034e-02,  4.3632e-01,  2.0930e-01,  1.6822e-01,\n",
      "         -2.1106e-02, -2.5414e-01, -1.1344e-02,  1.7114e-01, -3.4630e-01,\n",
      "          3.0182e-01, -1.7840e-01, -4.4955e-02,  4.7685e-01,  2.3429e-01,\n",
      "         -2.0993e-04, -3.1590e-01,  1.4933e-01, -3.4794e-01,  4.0296e-02,\n",
      "         -5.2708e-02, -3.9691e-01,  3.8710e-02, -7.3864e-02,  3.9559e-01,\n",
      "         -2.1087e-01,  4.0306e-01, -3.9320e-01, -9.2826e-02,  4.8890e-01,\n",
      "          4.7009e-01, -4.8984e-01,  1.8414e-02,  4.3571e-01,  3.3965e-02,\n",
      "         -3.1877e-01,  2.5281e-01, -3.5005e-01],\n",
      "        [ 2.8393e-02, -2.1507e-01,  7.3449e-02, -3.5425e-02,  8.4971e-03,\n",
      "          6.4568e-02, -4.1707e-01,  4.9329e-01,  6.2499e-02, -3.1489e-01,\n",
      "         -2.6528e-01, -2.0642e-01,  2.8555e-01,  2.0611e-01, -2.4231e-01,\n",
      "          1.9236e-01, -2.4110e-01,  3.8176e-01, -3.1023e-01,  5.9767e-02,\n",
      "         -3.1687e-01, -4.0727e-01, -2.7802e-01, -4.0448e-01, -2.9654e-01,\n",
      "          4.2772e-01, -2.7135e-02, -9.2047e-02, -1.7364e-01, -3.5465e-02,\n",
      "          4.5284e-02, -1.1347e-01,  1.1493e-01,  2.4035e-02,  4.2536e-01,\n",
      "         -2.7699e-01, -2.6960e-01,  4.5004e-01,  1.0166e-01,  4.5971e-01,\n",
      "         -4.6756e-01,  2.6671e-01,  6.4181e-02,  3.1845e-01, -4.9001e-01,\n",
      "          1.8032e-01,  1.0662e-01, -4.9253e-01, -8.3386e-02, -3.9306e-01,\n",
      "         -4.4291e-01, -7.9380e-02,  5.9529e-03,  1.9958e-02, -2.0477e-01,\n",
      "         -3.1541e-01, -1.0251e-01, -3.1129e-01, -2.1621e-02, -1.0289e-02,\n",
      "         -4.2008e-01, -4.8188e-02,  3.7436e-01, -1.1315e-01, -4.9122e-01,\n",
      "         -4.0513e-01,  2.0755e-01,  1.1163e-01, -4.9098e-01,  4.1759e-01,\n",
      "          1.6288e-01,  6.8331e-04, -3.7547e-01,  1.7793e-01, -1.0203e-01,\n",
      "         -3.3526e-01,  3.0938e-01,  3.5360e-01, -4.9011e-01, -3.8699e-01,\n",
      "          2.1129e-01, -4.3664e-01, -2.1207e-01, -3.9717e-01,  3.2691e-01,\n",
      "         -1.7923e-01,  3.1414e-01,  4.3294e-01,  4.4856e-01,  4.4316e-01,\n",
      "          2.7930e-01, -3.8357e-01,  3.5021e-01,  3.0501e-01,  3.6156e-01,\n",
      "         -3.5105e-01,  3.5966e-01,  4.6147e-01,  2.8881e-01,  1.0856e-02,\n",
      "          3.1861e-01,  3.0779e-02,  2.5312e-02, -1.1220e-01,  4.2442e-01,\n",
      "          8.0456e-02, -4.1062e-01,  4.2825e-01,  1.9142e-01,  1.2905e-01,\n",
      "         -4.1203e-01, -4.0837e-01,  1.0011e-01,  1.0266e-01,  1.1884e-01,\n",
      "         -2.9486e-01, -1.7023e-01, -3.9583e-01,  4.3081e-01, -4.8520e-01,\n",
      "          8.0888e-03,  1.3026e-02,  3.0812e-02,  3.1113e-01,  5.4653e-02,\n",
      "          3.3968e-01, -2.7318e-02, -1.7378e-01],\n",
      "        [ 2.4316e-01,  5.1094e-02,  5.5459e-02, -4.2938e-01, -1.7976e-01,\n",
      "          3.1237e-01, -3.2596e-02, -2.8321e-01,  2.7905e-01, -2.7461e-01,\n",
      "          1.0299e-01,  1.8728e-01, -1.6608e-01,  1.3264e-01,  2.7119e-01,\n",
      "          6.4278e-02,  3.1660e-01,  2.2600e-01, -4.8537e-01,  4.2158e-01,\n",
      "          4.8353e-01, -1.8455e-01,  8.0709e-02, -1.7443e-01,  4.0629e-01,\n",
      "          4.3725e-01, -1.5402e-01, -1.8739e-03,  4.4505e-01, -3.8958e-01,\n",
      "          1.8676e-01,  4.1720e-01, -4.8323e-01,  1.4650e-01,  3.3617e-01,\n",
      "          4.6191e-01,  4.8709e-01,  4.2077e-01,  2.7973e-01,  1.4151e-01,\n",
      "          4.8756e-01, -9.2613e-02, -8.1956e-02,  3.6287e-01, -4.3972e-01,\n",
      "         -3.8881e-01, -5.7135e-02,  3.9335e-01,  4.2260e-01, -4.6757e-01,\n",
      "         -2.9373e-01, -3.9142e-01, -4.4682e-01,  3.2180e-01,  3.9332e-01,\n",
      "          5.9391e-02, -4.0696e-01,  4.0756e-01, -3.7403e-01, -3.4418e-01,\n",
      "         -6.7751e-02,  3.9745e-01, -3.6439e-01,  3.8382e-01, -7.6625e-02,\n",
      "         -3.1299e-01,  4.9208e-02, -9.0591e-02,  2.8831e-01, -4.6503e-01,\n",
      "         -7.9162e-02, -1.3593e-01,  2.0823e-01,  1.3763e-01, -2.5408e-01,\n",
      "          1.9507e-01,  3.0950e-01, -4.6143e-01,  4.1417e-02, -3.1686e-02,\n",
      "         -8.1151e-02, -7.1416e-02,  1.4370e-01, -4.2339e-01, -3.9791e-01,\n",
      "          1.2989e-02,  9.9221e-02,  6.6198e-02,  2.0977e-01,  7.8000e-02,\n",
      "         -1.7535e-01,  4.6218e-01, -2.5272e-01, -4.7835e-01,  1.2324e-01,\n",
      "         -2.4548e-01,  1.5080e-01, -5.3550e-02,  1.2076e-01,  3.8934e-01,\n",
      "          3.8205e-01, -4.0994e-01, -1.1160e-01,  2.9541e-01, -3.4564e-01,\n",
      "          2.6261e-01,  5.1345e-02, -3.3273e-02,  3.0493e-01, -7.7605e-02,\n",
      "         -1.5532e-01,  1.4562e-01, -2.4403e-01,  2.2133e-01, -4.4685e-01,\n",
      "         -9.4684e-02,  2.7953e-01, -4.0533e-01,  2.5822e-01,  4.4835e-01,\n",
      "          3.3890e-01,  1.3830e-01, -2.2497e-01,  9.8689e-02,  2.8423e-01,\n",
      "         -2.7537e-01, -2.9027e-02,  3.8509e-01],\n",
      "        [ 2.1939e-02, -2.1620e-01, -1.3989e-01,  2.0001e-01, -2.6691e-01,\n",
      "         -2.2999e-01,  3.5089e-01,  4.2780e-01, -2.5216e-01,  2.5742e-01,\n",
      "         -4.2682e-01, -2.3400e-01,  2.5624e-01,  5.2858e-02, -4.4735e-02,\n",
      "         -2.7120e-01,  2.7121e-01, -1.1131e-02, -2.0003e-01,  3.6435e-01,\n",
      "         -1.3997e-01, -3.0246e-01,  2.5170e-01, -1.9140e-01, -4.2226e-01,\n",
      "         -2.8699e-01,  4.4266e-02,  6.2823e-02,  1.7323e-01, -8.4048e-02,\n",
      "          2.8983e-02,  7.1588e-02, -4.9993e-01,  4.3573e-01, -1.6429e-01,\n",
      "         -2.9448e-01, -4.0873e-01,  2.7543e-01, -3.6739e-01,  1.9690e-01,\n",
      "         -2.6216e-01,  2.5825e-01, -1.5555e-01, -1.9827e-01,  3.1932e-01,\n",
      "         -2.0889e-01, -2.8329e-01, -1.9644e-01,  7.6186e-02, -4.7227e-01,\n",
      "          1.4629e-01,  3.2163e-01,  4.3594e-01, -2.9314e-01,  2.6589e-01,\n",
      "         -6.6499e-02,  2.4181e-01,  4.1164e-01,  4.9924e-01,  9.3671e-02,\n",
      "         -4.7423e-01, -3.0037e-01, -3.6135e-01, -4.7988e-01,  4.1612e-01,\n",
      "          7.5813e-02, -4.3853e-01, -1.3715e-01,  2.6466e-01, -4.1169e-01,\n",
      "         -2.6106e-01,  2.4077e-01,  2.4765e-01,  2.0474e-01, -4.8767e-01,\n",
      "          3.4312e-01,  3.0089e-01, -2.6358e-01, -2.3430e-01,  3.1284e-01,\n",
      "          4.9953e-01, -3.5684e-02,  1.9416e-01,  4.6613e-01,  4.7486e-01,\n",
      "          7.1292e-02, -1.6498e-01,  2.6788e-01,  3.3629e-01,  4.4775e-01,\n",
      "          4.7669e-01, -4.7820e-01, -4.4081e-01, -4.0668e-01,  2.4537e-01,\n",
      "         -2.8373e-01,  2.6604e-01, -1.5824e-01, -2.7501e-01,  4.6280e-01,\n",
      "          4.3243e-01, -6.9161e-03,  3.4847e-01, -4.3560e-01, -3.1419e-01,\n",
      "         -5.0649e-02, -1.4905e-01, -2.1218e-01,  4.0496e-01,  9.8498e-02,\n",
      "          2.1212e-02, -2.1080e-01,  3.0203e-01, -3.2247e-02, -3.5137e-01,\n",
      "         -4.0825e-01,  3.6829e-01,  7.6268e-02, -4.5142e-01, -7.0428e-02,\n",
      "         -3.8538e-02, -4.0860e-01, -2.3721e-01, -2.6784e-01,  2.6255e-01,\n",
      "          4.1158e-01, -1.2125e-01,  4.8033e-01]], grad_fn=<CatBackward0>)\n",
      "Wk spliced: torch.Size([4, 32])\n",
      "tensor([[-2.3904e-01,  4.2611e-01, -2.9801e-01,  8.4876e-02,  4.0464e-01,\n",
      "         -1.8968e-01, -3.6074e-01, -3.8140e-03, -3.8703e-01, -1.0132e-01,\n",
      "         -4.1207e-01, -1.3494e-04,  3.9459e-01,  2.9657e-01,  2.0455e-01,\n",
      "         -1.1478e-01,  4.8224e-01, -2.4734e-01, -1.0230e-01,  9.2335e-02,\n",
      "          2.4343e-02, -3.5300e-01,  2.0875e-01,  4.3911e-01, -4.2997e-01,\n",
      "         -1.1652e-01,  2.6931e-01,  1.5951e-01,  4.8167e-01, -1.0938e-01,\n",
      "          6.8503e-02,  9.3351e-02],\n",
      "        [-1.9805e-01,  1.3532e-02, -7.3870e-02, -2.5023e-01,  4.2612e-01,\n",
      "          1.9641e-01,  4.4966e-01,  3.5824e-01, -3.8031e-01,  6.2354e-02,\n",
      "          2.4223e-01, -2.8999e-01, -2.9022e-01,  1.9836e-01,  2.7220e-01,\n",
      "          3.3876e-01,  2.3070e-01, -3.7776e-01,  3.4646e-01,  3.5032e-01,\n",
      "         -2.7519e-01, -3.7461e-01, -1.4845e-01, -4.0748e-01,  4.1486e-01,\n",
      "          4.7666e-01,  2.3587e-01,  3.7843e-01,  4.7986e-01,  3.1608e-01,\n",
      "          4.0587e-01,  4.5123e-01],\n",
      "        [-1.0216e-01,  8.3434e-02, -4.5676e-01, -3.2859e-01, -1.8846e-01,\n",
      "         -3.8350e-01,  4.8841e-01,  7.9258e-02, -3.2171e-01, -4.3615e-02,\n",
      "         -3.8480e-01, -6.7773e-02, -4.7649e-01, -4.3129e-01, -4.9387e-01,\n",
      "          4.9946e-01,  2.1872e-01,  1.6125e-01,  4.2134e-01,  4.6237e-01,\n",
      "         -2.6883e-02, -6.6878e-02, -3.0271e-01,  4.1654e-01,  3.9426e-01,\n",
      "          6.2786e-03,  3.8204e-01, -2.3100e-01,  1.6674e-01,  1.9269e-02,\n",
      "         -1.5630e-01,  1.3106e-01],\n",
      "        [-3.1060e-01,  4.8208e-04, -1.2071e-01,  3.9326e-02,  3.2534e-01,\n",
      "          1.0235e-01,  2.3560e-01,  3.7913e-01, -4.0633e-01, -3.9607e-01,\n",
      "          4.2034e-01,  9.0152e-02, -3.9707e-01,  2.2478e-01,  4.7630e-01,\n",
      "          8.5905e-02, -3.4227e-01, -2.6248e-01,  2.0666e-01,  2.5354e-01,\n",
      "          4.9642e-01, -3.3560e-01, -2.5921e-01, -1.5643e-01,  2.4026e-01,\n",
      "          2.8152e-01,  7.2462e-02,  5.1272e-02,  4.5968e-01,  2.5359e-02,\n",
      "         -3.2823e-01, -9.0428e-02]], grad_fn=<CatBackward0>)\n",
      "Wv spliced: torch.Size([4, 32])\n",
      "tensor([[-0.3861, -0.0047, -0.3063, -0.3145,  0.3639,  0.3937, -0.4880, -0.2338,\n",
      "         -0.2982, -0.3085,  0.2963,  0.0734, -0.0123,  0.1854, -0.0419, -0.4036,\n",
      "         -0.1026,  0.2818, -0.4908,  0.2115,  0.3683,  0.3095,  0.1572,  0.1417,\n",
      "         -0.4228, -0.4791,  0.3831, -0.3412, -0.2978, -0.1592,  0.3195,  0.3858],\n",
      "        [ 0.1613,  0.0348, -0.2421,  0.0833,  0.4053, -0.2057,  0.1263, -0.1411,\n",
      "         -0.1383, -0.1747,  0.2693,  0.1726, -0.3674, -0.2290,  0.0477, -0.3855,\n",
      "         -0.3565,  0.1225,  0.2639,  0.1409,  0.4857,  0.3976, -0.4630, -0.4086,\n",
      "         -0.0102,  0.2223, -0.0178,  0.2438, -0.1798, -0.4659, -0.4809,  0.4019],\n",
      "        [ 0.3421, -0.2811, -0.4621,  0.3267, -0.1484, -0.3590, -0.4294,  0.3745,\n",
      "          0.2690, -0.2680,  0.4643, -0.4175,  0.3907, -0.2853,  0.2783, -0.0148,\n",
      "         -0.0779, -0.2489,  0.4996,  0.3118,  0.4258, -0.0356,  0.2671, -0.2102,\n",
      "         -0.4692, -0.3033, -0.0888,  0.1352, -0.1219,  0.2907,  0.1419,  0.2986],\n",
      "        [-0.4156,  0.1790, -0.1572,  0.3132,  0.1497, -0.3757, -0.3237,  0.0733,\n",
      "         -0.1786, -0.0769, -0.1552, -0.2539, -0.3796, -0.3270,  0.0506, -0.2052,\n",
      "         -0.0535,  0.0741,  0.4252, -0.2110,  0.1130,  0.4561, -0.0444,  0.3839,\n",
      "         -0.2257, -0.2363, -0.4075, -0.4276, -0.0408, -0.4145,  0.4590, -0.1103]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Wqkv_spliced: torch.Size([4, 192])\n",
      "tensor([[ 1.8937e-01, -2.8415e-01, -4.6238e-01, -4.0144e-01, -4.4552e-01,\n",
      "          1.9680e-01,  4.4328e-02,  4.4592e-01, -2.1865e-01,  2.1341e-01,\n",
      "         -3.0257e-03, -4.0493e-01,  1.3867e-01,  2.9379e-01,  3.5597e-01,\n",
      "          4.7860e-01, -3.5840e-01,  8.7989e-02, -2.9445e-01, -9.1477e-02,\n",
      "         -3.5326e-02, -1.1366e-01,  1.4490e-01, -2.6402e-03,  6.1200e-02,\n",
      "          6.7722e-02, -3.2172e-01, -3.8489e-01,  2.5546e-01, -2.6750e-01,\n",
      "         -1.2889e-01, -4.3475e-01,  1.1832e-01,  1.9844e-01,  2.2294e-01,\n",
      "          2.2098e-01,  3.4316e-01,  4.0263e-01,  1.8223e-01, -3.1401e-01,\n",
      "          1.4572e-01,  2.5463e-01,  1.4850e-01,  4.2974e-01,  3.9195e-01,\n",
      "         -4.2508e-01,  1.2758e-01,  4.7005e-01,  2.9028e-01,  2.7730e-01,\n",
      "          2.4163e-01,  4.1844e-01, -3.1530e-01,  3.5747e-01,  2.3997e-02,\n",
      "          2.9750e-01,  4.4569e-01, -3.0182e-01, -1.2491e-01,  2.3174e-01,\n",
      "          4.1144e-01, -1.1239e-01, -3.2242e-01,  2.1355e-01, -2.8637e-01,\n",
      "         -4.6897e-01,  3.3208e-01,  3.4030e-02,  4.2027e-01,  1.1778e-01,\n",
      "         -2.8841e-01,  3.7903e-01, -9.4648e-02, -5.8460e-02, -2.6424e-01,\n",
      "          2.9487e-01,  5.8353e-02, -1.1010e-01, -3.4630e-01,  2.7946e-01,\n",
      "          3.0002e-01,  2.4929e-01, -4.9700e-01, -5.6904e-02, -2.3638e-01,\n",
      "          2.2197e-01, -3.2396e-01,  1.0468e-01, -1.2194e-01,  2.9137e-01,\n",
      "         -2.7048e-01, -2.6034e-02,  4.3632e-01,  2.0930e-01,  1.6822e-01,\n",
      "         -2.1106e-02, -2.5414e-01, -1.1344e-02,  1.7114e-01, -3.4630e-01,\n",
      "          3.0182e-01, -1.7840e-01, -4.4955e-02,  4.7685e-01,  2.3429e-01,\n",
      "         -2.0993e-04, -3.1590e-01,  1.4933e-01, -3.4794e-01,  4.0296e-02,\n",
      "         -5.2708e-02, -3.9691e-01,  3.8710e-02, -7.3864e-02,  3.9559e-01,\n",
      "         -2.1087e-01,  4.0306e-01, -3.9320e-01, -9.2826e-02,  4.8890e-01,\n",
      "          4.7009e-01, -4.8984e-01,  1.8414e-02,  4.3571e-01,  3.3965e-02,\n",
      "         -3.1877e-01,  2.5281e-01, -3.5005e-01, -2.3904e-01,  4.2611e-01,\n",
      "         -2.9801e-01,  8.4876e-02,  4.0464e-01, -1.8968e-01, -3.6074e-01,\n",
      "         -3.8140e-03, -3.8703e-01, -1.0132e-01, -4.1207e-01, -1.3494e-04,\n",
      "          3.9459e-01,  2.9657e-01,  2.0455e-01, -1.1478e-01,  4.8224e-01,\n",
      "         -2.4734e-01, -1.0230e-01,  9.2335e-02,  2.4343e-02, -3.5300e-01,\n",
      "          2.0875e-01,  4.3911e-01, -4.2997e-01, -1.1652e-01,  2.6931e-01,\n",
      "          1.5951e-01,  4.8167e-01, -1.0938e-01,  6.8503e-02,  9.3351e-02,\n",
      "         -3.8608e-01, -4.6630e-03, -3.0631e-01, -3.1452e-01,  3.6388e-01,\n",
      "          3.9371e-01, -4.8803e-01, -2.3385e-01, -2.9818e-01, -3.0848e-01,\n",
      "          2.9630e-01,  7.3386e-02, -1.2276e-02,  1.8542e-01, -4.1924e-02,\n",
      "         -4.0357e-01, -1.0265e-01,  2.8177e-01, -4.9081e-01,  2.1151e-01,\n",
      "          3.6830e-01,  3.0946e-01,  1.5720e-01,  1.4172e-01, -4.2283e-01,\n",
      "         -4.7915e-01,  3.8314e-01, -3.4119e-01, -2.9776e-01, -1.5916e-01,\n",
      "          3.1954e-01,  3.8582e-01],\n",
      "        [ 2.8393e-02, -2.1507e-01,  7.3449e-02, -3.5425e-02,  8.4971e-03,\n",
      "          6.4568e-02, -4.1707e-01,  4.9329e-01,  6.2499e-02, -3.1489e-01,\n",
      "         -2.6528e-01, -2.0642e-01,  2.8555e-01,  2.0611e-01, -2.4231e-01,\n",
      "          1.9236e-01, -2.4110e-01,  3.8176e-01, -3.1023e-01,  5.9767e-02,\n",
      "         -3.1687e-01, -4.0727e-01, -2.7802e-01, -4.0448e-01, -2.9654e-01,\n",
      "          4.2772e-01, -2.7135e-02, -9.2047e-02, -1.7364e-01, -3.5465e-02,\n",
      "          4.5284e-02, -1.1347e-01,  1.1493e-01,  2.4035e-02,  4.2536e-01,\n",
      "         -2.7699e-01, -2.6960e-01,  4.5004e-01,  1.0166e-01,  4.5971e-01,\n",
      "         -4.6756e-01,  2.6671e-01,  6.4181e-02,  3.1845e-01, -4.9001e-01,\n",
      "          1.8032e-01,  1.0662e-01, -4.9253e-01, -8.3386e-02, -3.9306e-01,\n",
      "         -4.4291e-01, -7.9380e-02,  5.9529e-03,  1.9958e-02, -2.0477e-01,\n",
      "         -3.1541e-01, -1.0251e-01, -3.1129e-01, -2.1621e-02, -1.0289e-02,\n",
      "         -4.2008e-01, -4.8188e-02,  3.7436e-01, -1.1315e-01, -4.9122e-01,\n",
      "         -4.0513e-01,  2.0755e-01,  1.1163e-01, -4.9098e-01,  4.1759e-01,\n",
      "          1.6288e-01,  6.8331e-04, -3.7547e-01,  1.7793e-01, -1.0203e-01,\n",
      "         -3.3526e-01,  3.0938e-01,  3.5360e-01, -4.9011e-01, -3.8699e-01,\n",
      "          2.1129e-01, -4.3664e-01, -2.1207e-01, -3.9717e-01,  3.2691e-01,\n",
      "         -1.7923e-01,  3.1414e-01,  4.3294e-01,  4.4856e-01,  4.4316e-01,\n",
      "          2.7930e-01, -3.8357e-01,  3.5021e-01,  3.0501e-01,  3.6156e-01,\n",
      "         -3.5105e-01,  3.5966e-01,  4.6147e-01,  2.8881e-01,  1.0856e-02,\n",
      "          3.1861e-01,  3.0779e-02,  2.5312e-02, -1.1220e-01,  4.2442e-01,\n",
      "          8.0456e-02, -4.1062e-01,  4.2825e-01,  1.9142e-01,  1.2905e-01,\n",
      "         -4.1203e-01, -4.0837e-01,  1.0011e-01,  1.0266e-01,  1.1884e-01,\n",
      "         -2.9486e-01, -1.7023e-01, -3.9583e-01,  4.3081e-01, -4.8520e-01,\n",
      "          8.0888e-03,  1.3026e-02,  3.0812e-02,  3.1113e-01,  5.4653e-02,\n",
      "          3.3968e-01, -2.7318e-02, -1.7378e-01, -1.9805e-01,  1.3532e-02,\n",
      "         -7.3870e-02, -2.5023e-01,  4.2612e-01,  1.9641e-01,  4.4966e-01,\n",
      "          3.5824e-01, -3.8031e-01,  6.2354e-02,  2.4223e-01, -2.8999e-01,\n",
      "         -2.9022e-01,  1.9836e-01,  2.7220e-01,  3.3876e-01,  2.3070e-01,\n",
      "         -3.7776e-01,  3.4646e-01,  3.5032e-01, -2.7519e-01, -3.7461e-01,\n",
      "         -1.4845e-01, -4.0748e-01,  4.1486e-01,  4.7666e-01,  2.3587e-01,\n",
      "          3.7843e-01,  4.7986e-01,  3.1608e-01,  4.0587e-01,  4.5123e-01,\n",
      "          1.6128e-01,  3.4837e-02, -2.4213e-01,  8.3348e-02,  4.0535e-01,\n",
      "         -2.0574e-01,  1.2630e-01, -1.4109e-01, -1.3830e-01, -1.7473e-01,\n",
      "          2.6931e-01,  1.7263e-01, -3.6738e-01, -2.2899e-01,  4.7750e-02,\n",
      "         -3.8553e-01, -3.5645e-01,  1.2255e-01,  2.6394e-01,  1.4087e-01,\n",
      "          4.8568e-01,  3.9760e-01, -4.6300e-01, -4.0858e-01, -1.0156e-02,\n",
      "          2.2234e-01, -1.7810e-02,  2.4384e-01, -1.7978e-01, -4.6589e-01,\n",
      "         -4.8086e-01,  4.0185e-01],\n",
      "        [ 2.4316e-01,  5.1094e-02,  5.5459e-02, -4.2938e-01, -1.7976e-01,\n",
      "          3.1237e-01, -3.2596e-02, -2.8321e-01,  2.7905e-01, -2.7461e-01,\n",
      "          1.0299e-01,  1.8728e-01, -1.6608e-01,  1.3264e-01,  2.7119e-01,\n",
      "          6.4278e-02,  3.1660e-01,  2.2600e-01, -4.8537e-01,  4.2158e-01,\n",
      "          4.8353e-01, -1.8455e-01,  8.0709e-02, -1.7443e-01,  4.0629e-01,\n",
      "          4.3725e-01, -1.5402e-01, -1.8739e-03,  4.4505e-01, -3.8958e-01,\n",
      "          1.8676e-01,  4.1720e-01, -4.8323e-01,  1.4650e-01,  3.3617e-01,\n",
      "          4.6191e-01,  4.8709e-01,  4.2077e-01,  2.7973e-01,  1.4151e-01,\n",
      "          4.8756e-01, -9.2613e-02, -8.1956e-02,  3.6287e-01, -4.3972e-01,\n",
      "         -3.8881e-01, -5.7135e-02,  3.9335e-01,  4.2260e-01, -4.6757e-01,\n",
      "         -2.9373e-01, -3.9142e-01, -4.4682e-01,  3.2180e-01,  3.9332e-01,\n",
      "          5.9391e-02, -4.0696e-01,  4.0756e-01, -3.7403e-01, -3.4418e-01,\n",
      "         -6.7751e-02,  3.9745e-01, -3.6439e-01,  3.8382e-01, -7.6625e-02,\n",
      "         -3.1299e-01,  4.9208e-02, -9.0591e-02,  2.8831e-01, -4.6503e-01,\n",
      "         -7.9162e-02, -1.3593e-01,  2.0823e-01,  1.3763e-01, -2.5408e-01,\n",
      "          1.9507e-01,  3.0950e-01, -4.6143e-01,  4.1417e-02, -3.1686e-02,\n",
      "         -8.1151e-02, -7.1416e-02,  1.4370e-01, -4.2339e-01, -3.9791e-01,\n",
      "          1.2989e-02,  9.9221e-02,  6.6198e-02,  2.0977e-01,  7.8000e-02,\n",
      "         -1.7535e-01,  4.6218e-01, -2.5272e-01, -4.7835e-01,  1.2324e-01,\n",
      "         -2.4548e-01,  1.5080e-01, -5.3550e-02,  1.2076e-01,  3.8934e-01,\n",
      "          3.8205e-01, -4.0994e-01, -1.1160e-01,  2.9541e-01, -3.4564e-01,\n",
      "          2.6261e-01,  5.1345e-02, -3.3273e-02,  3.0493e-01, -7.7605e-02,\n",
      "         -1.5532e-01,  1.4562e-01, -2.4403e-01,  2.2133e-01, -4.4685e-01,\n",
      "         -9.4684e-02,  2.7953e-01, -4.0533e-01,  2.5822e-01,  4.4835e-01,\n",
      "          3.3890e-01,  1.3830e-01, -2.2497e-01,  9.8689e-02,  2.8423e-01,\n",
      "         -2.7537e-01, -2.9027e-02,  3.8509e-01, -1.0216e-01,  8.3434e-02,\n",
      "         -4.5676e-01, -3.2859e-01, -1.8846e-01, -3.8350e-01,  4.8841e-01,\n",
      "          7.9258e-02, -3.2171e-01, -4.3615e-02, -3.8480e-01, -6.7773e-02,\n",
      "         -4.7649e-01, -4.3129e-01, -4.9387e-01,  4.9946e-01,  2.1872e-01,\n",
      "          1.6125e-01,  4.2134e-01,  4.6237e-01, -2.6883e-02, -6.6878e-02,\n",
      "         -3.0271e-01,  4.1654e-01,  3.9426e-01,  6.2786e-03,  3.8204e-01,\n",
      "         -2.3100e-01,  1.6674e-01,  1.9269e-02, -1.5630e-01,  1.3106e-01,\n",
      "          3.4213e-01, -2.8111e-01, -4.6205e-01,  3.2671e-01, -1.4839e-01,\n",
      "         -3.5897e-01, -4.2942e-01,  3.7445e-01,  2.6903e-01, -2.6800e-01,\n",
      "          4.6427e-01, -4.1747e-01,  3.9074e-01, -2.8527e-01,  2.7827e-01,\n",
      "         -1.4761e-02, -7.7906e-02, -2.4888e-01,  4.9965e-01,  3.1182e-01,\n",
      "          4.2581e-01, -3.5637e-02,  2.6706e-01, -2.1018e-01, -4.6920e-01,\n",
      "         -3.0328e-01, -8.8848e-02,  1.3523e-01, -1.2193e-01,  2.9067e-01,\n",
      "          1.4192e-01,  2.9863e-01],\n",
      "        [ 2.1939e-02, -2.1620e-01, -1.3989e-01,  2.0001e-01, -2.6691e-01,\n",
      "         -2.2999e-01,  3.5089e-01,  4.2780e-01, -2.5216e-01,  2.5742e-01,\n",
      "         -4.2682e-01, -2.3400e-01,  2.5624e-01,  5.2858e-02, -4.4735e-02,\n",
      "         -2.7120e-01,  2.7121e-01, -1.1131e-02, -2.0003e-01,  3.6435e-01,\n",
      "         -1.3997e-01, -3.0246e-01,  2.5170e-01, -1.9140e-01, -4.2226e-01,\n",
      "         -2.8699e-01,  4.4266e-02,  6.2823e-02,  1.7323e-01, -8.4048e-02,\n",
      "          2.8983e-02,  7.1588e-02, -4.9993e-01,  4.3573e-01, -1.6429e-01,\n",
      "         -2.9448e-01, -4.0873e-01,  2.7543e-01, -3.6739e-01,  1.9690e-01,\n",
      "         -2.6216e-01,  2.5825e-01, -1.5555e-01, -1.9827e-01,  3.1932e-01,\n",
      "         -2.0889e-01, -2.8329e-01, -1.9644e-01,  7.6186e-02, -4.7227e-01,\n",
      "          1.4629e-01,  3.2163e-01,  4.3594e-01, -2.9314e-01,  2.6589e-01,\n",
      "         -6.6499e-02,  2.4181e-01,  4.1164e-01,  4.9924e-01,  9.3671e-02,\n",
      "         -4.7423e-01, -3.0037e-01, -3.6135e-01, -4.7988e-01,  4.1612e-01,\n",
      "          7.5813e-02, -4.3853e-01, -1.3715e-01,  2.6466e-01, -4.1169e-01,\n",
      "         -2.6106e-01,  2.4077e-01,  2.4765e-01,  2.0474e-01, -4.8767e-01,\n",
      "          3.4312e-01,  3.0089e-01, -2.6358e-01, -2.3430e-01,  3.1284e-01,\n",
      "          4.9953e-01, -3.5684e-02,  1.9416e-01,  4.6613e-01,  4.7486e-01,\n",
      "          7.1292e-02, -1.6498e-01,  2.6788e-01,  3.3629e-01,  4.4775e-01,\n",
      "          4.7669e-01, -4.7820e-01, -4.4081e-01, -4.0668e-01,  2.4537e-01,\n",
      "         -2.8373e-01,  2.6604e-01, -1.5824e-01, -2.7501e-01,  4.6280e-01,\n",
      "          4.3243e-01, -6.9161e-03,  3.4847e-01, -4.3560e-01, -3.1419e-01,\n",
      "         -5.0649e-02, -1.4905e-01, -2.1218e-01,  4.0496e-01,  9.8498e-02,\n",
      "          2.1212e-02, -2.1080e-01,  3.0203e-01, -3.2247e-02, -3.5137e-01,\n",
      "         -4.0825e-01,  3.6829e-01,  7.6268e-02, -4.5142e-01, -7.0428e-02,\n",
      "         -3.8538e-02, -4.0860e-01, -2.3721e-01, -2.6784e-01,  2.6255e-01,\n",
      "          4.1158e-01, -1.2125e-01,  4.8033e-01, -3.1060e-01,  4.8208e-04,\n",
      "         -1.2071e-01,  3.9326e-02,  3.2534e-01,  1.0235e-01,  2.3560e-01,\n",
      "          3.7913e-01, -4.0633e-01, -3.9607e-01,  4.2034e-01,  9.0152e-02,\n",
      "         -3.9707e-01,  2.2478e-01,  4.7630e-01,  8.5905e-02, -3.4227e-01,\n",
      "         -2.6248e-01,  2.0666e-01,  2.5354e-01,  4.9642e-01, -3.3560e-01,\n",
      "         -2.5921e-01, -1.5643e-01,  2.4026e-01,  2.8152e-01,  7.2462e-02,\n",
      "          5.1272e-02,  4.5968e-01,  2.5359e-02, -3.2823e-01, -9.0428e-02,\n",
      "         -4.1559e-01,  1.7903e-01, -1.5719e-01,  3.1318e-01,  1.4970e-01,\n",
      "         -3.7570e-01, -3.2366e-01,  7.3289e-02, -1.7857e-01, -7.6912e-02,\n",
      "         -1.5517e-01, -2.5391e-01, -3.7961e-01, -3.2698e-01,  5.0610e-02,\n",
      "         -2.0522e-01, -5.3520e-02,  7.4136e-02,  4.2519e-01, -2.1097e-01,\n",
      "          1.1299e-01,  4.5611e-01, -4.4404e-02,  3.8389e-01, -2.2574e-01,\n",
      "         -2.3627e-01, -4.0755e-01, -4.2760e-01, -4.0825e-02, -4.1447e-01,\n",
      "          4.5897e-01, -1.1031e-01]], grad_fn=<CatBackward0>)\n",
      "xqkv: torch.Size([1, 3, 192])\n",
      "tensor([[[ 1.6549e-01,  5.4076e-01,  4.8795e-01, -6.3072e-01,  4.0208e-01,\n",
      "           6.2984e-01, -5.6808e-01, -1.2378e+00,  8.1524e-01, -7.9636e-01,\n",
      "           8.0192e-01,  7.8446e-01, -6.7752e-01, -7.4751e-02,  2.4583e-01,\n",
      "           2.6009e-01,  1.4017e-01,  2.1559e-01, -1.0835e-01, -2.4438e-02,\n",
      "           8.2970e-01,  3.3080e-01, -3.3757e-01,  1.2043e-01,  1.1275e+00,\n",
      "           8.9723e-01, -1.0124e-01,  8.4529e-02,  1.5678e-01, -2.0698e-01,\n",
      "           2.3246e-01,  5.9252e-01,  1.3468e-01, -5.9097e-01,  5.1714e-01,\n",
      "           9.2165e-01,  1.0710e+00, -1.4484e-01,  8.0683e-01, -2.6919e-02,\n",
      "           9.5135e-01, -6.4525e-01,  6.9601e-02,  5.1452e-01, -1.1561e+00,\n",
      "           4.2188e-02,  3.0234e-01,  5.9142e-01,  2.5691e-01,  7.6558e-02,\n",
      "          -6.5083e-01, -1.1460e+00, -1.0578e+00,  6.6808e-01,  6.1143e-02,\n",
      "           6.0631e-02, -1.0521e+00,  1.0921e-02, -1.1544e+00, -6.5812e-01,\n",
      "           4.9425e-01,  9.8971e-01,  2.4428e-01,  1.1058e+00, -5.6350e-01,\n",
      "          -2.4191e-01,  5.6628e-01,  7.9870e-02, -2.2138e-01, -2.7185e-03,\n",
      "           4.2848e-01, -7.0637e-01, -6.2744e-02, -1.3927e-01,  5.7964e-01,\n",
      "          -4.0731e-01, -1.4734e-01, -1.1736e-01,  6.0675e-01, -6.1850e-01,\n",
      "          -1.0202e+00, -1.1061e-01,  1.1528e-01, -1.1643e+00, -1.1206e+00,\n",
      "          -1.8263e-01,  4.9662e-01, -4.1570e-01, -2.4815e-01, -7.6545e-01,\n",
      "          -8.4023e-01,  1.3267e+00,  1.5299e-01, -5.9590e-02, -3.3702e-01,\n",
      "           1.8291e-01, -1.4230e-01,  1.5036e-01,  4.6662e-01, -9.3812e-02,\n",
      "          -3.7652e-01, -3.9600e-01, -6.5096e-01,  8.1178e-01, -6.5435e-02,\n",
      "           3.8346e-01,  4.6706e-01,  1.8640e-01, -1.1781e-01, -2.7227e-01,\n",
      "          -1.6118e-01,  7.1106e-01, -7.8050e-01,  3.3822e-01, -1.7873e-01,\n",
      "           6.3649e-01, -4.0824e-01, -3.8760e-01,  1.0118e+00,  4.5315e-01,\n",
      "           2.4505e-01,  1.0177e+00,  8.8149e-02,  3.0631e-01, -8.7508e-02,\n",
      "          -8.4092e-01,  3.8644e-02, -1.1003e-01,  4.8256e-01, -9.8314e-02,\n",
      "          -2.1318e-01, -4.7010e-01, -9.4338e-01, -5.4062e-01,  3.4701e-01,\n",
      "          -5.1625e-01,  4.5164e-01,  6.0066e-01, -9.3394e-01, -1.9711e-01,\n",
      "          -1.1165e-01, -1.0093e+00, -1.4347e+00,  4.8658e-01,  5.4822e-01,\n",
      "           7.3833e-01,  2.0144e-01,  8.8228e-02, -7.8738e-01,  6.2875e-01,\n",
      "          -4.3729e-02,  5.6492e-01,  2.6260e-01, -4.0964e-01,  1.9974e-01,\n",
      "          -4.5514e-01, -7.6873e-01,  9.7344e-03,  2.5824e-01,  2.1749e-01,\n",
      "           1.2113e+00, -6.0993e-01, -1.4653e-01,  4.2472e-02, -6.0475e-01,\n",
      "          -1.1302e-02,  2.0400e-01,  4.4914e-01,  7.4175e-01, -4.4345e-02,\n",
      "           6.3327e-01, -1.5046e-01,  1.0824e+00,  9.8328e-02,  2.6755e-01,\n",
      "           5.1357e-01,  6.4508e-02, -5.4810e-01,  1.4191e-01,  5.8725e-01,\n",
      "           1.2462e-01, -9.1792e-01,  3.4850e-01, -8.7489e-01, -1.3622e-02,\n",
      "           2.0743e-01,  3.4861e-01,  9.5751e-01,  6.8704e-02,  1.0924e+00,\n",
      "          -6.4916e-01,  3.1646e-01],\n",
      "         [-5.4763e-01, -5.3452e-02,  2.4608e-01,  1.1239e+00,  5.5752e-01,\n",
      "          -7.5870e-01, -3.1793e-02,  5.4430e-01, -3.8361e-01,  2.6360e-01,\n",
      "          -4.7404e-01, -1.9703e-01,  4.1643e-01, -3.4265e-01, -8.8174e-01,\n",
      "          -5.0832e-01, -2.6820e-01, -2.8545e-01,  8.4457e-01, -4.8091e-01,\n",
      "          -1.0145e+00,  9.4430e-02, -2.8207e-01,  3.5907e-02, -1.0622e+00,\n",
      "          -7.2554e-01,  5.2526e-01,  2.8888e-01, -9.8235e-01,  8.3390e-01,\n",
      "          -1.8843e-01, -4.0329e-01,  5.9334e-01, -2.2129e-01, -6.2421e-01,\n",
      "          -1.2232e+00, -1.4060e+00, -7.2171e-01, -7.2921e-01,  2.9879e-01,\n",
      "          -1.2846e+00,  1.8991e-01, -8.3729e-03, -8.9744e-01,  3.5141e-01,\n",
      "           1.0074e+00, -6.6844e-02, -1.3623e+00, -9.6802e-01,  2.1351e-01,\n",
      "           1.6907e-01,  4.4087e-01,  1.2016e+00, -9.4827e-01, -6.8738e-01,\n",
      "          -5.1246e-01,  4.0295e-01, -4.4600e-01,  9.3841e-01,  4.4603e-01,\n",
      "          -5.9735e-01, -7.4381e-01,  9.1339e-01, -1.0807e+00,  2.9669e-01,\n",
      "           7.5152e-01, -4.2770e-01,  1.2656e-01, -9.5302e-01,  7.4039e-01,\n",
      "           3.3459e-01,  3.4687e-02, -3.6131e-01, -2.5128e-02,  4.0147e-01,\n",
      "          -5.8782e-01, -3.1363e-01,  9.4393e-01, -1.2432e-01, -2.2002e-01,\n",
      "           2.0641e-01, -2.9261e-01,  1.2243e-01,  7.8026e-01,  1.2214e+00,\n",
      "          -2.5263e-01,  1.6418e-01,  1.1538e-01,  8.0786e-02,  2.5797e-02,\n",
      "           8.4165e-01, -1.1538e+00,  7.8557e-02,  6.4045e-01, -7.6072e-02,\n",
      "           1.6093e-01,  2.1664e-01,  2.5405e-01, -3.1994e-01, -2.0724e-01,\n",
      "          -5.7318e-01,  8.6142e-01,  3.8214e-01, -1.1168e+00,  4.8492e-01,\n",
      "          -4.3693e-01, -9.3433e-02,  5.4788e-02,  1.3893e-03,  2.0320e-01,\n",
      "           1.2525e-01, -2.1674e-01,  5.6154e-01, -2.8954e-01,  3.7415e-01,\n",
      "           2.5376e-02, -7.3140e-01,  8.5610e-01, -3.5473e-01, -1.4176e+00,\n",
      "          -9.6864e-01, -1.3288e-02,  2.9266e-01, -4.7687e-01, -3.8599e-01,\n",
      "           1.0546e+00, -2.1119e-01, -2.7657e-01,  1.4572e-01, -4.7360e-01,\n",
      "           9.4121e-01,  3.9996e-01,  3.3968e-01,  9.4683e-01, -2.5347e-01,\n",
      "           1.8845e-01,  5.1758e-01,  2.3493e-02,  1.2752e+00,  1.7756e-02,\n",
      "           2.1542e-01,  6.9701e-01,  1.0148e+00, -5.7945e-01, -7.8925e-01,\n",
      "          -3.6830e-01, -4.0139e-01, -6.0489e-01,  9.9941e-02,  8.1360e-02,\n",
      "           1.8396e-01, -1.3217e+00, -5.0506e-02,  4.1946e-01, -7.3284e-01,\n",
      "           4.7257e-01, -2.5559e-01,  2.1171e-01,  2.7360e-01, -1.2506e-01,\n",
      "          -3.8085e-01,  5.7916e-01,  8.6301e-01, -1.5075e-01,  2.2129e-01,\n",
      "           6.1207e-02,  1.0552e+00, -4.9996e-01, -3.6802e-01,  5.9334e-01,\n",
      "          -9.7324e-01,  6.4218e-01, -9.9331e-01,  1.0675e-01, -4.0532e-01,\n",
      "           7.8967e-02,  2.6407e-02,  2.9622e-01, -1.8056e-01, -7.2573e-01,\n",
      "          -7.5270e-01,  1.9012e-01, -8.2151e-01,  2.1607e-01,  1.0483e+00,\n",
      "           9.1064e-01, -3.2241e-01, -2.4386e-02,  3.4452e-01, -7.6491e-01,\n",
      "          -5.3659e-01, -6.7671e-01],\n",
      "         [ 1.2376e-01,  4.0339e-01,  2.3305e-01, -6.8447e-01,  3.4414e-01,\n",
      "           6.4568e-01, -6.8054e-01, -9.1918e-01,  6.1008e-01, -6.1686e-01,\n",
      "           8.7578e-01,  4.9808e-01, -5.6670e-01,  1.5259e-02,  2.9244e-01,\n",
      "           6.2184e-01, -3.8680e-01,  1.6070e-01,  6.4926e-02, -4.6873e-01,\n",
      "           5.4078e-01,  4.5886e-01, -4.1276e-01,  2.6811e-01,  1.0497e+00,\n",
      "           8.0557e-01, -2.1812e-01, -1.7520e-01, -3.9435e-02, -9.9788e-02,\n",
      "           3.2815e-02,  4.0657e-02,  6.9540e-01, -7.2106e-01,  5.3530e-01,\n",
      "           8.5985e-01,  1.1099e+00, -2.3143e-01,  8.8736e-01, -3.4270e-01,\n",
      "           8.0318e-01, -5.1199e-01,  2.7114e-01,  6.4548e-01, -8.0320e-01,\n",
      "           1.1564e-01,  5.2626e-01,  6.6987e-01,  1.3762e-01,  6.7702e-01,\n",
      "          -4.1063e-01, -7.7865e-01, -1.1334e+00,  7.9482e-01, -2.7875e-01,\n",
      "           2.0510e-01, -6.3097e-01, -5.9479e-01, -1.1856e+00, -3.4285e-01,\n",
      "           9.2849e-01,  7.8564e-01,  4.3443e-01,  1.1676e+00, -8.7688e-01,\n",
      "          -3.8796e-01,  9.1218e-01,  2.1451e-01, -2.7854e-01,  5.3529e-01,\n",
      "           4.1184e-01, -4.8413e-01, -3.6553e-01, -3.2199e-01,  7.4997e-01,\n",
      "          -5.0050e-01, -3.9171e-01,  2.2233e-01,  4.2524e-01, -5.7412e-01,\n",
      "          -9.5982e-01,  6.5305e-02, -3.5821e-01, -1.1387e+00, -1.1705e+00,\n",
      "          -9.6298e-02,  3.2425e-01, -4.6186e-01, -5.4277e-01, -7.7247e-01,\n",
      "          -1.0513e+00,  1.1767e+00,  7.5789e-01,  5.3176e-01, -3.7679e-01,\n",
      "           4.0103e-01, -4.6044e-01,  2.6752e-01,  6.1724e-01, -7.1117e-01,\n",
      "          -5.6717e-01, -2.4660e-01, -7.3636e-01,  1.0697e+00,  4.3367e-01,\n",
      "           2.4642e-01,  2.7184e-01,  4.0518e-01, -6.5018e-01, -2.2766e-01,\n",
      "          -1.3438e-01,  4.3224e-01, -7.1186e-01,  1.7707e-01,  4.7156e-01,\n",
      "           6.9799e-01, -4.8596e-01, -4.3092e-01,  9.9470e-01,  4.6310e-01,\n",
      "           3.3402e-01,  7.9046e-01,  3.2722e-01,  6.2832e-01, -3.3484e-01,\n",
      "          -9.9128e-01,  2.5118e-01, -7.4657e-01,  5.0250e-01,  1.0714e-01,\n",
      "          -7.2217e-02, -2.4949e-01, -6.7467e-01, -4.4289e-01, -2.2493e-01,\n",
      "          -6.8197e-01,  5.4005e-01,  7.1734e-01, -1.0832e+00, -2.0937e-01,\n",
      "           5.4423e-01, -6.3525e-01, -1.1652e+00,  1.0305e-01,  8.4625e-01,\n",
      "           5.6099e-01, -1.7069e-01, -2.0925e-01, -9.5910e-01,  5.5533e-01,\n",
      "           3.5286e-01,  6.0178e-01, -2.9727e-01, -5.5380e-01,  1.1671e-01,\n",
      "          -2.0964e-01, -7.1778e-01, -5.4826e-02,  5.4534e-01,  2.5835e-01,\n",
      "           9.3418e-01, -5.0373e-01, -5.7106e-03, -4.5693e-01, -3.2171e-01,\n",
      "           5.6993e-01,  3.0318e-01,  4.1628e-02,  4.5360e-01, -4.8925e-02,\n",
      "           6.0241e-01,  2.5607e-01,  9.4932e-01,  4.8954e-01,  5.6108e-02,\n",
      "           3.2881e-01,  4.5292e-02, -2.4472e-01, -5.9822e-01,  6.1065e-01,\n",
      "           7.6841e-02, -8.5058e-01,  2.6255e-01, -8.3137e-01,  1.0332e-01,\n",
      "           2.0895e-01,  7.8262e-01,  8.4474e-01, -3.2852e-02,  9.3850e-01,\n",
      "          -7.4797e-01,  4.3390e-01]]], grad_fn=<UnsafeViewBackward0>)\n",
      "xq: torch.Size([1, 3, 128])\n",
      "tensor([[[ 1.6549e-01,  5.4076e-01,  4.8795e-01, -6.3072e-01,  4.0208e-01,\n",
      "           6.2984e-01, -5.6808e-01, -1.2378e+00,  8.1524e-01, -7.9636e-01,\n",
      "           8.0192e-01,  7.8446e-01, -6.7752e-01, -7.4751e-02,  2.4583e-01,\n",
      "           2.6009e-01,  1.4017e-01,  2.1559e-01, -1.0835e-01, -2.4438e-02,\n",
      "           8.2970e-01,  3.3080e-01, -3.3757e-01,  1.2043e-01,  1.1275e+00,\n",
      "           8.9723e-01, -1.0124e-01,  8.4529e-02,  1.5678e-01, -2.0698e-01,\n",
      "           2.3246e-01,  5.9252e-01,  1.3468e-01, -5.9097e-01,  5.1714e-01,\n",
      "           9.2165e-01,  1.0710e+00, -1.4484e-01,  8.0683e-01, -2.6919e-02,\n",
      "           9.5135e-01, -6.4525e-01,  6.9601e-02,  5.1452e-01, -1.1561e+00,\n",
      "           4.2188e-02,  3.0234e-01,  5.9142e-01,  2.5691e-01,  7.6558e-02,\n",
      "          -6.5083e-01, -1.1460e+00, -1.0578e+00,  6.6808e-01,  6.1143e-02,\n",
      "           6.0631e-02, -1.0521e+00,  1.0921e-02, -1.1544e+00, -6.5812e-01,\n",
      "           4.9425e-01,  9.8971e-01,  2.4428e-01,  1.1058e+00, -5.6350e-01,\n",
      "          -2.4191e-01,  5.6628e-01,  7.9870e-02, -2.2138e-01, -2.7185e-03,\n",
      "           4.2848e-01, -7.0637e-01, -6.2744e-02, -1.3927e-01,  5.7964e-01,\n",
      "          -4.0731e-01, -1.4734e-01, -1.1736e-01,  6.0675e-01, -6.1850e-01,\n",
      "          -1.0202e+00, -1.1061e-01,  1.1528e-01, -1.1643e+00, -1.1206e+00,\n",
      "          -1.8263e-01,  4.9662e-01, -4.1570e-01, -2.4815e-01, -7.6545e-01,\n",
      "          -8.4023e-01,  1.3267e+00,  1.5299e-01, -5.9590e-02, -3.3702e-01,\n",
      "           1.8291e-01, -1.4230e-01,  1.5036e-01,  4.6662e-01, -9.3812e-02,\n",
      "          -3.7652e-01, -3.9600e-01, -6.5096e-01,  8.1178e-01, -6.5435e-02,\n",
      "           3.8346e-01,  4.6706e-01,  1.8640e-01, -1.1781e-01, -2.7227e-01,\n",
      "          -1.6118e-01,  7.1106e-01, -7.8050e-01,  3.3822e-01, -1.7873e-01,\n",
      "           6.3649e-01, -4.0824e-01, -3.8760e-01,  1.0118e+00,  4.5315e-01,\n",
      "           2.4505e-01,  1.0177e+00,  8.8149e-02,  3.0631e-01, -8.7508e-02,\n",
      "          -8.4092e-01,  3.8644e-02, -1.1003e-01],\n",
      "         [-5.4763e-01, -5.3452e-02,  2.4608e-01,  1.1239e+00,  5.5752e-01,\n",
      "          -7.5870e-01, -3.1793e-02,  5.4430e-01, -3.8361e-01,  2.6360e-01,\n",
      "          -4.7404e-01, -1.9703e-01,  4.1643e-01, -3.4265e-01, -8.8174e-01,\n",
      "          -5.0832e-01, -2.6820e-01, -2.8545e-01,  8.4457e-01, -4.8091e-01,\n",
      "          -1.0145e+00,  9.4430e-02, -2.8207e-01,  3.5907e-02, -1.0622e+00,\n",
      "          -7.2554e-01,  5.2526e-01,  2.8888e-01, -9.8235e-01,  8.3390e-01,\n",
      "          -1.8843e-01, -4.0329e-01,  5.9334e-01, -2.2129e-01, -6.2421e-01,\n",
      "          -1.2232e+00, -1.4060e+00, -7.2171e-01, -7.2921e-01,  2.9879e-01,\n",
      "          -1.2846e+00,  1.8991e-01, -8.3729e-03, -8.9744e-01,  3.5141e-01,\n",
      "           1.0074e+00, -6.6844e-02, -1.3623e+00, -9.6802e-01,  2.1351e-01,\n",
      "           1.6907e-01,  4.4087e-01,  1.2016e+00, -9.4827e-01, -6.8738e-01,\n",
      "          -5.1246e-01,  4.0295e-01, -4.4600e-01,  9.3841e-01,  4.4603e-01,\n",
      "          -5.9735e-01, -7.4381e-01,  9.1339e-01, -1.0807e+00,  2.9669e-01,\n",
      "           7.5152e-01, -4.2770e-01,  1.2656e-01, -9.5302e-01,  7.4039e-01,\n",
      "           3.3459e-01,  3.4687e-02, -3.6131e-01, -2.5128e-02,  4.0147e-01,\n",
      "          -5.8782e-01, -3.1363e-01,  9.4393e-01, -1.2432e-01, -2.2002e-01,\n",
      "           2.0641e-01, -2.9261e-01,  1.2243e-01,  7.8026e-01,  1.2214e+00,\n",
      "          -2.5263e-01,  1.6418e-01,  1.1538e-01,  8.0786e-02,  2.5797e-02,\n",
      "           8.4165e-01, -1.1538e+00,  7.8557e-02,  6.4045e-01, -7.6072e-02,\n",
      "           1.6093e-01,  2.1664e-01,  2.5405e-01, -3.1994e-01, -2.0724e-01,\n",
      "          -5.7318e-01,  8.6142e-01,  3.8214e-01, -1.1168e+00,  4.8492e-01,\n",
      "          -4.3693e-01, -9.3433e-02,  5.4788e-02,  1.3893e-03,  2.0320e-01,\n",
      "           1.2525e-01, -2.1674e-01,  5.6154e-01, -2.8954e-01,  3.7415e-01,\n",
      "           2.5376e-02, -7.3140e-01,  8.5610e-01, -3.5473e-01, -1.4176e+00,\n",
      "          -9.6864e-01, -1.3288e-02,  2.9266e-01, -4.7687e-01, -3.8599e-01,\n",
      "           1.0546e+00, -2.1119e-01, -2.7657e-01],\n",
      "         [ 1.2376e-01,  4.0339e-01,  2.3305e-01, -6.8447e-01,  3.4414e-01,\n",
      "           6.4568e-01, -6.8054e-01, -9.1918e-01,  6.1008e-01, -6.1686e-01,\n",
      "           8.7578e-01,  4.9808e-01, -5.6670e-01,  1.5259e-02,  2.9244e-01,\n",
      "           6.2184e-01, -3.8680e-01,  1.6070e-01,  6.4926e-02, -4.6873e-01,\n",
      "           5.4078e-01,  4.5886e-01, -4.1276e-01,  2.6811e-01,  1.0497e+00,\n",
      "           8.0557e-01, -2.1812e-01, -1.7520e-01, -3.9435e-02, -9.9788e-02,\n",
      "           3.2815e-02,  4.0657e-02,  6.9540e-01, -7.2106e-01,  5.3530e-01,\n",
      "           8.5985e-01,  1.1099e+00, -2.3143e-01,  8.8736e-01, -3.4270e-01,\n",
      "           8.0318e-01, -5.1199e-01,  2.7114e-01,  6.4548e-01, -8.0320e-01,\n",
      "           1.1564e-01,  5.2626e-01,  6.6987e-01,  1.3762e-01,  6.7702e-01,\n",
      "          -4.1063e-01, -7.7865e-01, -1.1334e+00,  7.9482e-01, -2.7875e-01,\n",
      "           2.0510e-01, -6.3097e-01, -5.9479e-01, -1.1856e+00, -3.4285e-01,\n",
      "           9.2849e-01,  7.8564e-01,  4.3443e-01,  1.1676e+00, -8.7688e-01,\n",
      "          -3.8796e-01,  9.1218e-01,  2.1451e-01, -2.7854e-01,  5.3529e-01,\n",
      "           4.1184e-01, -4.8413e-01, -3.6553e-01, -3.2199e-01,  7.4997e-01,\n",
      "          -5.0050e-01, -3.9171e-01,  2.2233e-01,  4.2524e-01, -5.7412e-01,\n",
      "          -9.5982e-01,  6.5305e-02, -3.5821e-01, -1.1387e+00, -1.1705e+00,\n",
      "          -9.6298e-02,  3.2425e-01, -4.6186e-01, -5.4277e-01, -7.7247e-01,\n",
      "          -1.0513e+00,  1.1767e+00,  7.5789e-01,  5.3176e-01, -3.7679e-01,\n",
      "           4.0103e-01, -4.6044e-01,  2.6752e-01,  6.1724e-01, -7.1117e-01,\n",
      "          -5.6717e-01, -2.4660e-01, -7.3636e-01,  1.0697e+00,  4.3367e-01,\n",
      "           2.4642e-01,  2.7184e-01,  4.0518e-01, -6.5018e-01, -2.2766e-01,\n",
      "          -1.3438e-01,  4.3224e-01, -7.1186e-01,  1.7707e-01,  4.7156e-01,\n",
      "           6.9799e-01, -4.8596e-01, -4.3092e-01,  9.9470e-01,  4.6310e-01,\n",
      "           3.3402e-01,  7.9046e-01,  3.2722e-01,  6.2832e-01, -3.3484e-01,\n",
      "          -9.9128e-01,  2.5118e-01, -7.4657e-01]]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "xk: torch.Size([1, 3, 32])\n",
      "tensor([[[ 0.4826, -0.0983, -0.2132, -0.4701, -0.9434, -0.5406,  0.3470,\n",
      "          -0.5162,  0.4516,  0.6007, -0.9339, -0.1971, -0.1116, -1.0093,\n",
      "          -1.4347,  0.4866,  0.5482,  0.7383,  0.2014,  0.0882, -0.7874,\n",
      "           0.6287, -0.0437,  0.5649,  0.2626, -0.4096,  0.1997, -0.4551,\n",
      "          -0.7687,  0.0097,  0.2582,  0.2175],\n",
      "         [ 0.1457, -0.4736,  0.9412,  0.4000,  0.3397,  0.9468, -0.2535,\n",
      "           0.1884,  0.5176,  0.0235,  1.2752,  0.0178,  0.2154,  0.6970,\n",
      "           1.0148, -0.5795, -0.7893, -0.3683, -0.4014, -0.6049,  0.0999,\n",
      "           0.0814,  0.1840, -1.3217, -0.0505,  0.4195, -0.7328,  0.4726,\n",
      "          -0.2556,  0.2117,  0.2736, -0.1251],\n",
      "         [ 0.5025,  0.1071, -0.0722, -0.2495, -0.6747, -0.4429, -0.2249,\n",
      "          -0.6820,  0.5401,  0.7173, -1.0832, -0.2094,  0.5442, -0.6353,\n",
      "          -1.1652,  0.1030,  0.8463,  0.5610, -0.1707, -0.2092, -0.9591,\n",
      "           0.5553,  0.3529,  0.6018, -0.2973, -0.5538,  0.1167, -0.2096,\n",
      "          -0.7178, -0.0548,  0.5453,  0.2583]]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "xv: torch.Size([1, 3, 32])\n",
      "tensor([[[ 1.2113, -0.6099, -0.1465,  0.0425, -0.6048, -0.0113,  0.2040,\n",
      "           0.4491,  0.7418, -0.0443,  0.6333, -0.1505,  1.0824,  0.0983,\n",
      "           0.2675,  0.5136,  0.0645, -0.5481,  0.1419,  0.5873,  0.1246,\n",
      "          -0.9179,  0.3485, -0.8749, -0.0136,  0.2074,  0.3486,  0.9575,\n",
      "           0.0687,  1.0924, -0.6492,  0.3165],\n",
      "         [-0.3809,  0.5792,  0.8630, -0.1507,  0.2213,  0.0612,  1.0552,\n",
      "          -0.5000, -0.3680,  0.5933, -0.9732,  0.6422, -0.9933,  0.1067,\n",
      "          -0.4053,  0.0790,  0.0264,  0.2962, -0.1806, -0.7257, -0.7527,\n",
      "           0.1901, -0.8215,  0.2161,  1.0483,  0.9106, -0.3224, -0.0244,\n",
      "           0.3445, -0.7649, -0.5366, -0.6767],\n",
      "         [ 0.9342, -0.5037, -0.0057, -0.4569, -0.3217,  0.5699,  0.3032,\n",
      "           0.0416,  0.4536, -0.0489,  0.6024,  0.2561,  0.9493,  0.4895,\n",
      "           0.0561,  0.3288,  0.0453, -0.2447, -0.5982,  0.6106,  0.0768,\n",
      "          -0.8506,  0.2625, -0.8314,  0.1033,  0.2090,  0.7826,  0.8447,\n",
      "          -0.0329,  0.9385, -0.7480,  0.4339]]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "xq reshaped: torch.Size([1, 3, 4, 32])\n",
      "tensor([[[[ 1.6549e-01,  5.4076e-01,  4.8795e-01, -6.3072e-01,  4.0208e-01,\n",
      "            6.2984e-01, -5.6808e-01, -1.2378e+00,  8.1524e-01, -7.9636e-01,\n",
      "            8.0192e-01,  7.8446e-01, -6.7752e-01, -7.4751e-02,  2.4583e-01,\n",
      "            2.6009e-01,  1.4017e-01,  2.1559e-01, -1.0835e-01, -2.4438e-02,\n",
      "            8.2970e-01,  3.3080e-01, -3.3757e-01,  1.2043e-01,  1.1275e+00,\n",
      "            8.9723e-01, -1.0124e-01,  8.4529e-02,  1.5678e-01, -2.0698e-01,\n",
      "            2.3246e-01,  5.9252e-01],\n",
      "          [ 1.3468e-01, -5.9097e-01,  5.1714e-01,  9.2165e-01,  1.0710e+00,\n",
      "           -1.4484e-01,  8.0683e-01, -2.6919e-02,  9.5135e-01, -6.4525e-01,\n",
      "            6.9601e-02,  5.1452e-01, -1.1561e+00,  4.2188e-02,  3.0234e-01,\n",
      "            5.9142e-01,  2.5691e-01,  7.6558e-02, -6.5083e-01, -1.1460e+00,\n",
      "           -1.0578e+00,  6.6808e-01,  6.1143e-02,  6.0631e-02, -1.0521e+00,\n",
      "            1.0921e-02, -1.1544e+00, -6.5812e-01,  4.9425e-01,  9.8971e-01,\n",
      "            2.4428e-01,  1.1058e+00],\n",
      "          [-5.6350e-01, -2.4191e-01,  5.6628e-01,  7.9870e-02, -2.2138e-01,\n",
      "           -2.7185e-03,  4.2848e-01, -7.0637e-01, -6.2744e-02, -1.3927e-01,\n",
      "            5.7964e-01, -4.0731e-01, -1.4734e-01, -1.1736e-01,  6.0675e-01,\n",
      "           -6.1850e-01, -1.0202e+00, -1.1061e-01,  1.1528e-01, -1.1643e+00,\n",
      "           -1.1206e+00, -1.8263e-01,  4.9662e-01, -4.1570e-01, -2.4815e-01,\n",
      "           -7.6545e-01, -8.4023e-01,  1.3267e+00,  1.5299e-01, -5.9590e-02,\n",
      "           -3.3702e-01,  1.8291e-01],\n",
      "          [-1.4230e-01,  1.5036e-01,  4.6662e-01, -9.3812e-02, -3.7652e-01,\n",
      "           -3.9600e-01, -6.5096e-01,  8.1178e-01, -6.5435e-02,  3.8346e-01,\n",
      "            4.6706e-01,  1.8640e-01, -1.1781e-01, -2.7227e-01, -1.6118e-01,\n",
      "            7.1106e-01, -7.8050e-01,  3.3822e-01, -1.7873e-01,  6.3649e-01,\n",
      "           -4.0824e-01, -3.8760e-01,  1.0118e+00,  4.5315e-01,  2.4505e-01,\n",
      "            1.0177e+00,  8.8149e-02,  3.0631e-01, -8.7508e-02, -8.4092e-01,\n",
      "            3.8644e-02, -1.1003e-01]],\n",
      "\n",
      "         [[-5.4763e-01, -5.3452e-02,  2.4608e-01,  1.1239e+00,  5.5752e-01,\n",
      "           -7.5870e-01, -3.1793e-02,  5.4430e-01, -3.8361e-01,  2.6360e-01,\n",
      "           -4.7404e-01, -1.9703e-01,  4.1643e-01, -3.4265e-01, -8.8174e-01,\n",
      "           -5.0832e-01, -2.6820e-01, -2.8545e-01,  8.4457e-01, -4.8091e-01,\n",
      "           -1.0145e+00,  9.4430e-02, -2.8207e-01,  3.5907e-02, -1.0622e+00,\n",
      "           -7.2554e-01,  5.2526e-01,  2.8888e-01, -9.8235e-01,  8.3390e-01,\n",
      "           -1.8843e-01, -4.0329e-01],\n",
      "          [ 5.9334e-01, -2.2129e-01, -6.2421e-01, -1.2232e+00, -1.4060e+00,\n",
      "           -7.2171e-01, -7.2921e-01,  2.9879e-01, -1.2846e+00,  1.8991e-01,\n",
      "           -8.3729e-03, -8.9744e-01,  3.5141e-01,  1.0074e+00, -6.6844e-02,\n",
      "           -1.3623e+00, -9.6802e-01,  2.1351e-01,  1.6907e-01,  4.4087e-01,\n",
      "            1.2016e+00, -9.4827e-01, -6.8738e-01, -5.1246e-01,  4.0295e-01,\n",
      "           -4.4600e-01,  9.3841e-01,  4.4603e-01, -5.9735e-01, -7.4381e-01,\n",
      "            9.1339e-01, -1.0807e+00],\n",
      "          [ 2.9669e-01,  7.5152e-01, -4.2770e-01,  1.2656e-01, -9.5302e-01,\n",
      "            7.4039e-01,  3.3459e-01,  3.4687e-02, -3.6131e-01, -2.5128e-02,\n",
      "            4.0147e-01, -5.8782e-01, -3.1363e-01,  9.4393e-01, -1.2432e-01,\n",
      "           -2.2002e-01,  2.0641e-01, -2.9261e-01,  1.2243e-01,  7.8026e-01,\n",
      "            1.2214e+00, -2.5263e-01,  1.6418e-01,  1.1538e-01,  8.0786e-02,\n",
      "            2.5797e-02,  8.4165e-01, -1.1538e+00,  7.8557e-02,  6.4045e-01,\n",
      "           -7.6072e-02,  1.6093e-01],\n",
      "          [ 2.1664e-01,  2.5405e-01, -3.1994e-01, -2.0724e-01, -5.7318e-01,\n",
      "            8.6142e-01,  3.8214e-01, -1.1168e+00,  4.8492e-01, -4.3693e-01,\n",
      "           -9.3433e-02,  5.4788e-02,  1.3893e-03,  2.0320e-01,  1.2525e-01,\n",
      "           -2.1674e-01,  5.6154e-01, -2.8954e-01,  3.7415e-01,  2.5376e-02,\n",
      "           -7.3140e-01,  8.5610e-01, -3.5473e-01, -1.4176e+00, -9.6864e-01,\n",
      "           -1.3288e-02,  2.9266e-01, -4.7687e-01, -3.8599e-01,  1.0546e+00,\n",
      "           -2.1119e-01, -2.7657e-01]],\n",
      "\n",
      "         [[ 1.2376e-01,  4.0339e-01,  2.3305e-01, -6.8447e-01,  3.4414e-01,\n",
      "            6.4568e-01, -6.8054e-01, -9.1918e-01,  6.1008e-01, -6.1686e-01,\n",
      "            8.7578e-01,  4.9808e-01, -5.6670e-01,  1.5259e-02,  2.9244e-01,\n",
      "            6.2184e-01, -3.8680e-01,  1.6070e-01,  6.4926e-02, -4.6873e-01,\n",
      "            5.4078e-01,  4.5886e-01, -4.1276e-01,  2.6811e-01,  1.0497e+00,\n",
      "            8.0557e-01, -2.1812e-01, -1.7520e-01, -3.9435e-02, -9.9788e-02,\n",
      "            3.2815e-02,  4.0657e-02],\n",
      "          [ 6.9540e-01, -7.2106e-01,  5.3530e-01,  8.5985e-01,  1.1099e+00,\n",
      "           -2.3143e-01,  8.8736e-01, -3.4270e-01,  8.0318e-01, -5.1199e-01,\n",
      "            2.7114e-01,  6.4548e-01, -8.0320e-01,  1.1564e-01,  5.2626e-01,\n",
      "            6.6987e-01,  1.3762e-01,  6.7702e-01, -4.1063e-01, -7.7865e-01,\n",
      "           -1.1334e+00,  7.9482e-01, -2.7875e-01,  2.0510e-01, -6.3097e-01,\n",
      "           -5.9479e-01, -1.1856e+00, -3.4285e-01,  9.2849e-01,  7.8564e-01,\n",
      "            4.3443e-01,  1.1676e+00],\n",
      "          [-8.7688e-01, -3.8796e-01,  9.1218e-01,  2.1451e-01, -2.7854e-01,\n",
      "            5.3529e-01,  4.1184e-01, -4.8413e-01, -3.6553e-01, -3.2199e-01,\n",
      "            7.4997e-01, -5.0050e-01, -3.9171e-01,  2.2233e-01,  4.2524e-01,\n",
      "           -5.7412e-01, -9.5982e-01,  6.5305e-02, -3.5821e-01, -1.1387e+00,\n",
      "           -1.1705e+00, -9.6298e-02,  3.2425e-01, -4.6186e-01, -5.4277e-01,\n",
      "           -7.7247e-01, -1.0513e+00,  1.1767e+00,  7.5789e-01,  5.3176e-01,\n",
      "           -3.7679e-01,  4.0103e-01],\n",
      "          [-4.6044e-01,  2.6752e-01,  6.1724e-01, -7.1117e-01, -5.6717e-01,\n",
      "           -2.4660e-01, -7.3636e-01,  1.0697e+00,  4.3367e-01,  2.4642e-01,\n",
      "            2.7184e-01,  4.0518e-01, -6.5018e-01, -2.2766e-01, -1.3438e-01,\n",
      "            4.3224e-01, -7.1186e-01,  1.7707e-01,  4.7156e-01,  6.9799e-01,\n",
      "           -4.8596e-01, -4.3092e-01,  9.9470e-01,  4.6310e-01,  3.3402e-01,\n",
      "            7.9046e-01,  3.2722e-01,  6.2832e-01, -3.3484e-01, -9.9128e-01,\n",
      "            2.5118e-01, -7.4657e-01]]]], grad_fn=<ViewBackward0>)\n",
      "xk reshaped: torch.Size([1, 3, 1, 32])\n",
      "tensor([[[[ 0.4826, -0.0983, -0.2132, -0.4701, -0.9434, -0.5406,  0.3470,\n",
      "           -0.5162,  0.4516,  0.6007, -0.9339, -0.1971, -0.1116, -1.0093,\n",
      "           -1.4347,  0.4866,  0.5482,  0.7383,  0.2014,  0.0882, -0.7874,\n",
      "            0.6287, -0.0437,  0.5649,  0.2626, -0.4096,  0.1997, -0.4551,\n",
      "           -0.7687,  0.0097,  0.2582,  0.2175]],\n",
      "\n",
      "         [[ 0.1457, -0.4736,  0.9412,  0.4000,  0.3397,  0.9468, -0.2535,\n",
      "            0.1884,  0.5176,  0.0235,  1.2752,  0.0178,  0.2154,  0.6970,\n",
      "            1.0148, -0.5795, -0.7893, -0.3683, -0.4014, -0.6049,  0.0999,\n",
      "            0.0814,  0.1840, -1.3217, -0.0505,  0.4195, -0.7328,  0.4726,\n",
      "           -0.2556,  0.2117,  0.2736, -0.1251]],\n",
      "\n",
      "         [[ 0.5025,  0.1071, -0.0722, -0.2495, -0.6747, -0.4429, -0.2249,\n",
      "           -0.6820,  0.5401,  0.7173, -1.0832, -0.2094,  0.5442, -0.6353,\n",
      "           -1.1652,  0.1030,  0.8463,  0.5610, -0.1707, -0.2092, -0.9591,\n",
      "            0.5553,  0.3529,  0.6018, -0.2973, -0.5538,  0.1167, -0.2096,\n",
      "           -0.7178, -0.0548,  0.5453,  0.2583]]]], grad_fn=<ViewBackward0>)\n",
      "xv reshaped: torch.Size([1, 3, 1, 32])\n",
      "tensor([[[[ 1.2113, -0.6099, -0.1465,  0.0425, -0.6048, -0.0113,  0.2040,\n",
      "            0.4491,  0.7418, -0.0443,  0.6333, -0.1505,  1.0824,  0.0983,\n",
      "            0.2675,  0.5136,  0.0645, -0.5481,  0.1419,  0.5873,  0.1246,\n",
      "           -0.9179,  0.3485, -0.8749, -0.0136,  0.2074,  0.3486,  0.9575,\n",
      "            0.0687,  1.0924, -0.6492,  0.3165]],\n",
      "\n",
      "         [[-0.3809,  0.5792,  0.8630, -0.1507,  0.2213,  0.0612,  1.0552,\n",
      "           -0.5000, -0.3680,  0.5933, -0.9732,  0.6422, -0.9933,  0.1067,\n",
      "           -0.4053,  0.0790,  0.0264,  0.2962, -0.1806, -0.7257, -0.7527,\n",
      "            0.1901, -0.8215,  0.2161,  1.0483,  0.9106, -0.3224, -0.0244,\n",
      "            0.3445, -0.7649, -0.5366, -0.6767]],\n",
      "\n",
      "         [[ 0.9342, -0.5037, -0.0057, -0.4569, -0.3217,  0.5699,  0.3032,\n",
      "            0.0416,  0.4536, -0.0489,  0.6024,  0.2561,  0.9493,  0.4895,\n",
      "            0.0561,  0.3288,  0.0453, -0.2447, -0.5982,  0.6106,  0.0768,\n",
      "           -0.8506,  0.2625, -0.8314,  0.1033,  0.2090,  0.7826,  0.8447,\n",
      "           -0.0329,  0.9385, -0.7480,  0.4339]]]], grad_fn=<ViewBackward0>)\n",
      "rotated xq: torch.Size([1, 3, 4, 32])\n",
      "tensor([[[[ 0.1655,  0.5408,  0.4880, -0.6307,  0.4021,  0.6298, -0.5681,\n",
      "           -1.2378,  0.8152, -0.7964,  0.8019,  0.7845, -0.6775, -0.0748,\n",
      "            0.2458,  0.2601,  0.1402,  0.2156, -0.1084, -0.0244,  0.8297,\n",
      "            0.3308, -0.3376,  0.1204,  1.1275,  0.8972, -0.1012,  0.0845,\n",
      "            0.1568, -0.2070,  0.2325,  0.5925],\n",
      "          [ 0.1347, -0.5910,  0.5171,  0.9217,  1.0710, -0.1448,  0.8068,\n",
      "           -0.0269,  0.9513, -0.6452,  0.0696,  0.5145, -1.1561,  0.0422,\n",
      "            0.3023,  0.5914,  0.2569,  0.0766, -0.6508, -1.1460, -1.0578,\n",
      "            0.6681,  0.0611,  0.0606, -1.0521,  0.0109, -1.1544, -0.6581,\n",
      "            0.4942,  0.9897,  0.2443,  1.1058],\n",
      "          [-0.5635, -0.2419,  0.5663,  0.0799, -0.2214, -0.0027,  0.4285,\n",
      "           -0.7064, -0.0627, -0.1393,  0.5796, -0.4073, -0.1473, -0.1174,\n",
      "            0.6068, -0.6185, -1.0202, -0.1106,  0.1153, -1.1643, -1.1206,\n",
      "           -0.1826,  0.4966, -0.4157, -0.2481, -0.7655, -0.8402,  1.3267,\n",
      "            0.1530, -0.0596, -0.3370,  0.1829],\n",
      "          [-0.1423,  0.1504,  0.4666, -0.0938, -0.3765, -0.3960, -0.6510,\n",
      "            0.8118, -0.0654,  0.3835,  0.4671,  0.1864, -0.1178, -0.2723,\n",
      "           -0.1612,  0.7111, -0.7805,  0.3382, -0.1787,  0.6365, -0.4082,\n",
      "           -0.3876,  1.0118,  0.4531,  0.2450,  1.0177,  0.0881,  0.3063,\n",
      "           -0.0875, -0.8409,  0.0386, -0.1100]],\n",
      "\n",
      "         [[-0.0702,  0.1554, -0.2421,  1.2223,  0.8454, -0.7596,  0.0186,\n",
      "            0.5347, -0.2757,  0.3172, -0.5028, -0.2090,  0.4473, -0.3623,\n",
      "           -0.8782, -0.5029, -0.6057, -0.2453,  0.8457,  0.0212, -0.7908,\n",
      "           -0.0864, -0.2832,  0.1080, -1.0952, -0.7038,  0.4978,  0.2803,\n",
      "           -0.9687,  0.8255, -0.2041, -0.4100],\n",
      "          [ 1.1351, -0.3075, -0.6182, -1.2965, -1.7100, -0.4787, -0.5961,\n",
      "            0.3643, -1.3184,  0.2228, -0.0611, -0.9154,  0.3701,  1.0247,\n",
      "           -0.0831, -1.3477, -0.0237,  0.0054, -0.1898, -0.0984,  0.7047,\n",
      "           -1.0913, -0.8055, -0.4682,  0.2727, -0.4305,  0.9365,  0.4078,\n",
      "           -0.5859, -0.7197,  0.9121, -1.0987],\n",
      "          [-0.0134,  0.7494, -0.4271, -0.2039, -1.2856,  0.7790,  0.3003,\n",
      "            0.0190, -0.3676, -0.0270,  0.3535, -0.5387, -0.3160,  0.9285,\n",
      "           -0.1229, -0.2222,  0.3612,  0.2981, -0.1245,  0.7637,  0.8645,\n",
      "           -0.0716,  0.2208,  0.1190,  0.0443,  0.0238,  0.8629, -1.1775,\n",
      "            0.0686,  0.6627, -0.0783,  0.1580],\n",
      "          [-0.3555,  0.3832, -0.4702, -0.1995, -0.3173,  0.6362,  0.4389,\n",
      "           -0.9184,  0.5792, -0.4347, -0.1097,  0.0748,  0.0136,  0.1781,\n",
      "            0.1290, -0.2130,  0.4857, -0.0387,  0.1459, -0.0617, -0.8734,\n",
      "            1.0345, -0.2815, -1.5535, -0.9154, -0.0460,  0.2869, -0.4741,\n",
      "           -0.3858,  1.0591, -0.2089, -0.2794]],\n",
      "\n",
      "         [[ 0.3002, -0.1317,  0.0420, -0.1050, -0.0421,  0.3649, -0.4942,\n",
      "           -0.9573,  0.3894, -0.7303,  0.8947,  0.5111, -0.5631,  0.0200,\n",
      "            0.2911,  0.6205,  0.2735,  0.4138,  0.2383, -0.8229,  0.6396,\n",
      "            0.7031, -0.6239,  0.0164,  1.1500,  0.7044, -0.1185, -0.1326,\n",
      "           -0.0752, -0.0990,  0.0432,  0.0572],\n",
      "          [-0.4145, -0.7265,  0.6014,  1.1533,  1.5652, -0.5689,  0.9289,\n",
      "           -0.3846,  0.9125, -0.4174,  0.4025,  0.6721, -0.8603,  0.0783,\n",
      "            0.5105,  0.6385,  0.5751, -0.6712,  0.3057,  0.1245, -0.2581,\n",
      "            0.6014,  0.0477,  0.1075, -0.4588, -0.6646, -1.1477, -0.2873,\n",
      "            0.8759,  0.7902,  0.4529,  1.1850],\n",
      "          [ 1.2377, -0.0927,  0.7167,  0.9931,  0.4673,  0.5202,  0.2732,\n",
      "           -0.3453, -0.2504, -0.2030,  0.8632, -0.5979, -0.4388,  0.1969,\n",
      "            0.4384, -0.5846, -0.3979, -0.3823,  0.6684, -0.5969, -1.1088,\n",
      "            0.1588,  0.4474, -0.5731, -0.6046, -0.8119, -0.9605,  1.1304,\n",
      "            0.7316,  0.5417, -0.3614,  0.3856],\n",
      "          [ 0.8389, -0.1576, -0.1591, -0.9942, -0.1702, -0.0226, -1.0366,\n",
      "            0.9098,  0.3587,  0.1255,  0.2334,  0.3508, -0.6277, -0.1804,\n",
      "           -0.1432,  0.4520, -0.1224,  0.2794,  0.7603, -0.0671, -0.7272,\n",
      "           -0.4960,  0.6761,  0.7287,  0.4135,  0.8184,  0.3557,  0.6602,\n",
      "           -0.3753, -1.0010,  0.2462, -0.7348]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "rotated xk: torch.Size([1, 3, 1, 32])\n",
      "tensor([[[[ 4.8256e-01, -9.8314e-02, -2.1318e-01, -4.7010e-01, -9.4338e-01,\n",
      "           -5.4062e-01,  3.4701e-01, -5.1625e-01,  4.5164e-01,  6.0066e-01,\n",
      "           -9.3394e-01, -1.9711e-01, -1.1165e-01, -1.0093e+00, -1.4347e+00,\n",
      "            4.8658e-01,  5.4822e-01,  7.3833e-01,  2.0144e-01,  8.8228e-02,\n",
      "           -7.8738e-01,  6.2875e-01, -4.3729e-02,  5.6492e-01,  2.6260e-01,\n",
      "           -4.0964e-01,  1.9974e-01, -4.5514e-01, -7.6873e-01,  9.7344e-03,\n",
      "            2.5824e-01,  2.1749e-01]],\n",
      "\n",
      "         [[ 7.4286e-01, -9.5548e-02,  1.0103e+00,  6.1251e-01,  2.9176e-01,\n",
      "            9.0122e-01, -2.8202e-01,  3.6251e-01,  5.2003e-01, -7.9983e-03,\n",
      "            1.3144e+00, -2.1820e-03,  2.2339e-01,  6.9179e-01,  1.0098e+00,\n",
      "           -5.7773e-01, -3.0382e-01, -5.9230e-01,  1.6224e-01, -3.8819e-01,\n",
      "            2.0062e-01,  3.0151e-01,  1.3622e-01, -1.2850e+00,  1.4174e-03,\n",
      "            4.2004e-01, -6.6001e-01,  4.7290e-01, -2.4865e-01,  2.2818e-01,\n",
      "            2.9160e-01, -1.3277e-01]],\n",
      "\n",
      "         [[-9.7861e-01, -5.5198e-01,  1.2282e-01, -9.6036e-03,  2.2779e-02,\n",
      "           -6.4762e-01, -3.3372e-01, -8.1646e-01,  5.8835e-01,  7.9203e-01,\n",
      "           -1.0895e+00, -1.9096e-01,  5.8851e-01, -6.3194e-01, -1.1839e+00,\n",
      "            9.6119e-02,  1.0476e-01,  1.4667e-01, -1.3879e-01, -3.2548e-01,\n",
      "           -1.1724e+00,  2.9177e-01,  2.5246e-01,  4.0076e-01, -1.8405e-01,\n",
      "           -4.4040e-01, -5.6033e-03, -2.2653e-01, -6.8195e-01, -8.4881e-02,\n",
      "            5.0357e-01,  2.6100e-01]]]], grad_fn=<TransposeBackward0>)\n",
      "repeat_interleaved xk: torch.Size([1, 3, 4, 32])\n",
      "tensor([[[[ 4.8256e-01, -9.8314e-02, -2.1318e-01, -4.7010e-01, -9.4338e-01,\n",
      "           -5.4062e-01,  3.4701e-01, -5.1625e-01,  4.5164e-01,  6.0066e-01,\n",
      "           -9.3394e-01, -1.9711e-01, -1.1165e-01, -1.0093e+00, -1.4347e+00,\n",
      "            4.8658e-01,  5.4822e-01,  7.3833e-01,  2.0144e-01,  8.8228e-02,\n",
      "           -7.8738e-01,  6.2875e-01, -4.3729e-02,  5.6492e-01,  2.6260e-01,\n",
      "           -4.0964e-01,  1.9974e-01, -4.5514e-01, -7.6873e-01,  9.7344e-03,\n",
      "            2.5824e-01,  2.1749e-01],\n",
      "          [ 4.8256e-01, -9.8314e-02, -2.1318e-01, -4.7010e-01, -9.4338e-01,\n",
      "           -5.4062e-01,  3.4701e-01, -5.1625e-01,  4.5164e-01,  6.0066e-01,\n",
      "           -9.3394e-01, -1.9711e-01, -1.1165e-01, -1.0093e+00, -1.4347e+00,\n",
      "            4.8658e-01,  5.4822e-01,  7.3833e-01,  2.0144e-01,  8.8228e-02,\n",
      "           -7.8738e-01,  6.2875e-01, -4.3729e-02,  5.6492e-01,  2.6260e-01,\n",
      "           -4.0964e-01,  1.9974e-01, -4.5514e-01, -7.6873e-01,  9.7344e-03,\n",
      "            2.5824e-01,  2.1749e-01],\n",
      "          [ 4.8256e-01, -9.8314e-02, -2.1318e-01, -4.7010e-01, -9.4338e-01,\n",
      "           -5.4062e-01,  3.4701e-01, -5.1625e-01,  4.5164e-01,  6.0066e-01,\n",
      "           -9.3394e-01, -1.9711e-01, -1.1165e-01, -1.0093e+00, -1.4347e+00,\n",
      "            4.8658e-01,  5.4822e-01,  7.3833e-01,  2.0144e-01,  8.8228e-02,\n",
      "           -7.8738e-01,  6.2875e-01, -4.3729e-02,  5.6492e-01,  2.6260e-01,\n",
      "           -4.0964e-01,  1.9974e-01, -4.5514e-01, -7.6873e-01,  9.7344e-03,\n",
      "            2.5824e-01,  2.1749e-01],\n",
      "          [ 4.8256e-01, -9.8314e-02, -2.1318e-01, -4.7010e-01, -9.4338e-01,\n",
      "           -5.4062e-01,  3.4701e-01, -5.1625e-01,  4.5164e-01,  6.0066e-01,\n",
      "           -9.3394e-01, -1.9711e-01, -1.1165e-01, -1.0093e+00, -1.4347e+00,\n",
      "            4.8658e-01,  5.4822e-01,  7.3833e-01,  2.0144e-01,  8.8228e-02,\n",
      "           -7.8738e-01,  6.2875e-01, -4.3729e-02,  5.6492e-01,  2.6260e-01,\n",
      "           -4.0964e-01,  1.9974e-01, -4.5514e-01, -7.6873e-01,  9.7344e-03,\n",
      "            2.5824e-01,  2.1749e-01]],\n",
      "\n",
      "         [[ 7.4286e-01, -9.5548e-02,  1.0103e+00,  6.1251e-01,  2.9176e-01,\n",
      "            9.0122e-01, -2.8202e-01,  3.6251e-01,  5.2003e-01, -7.9983e-03,\n",
      "            1.3144e+00, -2.1820e-03,  2.2339e-01,  6.9179e-01,  1.0098e+00,\n",
      "           -5.7773e-01, -3.0382e-01, -5.9230e-01,  1.6224e-01, -3.8819e-01,\n",
      "            2.0062e-01,  3.0151e-01,  1.3622e-01, -1.2850e+00,  1.4174e-03,\n",
      "            4.2004e-01, -6.6001e-01,  4.7290e-01, -2.4865e-01,  2.2818e-01,\n",
      "            2.9160e-01, -1.3277e-01],\n",
      "          [ 7.4286e-01, -9.5548e-02,  1.0103e+00,  6.1251e-01,  2.9176e-01,\n",
      "            9.0122e-01, -2.8202e-01,  3.6251e-01,  5.2003e-01, -7.9983e-03,\n",
      "            1.3144e+00, -2.1820e-03,  2.2339e-01,  6.9179e-01,  1.0098e+00,\n",
      "           -5.7773e-01, -3.0382e-01, -5.9230e-01,  1.6224e-01, -3.8819e-01,\n",
      "            2.0062e-01,  3.0151e-01,  1.3622e-01, -1.2850e+00,  1.4174e-03,\n",
      "            4.2004e-01, -6.6001e-01,  4.7290e-01, -2.4865e-01,  2.2818e-01,\n",
      "            2.9160e-01, -1.3277e-01],\n",
      "          [ 7.4286e-01, -9.5548e-02,  1.0103e+00,  6.1251e-01,  2.9176e-01,\n",
      "            9.0122e-01, -2.8202e-01,  3.6251e-01,  5.2003e-01, -7.9983e-03,\n",
      "            1.3144e+00, -2.1820e-03,  2.2339e-01,  6.9179e-01,  1.0098e+00,\n",
      "           -5.7773e-01, -3.0382e-01, -5.9230e-01,  1.6224e-01, -3.8819e-01,\n",
      "            2.0062e-01,  3.0151e-01,  1.3622e-01, -1.2850e+00,  1.4174e-03,\n",
      "            4.2004e-01, -6.6001e-01,  4.7290e-01, -2.4865e-01,  2.2818e-01,\n",
      "            2.9160e-01, -1.3277e-01],\n",
      "          [ 7.4286e-01, -9.5548e-02,  1.0103e+00,  6.1251e-01,  2.9176e-01,\n",
      "            9.0122e-01, -2.8202e-01,  3.6251e-01,  5.2003e-01, -7.9983e-03,\n",
      "            1.3144e+00, -2.1820e-03,  2.2339e-01,  6.9179e-01,  1.0098e+00,\n",
      "           -5.7773e-01, -3.0382e-01, -5.9230e-01,  1.6224e-01, -3.8819e-01,\n",
      "            2.0062e-01,  3.0151e-01,  1.3622e-01, -1.2850e+00,  1.4174e-03,\n",
      "            4.2004e-01, -6.6001e-01,  4.7290e-01, -2.4865e-01,  2.2818e-01,\n",
      "            2.9160e-01, -1.3277e-01]],\n",
      "\n",
      "         [[-9.7861e-01, -5.5198e-01,  1.2282e-01, -9.6036e-03,  2.2779e-02,\n",
      "           -6.4762e-01, -3.3372e-01, -8.1646e-01,  5.8835e-01,  7.9203e-01,\n",
      "           -1.0895e+00, -1.9096e-01,  5.8851e-01, -6.3194e-01, -1.1839e+00,\n",
      "            9.6119e-02,  1.0476e-01,  1.4667e-01, -1.3879e-01, -3.2548e-01,\n",
      "           -1.1724e+00,  2.9177e-01,  2.5246e-01,  4.0076e-01, -1.8405e-01,\n",
      "           -4.4040e-01, -5.6033e-03, -2.2653e-01, -6.8195e-01, -8.4881e-02,\n",
      "            5.0357e-01,  2.6100e-01],\n",
      "          [-9.7861e-01, -5.5198e-01,  1.2282e-01, -9.6036e-03,  2.2779e-02,\n",
      "           -6.4762e-01, -3.3372e-01, -8.1646e-01,  5.8835e-01,  7.9203e-01,\n",
      "           -1.0895e+00, -1.9096e-01,  5.8851e-01, -6.3194e-01, -1.1839e+00,\n",
      "            9.6119e-02,  1.0476e-01,  1.4667e-01, -1.3879e-01, -3.2548e-01,\n",
      "           -1.1724e+00,  2.9177e-01,  2.5246e-01,  4.0076e-01, -1.8405e-01,\n",
      "           -4.4040e-01, -5.6033e-03, -2.2653e-01, -6.8195e-01, -8.4881e-02,\n",
      "            5.0357e-01,  2.6100e-01],\n",
      "          [-9.7861e-01, -5.5198e-01,  1.2282e-01, -9.6036e-03,  2.2779e-02,\n",
      "           -6.4762e-01, -3.3372e-01, -8.1646e-01,  5.8835e-01,  7.9203e-01,\n",
      "           -1.0895e+00, -1.9096e-01,  5.8851e-01, -6.3194e-01, -1.1839e+00,\n",
      "            9.6119e-02,  1.0476e-01,  1.4667e-01, -1.3879e-01, -3.2548e-01,\n",
      "           -1.1724e+00,  2.9177e-01,  2.5246e-01,  4.0076e-01, -1.8405e-01,\n",
      "           -4.4040e-01, -5.6033e-03, -2.2653e-01, -6.8195e-01, -8.4881e-02,\n",
      "            5.0357e-01,  2.6100e-01],\n",
      "          [-9.7861e-01, -5.5198e-01,  1.2282e-01, -9.6036e-03,  2.2779e-02,\n",
      "           -6.4762e-01, -3.3372e-01, -8.1646e-01,  5.8835e-01,  7.9203e-01,\n",
      "           -1.0895e+00, -1.9096e-01,  5.8851e-01, -6.3194e-01, -1.1839e+00,\n",
      "            9.6119e-02,  1.0476e-01,  1.4667e-01, -1.3879e-01, -3.2548e-01,\n",
      "           -1.1724e+00,  2.9177e-01,  2.5246e-01,  4.0076e-01, -1.8405e-01,\n",
      "           -4.4040e-01, -5.6033e-03, -2.2653e-01, -6.8195e-01, -8.4881e-02,\n",
      "            5.0357e-01,  2.6100e-01]]]], grad_fn=<ViewBackward0>)\n",
      "repeat_interleaved xv: torch.Size([1, 3, 4, 32])\n",
      "tensor([[[[ 1.2113, -0.6099, -0.1465,  0.0425, -0.6048, -0.0113,  0.2040,\n",
      "            0.4491,  0.7418, -0.0443,  0.6333, -0.1505,  1.0824,  0.0983,\n",
      "            0.2675,  0.5136,  0.0645, -0.5481,  0.1419,  0.5873,  0.1246,\n",
      "           -0.9179,  0.3485, -0.8749, -0.0136,  0.2074,  0.3486,  0.9575,\n",
      "            0.0687,  1.0924, -0.6492,  0.3165],\n",
      "          [ 1.2113, -0.6099, -0.1465,  0.0425, -0.6048, -0.0113,  0.2040,\n",
      "            0.4491,  0.7418, -0.0443,  0.6333, -0.1505,  1.0824,  0.0983,\n",
      "            0.2675,  0.5136,  0.0645, -0.5481,  0.1419,  0.5873,  0.1246,\n",
      "           -0.9179,  0.3485, -0.8749, -0.0136,  0.2074,  0.3486,  0.9575,\n",
      "            0.0687,  1.0924, -0.6492,  0.3165],\n",
      "          [ 1.2113, -0.6099, -0.1465,  0.0425, -0.6048, -0.0113,  0.2040,\n",
      "            0.4491,  0.7418, -0.0443,  0.6333, -0.1505,  1.0824,  0.0983,\n",
      "            0.2675,  0.5136,  0.0645, -0.5481,  0.1419,  0.5873,  0.1246,\n",
      "           -0.9179,  0.3485, -0.8749, -0.0136,  0.2074,  0.3486,  0.9575,\n",
      "            0.0687,  1.0924, -0.6492,  0.3165],\n",
      "          [ 1.2113, -0.6099, -0.1465,  0.0425, -0.6048, -0.0113,  0.2040,\n",
      "            0.4491,  0.7418, -0.0443,  0.6333, -0.1505,  1.0824,  0.0983,\n",
      "            0.2675,  0.5136,  0.0645, -0.5481,  0.1419,  0.5873,  0.1246,\n",
      "           -0.9179,  0.3485, -0.8749, -0.0136,  0.2074,  0.3486,  0.9575,\n",
      "            0.0687,  1.0924, -0.6492,  0.3165]],\n",
      "\n",
      "         [[-0.3809,  0.5792,  0.8630, -0.1507,  0.2213,  0.0612,  1.0552,\n",
      "           -0.5000, -0.3680,  0.5933, -0.9732,  0.6422, -0.9933,  0.1067,\n",
      "           -0.4053,  0.0790,  0.0264,  0.2962, -0.1806, -0.7257, -0.7527,\n",
      "            0.1901, -0.8215,  0.2161,  1.0483,  0.9106, -0.3224, -0.0244,\n",
      "            0.3445, -0.7649, -0.5366, -0.6767],\n",
      "          [-0.3809,  0.5792,  0.8630, -0.1507,  0.2213,  0.0612,  1.0552,\n",
      "           -0.5000, -0.3680,  0.5933, -0.9732,  0.6422, -0.9933,  0.1067,\n",
      "           -0.4053,  0.0790,  0.0264,  0.2962, -0.1806, -0.7257, -0.7527,\n",
      "            0.1901, -0.8215,  0.2161,  1.0483,  0.9106, -0.3224, -0.0244,\n",
      "            0.3445, -0.7649, -0.5366, -0.6767],\n",
      "          [-0.3809,  0.5792,  0.8630, -0.1507,  0.2213,  0.0612,  1.0552,\n",
      "           -0.5000, -0.3680,  0.5933, -0.9732,  0.6422, -0.9933,  0.1067,\n",
      "           -0.4053,  0.0790,  0.0264,  0.2962, -0.1806, -0.7257, -0.7527,\n",
      "            0.1901, -0.8215,  0.2161,  1.0483,  0.9106, -0.3224, -0.0244,\n",
      "            0.3445, -0.7649, -0.5366, -0.6767],\n",
      "          [-0.3809,  0.5792,  0.8630, -0.1507,  0.2213,  0.0612,  1.0552,\n",
      "           -0.5000, -0.3680,  0.5933, -0.9732,  0.6422, -0.9933,  0.1067,\n",
      "           -0.4053,  0.0790,  0.0264,  0.2962, -0.1806, -0.7257, -0.7527,\n",
      "            0.1901, -0.8215,  0.2161,  1.0483,  0.9106, -0.3224, -0.0244,\n",
      "            0.3445, -0.7649, -0.5366, -0.6767]],\n",
      "\n",
      "         [[ 0.9342, -0.5037, -0.0057, -0.4569, -0.3217,  0.5699,  0.3032,\n",
      "            0.0416,  0.4536, -0.0489,  0.6024,  0.2561,  0.9493,  0.4895,\n",
      "            0.0561,  0.3288,  0.0453, -0.2447, -0.5982,  0.6106,  0.0768,\n",
      "           -0.8506,  0.2625, -0.8314,  0.1033,  0.2090,  0.7826,  0.8447,\n",
      "           -0.0329,  0.9385, -0.7480,  0.4339],\n",
      "          [ 0.9342, -0.5037, -0.0057, -0.4569, -0.3217,  0.5699,  0.3032,\n",
      "            0.0416,  0.4536, -0.0489,  0.6024,  0.2561,  0.9493,  0.4895,\n",
      "            0.0561,  0.3288,  0.0453, -0.2447, -0.5982,  0.6106,  0.0768,\n",
      "           -0.8506,  0.2625, -0.8314,  0.1033,  0.2090,  0.7826,  0.8447,\n",
      "           -0.0329,  0.9385, -0.7480,  0.4339],\n",
      "          [ 0.9342, -0.5037, -0.0057, -0.4569, -0.3217,  0.5699,  0.3032,\n",
      "            0.0416,  0.4536, -0.0489,  0.6024,  0.2561,  0.9493,  0.4895,\n",
      "            0.0561,  0.3288,  0.0453, -0.2447, -0.5982,  0.6106,  0.0768,\n",
      "           -0.8506,  0.2625, -0.8314,  0.1033,  0.2090,  0.7826,  0.8447,\n",
      "           -0.0329,  0.9385, -0.7480,  0.4339],\n",
      "          [ 0.9342, -0.5037, -0.0057, -0.4569, -0.3217,  0.5699,  0.3032,\n",
      "            0.0416,  0.4536, -0.0489,  0.6024,  0.2561,  0.9493,  0.4895,\n",
      "            0.0561,  0.3288,  0.0453, -0.2447, -0.5982,  0.6106,  0.0768,\n",
      "           -0.8506,  0.2625, -0.8314,  0.1033,  0.2090,  0.7826,  0.8447,\n",
      "           -0.0329,  0.9385, -0.7480,  0.4339]]]], grad_fn=<ViewBackward0>)\n",
      "transposed xq: torch.Size([1, 4, 3, 32])\n",
      "tensor([[[[ 0.1655,  0.5408,  0.4880, -0.6307,  0.4021,  0.6298, -0.5681,\n",
      "           -1.2378,  0.8152, -0.7964,  0.8019,  0.7845, -0.6775, -0.0748,\n",
      "            0.2458,  0.2601,  0.1402,  0.2156, -0.1084, -0.0244,  0.8297,\n",
      "            0.3308, -0.3376,  0.1204,  1.1275,  0.8972, -0.1012,  0.0845,\n",
      "            0.1568, -0.2070,  0.2325,  0.5925],\n",
      "          [-0.0702,  0.1554, -0.2421,  1.2223,  0.8454, -0.7596,  0.0186,\n",
      "            0.5347, -0.2757,  0.3172, -0.5028, -0.2090,  0.4473, -0.3623,\n",
      "           -0.8782, -0.5029, -0.6057, -0.2453,  0.8457,  0.0212, -0.7908,\n",
      "           -0.0864, -0.2832,  0.1080, -1.0952, -0.7038,  0.4978,  0.2803,\n",
      "           -0.9687,  0.8255, -0.2041, -0.4100],\n",
      "          [ 0.3002, -0.1317,  0.0420, -0.1050, -0.0421,  0.3649, -0.4942,\n",
      "           -0.9573,  0.3894, -0.7303,  0.8947,  0.5111, -0.5631,  0.0200,\n",
      "            0.2911,  0.6205,  0.2735,  0.4138,  0.2383, -0.8229,  0.6396,\n",
      "            0.7031, -0.6239,  0.0164,  1.1500,  0.7044, -0.1185, -0.1326,\n",
      "           -0.0752, -0.0990,  0.0432,  0.0572]],\n",
      "\n",
      "         [[ 0.1347, -0.5910,  0.5171,  0.9217,  1.0710, -0.1448,  0.8068,\n",
      "           -0.0269,  0.9513, -0.6452,  0.0696,  0.5145, -1.1561,  0.0422,\n",
      "            0.3023,  0.5914,  0.2569,  0.0766, -0.6508, -1.1460, -1.0578,\n",
      "            0.6681,  0.0611,  0.0606, -1.0521,  0.0109, -1.1544, -0.6581,\n",
      "            0.4942,  0.9897,  0.2443,  1.1058],\n",
      "          [ 1.1351, -0.3075, -0.6182, -1.2965, -1.7100, -0.4787, -0.5961,\n",
      "            0.3643, -1.3184,  0.2228, -0.0611, -0.9154,  0.3701,  1.0247,\n",
      "           -0.0831, -1.3477, -0.0237,  0.0054, -0.1898, -0.0984,  0.7047,\n",
      "           -1.0913, -0.8055, -0.4682,  0.2727, -0.4305,  0.9365,  0.4078,\n",
      "           -0.5859, -0.7197,  0.9121, -1.0987],\n",
      "          [-0.4145, -0.7265,  0.6014,  1.1533,  1.5652, -0.5689,  0.9289,\n",
      "           -0.3846,  0.9125, -0.4174,  0.4025,  0.6721, -0.8603,  0.0783,\n",
      "            0.5105,  0.6385,  0.5751, -0.6712,  0.3057,  0.1245, -0.2581,\n",
      "            0.6014,  0.0477,  0.1075, -0.4588, -0.6646, -1.1477, -0.2873,\n",
      "            0.8759,  0.7902,  0.4529,  1.1850]],\n",
      "\n",
      "         [[-0.5635, -0.2419,  0.5663,  0.0799, -0.2214, -0.0027,  0.4285,\n",
      "           -0.7064, -0.0627, -0.1393,  0.5796, -0.4073, -0.1473, -0.1174,\n",
      "            0.6068, -0.6185, -1.0202, -0.1106,  0.1153, -1.1643, -1.1206,\n",
      "           -0.1826,  0.4966, -0.4157, -0.2481, -0.7655, -0.8402,  1.3267,\n",
      "            0.1530, -0.0596, -0.3370,  0.1829],\n",
      "          [-0.0134,  0.7494, -0.4271, -0.2039, -1.2856,  0.7790,  0.3003,\n",
      "            0.0190, -0.3676, -0.0270,  0.3535, -0.5387, -0.3160,  0.9285,\n",
      "           -0.1229, -0.2222,  0.3612,  0.2981, -0.1245,  0.7637,  0.8645,\n",
      "           -0.0716,  0.2208,  0.1190,  0.0443,  0.0238,  0.8629, -1.1775,\n",
      "            0.0686,  0.6627, -0.0783,  0.1580],\n",
      "          [ 1.2377, -0.0927,  0.7167,  0.9931,  0.4673,  0.5202,  0.2732,\n",
      "           -0.3453, -0.2504, -0.2030,  0.8632, -0.5979, -0.4388,  0.1969,\n",
      "            0.4384, -0.5846, -0.3979, -0.3823,  0.6684, -0.5969, -1.1088,\n",
      "            0.1588,  0.4474, -0.5731, -0.6046, -0.8119, -0.9605,  1.1304,\n",
      "            0.7316,  0.5417, -0.3614,  0.3856]],\n",
      "\n",
      "         [[-0.1423,  0.1504,  0.4666, -0.0938, -0.3765, -0.3960, -0.6510,\n",
      "            0.8118, -0.0654,  0.3835,  0.4671,  0.1864, -0.1178, -0.2723,\n",
      "           -0.1612,  0.7111, -0.7805,  0.3382, -0.1787,  0.6365, -0.4082,\n",
      "           -0.3876,  1.0118,  0.4531,  0.2450,  1.0177,  0.0881,  0.3063,\n",
      "           -0.0875, -0.8409,  0.0386, -0.1100],\n",
      "          [-0.3555,  0.3832, -0.4702, -0.1995, -0.3173,  0.6362,  0.4389,\n",
      "           -0.9184,  0.5792, -0.4347, -0.1097,  0.0748,  0.0136,  0.1781,\n",
      "            0.1290, -0.2130,  0.4857, -0.0387,  0.1459, -0.0617, -0.8734,\n",
      "            1.0345, -0.2815, -1.5535, -0.9154, -0.0460,  0.2869, -0.4741,\n",
      "           -0.3858,  1.0591, -0.2089, -0.2794],\n",
      "          [ 0.8389, -0.1576, -0.1591, -0.9942, -0.1702, -0.0226, -1.0366,\n",
      "            0.9098,  0.3587,  0.1255,  0.2334,  0.3508, -0.6277, -0.1804,\n",
      "           -0.1432,  0.4520, -0.1224,  0.2794,  0.7603, -0.0671, -0.7272,\n",
      "           -0.4960,  0.6761,  0.7287,  0.4135,  0.8184,  0.3557,  0.6602,\n",
      "           -0.3753, -1.0010,  0.2462, -0.7348]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "transposed xk: torch.Size([1, 4, 3, 32])\n",
      "tensor([[[[ 4.8256e-01, -9.8314e-02, -2.1318e-01, -4.7010e-01, -9.4338e-01,\n",
      "           -5.4062e-01,  3.4701e-01, -5.1625e-01,  4.5164e-01,  6.0066e-01,\n",
      "           -9.3394e-01, -1.9711e-01, -1.1165e-01, -1.0093e+00, -1.4347e+00,\n",
      "            4.8658e-01,  5.4822e-01,  7.3833e-01,  2.0144e-01,  8.8228e-02,\n",
      "           -7.8738e-01,  6.2875e-01, -4.3729e-02,  5.6492e-01,  2.6260e-01,\n",
      "           -4.0964e-01,  1.9974e-01, -4.5514e-01, -7.6873e-01,  9.7344e-03,\n",
      "            2.5824e-01,  2.1749e-01],\n",
      "          [ 7.4286e-01, -9.5548e-02,  1.0103e+00,  6.1251e-01,  2.9176e-01,\n",
      "            9.0122e-01, -2.8202e-01,  3.6251e-01,  5.2003e-01, -7.9983e-03,\n",
      "            1.3144e+00, -2.1820e-03,  2.2339e-01,  6.9179e-01,  1.0098e+00,\n",
      "           -5.7773e-01, -3.0382e-01, -5.9230e-01,  1.6224e-01, -3.8819e-01,\n",
      "            2.0062e-01,  3.0151e-01,  1.3622e-01, -1.2850e+00,  1.4174e-03,\n",
      "            4.2004e-01, -6.6001e-01,  4.7290e-01, -2.4865e-01,  2.2818e-01,\n",
      "            2.9160e-01, -1.3277e-01],\n",
      "          [-9.7861e-01, -5.5198e-01,  1.2282e-01, -9.6036e-03,  2.2779e-02,\n",
      "           -6.4762e-01, -3.3372e-01, -8.1646e-01,  5.8835e-01,  7.9203e-01,\n",
      "           -1.0895e+00, -1.9096e-01,  5.8851e-01, -6.3194e-01, -1.1839e+00,\n",
      "            9.6119e-02,  1.0476e-01,  1.4667e-01, -1.3879e-01, -3.2548e-01,\n",
      "           -1.1724e+00,  2.9177e-01,  2.5246e-01,  4.0076e-01, -1.8405e-01,\n",
      "           -4.4040e-01, -5.6033e-03, -2.2653e-01, -6.8195e-01, -8.4881e-02,\n",
      "            5.0357e-01,  2.6100e-01]],\n",
      "\n",
      "         [[ 4.8256e-01, -9.8314e-02, -2.1318e-01, -4.7010e-01, -9.4338e-01,\n",
      "           -5.4062e-01,  3.4701e-01, -5.1625e-01,  4.5164e-01,  6.0066e-01,\n",
      "           -9.3394e-01, -1.9711e-01, -1.1165e-01, -1.0093e+00, -1.4347e+00,\n",
      "            4.8658e-01,  5.4822e-01,  7.3833e-01,  2.0144e-01,  8.8228e-02,\n",
      "           -7.8738e-01,  6.2875e-01, -4.3729e-02,  5.6492e-01,  2.6260e-01,\n",
      "           -4.0964e-01,  1.9974e-01, -4.5514e-01, -7.6873e-01,  9.7344e-03,\n",
      "            2.5824e-01,  2.1749e-01],\n",
      "          [ 7.4286e-01, -9.5548e-02,  1.0103e+00,  6.1251e-01,  2.9176e-01,\n",
      "            9.0122e-01, -2.8202e-01,  3.6251e-01,  5.2003e-01, -7.9983e-03,\n",
      "            1.3144e+00, -2.1820e-03,  2.2339e-01,  6.9179e-01,  1.0098e+00,\n",
      "           -5.7773e-01, -3.0382e-01, -5.9230e-01,  1.6224e-01, -3.8819e-01,\n",
      "            2.0062e-01,  3.0151e-01,  1.3622e-01, -1.2850e+00,  1.4174e-03,\n",
      "            4.2004e-01, -6.6001e-01,  4.7290e-01, -2.4865e-01,  2.2818e-01,\n",
      "            2.9160e-01, -1.3277e-01],\n",
      "          [-9.7861e-01, -5.5198e-01,  1.2282e-01, -9.6036e-03,  2.2779e-02,\n",
      "           -6.4762e-01, -3.3372e-01, -8.1646e-01,  5.8835e-01,  7.9203e-01,\n",
      "           -1.0895e+00, -1.9096e-01,  5.8851e-01, -6.3194e-01, -1.1839e+00,\n",
      "            9.6119e-02,  1.0476e-01,  1.4667e-01, -1.3879e-01, -3.2548e-01,\n",
      "           -1.1724e+00,  2.9177e-01,  2.5246e-01,  4.0076e-01, -1.8405e-01,\n",
      "           -4.4040e-01, -5.6033e-03, -2.2653e-01, -6.8195e-01, -8.4881e-02,\n",
      "            5.0357e-01,  2.6100e-01]],\n",
      "\n",
      "         [[ 4.8256e-01, -9.8314e-02, -2.1318e-01, -4.7010e-01, -9.4338e-01,\n",
      "           -5.4062e-01,  3.4701e-01, -5.1625e-01,  4.5164e-01,  6.0066e-01,\n",
      "           -9.3394e-01, -1.9711e-01, -1.1165e-01, -1.0093e+00, -1.4347e+00,\n",
      "            4.8658e-01,  5.4822e-01,  7.3833e-01,  2.0144e-01,  8.8228e-02,\n",
      "           -7.8738e-01,  6.2875e-01, -4.3729e-02,  5.6492e-01,  2.6260e-01,\n",
      "           -4.0964e-01,  1.9974e-01, -4.5514e-01, -7.6873e-01,  9.7344e-03,\n",
      "            2.5824e-01,  2.1749e-01],\n",
      "          [ 7.4286e-01, -9.5548e-02,  1.0103e+00,  6.1251e-01,  2.9176e-01,\n",
      "            9.0122e-01, -2.8202e-01,  3.6251e-01,  5.2003e-01, -7.9983e-03,\n",
      "            1.3144e+00, -2.1820e-03,  2.2339e-01,  6.9179e-01,  1.0098e+00,\n",
      "           -5.7773e-01, -3.0382e-01, -5.9230e-01,  1.6224e-01, -3.8819e-01,\n",
      "            2.0062e-01,  3.0151e-01,  1.3622e-01, -1.2850e+00,  1.4174e-03,\n",
      "            4.2004e-01, -6.6001e-01,  4.7290e-01, -2.4865e-01,  2.2818e-01,\n",
      "            2.9160e-01, -1.3277e-01],\n",
      "          [-9.7861e-01, -5.5198e-01,  1.2282e-01, -9.6036e-03,  2.2779e-02,\n",
      "           -6.4762e-01, -3.3372e-01, -8.1646e-01,  5.8835e-01,  7.9203e-01,\n",
      "           -1.0895e+00, -1.9096e-01,  5.8851e-01, -6.3194e-01, -1.1839e+00,\n",
      "            9.6119e-02,  1.0476e-01,  1.4667e-01, -1.3879e-01, -3.2548e-01,\n",
      "           -1.1724e+00,  2.9177e-01,  2.5246e-01,  4.0076e-01, -1.8405e-01,\n",
      "           -4.4040e-01, -5.6033e-03, -2.2653e-01, -6.8195e-01, -8.4881e-02,\n",
      "            5.0357e-01,  2.6100e-01]],\n",
      "\n",
      "         [[ 4.8256e-01, -9.8314e-02, -2.1318e-01, -4.7010e-01, -9.4338e-01,\n",
      "           -5.4062e-01,  3.4701e-01, -5.1625e-01,  4.5164e-01,  6.0066e-01,\n",
      "           -9.3394e-01, -1.9711e-01, -1.1165e-01, -1.0093e+00, -1.4347e+00,\n",
      "            4.8658e-01,  5.4822e-01,  7.3833e-01,  2.0144e-01,  8.8228e-02,\n",
      "           -7.8738e-01,  6.2875e-01, -4.3729e-02,  5.6492e-01,  2.6260e-01,\n",
      "           -4.0964e-01,  1.9974e-01, -4.5514e-01, -7.6873e-01,  9.7344e-03,\n",
      "            2.5824e-01,  2.1749e-01],\n",
      "          [ 7.4286e-01, -9.5548e-02,  1.0103e+00,  6.1251e-01,  2.9176e-01,\n",
      "            9.0122e-01, -2.8202e-01,  3.6251e-01,  5.2003e-01, -7.9983e-03,\n",
      "            1.3144e+00, -2.1820e-03,  2.2339e-01,  6.9179e-01,  1.0098e+00,\n",
      "           -5.7773e-01, -3.0382e-01, -5.9230e-01,  1.6224e-01, -3.8819e-01,\n",
      "            2.0062e-01,  3.0151e-01,  1.3622e-01, -1.2850e+00,  1.4174e-03,\n",
      "            4.2004e-01, -6.6001e-01,  4.7290e-01, -2.4865e-01,  2.2818e-01,\n",
      "            2.9160e-01, -1.3277e-01],\n",
      "          [-9.7861e-01, -5.5198e-01,  1.2282e-01, -9.6036e-03,  2.2779e-02,\n",
      "           -6.4762e-01, -3.3372e-01, -8.1646e-01,  5.8835e-01,  7.9203e-01,\n",
      "           -1.0895e+00, -1.9096e-01,  5.8851e-01, -6.3194e-01, -1.1839e+00,\n",
      "            9.6119e-02,  1.0476e-01,  1.4667e-01, -1.3879e-01, -3.2548e-01,\n",
      "           -1.1724e+00,  2.9177e-01,  2.5246e-01,  4.0076e-01, -1.8405e-01,\n",
      "           -4.4040e-01, -5.6033e-03, -2.2653e-01, -6.8195e-01, -8.4881e-02,\n",
      "            5.0357e-01,  2.6100e-01]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xv: torch.Size([1, 4, 3, 32])\n",
      "tensor([[[[ 1.2113, -0.6099, -0.1465,  0.0425, -0.6048, -0.0113,  0.2040,\n",
      "            0.4491,  0.7418, -0.0443,  0.6333, -0.1505,  1.0824,  0.0983,\n",
      "            0.2675,  0.5136,  0.0645, -0.5481,  0.1419,  0.5873,  0.1246,\n",
      "           -0.9179,  0.3485, -0.8749, -0.0136,  0.2074,  0.3486,  0.9575,\n",
      "            0.0687,  1.0924, -0.6492,  0.3165],\n",
      "          [-0.3809,  0.5792,  0.8630, -0.1507,  0.2213,  0.0612,  1.0552,\n",
      "           -0.5000, -0.3680,  0.5933, -0.9732,  0.6422, -0.9933,  0.1067,\n",
      "           -0.4053,  0.0790,  0.0264,  0.2962, -0.1806, -0.7257, -0.7527,\n",
      "            0.1901, -0.8215,  0.2161,  1.0483,  0.9106, -0.3224, -0.0244,\n",
      "            0.3445, -0.7649, -0.5366, -0.6767],\n",
      "          [ 0.9342, -0.5037, -0.0057, -0.4569, -0.3217,  0.5699,  0.3032,\n",
      "            0.0416,  0.4536, -0.0489,  0.6024,  0.2561,  0.9493,  0.4895,\n",
      "            0.0561,  0.3288,  0.0453, -0.2447, -0.5982,  0.6106,  0.0768,\n",
      "           -0.8506,  0.2625, -0.8314,  0.1033,  0.2090,  0.7826,  0.8447,\n",
      "           -0.0329,  0.9385, -0.7480,  0.4339]],\n",
      "\n",
      "         [[ 1.2113, -0.6099, -0.1465,  0.0425, -0.6048, -0.0113,  0.2040,\n",
      "            0.4491,  0.7418, -0.0443,  0.6333, -0.1505,  1.0824,  0.0983,\n",
      "            0.2675,  0.5136,  0.0645, -0.5481,  0.1419,  0.5873,  0.1246,\n",
      "           -0.9179,  0.3485, -0.8749, -0.0136,  0.2074,  0.3486,  0.9575,\n",
      "            0.0687,  1.0924, -0.6492,  0.3165],\n",
      "          [-0.3809,  0.5792,  0.8630, -0.1507,  0.2213,  0.0612,  1.0552,\n",
      "           -0.5000, -0.3680,  0.5933, -0.9732,  0.6422, -0.9933,  0.1067,\n",
      "           -0.4053,  0.0790,  0.0264,  0.2962, -0.1806, -0.7257, -0.7527,\n",
      "            0.1901, -0.8215,  0.2161,  1.0483,  0.9106, -0.3224, -0.0244,\n",
      "            0.3445, -0.7649, -0.5366, -0.6767],\n",
      "          [ 0.9342, -0.5037, -0.0057, -0.4569, -0.3217,  0.5699,  0.3032,\n",
      "            0.0416,  0.4536, -0.0489,  0.6024,  0.2561,  0.9493,  0.4895,\n",
      "            0.0561,  0.3288,  0.0453, -0.2447, -0.5982,  0.6106,  0.0768,\n",
      "           -0.8506,  0.2625, -0.8314,  0.1033,  0.2090,  0.7826,  0.8447,\n",
      "           -0.0329,  0.9385, -0.7480,  0.4339]],\n",
      "\n",
      "         [[ 1.2113, -0.6099, -0.1465,  0.0425, -0.6048, -0.0113,  0.2040,\n",
      "            0.4491,  0.7418, -0.0443,  0.6333, -0.1505,  1.0824,  0.0983,\n",
      "            0.2675,  0.5136,  0.0645, -0.5481,  0.1419,  0.5873,  0.1246,\n",
      "           -0.9179,  0.3485, -0.8749, -0.0136,  0.2074,  0.3486,  0.9575,\n",
      "            0.0687,  1.0924, -0.6492,  0.3165],\n",
      "          [-0.3809,  0.5792,  0.8630, -0.1507,  0.2213,  0.0612,  1.0552,\n",
      "           -0.5000, -0.3680,  0.5933, -0.9732,  0.6422, -0.9933,  0.1067,\n",
      "           -0.4053,  0.0790,  0.0264,  0.2962, -0.1806, -0.7257, -0.7527,\n",
      "            0.1901, -0.8215,  0.2161,  1.0483,  0.9106, -0.3224, -0.0244,\n",
      "            0.3445, -0.7649, -0.5366, -0.6767],\n",
      "          [ 0.9342, -0.5037, -0.0057, -0.4569, -0.3217,  0.5699,  0.3032,\n",
      "            0.0416,  0.4536, -0.0489,  0.6024,  0.2561,  0.9493,  0.4895,\n",
      "            0.0561,  0.3288,  0.0453, -0.2447, -0.5982,  0.6106,  0.0768,\n",
      "           -0.8506,  0.2625, -0.8314,  0.1033,  0.2090,  0.7826,  0.8447,\n",
      "           -0.0329,  0.9385, -0.7480,  0.4339]],\n",
      "\n",
      "         [[ 1.2113, -0.6099, -0.1465,  0.0425, -0.6048, -0.0113,  0.2040,\n",
      "            0.4491,  0.7418, -0.0443,  0.6333, -0.1505,  1.0824,  0.0983,\n",
      "            0.2675,  0.5136,  0.0645, -0.5481,  0.1419,  0.5873,  0.1246,\n",
      "           -0.9179,  0.3485, -0.8749, -0.0136,  0.2074,  0.3486,  0.9575,\n",
      "            0.0687,  1.0924, -0.6492,  0.3165],\n",
      "          [-0.3809,  0.5792,  0.8630, -0.1507,  0.2213,  0.0612,  1.0552,\n",
      "           -0.5000, -0.3680,  0.5933, -0.9732,  0.6422, -0.9933,  0.1067,\n",
      "           -0.4053,  0.0790,  0.0264,  0.2962, -0.1806, -0.7257, -0.7527,\n",
      "            0.1901, -0.8215,  0.2161,  1.0483,  0.9106, -0.3224, -0.0244,\n",
      "            0.3445, -0.7649, -0.5366, -0.6767],\n",
      "          [ 0.9342, -0.5037, -0.0057, -0.4569, -0.3217,  0.5699,  0.3032,\n",
      "            0.0416,  0.4536, -0.0489,  0.6024,  0.2561,  0.9493,  0.4895,\n",
      "            0.0561,  0.3288,  0.0453, -0.2447, -0.5982,  0.6106,  0.0768,\n",
      "           -0.8506,  0.2625, -0.8314,  0.1033,  0.2090,  0.7826,  0.8447,\n",
      "           -0.0329,  0.9385, -0.7480,  0.4339]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "scores: torch.Size([1, 4, 3, 3])\n",
      "tensor([[[[-0.2407,  0.3937, -0.4716],\n",
      "          [ 0.2764, -0.2366,  0.6446],\n",
      "          [-0.0658,  0.3726, -0.4106]],\n",
      "\n",
      "         [[-0.0480,  0.3765,  0.2294],\n",
      "          [ 0.0218, -0.0623, -0.4340],\n",
      "          [-0.3366,  0.3771,  0.0976]],\n",
      "\n",
      "         [[-0.3872,  0.5697,  0.1799],\n",
      "          [ 0.0395, -0.2011, -0.5670],\n",
      "          [-0.5899,  1.1334, -0.4391]],\n",
      "\n",
      "         [[ 0.0143, -0.0741,  0.0258],\n",
      "          [ 0.2056,  0.2763,  0.1866],\n",
      "          [ 0.2737, -0.0744, -0.0677]]]], grad_fn=<MulBackward0>)\n",
      "mask: torch.Size([1, 1, 3, 3])\n",
      "tensor([[[[ 0.0000e+00, -2.3820e+38, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]]]])\n",
      "masked scores: torch.Size([1, 4, 3, 3])\n",
      "tensor([[[[-2.4074e-01, -2.3820e+38, -2.3820e+38],\n",
      "          [ 2.7639e-01, -2.3659e-01, -2.3820e+38],\n",
      "          [-6.5818e-02,  3.7256e-01, -4.1060e-01]],\n",
      "\n",
      "         [[-4.7955e-02, -2.3820e+38, -2.3820e+38],\n",
      "          [ 2.1776e-02, -6.2290e-02, -2.3820e+38],\n",
      "          [-3.3661e-01,  3.7709e-01,  9.7570e-02]],\n",
      "\n",
      "         [[-3.8724e-01, -2.3820e+38, -2.3820e+38],\n",
      "          [ 3.9476e-02, -2.0112e-01, -2.3820e+38],\n",
      "          [-5.8987e-01,  1.1334e+00, -4.3913e-01]],\n",
      "\n",
      "         [[ 1.4273e-02, -2.3820e+38, -2.3820e+38],\n",
      "          [ 2.0558e-01,  2.7632e-01, -2.3820e+38],\n",
      "          [ 2.7373e-01, -7.4419e-02, -6.7747e-02]]]], grad_fn=<AddBackward0>)\n",
      "softmaxed scores: torch.Size([1, 4, 3, 3])\n",
      "tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [0.6255, 0.3745, 0.0000],\n",
      "          [0.3069, 0.4757, 0.2174]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.5210, 0.4790, 0.0000],\n",
      "          [0.2181, 0.4452, 0.3367]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.5599, 0.4401, 0.0000],\n",
      "          [0.1288, 0.7215, 0.1497]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.4823, 0.5177, 0.0000],\n",
      "          [0.4138, 0.2921, 0.2941]]]], grad_fn=<SoftmaxBackward0>)\n",
      "attention: torch.Size([1, 4, 3, 32])\n",
      "tensor([[[[ 1.2113e+00, -6.0993e-01, -1.4653e-01,  4.2472e-02, -6.0475e-01,\n",
      "           -1.1302e-02,  2.0400e-01,  4.4914e-01,  7.4175e-01, -4.4345e-02,\n",
      "            6.3327e-01, -1.5046e-01,  1.0824e+00,  9.8328e-02,  2.6755e-01,\n",
      "            5.1357e-01,  6.4508e-02, -5.4810e-01,  1.4191e-01,  5.8725e-01,\n",
      "            1.2462e-01, -9.1792e-01,  3.4850e-01, -8.7489e-01, -1.3622e-02,\n",
      "            2.0743e-01,  3.4861e-01,  9.5751e-01,  6.8704e-02,  1.0924e+00,\n",
      "           -6.4916e-01,  3.1646e-01],\n",
      "          [ 6.1504e-01, -1.6462e-01,  2.3154e-01, -2.9887e-02, -2.9540e-01,\n",
      "            1.5852e-02,  5.2276e-01,  9.3711e-02,  3.2615e-01,  1.9447e-01,\n",
      "            3.1646e-02,  1.4638e-01,  3.0508e-01,  1.0148e-01,  1.5562e-02,\n",
      "            3.5081e-01,  5.0239e-02, -2.3191e-01,  2.1146e-02,  9.5545e-02,\n",
      "           -2.0393e-01, -5.0297e-01, -8.9661e-02, -4.6633e-01,  3.8407e-01,\n",
      "            4.7078e-01,  9.7317e-02,  5.8980e-01,  1.7200e-01,  3.9688e-01,\n",
      "           -6.0700e-01, -5.5476e-02],\n",
      "          [ 3.9362e-01, -2.1159e-02,  3.6435e-01, -1.5801e-01, -1.5025e-01,\n",
      "            1.4955e-01,  6.3049e-01, -9.0960e-02,  1.5116e-01,  2.5803e-01,\n",
      "           -1.3770e-01,  3.1500e-01,  6.6003e-02,  1.8738e-01, -9.8519e-02,\n",
      "            2.6665e-01,  4.2205e-02, -8.0481e-02, -1.7239e-01, -3.2288e-02,\n",
      "           -3.0313e-01, -3.7615e-01, -2.2679e-01, -3.4643e-01,  5.1700e-01,\n",
      "            5.4230e-01,  1.2373e-01,  4.6588e-01,  1.7784e-01,  1.7538e-01,\n",
      "           -6.1709e-01, -1.3049e-01]],\n",
      "\n",
      "         [[ 1.2113e+00, -6.0993e-01, -1.4653e-01,  4.2472e-02, -6.0475e-01,\n",
      "           -1.1302e-02,  2.0400e-01,  4.4914e-01,  7.4175e-01, -4.4345e-02,\n",
      "            6.3327e-01, -1.5046e-01,  1.0824e+00,  9.8328e-02,  2.6755e-01,\n",
      "            5.1357e-01,  6.4508e-02, -5.4810e-01,  1.4191e-01,  5.8725e-01,\n",
      "            1.2462e-01, -9.1792e-01,  3.4850e-01, -8.7489e-01, -1.3622e-02,\n",
      "            2.0743e-01,  3.4861e-01,  9.5751e-01,  6.8704e-02,  1.0924e+00,\n",
      "           -6.4916e-01,  3.1646e-01],\n",
      "          [ 4.4866e-01, -4.0360e-02,  3.3704e-01, -5.0078e-02, -2.0908e-01,\n",
      "            2.3430e-02,  6.1171e-01, -5.4714e-03,  2.1017e-01,  2.6111e-01,\n",
      "           -1.3624e-01,  2.2921e-01,  8.8159e-02,  1.0236e-01, -5.4754e-02,\n",
      "            3.0539e-01,  4.6258e-02, -1.4367e-01, -1.2552e-02, -4.1664e-02,\n",
      "           -2.9561e-01, -3.8717e-01, -2.1193e-01, -3.5232e-01,  4.9505e-01,\n",
      "            5.4426e-01,  2.7194e-02,  4.8719e-01,  2.0082e-01,  2.0278e-01,\n",
      "           -5.9524e-01, -1.5926e-01],\n",
      "          [ 4.0911e-01, -4.4745e-02,  3.5037e-01, -2.1169e-01, -1.4167e-01,\n",
      "            2.1666e-01,  6.1637e-01, -1.1063e-01,  1.5062e-01,  2.3804e-01,\n",
      "           -9.2400e-02,  3.3932e-01,  1.1341e-01,  2.3379e-01, -1.0322e-01,\n",
      "            2.5787e-01,  4.1074e-02, -7.0036e-02, -2.5085e-01,  1.0533e-02,\n",
      "           -2.8208e-01, -4.0190e-01, -2.0137e-01, -3.7450e-01,  4.9857e-01,\n",
      "            5.2104e-01,  1.9596e-01,  4.8236e-01,  1.5732e-01,  2.1365e-01,\n",
      "           -6.3231e-01, -8.6200e-02]],\n",
      "\n",
      "         [[ 1.2113e+00, -6.0993e-01, -1.4653e-01,  4.2472e-02, -6.0475e-01,\n",
      "           -1.1302e-02,  2.0400e-01,  4.4914e-01,  7.4175e-01, -4.4345e-02,\n",
      "            6.3327e-01, -1.5046e-01,  1.0824e+00,  9.8328e-02,  2.6755e-01,\n",
      "            5.1357e-01,  6.4508e-02, -5.4810e-01,  1.4191e-01,  5.8725e-01,\n",
      "            1.2462e-01, -9.1792e-01,  3.4850e-01, -8.7489e-01, -1.3622e-02,\n",
      "            2.0743e-01,  3.4861e-01,  9.5751e-01,  6.8704e-02,  1.0924e+00,\n",
      "           -6.4916e-01,  3.1646e-01],\n",
      "          [ 5.1052e-01, -8.6564e-02,  2.9781e-01, -4.2571e-02, -2.4118e-01,\n",
      "            2.0612e-02,  5.7864e-01,  3.1407e-02,  2.5330e-01,  2.3633e-01,\n",
      "           -7.3814e-02,  1.9841e-01,  1.6882e-01,  1.0203e-01, -2.8609e-02,\n",
      "            3.2228e-01,  4.7738e-02, -1.7648e-01, -2.2136e-05,  9.3543e-03,\n",
      "           -2.6152e-01, -4.3023e-01, -1.6647e-01, -3.9472e-01,  4.5378e-01,\n",
      "            5.1694e-01,  5.3268e-02,  5.2534e-01,  1.9010e-01,  2.7495e-01,\n",
      "           -5.9961e-01, -1.2067e-01],\n",
      "          [ 2.1065e-02,  2.6390e-01,  6.0294e-01, -1.7171e-01,  3.3619e-02,\n",
      "            1.2804e-01,  8.3297e-01, -2.9665e-01, -1.0210e-01,  4.1506e-01,\n",
      "           -5.3045e-01,  4.8230e-01, -4.3515e-01,  1.6298e-01, -2.4958e-01,\n",
      "            1.7234e-01,  3.4141e-02,  1.0650e-01, -2.0157e-01, -3.5657e-01,\n",
      "           -5.1552e-01, -1.0838e-01, -5.0853e-01, -8.1245e-02,  7.7009e-01,\n",
      "            7.1502e-01, -7.0552e-02,  2.3219e-01,  2.5250e-01, -2.7069e-01,\n",
      "           -5.8274e-01, -3.8253e-01]],\n",
      "\n",
      "         [[ 1.2113e+00, -6.0993e-01, -1.4653e-01,  4.2472e-02, -6.0475e-01,\n",
      "           -1.1302e-02,  2.0400e-01,  4.4914e-01,  7.4175e-01, -4.4345e-02,\n",
      "            6.3327e-01, -1.5046e-01,  1.0824e+00,  9.8328e-02,  2.6755e-01,\n",
      "            5.1357e-01,  6.4508e-02, -5.4810e-01,  1.4191e-01,  5.8725e-01,\n",
      "            1.2462e-01, -9.1792e-01,  3.4850e-01, -8.7489e-01, -1.3622e-02,\n",
      "            2.0743e-01,  3.4861e-01,  9.5751e-01,  6.8704e-02,  1.0924e+00,\n",
      "           -6.4916e-01,  3.1646e-01],\n",
      "          [ 3.8707e-01,  5.6370e-03,  3.7609e-01, -5.7553e-02, -1.7713e-01,\n",
      "            2.6234e-02,  6.4464e-01, -4.2185e-02,  1.6725e-01,  2.8577e-01,\n",
      "           -1.9838e-01,  2.5987e-01,  7.8648e-03,  1.0269e-01, -8.0782e-02,\n",
      "            2.8858e-01,  4.4784e-02, -1.1101e-01, -2.5026e-02, -9.2453e-02,\n",
      "           -3.2955e-01, -3.4431e-01, -2.5719e-01, -3.1012e-01,  5.3613e-01,\n",
      "            5.7146e-01,  1.2377e-03,  4.4921e-01,  2.1149e-01,  1.3093e-01,\n",
      "           -5.9089e-01, -1.9768e-01],\n",
      "          [ 6.6468e-01, -2.3133e-01,  1.8980e-01, -1.6084e-01, -2.8020e-01,\n",
      "            1.8081e-01,  4.8182e-01,  5.2040e-02,  3.3281e-01,  1.4060e-01,\n",
      "            1.5489e-01,  2.0065e-01,  4.3690e-01,  2.1584e-01,  8.8017e-03,\n",
      "            3.3227e-01,  4.7726e-02, -2.1223e-01, -1.6996e-01,  2.1057e-01,\n",
      "           -1.4572e-01, -5.7443e-01, -1.8571e-02, -5.4339e-01,  3.3100e-01,\n",
      "            4.1331e-01,  2.8022e-01,  6.3751e-01,  1.1941e-01,  5.0458e-01,\n",
      "           -6.4533e-01,  6.0865e-02]]]], grad_fn=<UnsafeViewBackward0>)\n",
      "reshaped attention: torch.Size([1, 3, 128])\n",
      "tensor([[[ 1.2113e+00, -6.0993e-01, -1.4653e-01,  4.2472e-02, -6.0475e-01,\n",
      "          -1.1302e-02,  2.0400e-01,  4.4914e-01,  7.4175e-01, -4.4345e-02,\n",
      "           6.3327e-01, -1.5046e-01,  1.0824e+00,  9.8328e-02,  2.6755e-01,\n",
      "           5.1357e-01,  6.4508e-02, -5.4810e-01,  1.4191e-01,  5.8725e-01,\n",
      "           1.2462e-01, -9.1792e-01,  3.4850e-01, -8.7489e-01, -1.3622e-02,\n",
      "           2.0743e-01,  3.4861e-01,  9.5751e-01,  6.8704e-02,  1.0924e+00,\n",
      "          -6.4916e-01,  3.1646e-01,  1.2113e+00, -6.0993e-01, -1.4653e-01,\n",
      "           4.2472e-02, -6.0475e-01, -1.1302e-02,  2.0400e-01,  4.4914e-01,\n",
      "           7.4175e-01, -4.4345e-02,  6.3327e-01, -1.5046e-01,  1.0824e+00,\n",
      "           9.8328e-02,  2.6755e-01,  5.1357e-01,  6.4508e-02, -5.4810e-01,\n",
      "           1.4191e-01,  5.8725e-01,  1.2462e-01, -9.1792e-01,  3.4850e-01,\n",
      "          -8.7489e-01, -1.3622e-02,  2.0743e-01,  3.4861e-01,  9.5751e-01,\n",
      "           6.8704e-02,  1.0924e+00, -6.4916e-01,  3.1646e-01,  1.2113e+00,\n",
      "          -6.0993e-01, -1.4653e-01,  4.2472e-02, -6.0475e-01, -1.1302e-02,\n",
      "           2.0400e-01,  4.4914e-01,  7.4175e-01, -4.4345e-02,  6.3327e-01,\n",
      "          -1.5046e-01,  1.0824e+00,  9.8328e-02,  2.6755e-01,  5.1357e-01,\n",
      "           6.4508e-02, -5.4810e-01,  1.4191e-01,  5.8725e-01,  1.2462e-01,\n",
      "          -9.1792e-01,  3.4850e-01, -8.7489e-01, -1.3622e-02,  2.0743e-01,\n",
      "           3.4861e-01,  9.5751e-01,  6.8704e-02,  1.0924e+00, -6.4916e-01,\n",
      "           3.1646e-01,  1.2113e+00, -6.0993e-01, -1.4653e-01,  4.2472e-02,\n",
      "          -6.0475e-01, -1.1302e-02,  2.0400e-01,  4.4914e-01,  7.4175e-01,\n",
      "          -4.4345e-02,  6.3327e-01, -1.5046e-01,  1.0824e+00,  9.8328e-02,\n",
      "           2.6755e-01,  5.1357e-01,  6.4508e-02, -5.4810e-01,  1.4191e-01,\n",
      "           5.8725e-01,  1.2462e-01, -9.1792e-01,  3.4850e-01, -8.7489e-01,\n",
      "          -1.3622e-02,  2.0743e-01,  3.4861e-01,  9.5751e-01,  6.8704e-02,\n",
      "           1.0924e+00, -6.4916e-01,  3.1646e-01],\n",
      "         [ 6.1504e-01, -1.6462e-01,  2.3154e-01, -2.9887e-02, -2.9540e-01,\n",
      "           1.5852e-02,  5.2276e-01,  9.3711e-02,  3.2615e-01,  1.9447e-01,\n",
      "           3.1646e-02,  1.4638e-01,  3.0508e-01,  1.0148e-01,  1.5562e-02,\n",
      "           3.5081e-01,  5.0239e-02, -2.3191e-01,  2.1146e-02,  9.5545e-02,\n",
      "          -2.0393e-01, -5.0297e-01, -8.9661e-02, -4.6633e-01,  3.8407e-01,\n",
      "           4.7078e-01,  9.7317e-02,  5.8980e-01,  1.7200e-01,  3.9688e-01,\n",
      "          -6.0700e-01, -5.5476e-02,  4.4866e-01, -4.0360e-02,  3.3704e-01,\n",
      "          -5.0078e-02, -2.0908e-01,  2.3430e-02,  6.1171e-01, -5.4714e-03,\n",
      "           2.1017e-01,  2.6111e-01, -1.3624e-01,  2.2921e-01,  8.8159e-02,\n",
      "           1.0236e-01, -5.4754e-02,  3.0539e-01,  4.6258e-02, -1.4367e-01,\n",
      "          -1.2552e-02, -4.1664e-02, -2.9561e-01, -3.8717e-01, -2.1193e-01,\n",
      "          -3.5232e-01,  4.9505e-01,  5.4426e-01,  2.7194e-02,  4.8719e-01,\n",
      "           2.0082e-01,  2.0278e-01, -5.9524e-01, -1.5926e-01,  5.1052e-01,\n",
      "          -8.6564e-02,  2.9781e-01, -4.2571e-02, -2.4118e-01,  2.0612e-02,\n",
      "           5.7864e-01,  3.1407e-02,  2.5330e-01,  2.3633e-01, -7.3814e-02,\n",
      "           1.9841e-01,  1.6882e-01,  1.0203e-01, -2.8609e-02,  3.2228e-01,\n",
      "           4.7738e-02, -1.7648e-01, -2.2136e-05,  9.3543e-03, -2.6152e-01,\n",
      "          -4.3023e-01, -1.6647e-01, -3.9472e-01,  4.5378e-01,  5.1694e-01,\n",
      "           5.3268e-02,  5.2534e-01,  1.9010e-01,  2.7495e-01, -5.9961e-01,\n",
      "          -1.2067e-01,  3.8707e-01,  5.6370e-03,  3.7609e-01, -5.7553e-02,\n",
      "          -1.7713e-01,  2.6234e-02,  6.4464e-01, -4.2185e-02,  1.6725e-01,\n",
      "           2.8577e-01, -1.9838e-01,  2.5987e-01,  7.8648e-03,  1.0269e-01,\n",
      "          -8.0782e-02,  2.8858e-01,  4.4784e-02, -1.1101e-01, -2.5026e-02,\n",
      "          -9.2453e-02, -3.2955e-01, -3.4431e-01, -2.5719e-01, -3.1012e-01,\n",
      "           5.3613e-01,  5.7146e-01,  1.2377e-03,  4.4921e-01,  2.1149e-01,\n",
      "           1.3093e-01, -5.9089e-01, -1.9768e-01],\n",
      "         [ 3.9362e-01, -2.1159e-02,  3.6435e-01, -1.5801e-01, -1.5025e-01,\n",
      "           1.4955e-01,  6.3049e-01, -9.0960e-02,  1.5116e-01,  2.5803e-01,\n",
      "          -1.3770e-01,  3.1500e-01,  6.6003e-02,  1.8738e-01, -9.8519e-02,\n",
      "           2.6665e-01,  4.2205e-02, -8.0481e-02, -1.7239e-01, -3.2288e-02,\n",
      "          -3.0313e-01, -3.7615e-01, -2.2679e-01, -3.4643e-01,  5.1700e-01,\n",
      "           5.4230e-01,  1.2373e-01,  4.6588e-01,  1.7784e-01,  1.7538e-01,\n",
      "          -6.1709e-01, -1.3049e-01,  4.0911e-01, -4.4745e-02,  3.5037e-01,\n",
      "          -2.1169e-01, -1.4167e-01,  2.1666e-01,  6.1637e-01, -1.1063e-01,\n",
      "           1.5062e-01,  2.3804e-01, -9.2400e-02,  3.3932e-01,  1.1341e-01,\n",
      "           2.3379e-01, -1.0322e-01,  2.5787e-01,  4.1074e-02, -7.0036e-02,\n",
      "          -2.5085e-01,  1.0533e-02, -2.8208e-01, -4.0190e-01, -2.0137e-01,\n",
      "          -3.7450e-01,  4.9857e-01,  5.2104e-01,  1.9596e-01,  4.8236e-01,\n",
      "           1.5732e-01,  2.1365e-01, -6.3231e-01, -8.6200e-02,  2.1065e-02,\n",
      "           2.6390e-01,  6.0294e-01, -1.7171e-01,  3.3619e-02,  1.2804e-01,\n",
      "           8.3297e-01, -2.9665e-01, -1.0210e-01,  4.1506e-01, -5.3045e-01,\n",
      "           4.8230e-01, -4.3515e-01,  1.6298e-01, -2.4958e-01,  1.7234e-01,\n",
      "           3.4141e-02,  1.0650e-01, -2.0157e-01, -3.5657e-01, -5.1552e-01,\n",
      "          -1.0838e-01, -5.0853e-01, -8.1245e-02,  7.7009e-01,  7.1502e-01,\n",
      "          -7.0552e-02,  2.3219e-01,  2.5250e-01, -2.7069e-01, -5.8274e-01,\n",
      "          -3.8253e-01,  6.6468e-01, -2.3133e-01,  1.8980e-01, -1.6084e-01,\n",
      "          -2.8020e-01,  1.8081e-01,  4.8182e-01,  5.2040e-02,  3.3281e-01,\n",
      "           1.4060e-01,  1.5489e-01,  2.0065e-01,  4.3690e-01,  2.1584e-01,\n",
      "           8.8017e-03,  3.3227e-01,  4.7726e-02, -2.1223e-01, -1.6996e-01,\n",
      "           2.1057e-01, -1.4572e-01, -5.7443e-01, -1.8571e-02, -5.4339e-01,\n",
      "           3.3100e-01,  4.1331e-01,  2.8022e-01,  6.3751e-01,  1.1941e-01,\n",
      "           5.0458e-01, -6.4533e-01,  6.0865e-02]]], grad_fn=<ViewBackward0>)\n",
      "self.Wo: torch.Size([128, 4])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0514,  0.0743,  0.0537,  0.0693],\n",
      "        [-0.0094, -0.0845, -0.0542, -0.0136],\n",
      "        [ 0.0229,  0.0264, -0.0093, -0.0026],\n",
      "        [-0.0316,  0.0430,  0.0125, -0.0616],\n",
      "        [ 0.0464,  0.0413,  0.0481,  0.0531],\n",
      "        [-0.0105, -0.0405,  0.0001, -0.0071],\n",
      "        [-0.0035, -0.0689, -0.0252, -0.0542],\n",
      "        [ 0.0835,  0.0865,  0.0467, -0.0720],\n",
      "        [-0.0548,  0.0473,  0.0302,  0.0020],\n",
      "        [ 0.0081, -0.0360,  0.0452, -0.0402],\n",
      "        [-0.0376, -0.0440,  0.0329, -0.0872],\n",
      "        [-0.0355, -0.0404, -0.0046,  0.0003],\n",
      "        [ 0.0848, -0.0524,  0.0606, -0.0696],\n",
      "        [-0.0693,  0.0483, -0.0576,  0.0117],\n",
      "        [-0.0017, -0.0049,  0.0201,  0.0202],\n",
      "        [-0.0724, -0.0261,  0.0836, -0.0099],\n",
      "        [-0.0679, -0.0854, -0.0672,  0.0221],\n",
      "        [-0.0875, -0.0013,  0.0315, -0.0219],\n",
      "        [ 0.0605,  0.0836, -0.0371, -0.0282],\n",
      "        [-0.0303,  0.0743, -0.0527, -0.0353],\n",
      "        [ 0.0282,  0.0691, -0.0066,  0.0627],\n",
      "        [-0.0791, -0.0368, -0.0722,  0.0095],\n",
      "        [ 0.0809, -0.0143, -0.0701, -0.0357],\n",
      "        [-0.0454,  0.0016, -0.0115,  0.0131],\n",
      "        [ 0.0266,  0.0383, -0.0735,  0.0114],\n",
      "        [-0.0617, -0.0287,  0.0471,  0.0024],\n",
      "        [ 0.0362,  0.0466,  0.0281, -0.0051],\n",
      "        [ 0.0618,  0.0556, -0.0811,  0.0084],\n",
      "        [-0.0073,  0.0854,  0.0814,  0.0430],\n",
      "        [-0.0234, -0.0501, -0.0794,  0.0883],\n",
      "        [ 0.0510,  0.0175,  0.0640,  0.0476],\n",
      "        [-0.0776, -0.0752, -0.0081, -0.0527],\n",
      "        [-0.0840, -0.0065,  0.0413, -0.0724],\n",
      "        [ 0.0094,  0.0614, -0.0162, -0.0418],\n",
      "        [ 0.0833,  0.0642,  0.0744,  0.0247],\n",
      "        [ 0.0155,  0.0746,  0.0227, -0.0774],\n",
      "        [-0.0599,  0.0262, -0.0539,  0.0297],\n",
      "        [-0.0088, -0.0238,  0.0177, -0.0479],\n",
      "        [ 0.0257, -0.0519,  0.0176,  0.0664],\n",
      "        [-0.0519,  0.0628, -0.0223, -0.0024],\n",
      "        [-0.0426,  0.0583, -0.0716,  0.0199],\n",
      "        [ 0.0391,  0.0258,  0.0776,  0.0818],\n",
      "        [-0.0625,  0.0565, -0.0284, -0.0667],\n",
      "        [ 0.0728,  0.0282,  0.0354, -0.0383],\n",
      "        [-0.0742,  0.0047,  0.0576,  0.0267],\n",
      "        [-0.0635, -0.0323, -0.0053, -0.0564],\n",
      "        [ 0.0178, -0.0213,  0.0196, -0.0009],\n",
      "        [-0.0677, -0.0137, -0.0435,  0.0596],\n",
      "        [-0.0611, -0.0526,  0.0458, -0.0519],\n",
      "        [-0.0120,  0.0083, -0.0181, -0.0460],\n",
      "        [ 0.0733, -0.0850,  0.0844, -0.0710],\n",
      "        [-0.0707,  0.0651,  0.0544, -0.0751],\n",
      "        [ 0.0044,  0.0845, -0.0804,  0.0732],\n",
      "        [-0.0220,  0.0340, -0.0049, -0.0684],\n",
      "        [-0.0223, -0.0597, -0.0738, -0.0661],\n",
      "        [ 0.0663,  0.0443,  0.0142,  0.0746],\n",
      "        [-0.0266,  0.0484,  0.0818, -0.0387],\n",
      "        [ 0.0486, -0.0462,  0.0570,  0.0004],\n",
      "        [-0.0711, -0.0846, -0.0152, -0.0648],\n",
      "        [ 0.0782, -0.0624,  0.0493,  0.0408],\n",
      "        [ 0.0517,  0.0578, -0.0004, -0.0706],\n",
      "        [ 0.0635,  0.0529,  0.0730,  0.0173],\n",
      "        [-0.0615, -0.0505,  0.0230, -0.0285],\n",
      "        [ 0.0780, -0.0163,  0.0638,  0.0295],\n",
      "        [-0.0351, -0.0091, -0.0593, -0.0044],\n",
      "        [ 0.0792,  0.0473,  0.0735, -0.0741],\n",
      "        [ 0.0105,  0.0763, -0.0420, -0.0469],\n",
      "        [ 0.0652,  0.0307, -0.0339, -0.0420],\n",
      "        [ 0.0687, -0.0324,  0.0191,  0.0743],\n",
      "        [ 0.0727, -0.0414,  0.0630,  0.0561],\n",
      "        [-0.0774,  0.0852,  0.0413,  0.0461],\n",
      "        [-0.0510, -0.0647, -0.0310,  0.0168],\n",
      "        [ 0.0367, -0.0794,  0.0471,  0.0470],\n",
      "        [-0.0112,  0.0241, -0.0717, -0.0274],\n",
      "        [-0.0728,  0.0839,  0.0003, -0.0810],\n",
      "        [ 0.0222, -0.0643,  0.0136, -0.0075],\n",
      "        [-0.0366,  0.0549,  0.0628, -0.0377],\n",
      "        [ 0.0760, -0.0411,  0.0003,  0.0797],\n",
      "        [ 0.0150, -0.0266, -0.0145,  0.0551],\n",
      "        [-0.0400, -0.0881,  0.0034, -0.0795],\n",
      "        [ 0.0611, -0.0265, -0.0585, -0.0675],\n",
      "        [-0.0521, -0.0122, -0.0485, -0.0295],\n",
      "        [ 0.0822,  0.0452, -0.0568, -0.0710],\n",
      "        [ 0.0450,  0.0645, -0.0702, -0.0811],\n",
      "        [-0.0225, -0.0715, -0.0430, -0.0110],\n",
      "        [ 0.0735, -0.0227,  0.0861, -0.0029],\n",
      "        [ 0.0307, -0.0863,  0.0438, -0.0778],\n",
      "        [ 0.0206, -0.0404,  0.0440, -0.0200],\n",
      "        [-0.0879,  0.0735, -0.0069,  0.0122],\n",
      "        [-0.0054,  0.0238, -0.0209, -0.0291],\n",
      "        [ 0.0112, -0.0083, -0.0622,  0.0861],\n",
      "        [ 0.0630, -0.0599,  0.0275, -0.0171],\n",
      "        [-0.0425,  0.0575, -0.0010,  0.0880],\n",
      "        [ 0.0031,  0.0430,  0.0718,  0.0703],\n",
      "        [ 0.0070,  0.0678, -0.0493, -0.0748],\n",
      "        [-0.0038,  0.0665,  0.0458, -0.0291],\n",
      "        [ 0.0548,  0.0803, -0.0577,  0.0824],\n",
      "        [-0.0199, -0.0854,  0.0256,  0.0154],\n",
      "        [ 0.0358, -0.0863, -0.0115,  0.0538],\n",
      "        [ 0.0343,  0.0434, -0.0784,  0.0476],\n",
      "        [-0.0750, -0.0764, -0.0024, -0.0658],\n",
      "        [ 0.0400, -0.0731, -0.0679, -0.0725],\n",
      "        [-0.0722, -0.0172, -0.0615,  0.0616],\n",
      "        [-0.0544, -0.0756,  0.0504, -0.0241],\n",
      "        [-0.0493,  0.0617,  0.0499,  0.0277],\n",
      "        [ 0.0236,  0.0027,  0.0016, -0.0365],\n",
      "        [-0.0200, -0.0603, -0.0211, -0.0676],\n",
      "        [ 0.0158, -0.0035, -0.0131,  0.0160],\n",
      "        [-0.0304, -0.0590, -0.0502,  0.0389],\n",
      "        [ 0.0793, -0.0155, -0.0462, -0.0514],\n",
      "        [-0.0697, -0.0389, -0.0095, -0.0075],\n",
      "        [ 0.0466,  0.0835, -0.0150, -0.0661],\n",
      "        [-0.0175,  0.0384,  0.0659,  0.0800],\n",
      "        [ 0.0198, -0.0148,  0.0801,  0.0679],\n",
      "        [ 0.0548,  0.0441, -0.0773,  0.0059],\n",
      "        [-0.0003,  0.0879,  0.0650,  0.0100],\n",
      "        [ 0.0017,  0.0040,  0.0177, -0.0550],\n",
      "        [-0.0189, -0.0250, -0.0162, -0.0413],\n",
      "        [-0.0369, -0.0653,  0.0759,  0.0452],\n",
      "        [-0.0196, -0.0653, -0.0633,  0.0211],\n",
      "        [-0.0218,  0.0105, -0.0293, -0.0554],\n",
      "        [-0.0503,  0.0465,  0.0322, -0.0693],\n",
      "        [-0.0623, -0.0767,  0.0390, -0.0007],\n",
      "        [ 0.0549, -0.0156,  0.0739,  0.0425],\n",
      "        [ 0.0659,  0.0031, -0.0220, -0.0762],\n",
      "        [ 0.0374,  0.0430,  0.0135, -0.0675],\n",
      "        [-0.0469, -0.0155, -0.0748,  0.0680],\n",
      "        [ 0.0117,  0.0004,  0.0271,  0.0802]], requires_grad=True)\n",
      "spliced Wo: torch.Size([128, 4])\n",
      "tensor([[ 0.0514,  0.0743,  0.0537,  0.0693],\n",
      "        [-0.0094, -0.0845, -0.0542, -0.0136],\n",
      "        [ 0.0229,  0.0264, -0.0093, -0.0026],\n",
      "        [-0.0316,  0.0430,  0.0125, -0.0616],\n",
      "        [ 0.0464,  0.0413,  0.0481,  0.0531],\n",
      "        [-0.0105, -0.0405,  0.0001, -0.0071],\n",
      "        [-0.0035, -0.0689, -0.0252, -0.0542],\n",
      "        [ 0.0835,  0.0865,  0.0467, -0.0720],\n",
      "        [-0.0548,  0.0473,  0.0302,  0.0020],\n",
      "        [ 0.0081, -0.0360,  0.0452, -0.0402],\n",
      "        [-0.0376, -0.0440,  0.0329, -0.0872],\n",
      "        [-0.0355, -0.0404, -0.0046,  0.0003],\n",
      "        [ 0.0848, -0.0524,  0.0606, -0.0696],\n",
      "        [-0.0693,  0.0483, -0.0576,  0.0117],\n",
      "        [-0.0017, -0.0049,  0.0201,  0.0202],\n",
      "        [-0.0724, -0.0261,  0.0836, -0.0099],\n",
      "        [-0.0679, -0.0854, -0.0672,  0.0221],\n",
      "        [-0.0875, -0.0013,  0.0315, -0.0219],\n",
      "        [ 0.0605,  0.0836, -0.0371, -0.0282],\n",
      "        [-0.0303,  0.0743, -0.0527, -0.0353],\n",
      "        [ 0.0282,  0.0691, -0.0066,  0.0627],\n",
      "        [-0.0791, -0.0368, -0.0722,  0.0095],\n",
      "        [ 0.0809, -0.0143, -0.0701, -0.0357],\n",
      "        [-0.0454,  0.0016, -0.0115,  0.0131],\n",
      "        [ 0.0266,  0.0383, -0.0735,  0.0114],\n",
      "        [-0.0617, -0.0287,  0.0471,  0.0024],\n",
      "        [ 0.0362,  0.0466,  0.0281, -0.0051],\n",
      "        [ 0.0618,  0.0556, -0.0811,  0.0084],\n",
      "        [-0.0073,  0.0854,  0.0814,  0.0430],\n",
      "        [-0.0234, -0.0501, -0.0794,  0.0883],\n",
      "        [ 0.0510,  0.0175,  0.0640,  0.0476],\n",
      "        [-0.0776, -0.0752, -0.0081, -0.0527],\n",
      "        [-0.0840, -0.0065,  0.0413, -0.0724],\n",
      "        [ 0.0094,  0.0614, -0.0162, -0.0418],\n",
      "        [ 0.0833,  0.0642,  0.0744,  0.0247],\n",
      "        [ 0.0155,  0.0746,  0.0227, -0.0774],\n",
      "        [-0.0599,  0.0262, -0.0539,  0.0297],\n",
      "        [-0.0088, -0.0238,  0.0177, -0.0479],\n",
      "        [ 0.0257, -0.0519,  0.0176,  0.0664],\n",
      "        [-0.0519,  0.0628, -0.0223, -0.0024],\n",
      "        [-0.0426,  0.0583, -0.0716,  0.0199],\n",
      "        [ 0.0391,  0.0258,  0.0776,  0.0818],\n",
      "        [-0.0625,  0.0565, -0.0284, -0.0667],\n",
      "        [ 0.0728,  0.0282,  0.0354, -0.0383],\n",
      "        [-0.0742,  0.0047,  0.0576,  0.0267],\n",
      "        [-0.0635, -0.0323, -0.0053, -0.0564],\n",
      "        [ 0.0178, -0.0213,  0.0196, -0.0009],\n",
      "        [-0.0677, -0.0137, -0.0435,  0.0596],\n",
      "        [-0.0611, -0.0526,  0.0458, -0.0519],\n",
      "        [-0.0120,  0.0083, -0.0181, -0.0460],\n",
      "        [ 0.0733, -0.0850,  0.0844, -0.0710],\n",
      "        [-0.0707,  0.0651,  0.0544, -0.0751],\n",
      "        [ 0.0044,  0.0845, -0.0804,  0.0732],\n",
      "        [-0.0220,  0.0340, -0.0049, -0.0684],\n",
      "        [-0.0223, -0.0597, -0.0738, -0.0661],\n",
      "        [ 0.0663,  0.0443,  0.0142,  0.0746],\n",
      "        [-0.0266,  0.0484,  0.0818, -0.0387],\n",
      "        [ 0.0486, -0.0462,  0.0570,  0.0004],\n",
      "        [-0.0711, -0.0846, -0.0152, -0.0648],\n",
      "        [ 0.0782, -0.0624,  0.0493,  0.0408],\n",
      "        [ 0.0517,  0.0578, -0.0004, -0.0706],\n",
      "        [ 0.0635,  0.0529,  0.0730,  0.0173],\n",
      "        [-0.0615, -0.0505,  0.0230, -0.0285],\n",
      "        [ 0.0780, -0.0163,  0.0638,  0.0295],\n",
      "        [-0.0351, -0.0091, -0.0593, -0.0044],\n",
      "        [ 0.0792,  0.0473,  0.0735, -0.0741],\n",
      "        [ 0.0105,  0.0763, -0.0420, -0.0469],\n",
      "        [ 0.0652,  0.0307, -0.0339, -0.0420],\n",
      "        [ 0.0687, -0.0324,  0.0191,  0.0743],\n",
      "        [ 0.0727, -0.0414,  0.0630,  0.0561],\n",
      "        [-0.0774,  0.0852,  0.0413,  0.0461],\n",
      "        [-0.0510, -0.0647, -0.0310,  0.0168],\n",
      "        [ 0.0367, -0.0794,  0.0471,  0.0470],\n",
      "        [-0.0112,  0.0241, -0.0717, -0.0274],\n",
      "        [-0.0728,  0.0839,  0.0003, -0.0810],\n",
      "        [ 0.0222, -0.0643,  0.0136, -0.0075],\n",
      "        [-0.0366,  0.0549,  0.0628, -0.0377],\n",
      "        [ 0.0760, -0.0411,  0.0003,  0.0797],\n",
      "        [ 0.0150, -0.0266, -0.0145,  0.0551],\n",
      "        [-0.0400, -0.0881,  0.0034, -0.0795],\n",
      "        [ 0.0611, -0.0265, -0.0585, -0.0675],\n",
      "        [-0.0521, -0.0122, -0.0485, -0.0295],\n",
      "        [ 0.0822,  0.0452, -0.0568, -0.0710],\n",
      "        [ 0.0450,  0.0645, -0.0702, -0.0811],\n",
      "        [-0.0225, -0.0715, -0.0430, -0.0110],\n",
      "        [ 0.0735, -0.0227,  0.0861, -0.0029],\n",
      "        [ 0.0307, -0.0863,  0.0438, -0.0778],\n",
      "        [ 0.0206, -0.0404,  0.0440, -0.0200],\n",
      "        [-0.0879,  0.0735, -0.0069,  0.0122],\n",
      "        [-0.0054,  0.0238, -0.0209, -0.0291],\n",
      "        [ 0.0112, -0.0083, -0.0622,  0.0861],\n",
      "        [ 0.0630, -0.0599,  0.0275, -0.0171],\n",
      "        [-0.0425,  0.0575, -0.0010,  0.0880],\n",
      "        [ 0.0031,  0.0430,  0.0718,  0.0703],\n",
      "        [ 0.0070,  0.0678, -0.0493, -0.0748],\n",
      "        [-0.0038,  0.0665,  0.0458, -0.0291],\n",
      "        [ 0.0548,  0.0803, -0.0577,  0.0824],\n",
      "        [-0.0199, -0.0854,  0.0256,  0.0154],\n",
      "        [ 0.0358, -0.0863, -0.0115,  0.0538],\n",
      "        [ 0.0343,  0.0434, -0.0784,  0.0476],\n",
      "        [-0.0750, -0.0764, -0.0024, -0.0658],\n",
      "        [ 0.0400, -0.0731, -0.0679, -0.0725],\n",
      "        [-0.0722, -0.0172, -0.0615,  0.0616],\n",
      "        [-0.0544, -0.0756,  0.0504, -0.0241],\n",
      "        [-0.0493,  0.0617,  0.0499,  0.0277],\n",
      "        [ 0.0236,  0.0027,  0.0016, -0.0365],\n",
      "        [-0.0200, -0.0603, -0.0211, -0.0676],\n",
      "        [ 0.0158, -0.0035, -0.0131,  0.0160],\n",
      "        [-0.0304, -0.0590, -0.0502,  0.0389],\n",
      "        [ 0.0793, -0.0155, -0.0462, -0.0514],\n",
      "        [-0.0697, -0.0389, -0.0095, -0.0075],\n",
      "        [ 0.0466,  0.0835, -0.0150, -0.0661],\n",
      "        [-0.0175,  0.0384,  0.0659,  0.0800],\n",
      "        [ 0.0198, -0.0148,  0.0801,  0.0679],\n",
      "        [ 0.0548,  0.0441, -0.0773,  0.0059],\n",
      "        [-0.0003,  0.0879,  0.0650,  0.0100],\n",
      "        [ 0.0017,  0.0040,  0.0177, -0.0550],\n",
      "        [-0.0189, -0.0250, -0.0162, -0.0413],\n",
      "        [-0.0369, -0.0653,  0.0759,  0.0452],\n",
      "        [-0.0196, -0.0653, -0.0633,  0.0211],\n",
      "        [-0.0218,  0.0105, -0.0293, -0.0554],\n",
      "        [-0.0503,  0.0465,  0.0322, -0.0693],\n",
      "        [-0.0623, -0.0767,  0.0390, -0.0007],\n",
      "        [ 0.0549, -0.0156,  0.0739,  0.0425],\n",
      "        [ 0.0659,  0.0031, -0.0220, -0.0762],\n",
      "        [ 0.0374,  0.0430,  0.0135, -0.0675],\n",
      "        [-0.0469, -0.0155, -0.0748,  0.0680],\n",
      "        [ 0.0117,  0.0004,  0.0271,  0.0802]], grad_fn=<CatBackward0>)\n",
      "projected output: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.0267,  0.3975,  0.3331, -0.0686],\n",
      "         [ 0.0995,  0.1867,  0.1749,  0.0773],\n",
      "         [ 0.0512,  0.1612,  0.2301,  0.1716]]], grad_fn=<UnsafeViewBackward0>)\n",
      "----------------- END MultiQueryAttention.forwardTensor() --------------------\n",
      "x in layer after MQA & resid connection and before MLP:\n",
      "tensor([[[-0.4243,  0.3311,  1.3610, -1.4050],\n",
      "         [-0.9206,  0.7949, -2.0642,  0.6038],\n",
      "         [ 0.1770,  0.1560,  0.7370, -1.5235]]], grad_fn=<AddBackward0>)\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.4243,  0.3311,  1.3610, -1.4050],\n",
      "         [-0.9206,  0.7949, -2.0642,  0.6038],\n",
      "         [ 0.1770,  0.1560,  0.7370, -1.5235]]], grad_fn=<AddBackward0>)\n",
      "normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.4183,  0.3264,  1.3417, -1.3851],\n",
      "         [-0.7452,  0.6435, -1.6709,  0.4887],\n",
      "         [ 0.2071,  0.1826,  0.8627, -1.7831]]], grad_fn=<MulBackward0>)\n",
      "dim: 4\n",
      "skip: 0\n",
      "spliced scale: torch.Size([4])\n",
      "tensor([0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.4183,  0.3264,  1.3417, -1.3851],\n",
      "         [-0.7452,  0.6435, -1.6709,  0.4887],\n",
      "         [ 0.2071,  0.1826,  0.8627, -1.7831]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "---------- MLP Input: torch.Tensor ------------\n",
      "------------- MLP.forwardTensor() ------------\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.4183,  0.3264,  1.3417, -1.3851],\n",
      "         [-0.7452,  0.6435, -1.6709,  0.4887],\n",
      "         [ 0.2071,  0.1826,  0.8627, -1.7831]]], grad_fn=<MulBackward0>)\n",
      "d_dim: 4\n",
      "d_skip: 0\n",
      "i_dim: 16\n",
      "i_skip: 0\n",
      "Wgate: torch.Size([4, 16])\n",
      "Parameter containing:\n",
      "tensor([[ 0.3095,  0.3247, -0.2348, -0.2684,  0.1481, -0.0336,  0.2918,  0.4104,\n",
      "          0.4233, -0.2854, -0.4600, -0.0114,  0.0436,  0.3290, -0.4695,  0.1966],\n",
      "        [-0.4550, -0.3347,  0.1588, -0.0416, -0.1350, -0.4122,  0.0433, -0.0343,\n",
      "          0.1841,  0.2340, -0.0984,  0.2009, -0.4429, -0.4785,  0.0231, -0.0917],\n",
      "        [-0.1875,  0.4671,  0.2435, -0.4029, -0.2097,  0.4350,  0.1203,  0.4133,\n",
      "          0.2601, -0.1608, -0.2288, -0.2892, -0.4386, -0.3339,  0.4059,  0.3157],\n",
      "        [-0.4021,  0.0384, -0.3442,  0.3066, -0.4698, -0.0893,  0.1899, -0.2196,\n",
      "          0.1504,  0.3536, -0.0917,  0.4688, -0.2037,  0.0865,  0.1056, -0.1437]],\n",
      "       requires_grad=True)\n",
      "Wgate spliced: torch.Size([4, 16])\n",
      "tensor([[ 0.3095,  0.3247, -0.2348, -0.2684,  0.1481, -0.0336,  0.2918,  0.4104,\n",
      "          0.4233, -0.2854, -0.4600, -0.0114,  0.0436,  0.3290, -0.4695,  0.1966],\n",
      "        [-0.4550, -0.3347,  0.1588, -0.0416, -0.1350, -0.4122,  0.0433, -0.0343,\n",
      "          0.1841,  0.2340, -0.0984,  0.2009, -0.4429, -0.4785,  0.0231, -0.0917],\n",
      "        [-0.1875,  0.4671,  0.2435, -0.4029, -0.2097,  0.4350,  0.1203,  0.4133,\n",
      "          0.2601, -0.1608, -0.2288, -0.2892, -0.4386, -0.3339,  0.4059,  0.3157],\n",
      "        [-0.4021,  0.0384, -0.3442,  0.3066, -0.4698, -0.0893,  0.1899, -0.2196,\n",
      "          0.1504,  0.3536, -0.0917,  0.4688, -0.2037,  0.0865,  0.1056, -0.1437]],\n",
      "       grad_fn=<AliasBackward0>)\n",
      "Bgate: torch.Size([16])\n",
      "Parameter containing:\n",
      "tensor([-0.3869, -0.0935, -0.0742,  0.3095, -0.0956,  0.3838, -0.1328, -0.3449,\n",
      "        -0.4261,  0.3397,  0.2771,  0.2628, -0.2119, -0.4128, -0.2219,  0.1706],\n",
      "       requires_grad=True)\n",
      "Bgate spliced: torch.Size([16])\n",
      "tensor([-0.3869, -0.0935, -0.0742,  0.3095, -0.0956,  0.3838, -0.1328, -0.3449,\n",
      "        -0.4261,  0.3397,  0.2771,  0.2628, -0.2119, -0.4128, -0.2219,  0.1706],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xgate: torch.Size([1, 3, 16])\n",
      "tensor([[[-0.3596,  0.2349,  0.8793, -0.5571,  0.1677,  0.9705, -0.3423,\n",
      "           0.3309, -0.4025, -0.1701,  0.2573, -0.7042, -0.6810, -1.2744,\n",
      "           0.3803,  0.6811],\n",
      "         [-0.7935, -1.3126, -0.3722,  1.3057, -0.1720, -0.6269, -0.4305,\n",
      "          -1.4708, -0.9841,  1.1445,  0.8940,  1.1128,  0.1039, -0.3657,\n",
      "          -0.4838, -0.6326],\n",
      "         [ 0.1493,  0.2471,  0.7300, -0.6480,  0.5673,  0.8360, -0.2993,\n",
      "           0.4820, -0.3487, -0.4460,  0.1299, -0.7883, -0.2989, -0.8743,\n",
      "          -0.1532,  0.7231]]], grad_fn=<AddBackward0>)\n",
      "GeLU'ed Xgate: torch.Size([1, 3, 16])\n",
      "tensor([[[-0.1293,  0.1393,  0.7126, -0.1608,  0.0950,  0.8095, -0.1253,\n",
      "           0.2084, -0.1383, -0.0736,  0.1548, -0.1695, -0.1688, -0.1291,\n",
      "           0.2465,  0.5122],\n",
      "         [-0.1696, -0.1243, -0.1321,  1.1806, -0.0743, -0.1664, -0.1435,\n",
      "          -0.1039, -0.1599,  1.0000,  0.7280,  0.9650,  0.0563, -0.1307,\n",
      "          -0.1520, -0.1667],\n",
      "         [ 0.0835,  0.1476,  0.5602, -0.1675,  0.4054,  0.6675, -0.1144,\n",
      "           0.3302, -0.1268, -0.1462,  0.0717, -0.1697, -0.1143, -0.1670,\n",
      "          -0.0673,  0.5533]]], grad_fn=<GeluBackward0>)\n",
      "Wup: torch.Size([4, 16])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0488, -0.1314, -0.0796, -0.4639,  0.2573,  0.1704,  0.0070,  0.3364,\n",
      "         -0.4475,  0.2511,  0.1227,  0.0093,  0.3071, -0.4281, -0.3575, -0.1387],\n",
      "        [-0.2920, -0.1945, -0.1729, -0.1217,  0.0128,  0.4984,  0.2747, -0.0651,\n",
      "         -0.2933,  0.0877, -0.3934, -0.1303, -0.4999, -0.2135,  0.0632,  0.2846],\n",
      "        [ 0.4273, -0.4888, -0.3618,  0.3541,  0.4532,  0.2270, -0.0352, -0.1797,\n",
      "          0.2048,  0.0527, -0.3789,  0.0368, -0.1809,  0.1246,  0.2698, -0.0715],\n",
      "        [-0.3377, -0.4910, -0.0681, -0.4828, -0.0743, -0.0438, -0.3095, -0.1333,\n",
      "          0.0544,  0.0970, -0.0450, -0.4722,  0.2919, -0.2446,  0.2606,  0.0314]],\n",
      "       requires_grad=True)\n",
      "Wup spliced: torch.Size([4, 16])\n",
      "tensor([[ 0.0488, -0.1314, -0.0796, -0.4639,  0.2573,  0.1704,  0.0070,  0.3364,\n",
      "         -0.4475,  0.2511,  0.1227,  0.0093,  0.3071, -0.4281, -0.3575, -0.1387],\n",
      "        [-0.2920, -0.1945, -0.1729, -0.1217,  0.0128,  0.4984,  0.2747, -0.0651,\n",
      "         -0.2933,  0.0877, -0.3934, -0.1303, -0.4999, -0.2135,  0.0632,  0.2846],\n",
      "        [ 0.4273, -0.4888, -0.3618,  0.3541,  0.4532,  0.2270, -0.0352, -0.1797,\n",
      "          0.2048,  0.0527, -0.3789,  0.0368, -0.1809,  0.1246,  0.2698, -0.0715],\n",
      "        [-0.3377, -0.4910, -0.0681, -0.4828, -0.0743, -0.0438, -0.3095, -0.1333,\n",
      "          0.0544,  0.0970, -0.0450, -0.4722,  0.2919, -0.2446,  0.2606,  0.0314]],\n",
      "       grad_fn=<AliasBackward0>)\n",
      "Bup: torch.Size([16])\n",
      "Parameter containing:\n",
      "tensor([-0.1737,  0.4697,  0.0072, -0.1991,  0.3706, -0.0403, -0.2663,  0.3325,\n",
      "        -0.3660,  0.3473, -0.3763, -0.3744,  0.2100,  0.1556, -0.0674,  0.4764],\n",
      "       requires_grad=True)\n",
      "Bup spliced: torch.Size([16])\n",
      "tensor([-0.1737,  0.4697,  0.0072, -0.1991,  0.3706, -0.0403, -0.2663,  0.3325,\n",
      "        -0.3660,  0.3473, -0.3763, -0.3744,  0.2100,  0.1556, -0.0674,  0.4764],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xup: torch.Size([1, 3, 16])\n",
      "tensor([[[ 0.7516,  0.4854, -0.4071,  1.0990,  0.9781,  0.4164,  0.2019,\n",
      "           0.1141, -0.0751,  0.2072, -1.0020,  0.2826, -0.7286,  0.7710,\n",
      "           0.1038,  0.4878],\n",
      "         [-1.2771,  1.0192,  0.5265, -0.7594, -0.6064, -0.2473, -0.1871,\n",
      "           0.2750, -0.5368,  0.1760, -0.1098, -0.7575,  0.1043,  0.0094,\n",
      "          -0.0838,  0.8977],\n",
      "         [ 0.7539,  0.8608, -0.2316,  0.8490,  0.9497,  0.3600,  0.3068,\n",
      "           0.4730, -0.4325,  0.2877, -0.6693,  0.4775, -0.4941,  0.5716,\n",
      "          -0.3618,  0.3819]]], grad_fn=<AddBackward0>)\n",
      "Xfuse: torch.Size([1, 3, 16])\n",
      "tensor([[[-0.0972,  0.0676, -0.2901, -0.1768,  0.0929,  0.3371, -0.0253,\n",
      "           0.0238,  0.0104, -0.0152, -0.1551, -0.0479,  0.1230, -0.0995,\n",
      "           0.0256,  0.2499],\n",
      "         [ 0.2166, -0.1266, -0.0695, -0.8966,  0.0450,  0.0411,  0.0269,\n",
      "          -0.0286,  0.0859,  0.1760, -0.0799, -0.7309,  0.0059, -0.0012,\n",
      "           0.0127, -0.1496],\n",
      "         [ 0.0630,  0.1271, -0.1297, -0.1422,  0.3850,  0.2403, -0.0351,\n",
      "           0.1562,  0.0549, -0.0421, -0.0480, -0.0810,  0.0565, -0.0954,\n",
      "           0.0243,  0.2113]]], grad_fn=<MulBackward0>)\n",
      "Wdown: torch.Size([16, 4])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0028,  0.1688,  0.1252, -0.2132],\n",
      "        [ 0.2146,  0.2054, -0.1145,  0.1425],\n",
      "        [ 0.2420, -0.1323,  0.0254,  0.0988],\n",
      "        [-0.2279, -0.2429,  0.0742,  0.1267],\n",
      "        [ 0.1254,  0.0361,  0.2388, -0.2003],\n",
      "        [-0.0737, -0.1731, -0.2045, -0.1040],\n",
      "        [ 0.0186, -0.1040, -0.0075,  0.1881],\n",
      "        [ 0.0218, -0.1834,  0.1468, -0.1363],\n",
      "        [ 0.0217,  0.2413,  0.1934, -0.0885],\n",
      "        [-0.0613,  0.1515, -0.0892,  0.0997],\n",
      "        [ 0.1085,  0.1849, -0.1100,  0.0362],\n",
      "        [-0.0083,  0.1678, -0.1493,  0.0104],\n",
      "        [ 0.2329, -0.2497, -0.1154,  0.0158],\n",
      "        [ 0.2373,  0.1807,  0.2185,  0.2416],\n",
      "        [-0.0733,  0.1733,  0.0052,  0.2215],\n",
      "        [ 0.1725, -0.0791,  0.0407, -0.1026]], requires_grad=True)\n",
      "Wdown spliced: torch.Size([16, 4])\n",
      "tensor([[ 0.0028,  0.1688,  0.1252, -0.2132],\n",
      "        [ 0.2146,  0.2054, -0.1145,  0.1425],\n",
      "        [ 0.2420, -0.1323,  0.0254,  0.0988],\n",
      "        [-0.2279, -0.2429,  0.0742,  0.1267],\n",
      "        [ 0.1254,  0.0361,  0.2388, -0.2003],\n",
      "        [-0.0737, -0.1731, -0.2045, -0.1040],\n",
      "        [ 0.0186, -0.1040, -0.0075,  0.1881],\n",
      "        [ 0.0218, -0.1834,  0.1468, -0.1363],\n",
      "        [ 0.0217,  0.2413,  0.1934, -0.0885],\n",
      "        [-0.0613,  0.1515, -0.0892,  0.0997],\n",
      "        [ 0.1085,  0.1849, -0.1100,  0.0362],\n",
      "        [-0.0083,  0.1678, -0.1493,  0.0104],\n",
      "        [ 0.2329, -0.2497, -0.1154,  0.0158],\n",
      "        [ 0.2373,  0.1807,  0.2185,  0.2416],\n",
      "        [-0.0733,  0.1733,  0.0052,  0.2215],\n",
      "        [ 0.1725, -0.0791,  0.0407, -0.1026]], grad_fn=<AliasBackward0>)\n",
      "Bdown: torch.Size([4])\n",
      "Parameter containing:\n",
      "tensor([-0.1299, -0.0603, -0.1047, -0.2112], requires_grad=True)\n",
      "Bdown spliced: torch.Size([4])\n",
      "tensor([-0.1299, -0.0603, -0.1047, -0.2112], grad_fn=<SliceBackward0>)\n",
      "outputs: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.1277, -0.1387, -0.1862, -0.3442],\n",
      "         [-0.0037,  0.0964, -0.0216, -0.3827],\n",
      "         [-0.0437, -0.0840, -0.0461, -0.4174]]], grad_fn=<AddBackward0>)\n",
      "------------- END MLP.forwardTensor() ------------\n",
      "layer's final residual state:\n",
      "tensor([[[-0.5520,  0.1924,  1.1748, -1.7492],\n",
      "         [-0.9243,  0.8913, -2.0857,  0.2210],\n",
      "         [ 0.1333,  0.0720,  0.6909, -1.9409]]], grad_fn=<AddBackward0>)\n",
      "----------------- END Layer.forwardTensor() --------------------\n",
      "forwardTensor() output: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.5520,  0.1924,  1.1748, -1.7492],\n",
      "         [-0.9243,  0.8913, -2.0857,  0.2210],\n",
      "         [ 0.1333,  0.0720,  0.6909, -1.9409]]], grad_fn=<AddBackward0>)\n",
      "Level 1 from range(2)\n",
      "Model 0 from range(2)\n",
      "----------------- Layer.forwardTensor() --------------------\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 3, 2])\n",
      "tensor([[[ 0.1415,  1.4670],\n",
      "         [-0.9165,  0.4451],\n",
      "         [-0.5364, -0.1246]]])\n",
      "normed x: torch.Size([1, 3, 2])\n",
      "tensor([[[ 0.1358,  1.4077],\n",
      "         [-1.2721,  0.6178],\n",
      "         [-1.3775, -0.3200]]])\n",
      "dim: 2\n",
      "skip: 0\n",
      "spliced scale: torch.Size([2])\n",
      "tensor([0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 3, 2])\n",
      "tensor([[[ 0.1358,  1.4077],\n",
      "         [-1.2721,  0.6178],\n",
      "         [-1.3775, -0.3200]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "---------- Attention Input: torch.Tensor ------------\n",
      "----------------- MultiQueryAttention.forwardTensor() --------------------\n",
      "x shape: torch.Size([1, 3, 2])\n",
      "d_skip: 0\n",
      "models_in_this_level: 2\n",
      "h_dim: 16\n",
      "h_skip: 0\n",
      "self.Wqkv: torch.Size([4, 192])\n",
      "Parameter containing:\n",
      "tensor([[ 1.8937e-01, -2.8415e-01, -4.6238e-01, -4.0144e-01, -4.4552e-01,\n",
      "          1.9680e-01,  4.4328e-02,  4.4592e-01, -2.1865e-01,  2.1341e-01,\n",
      "         -3.0257e-03, -4.0493e-01,  1.3867e-01,  2.9379e-01,  3.5597e-01,\n",
      "          4.7860e-01, -3.5840e-01,  8.7989e-02, -2.9445e-01, -9.1477e-02,\n",
      "         -3.5326e-02, -1.1366e-01,  1.4490e-01, -2.6402e-03,  6.1200e-02,\n",
      "          6.7722e-02, -3.2172e-01, -3.8489e-01,  2.5546e-01, -2.6750e-01,\n",
      "         -1.2889e-01, -4.3475e-01,  1.1832e-01,  1.9844e-01,  2.2294e-01,\n",
      "          2.2098e-01,  3.4316e-01,  4.0263e-01,  1.8223e-01, -3.1401e-01,\n",
      "          1.4572e-01,  2.5463e-01,  1.4850e-01,  4.2974e-01,  3.9195e-01,\n",
      "         -4.2508e-01,  1.2758e-01,  4.7005e-01,  2.9028e-01,  2.7730e-01,\n",
      "          2.4163e-01,  4.1844e-01, -3.1530e-01,  3.5747e-01,  2.3997e-02,\n",
      "          2.9750e-01,  4.4569e-01, -3.0182e-01, -1.2491e-01,  2.3174e-01,\n",
      "          4.1144e-01, -1.1239e-01, -3.2242e-01,  2.1355e-01, -2.8637e-01,\n",
      "         -4.6897e-01,  3.3208e-01,  3.4030e-02,  4.2027e-01,  1.1778e-01,\n",
      "         -2.8841e-01,  3.7903e-01, -9.4648e-02, -5.8460e-02, -2.6424e-01,\n",
      "          2.9487e-01,  5.8353e-02, -1.1010e-01, -3.4630e-01,  2.7946e-01,\n",
      "          3.0002e-01,  2.4929e-01, -4.9700e-01, -5.6904e-02, -2.3638e-01,\n",
      "          2.2197e-01, -3.2396e-01,  1.0468e-01, -1.2194e-01,  2.9137e-01,\n",
      "         -2.7048e-01, -2.6034e-02,  4.3632e-01,  2.0930e-01,  1.6822e-01,\n",
      "         -2.1106e-02, -2.5414e-01, -1.1344e-02,  1.7114e-01, -3.4630e-01,\n",
      "          3.0182e-01, -1.7840e-01, -4.4955e-02,  4.7685e-01,  2.3429e-01,\n",
      "         -2.0993e-04, -3.1590e-01,  1.4933e-01, -3.4794e-01,  4.0296e-02,\n",
      "         -5.2708e-02, -3.9691e-01,  3.8710e-02, -7.3864e-02,  3.9559e-01,\n",
      "         -2.1087e-01,  4.0306e-01, -3.9320e-01, -9.2826e-02,  4.8890e-01,\n",
      "          4.7009e-01, -4.8984e-01,  1.8414e-02,  4.3571e-01,  3.3965e-02,\n",
      "         -3.1877e-01,  2.5281e-01, -3.5005e-01, -2.3904e-01,  4.2611e-01,\n",
      "         -2.9801e-01,  8.4876e-02,  4.0464e-01, -1.8968e-01, -3.6074e-01,\n",
      "         -3.8140e-03, -3.8703e-01, -1.0132e-01, -4.1207e-01, -1.3494e-04,\n",
      "          3.9459e-01,  2.9657e-01,  2.0455e-01, -1.1478e-01,  4.8224e-01,\n",
      "         -2.4734e-01, -1.0230e-01,  9.2335e-02,  2.4343e-02, -3.5300e-01,\n",
      "          2.0875e-01,  4.3911e-01, -4.2997e-01, -1.1652e-01,  2.6931e-01,\n",
      "          1.5951e-01,  4.8167e-01, -1.0938e-01,  6.8503e-02,  9.3351e-02,\n",
      "         -3.8608e-01, -4.6630e-03, -3.0631e-01, -3.1452e-01,  3.6388e-01,\n",
      "          3.9371e-01, -4.8803e-01, -2.3385e-01, -2.9818e-01, -3.0848e-01,\n",
      "          2.9630e-01,  7.3386e-02, -1.2276e-02,  1.8542e-01, -4.1924e-02,\n",
      "         -4.0357e-01, -1.0265e-01,  2.8177e-01, -4.9081e-01,  2.1151e-01,\n",
      "          3.6830e-01,  3.0946e-01,  1.5720e-01,  1.4172e-01, -4.2283e-01,\n",
      "         -4.7915e-01,  3.8314e-01, -3.4119e-01, -2.9776e-01, -1.5916e-01,\n",
      "          3.1954e-01,  3.8582e-01],\n",
      "        [ 2.8393e-02, -2.1507e-01,  7.3449e-02, -3.5425e-02,  8.4971e-03,\n",
      "          6.4568e-02, -4.1707e-01,  4.9329e-01,  6.2499e-02, -3.1489e-01,\n",
      "         -2.6528e-01, -2.0642e-01,  2.8555e-01,  2.0611e-01, -2.4231e-01,\n",
      "          1.9236e-01, -2.4110e-01,  3.8176e-01, -3.1023e-01,  5.9767e-02,\n",
      "         -3.1687e-01, -4.0727e-01, -2.7802e-01, -4.0448e-01, -2.9654e-01,\n",
      "          4.2772e-01, -2.7135e-02, -9.2047e-02, -1.7364e-01, -3.5465e-02,\n",
      "          4.5284e-02, -1.1347e-01,  1.1493e-01,  2.4035e-02,  4.2536e-01,\n",
      "         -2.7699e-01, -2.6960e-01,  4.5004e-01,  1.0166e-01,  4.5971e-01,\n",
      "         -4.6756e-01,  2.6671e-01,  6.4181e-02,  3.1845e-01, -4.9001e-01,\n",
      "          1.8032e-01,  1.0662e-01, -4.9253e-01, -8.3386e-02, -3.9306e-01,\n",
      "         -4.4291e-01, -7.9380e-02,  5.9529e-03,  1.9958e-02, -2.0477e-01,\n",
      "         -3.1541e-01, -1.0251e-01, -3.1129e-01, -2.1621e-02, -1.0289e-02,\n",
      "         -4.2008e-01, -4.8188e-02,  3.7436e-01, -1.1315e-01, -4.9122e-01,\n",
      "         -4.0513e-01,  2.0755e-01,  1.1163e-01, -4.9098e-01,  4.1759e-01,\n",
      "          1.6288e-01,  6.8331e-04, -3.7547e-01,  1.7793e-01, -1.0203e-01,\n",
      "         -3.3526e-01,  3.0938e-01,  3.5360e-01, -4.9011e-01, -3.8699e-01,\n",
      "          2.1129e-01, -4.3664e-01, -2.1207e-01, -3.9717e-01,  3.2691e-01,\n",
      "         -1.7923e-01,  3.1414e-01,  4.3294e-01,  4.4856e-01,  4.4316e-01,\n",
      "          2.7930e-01, -3.8357e-01,  3.5021e-01,  3.0501e-01,  3.6156e-01,\n",
      "         -3.5105e-01,  3.5966e-01,  4.6147e-01,  2.8881e-01,  1.0856e-02,\n",
      "          3.1861e-01,  3.0779e-02,  2.5312e-02, -1.1220e-01,  4.2442e-01,\n",
      "          8.0456e-02, -4.1062e-01,  4.2825e-01,  1.9142e-01,  1.2905e-01,\n",
      "         -4.1203e-01, -4.0837e-01,  1.0011e-01,  1.0266e-01,  1.1884e-01,\n",
      "         -2.9486e-01, -1.7023e-01, -3.9583e-01,  4.3081e-01, -4.8520e-01,\n",
      "          8.0888e-03,  1.3026e-02,  3.0812e-02,  3.1113e-01,  5.4653e-02,\n",
      "          3.3968e-01, -2.7318e-02, -1.7378e-01, -1.9805e-01,  1.3532e-02,\n",
      "         -7.3870e-02, -2.5023e-01,  4.2612e-01,  1.9641e-01,  4.4966e-01,\n",
      "          3.5824e-01, -3.8031e-01,  6.2354e-02,  2.4223e-01, -2.8999e-01,\n",
      "         -2.9022e-01,  1.9836e-01,  2.7220e-01,  3.3876e-01,  2.3070e-01,\n",
      "         -3.7776e-01,  3.4646e-01,  3.5032e-01, -2.7519e-01, -3.7461e-01,\n",
      "         -1.4845e-01, -4.0748e-01,  4.1486e-01,  4.7666e-01,  2.3587e-01,\n",
      "          3.7843e-01,  4.7986e-01,  3.1608e-01,  4.0587e-01,  4.5123e-01,\n",
      "          1.6128e-01,  3.4837e-02, -2.4213e-01,  8.3348e-02,  4.0535e-01,\n",
      "         -2.0574e-01,  1.2630e-01, -1.4109e-01, -1.3830e-01, -1.7473e-01,\n",
      "          2.6931e-01,  1.7263e-01, -3.6738e-01, -2.2899e-01,  4.7750e-02,\n",
      "         -3.8553e-01, -3.5645e-01,  1.2255e-01,  2.6394e-01,  1.4087e-01,\n",
      "          4.8568e-01,  3.9760e-01, -4.6300e-01, -4.0858e-01, -1.0156e-02,\n",
      "          2.2234e-01, -1.7810e-02,  2.4384e-01, -1.7978e-01, -4.6589e-01,\n",
      "         -4.8086e-01,  4.0185e-01],\n",
      "        [ 2.4316e-01,  5.1094e-02,  5.5459e-02, -4.2938e-01, -1.7976e-01,\n",
      "          3.1237e-01, -3.2596e-02, -2.8321e-01,  2.7905e-01, -2.7461e-01,\n",
      "          1.0299e-01,  1.8728e-01, -1.6608e-01,  1.3264e-01,  2.7119e-01,\n",
      "          6.4278e-02,  3.1660e-01,  2.2600e-01, -4.8537e-01,  4.2158e-01,\n",
      "          4.8353e-01, -1.8455e-01,  8.0709e-02, -1.7443e-01,  4.0629e-01,\n",
      "          4.3725e-01, -1.5402e-01, -1.8739e-03,  4.4505e-01, -3.8958e-01,\n",
      "          1.8676e-01,  4.1720e-01, -4.8323e-01,  1.4650e-01,  3.3617e-01,\n",
      "          4.6191e-01,  4.8709e-01,  4.2077e-01,  2.7973e-01,  1.4151e-01,\n",
      "          4.8756e-01, -9.2613e-02, -8.1956e-02,  3.6287e-01, -4.3972e-01,\n",
      "         -3.8881e-01, -5.7135e-02,  3.9335e-01,  4.2260e-01, -4.6757e-01,\n",
      "         -2.9373e-01, -3.9142e-01, -4.4682e-01,  3.2180e-01,  3.9332e-01,\n",
      "          5.9391e-02, -4.0696e-01,  4.0756e-01, -3.7403e-01, -3.4418e-01,\n",
      "         -6.7751e-02,  3.9745e-01, -3.6439e-01,  3.8382e-01, -7.6625e-02,\n",
      "         -3.1299e-01,  4.9208e-02, -9.0591e-02,  2.8831e-01, -4.6503e-01,\n",
      "         -7.9162e-02, -1.3593e-01,  2.0823e-01,  1.3763e-01, -2.5408e-01,\n",
      "          1.9507e-01,  3.0950e-01, -4.6143e-01,  4.1417e-02, -3.1686e-02,\n",
      "         -8.1151e-02, -7.1416e-02,  1.4370e-01, -4.2339e-01, -3.9791e-01,\n",
      "          1.2989e-02,  9.9221e-02,  6.6198e-02,  2.0977e-01,  7.8000e-02,\n",
      "         -1.7535e-01,  4.6218e-01, -2.5272e-01, -4.7835e-01,  1.2324e-01,\n",
      "         -2.4548e-01,  1.5080e-01, -5.3550e-02,  1.2076e-01,  3.8934e-01,\n",
      "          3.8205e-01, -4.0994e-01, -1.1160e-01,  2.9541e-01, -3.4564e-01,\n",
      "          2.6261e-01,  5.1345e-02, -3.3273e-02,  3.0493e-01, -7.7605e-02,\n",
      "         -1.5532e-01,  1.4562e-01, -2.4403e-01,  2.2133e-01, -4.4685e-01,\n",
      "         -9.4684e-02,  2.7953e-01, -4.0533e-01,  2.5822e-01,  4.4835e-01,\n",
      "          3.3890e-01,  1.3830e-01, -2.2497e-01,  9.8689e-02,  2.8423e-01,\n",
      "         -2.7537e-01, -2.9027e-02,  3.8509e-01, -1.0216e-01,  8.3434e-02,\n",
      "         -4.5676e-01, -3.2859e-01, -1.8846e-01, -3.8350e-01,  4.8841e-01,\n",
      "          7.9258e-02, -3.2171e-01, -4.3615e-02, -3.8480e-01, -6.7773e-02,\n",
      "         -4.7649e-01, -4.3129e-01, -4.9387e-01,  4.9946e-01,  2.1872e-01,\n",
      "          1.6125e-01,  4.2134e-01,  4.6237e-01, -2.6883e-02, -6.6878e-02,\n",
      "         -3.0271e-01,  4.1654e-01,  3.9426e-01,  6.2786e-03,  3.8204e-01,\n",
      "         -2.3100e-01,  1.6674e-01,  1.9269e-02, -1.5630e-01,  1.3106e-01,\n",
      "          3.4213e-01, -2.8111e-01, -4.6205e-01,  3.2671e-01, -1.4839e-01,\n",
      "         -3.5897e-01, -4.2942e-01,  3.7445e-01,  2.6903e-01, -2.6800e-01,\n",
      "          4.6427e-01, -4.1747e-01,  3.9074e-01, -2.8527e-01,  2.7827e-01,\n",
      "         -1.4761e-02, -7.7906e-02, -2.4888e-01,  4.9965e-01,  3.1182e-01,\n",
      "          4.2581e-01, -3.5637e-02,  2.6706e-01, -2.1018e-01, -4.6920e-01,\n",
      "         -3.0328e-01, -8.8848e-02,  1.3523e-01, -1.2193e-01,  2.9067e-01,\n",
      "          1.4192e-01,  2.9863e-01],\n",
      "        [ 2.1939e-02, -2.1620e-01, -1.3989e-01,  2.0001e-01, -2.6691e-01,\n",
      "         -2.2999e-01,  3.5089e-01,  4.2780e-01, -2.5216e-01,  2.5742e-01,\n",
      "         -4.2682e-01, -2.3400e-01,  2.5624e-01,  5.2858e-02, -4.4735e-02,\n",
      "         -2.7120e-01,  2.7121e-01, -1.1131e-02, -2.0003e-01,  3.6435e-01,\n",
      "         -1.3997e-01, -3.0246e-01,  2.5170e-01, -1.9140e-01, -4.2226e-01,\n",
      "         -2.8699e-01,  4.4266e-02,  6.2823e-02,  1.7323e-01, -8.4048e-02,\n",
      "          2.8983e-02,  7.1588e-02, -4.9993e-01,  4.3573e-01, -1.6429e-01,\n",
      "         -2.9448e-01, -4.0873e-01,  2.7543e-01, -3.6739e-01,  1.9690e-01,\n",
      "         -2.6216e-01,  2.5825e-01, -1.5555e-01, -1.9827e-01,  3.1932e-01,\n",
      "         -2.0889e-01, -2.8329e-01, -1.9644e-01,  7.6186e-02, -4.7227e-01,\n",
      "          1.4629e-01,  3.2163e-01,  4.3594e-01, -2.9314e-01,  2.6589e-01,\n",
      "         -6.6499e-02,  2.4181e-01,  4.1164e-01,  4.9924e-01,  9.3671e-02,\n",
      "         -4.7423e-01, -3.0037e-01, -3.6135e-01, -4.7988e-01,  4.1612e-01,\n",
      "          7.5813e-02, -4.3853e-01, -1.3715e-01,  2.6466e-01, -4.1169e-01,\n",
      "         -2.6106e-01,  2.4077e-01,  2.4765e-01,  2.0474e-01, -4.8767e-01,\n",
      "          3.4312e-01,  3.0089e-01, -2.6358e-01, -2.3430e-01,  3.1284e-01,\n",
      "          4.9953e-01, -3.5684e-02,  1.9416e-01,  4.6613e-01,  4.7486e-01,\n",
      "          7.1292e-02, -1.6498e-01,  2.6788e-01,  3.3629e-01,  4.4775e-01,\n",
      "          4.7669e-01, -4.7820e-01, -4.4081e-01, -4.0668e-01,  2.4537e-01,\n",
      "         -2.8373e-01,  2.6604e-01, -1.5824e-01, -2.7501e-01,  4.6280e-01,\n",
      "          4.3243e-01, -6.9161e-03,  3.4847e-01, -4.3560e-01, -3.1419e-01,\n",
      "         -5.0649e-02, -1.4905e-01, -2.1218e-01,  4.0496e-01,  9.8498e-02,\n",
      "          2.1212e-02, -2.1080e-01,  3.0203e-01, -3.2247e-02, -3.5137e-01,\n",
      "         -4.0825e-01,  3.6829e-01,  7.6268e-02, -4.5142e-01, -7.0428e-02,\n",
      "         -3.8538e-02, -4.0860e-01, -2.3721e-01, -2.6784e-01,  2.6255e-01,\n",
      "          4.1158e-01, -1.2125e-01,  4.8033e-01, -3.1060e-01,  4.8208e-04,\n",
      "         -1.2071e-01,  3.9326e-02,  3.2534e-01,  1.0235e-01,  2.3560e-01,\n",
      "          3.7913e-01, -4.0633e-01, -3.9607e-01,  4.2034e-01,  9.0152e-02,\n",
      "         -3.9707e-01,  2.2478e-01,  4.7630e-01,  8.5905e-02, -3.4227e-01,\n",
      "         -2.6248e-01,  2.0666e-01,  2.5354e-01,  4.9642e-01, -3.3560e-01,\n",
      "         -2.5921e-01, -1.5643e-01,  2.4026e-01,  2.8152e-01,  7.2462e-02,\n",
      "          5.1272e-02,  4.5968e-01,  2.5359e-02, -3.2823e-01, -9.0428e-02,\n",
      "         -4.1559e-01,  1.7903e-01, -1.5719e-01,  3.1318e-01,  1.4970e-01,\n",
      "         -3.7570e-01, -3.2366e-01,  7.3289e-02, -1.7857e-01, -7.6912e-02,\n",
      "         -1.5517e-01, -2.5391e-01, -3.7961e-01, -3.2698e-01,  5.0610e-02,\n",
      "         -2.0522e-01, -5.3520e-02,  7.4136e-02,  4.2519e-01, -2.1097e-01,\n",
      "          1.1299e-01,  4.5611e-01, -4.4404e-02,  3.8389e-01, -2.2574e-01,\n",
      "         -2.3627e-01, -4.0755e-01, -4.2760e-01, -4.0825e-02, -4.1447e-01,\n",
      "          4.5897e-01, -1.1031e-01]], requires_grad=True)\n",
      "Wq: torch.Size([4, 128])\n",
      "tensor([[ 1.8937e-01, -2.8415e-01, -4.6238e-01, -4.0144e-01, -4.4552e-01,\n",
      "          1.9680e-01,  4.4328e-02,  4.4592e-01, -2.1865e-01,  2.1341e-01,\n",
      "         -3.0257e-03, -4.0493e-01,  1.3867e-01,  2.9379e-01,  3.5597e-01,\n",
      "          4.7860e-01, -3.5840e-01,  8.7989e-02, -2.9445e-01, -9.1477e-02,\n",
      "         -3.5326e-02, -1.1366e-01,  1.4490e-01, -2.6402e-03,  6.1200e-02,\n",
      "          6.7722e-02, -3.2172e-01, -3.8489e-01,  2.5546e-01, -2.6750e-01,\n",
      "         -1.2889e-01, -4.3475e-01,  1.1832e-01,  1.9844e-01,  2.2294e-01,\n",
      "          2.2098e-01,  3.4316e-01,  4.0263e-01,  1.8223e-01, -3.1401e-01,\n",
      "          1.4572e-01,  2.5463e-01,  1.4850e-01,  4.2974e-01,  3.9195e-01,\n",
      "         -4.2508e-01,  1.2758e-01,  4.7005e-01,  2.9028e-01,  2.7730e-01,\n",
      "          2.4163e-01,  4.1844e-01, -3.1530e-01,  3.5747e-01,  2.3997e-02,\n",
      "          2.9750e-01,  4.4569e-01, -3.0182e-01, -1.2491e-01,  2.3174e-01,\n",
      "          4.1144e-01, -1.1239e-01, -3.2242e-01,  2.1355e-01, -2.8637e-01,\n",
      "         -4.6897e-01,  3.3208e-01,  3.4030e-02,  4.2027e-01,  1.1778e-01,\n",
      "         -2.8841e-01,  3.7903e-01, -9.4648e-02, -5.8460e-02, -2.6424e-01,\n",
      "          2.9487e-01,  5.8353e-02, -1.1010e-01, -3.4630e-01,  2.7946e-01,\n",
      "          3.0002e-01,  2.4929e-01, -4.9700e-01, -5.6904e-02, -2.3638e-01,\n",
      "          2.2197e-01, -3.2396e-01,  1.0468e-01, -1.2194e-01,  2.9137e-01,\n",
      "         -2.7048e-01, -2.6034e-02,  4.3632e-01,  2.0930e-01,  1.6822e-01,\n",
      "         -2.1106e-02, -2.5414e-01, -1.1344e-02,  1.7114e-01, -3.4630e-01,\n",
      "          3.0182e-01, -1.7840e-01, -4.4955e-02,  4.7685e-01,  2.3429e-01,\n",
      "         -2.0993e-04, -3.1590e-01,  1.4933e-01, -3.4794e-01,  4.0296e-02,\n",
      "         -5.2708e-02, -3.9691e-01,  3.8710e-02, -7.3864e-02,  3.9559e-01,\n",
      "         -2.1087e-01,  4.0306e-01, -3.9320e-01, -9.2826e-02,  4.8890e-01,\n",
      "          4.7009e-01, -4.8984e-01,  1.8414e-02,  4.3571e-01,  3.3965e-02,\n",
      "         -3.1877e-01,  2.5281e-01, -3.5005e-01],\n",
      "        [ 2.8393e-02, -2.1507e-01,  7.3449e-02, -3.5425e-02,  8.4971e-03,\n",
      "          6.4568e-02, -4.1707e-01,  4.9329e-01,  6.2499e-02, -3.1489e-01,\n",
      "         -2.6528e-01, -2.0642e-01,  2.8555e-01,  2.0611e-01, -2.4231e-01,\n",
      "          1.9236e-01, -2.4110e-01,  3.8176e-01, -3.1023e-01,  5.9767e-02,\n",
      "         -3.1687e-01, -4.0727e-01, -2.7802e-01, -4.0448e-01, -2.9654e-01,\n",
      "          4.2772e-01, -2.7135e-02, -9.2047e-02, -1.7364e-01, -3.5465e-02,\n",
      "          4.5284e-02, -1.1347e-01,  1.1493e-01,  2.4035e-02,  4.2536e-01,\n",
      "         -2.7699e-01, -2.6960e-01,  4.5004e-01,  1.0166e-01,  4.5971e-01,\n",
      "         -4.6756e-01,  2.6671e-01,  6.4181e-02,  3.1845e-01, -4.9001e-01,\n",
      "          1.8032e-01,  1.0662e-01, -4.9253e-01, -8.3386e-02, -3.9306e-01,\n",
      "         -4.4291e-01, -7.9380e-02,  5.9529e-03,  1.9958e-02, -2.0477e-01,\n",
      "         -3.1541e-01, -1.0251e-01, -3.1129e-01, -2.1621e-02, -1.0289e-02,\n",
      "         -4.2008e-01, -4.8188e-02,  3.7436e-01, -1.1315e-01, -4.9122e-01,\n",
      "         -4.0513e-01,  2.0755e-01,  1.1163e-01, -4.9098e-01,  4.1759e-01,\n",
      "          1.6288e-01,  6.8331e-04, -3.7547e-01,  1.7793e-01, -1.0203e-01,\n",
      "         -3.3526e-01,  3.0938e-01,  3.5360e-01, -4.9011e-01, -3.8699e-01,\n",
      "          2.1129e-01, -4.3664e-01, -2.1207e-01, -3.9717e-01,  3.2691e-01,\n",
      "         -1.7923e-01,  3.1414e-01,  4.3294e-01,  4.4856e-01,  4.4316e-01,\n",
      "          2.7930e-01, -3.8357e-01,  3.5021e-01,  3.0501e-01,  3.6156e-01,\n",
      "         -3.5105e-01,  3.5966e-01,  4.6147e-01,  2.8881e-01,  1.0856e-02,\n",
      "          3.1861e-01,  3.0779e-02,  2.5312e-02, -1.1220e-01,  4.2442e-01,\n",
      "          8.0456e-02, -4.1062e-01,  4.2825e-01,  1.9142e-01,  1.2905e-01,\n",
      "         -4.1203e-01, -4.0837e-01,  1.0011e-01,  1.0266e-01,  1.1884e-01,\n",
      "         -2.9486e-01, -1.7023e-01, -3.9583e-01,  4.3081e-01, -4.8520e-01,\n",
      "          8.0888e-03,  1.3026e-02,  3.0812e-02,  3.1113e-01,  5.4653e-02,\n",
      "          3.3968e-01, -2.7318e-02, -1.7378e-01],\n",
      "        [ 2.4316e-01,  5.1094e-02,  5.5459e-02, -4.2938e-01, -1.7976e-01,\n",
      "          3.1237e-01, -3.2596e-02, -2.8321e-01,  2.7905e-01, -2.7461e-01,\n",
      "          1.0299e-01,  1.8728e-01, -1.6608e-01,  1.3264e-01,  2.7119e-01,\n",
      "          6.4278e-02,  3.1660e-01,  2.2600e-01, -4.8537e-01,  4.2158e-01,\n",
      "          4.8353e-01, -1.8455e-01,  8.0709e-02, -1.7443e-01,  4.0629e-01,\n",
      "          4.3725e-01, -1.5402e-01, -1.8739e-03,  4.4505e-01, -3.8958e-01,\n",
      "          1.8676e-01,  4.1720e-01, -4.8323e-01,  1.4650e-01,  3.3617e-01,\n",
      "          4.6191e-01,  4.8709e-01,  4.2077e-01,  2.7973e-01,  1.4151e-01,\n",
      "          4.8756e-01, -9.2613e-02, -8.1956e-02,  3.6287e-01, -4.3972e-01,\n",
      "         -3.8881e-01, -5.7135e-02,  3.9335e-01,  4.2260e-01, -4.6757e-01,\n",
      "         -2.9373e-01, -3.9142e-01, -4.4682e-01,  3.2180e-01,  3.9332e-01,\n",
      "          5.9391e-02, -4.0696e-01,  4.0756e-01, -3.7403e-01, -3.4418e-01,\n",
      "         -6.7751e-02,  3.9745e-01, -3.6439e-01,  3.8382e-01, -7.6625e-02,\n",
      "         -3.1299e-01,  4.9208e-02, -9.0591e-02,  2.8831e-01, -4.6503e-01,\n",
      "         -7.9162e-02, -1.3593e-01,  2.0823e-01,  1.3763e-01, -2.5408e-01,\n",
      "          1.9507e-01,  3.0950e-01, -4.6143e-01,  4.1417e-02, -3.1686e-02,\n",
      "         -8.1151e-02, -7.1416e-02,  1.4370e-01, -4.2339e-01, -3.9791e-01,\n",
      "          1.2989e-02,  9.9221e-02,  6.6198e-02,  2.0977e-01,  7.8000e-02,\n",
      "         -1.7535e-01,  4.6218e-01, -2.5272e-01, -4.7835e-01,  1.2324e-01,\n",
      "         -2.4548e-01,  1.5080e-01, -5.3550e-02,  1.2076e-01,  3.8934e-01,\n",
      "          3.8205e-01, -4.0994e-01, -1.1160e-01,  2.9541e-01, -3.4564e-01,\n",
      "          2.6261e-01,  5.1345e-02, -3.3273e-02,  3.0493e-01, -7.7605e-02,\n",
      "         -1.5532e-01,  1.4562e-01, -2.4403e-01,  2.2133e-01, -4.4685e-01,\n",
      "         -9.4684e-02,  2.7953e-01, -4.0533e-01,  2.5822e-01,  4.4835e-01,\n",
      "          3.3890e-01,  1.3830e-01, -2.2497e-01,  9.8689e-02,  2.8423e-01,\n",
      "         -2.7537e-01, -2.9027e-02,  3.8509e-01],\n",
      "        [ 2.1939e-02, -2.1620e-01, -1.3989e-01,  2.0001e-01, -2.6691e-01,\n",
      "         -2.2999e-01,  3.5089e-01,  4.2780e-01, -2.5216e-01,  2.5742e-01,\n",
      "         -4.2682e-01, -2.3400e-01,  2.5624e-01,  5.2858e-02, -4.4735e-02,\n",
      "         -2.7120e-01,  2.7121e-01, -1.1131e-02, -2.0003e-01,  3.6435e-01,\n",
      "         -1.3997e-01, -3.0246e-01,  2.5170e-01, -1.9140e-01, -4.2226e-01,\n",
      "         -2.8699e-01,  4.4266e-02,  6.2823e-02,  1.7323e-01, -8.4048e-02,\n",
      "          2.8983e-02,  7.1588e-02, -4.9993e-01,  4.3573e-01, -1.6429e-01,\n",
      "         -2.9448e-01, -4.0873e-01,  2.7543e-01, -3.6739e-01,  1.9690e-01,\n",
      "         -2.6216e-01,  2.5825e-01, -1.5555e-01, -1.9827e-01,  3.1932e-01,\n",
      "         -2.0889e-01, -2.8329e-01, -1.9644e-01,  7.6186e-02, -4.7227e-01,\n",
      "          1.4629e-01,  3.2163e-01,  4.3594e-01, -2.9314e-01,  2.6589e-01,\n",
      "         -6.6499e-02,  2.4181e-01,  4.1164e-01,  4.9924e-01,  9.3671e-02,\n",
      "         -4.7423e-01, -3.0037e-01, -3.6135e-01, -4.7988e-01,  4.1612e-01,\n",
      "          7.5813e-02, -4.3853e-01, -1.3715e-01,  2.6466e-01, -4.1169e-01,\n",
      "         -2.6106e-01,  2.4077e-01,  2.4765e-01,  2.0474e-01, -4.8767e-01,\n",
      "          3.4312e-01,  3.0089e-01, -2.6358e-01, -2.3430e-01,  3.1284e-01,\n",
      "          4.9953e-01, -3.5684e-02,  1.9416e-01,  4.6613e-01,  4.7486e-01,\n",
      "          7.1292e-02, -1.6498e-01,  2.6788e-01,  3.3629e-01,  4.4775e-01,\n",
      "          4.7669e-01, -4.7820e-01, -4.4081e-01, -4.0668e-01,  2.4537e-01,\n",
      "         -2.8373e-01,  2.6604e-01, -1.5824e-01, -2.7501e-01,  4.6280e-01,\n",
      "          4.3243e-01, -6.9161e-03,  3.4847e-01, -4.3560e-01, -3.1419e-01,\n",
      "         -5.0649e-02, -1.4905e-01, -2.1218e-01,  4.0496e-01,  9.8498e-02,\n",
      "          2.1212e-02, -2.1080e-01,  3.0203e-01, -3.2247e-02, -3.5137e-01,\n",
      "         -4.0825e-01,  3.6829e-01,  7.6268e-02, -4.5142e-01, -7.0428e-02,\n",
      "         -3.8538e-02, -4.0860e-01, -2.3721e-01, -2.6784e-01,  2.6255e-01,\n",
      "          4.1158e-01, -1.2125e-01,  4.8033e-01]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wk: torch.Size([4, 32])\n",
      "tensor([[-2.3904e-01,  4.2611e-01, -2.9801e-01,  8.4876e-02,  4.0464e-01,\n",
      "         -1.8968e-01, -3.6074e-01, -3.8140e-03, -3.8703e-01, -1.0132e-01,\n",
      "         -4.1207e-01, -1.3494e-04,  3.9459e-01,  2.9657e-01,  2.0455e-01,\n",
      "         -1.1478e-01,  4.8224e-01, -2.4734e-01, -1.0230e-01,  9.2335e-02,\n",
      "          2.4343e-02, -3.5300e-01,  2.0875e-01,  4.3911e-01, -4.2997e-01,\n",
      "         -1.1652e-01,  2.6931e-01,  1.5951e-01,  4.8167e-01, -1.0938e-01,\n",
      "          6.8503e-02,  9.3351e-02],\n",
      "        [-1.9805e-01,  1.3532e-02, -7.3870e-02, -2.5023e-01,  4.2612e-01,\n",
      "          1.9641e-01,  4.4966e-01,  3.5824e-01, -3.8031e-01,  6.2354e-02,\n",
      "          2.4223e-01, -2.8999e-01, -2.9022e-01,  1.9836e-01,  2.7220e-01,\n",
      "          3.3876e-01,  2.3070e-01, -3.7776e-01,  3.4646e-01,  3.5032e-01,\n",
      "         -2.7519e-01, -3.7461e-01, -1.4845e-01, -4.0748e-01,  4.1486e-01,\n",
      "          4.7666e-01,  2.3587e-01,  3.7843e-01,  4.7986e-01,  3.1608e-01,\n",
      "          4.0587e-01,  4.5123e-01],\n",
      "        [-1.0216e-01,  8.3434e-02, -4.5676e-01, -3.2859e-01, -1.8846e-01,\n",
      "         -3.8350e-01,  4.8841e-01,  7.9258e-02, -3.2171e-01, -4.3615e-02,\n",
      "         -3.8480e-01, -6.7773e-02, -4.7649e-01, -4.3129e-01, -4.9387e-01,\n",
      "          4.9946e-01,  2.1872e-01,  1.6125e-01,  4.2134e-01,  4.6237e-01,\n",
      "         -2.6883e-02, -6.6878e-02, -3.0271e-01,  4.1654e-01,  3.9426e-01,\n",
      "          6.2786e-03,  3.8204e-01, -2.3100e-01,  1.6674e-01,  1.9269e-02,\n",
      "         -1.5630e-01,  1.3106e-01],\n",
      "        [-3.1060e-01,  4.8208e-04, -1.2071e-01,  3.9326e-02,  3.2534e-01,\n",
      "          1.0235e-01,  2.3560e-01,  3.7913e-01, -4.0633e-01, -3.9607e-01,\n",
      "          4.2034e-01,  9.0152e-02, -3.9707e-01,  2.2478e-01,  4.7630e-01,\n",
      "          8.5905e-02, -3.4227e-01, -2.6248e-01,  2.0666e-01,  2.5354e-01,\n",
      "          4.9642e-01, -3.3560e-01, -2.5921e-01, -1.5643e-01,  2.4026e-01,\n",
      "          2.8152e-01,  7.2462e-02,  5.1272e-02,  4.5968e-01,  2.5359e-02,\n",
      "         -3.2823e-01, -9.0428e-02]], grad_fn=<SplitWithSizesBackward0>)\n",
      "Wv: torch.Size([4, 32])\n",
      "tensor([[-0.3861, -0.0047, -0.3063, -0.3145,  0.3639,  0.3937, -0.4880, -0.2338,\n",
      "         -0.2982, -0.3085,  0.2963,  0.0734, -0.0123,  0.1854, -0.0419, -0.4036,\n",
      "         -0.1026,  0.2818, -0.4908,  0.2115,  0.3683,  0.3095,  0.1572,  0.1417,\n",
      "         -0.4228, -0.4791,  0.3831, -0.3412, -0.2978, -0.1592,  0.3195,  0.3858],\n",
      "        [ 0.1613,  0.0348, -0.2421,  0.0833,  0.4053, -0.2057,  0.1263, -0.1411,\n",
      "         -0.1383, -0.1747,  0.2693,  0.1726, -0.3674, -0.2290,  0.0477, -0.3855,\n",
      "         -0.3565,  0.1225,  0.2639,  0.1409,  0.4857,  0.3976, -0.4630, -0.4086,\n",
      "         -0.0102,  0.2223, -0.0178,  0.2438, -0.1798, -0.4659, -0.4809,  0.4019],\n",
      "        [ 0.3421, -0.2811, -0.4621,  0.3267, -0.1484, -0.3590, -0.4294,  0.3745,\n",
      "          0.2690, -0.2680,  0.4643, -0.4175,  0.3907, -0.2853,  0.2783, -0.0148,\n",
      "         -0.0779, -0.2489,  0.4996,  0.3118,  0.4258, -0.0356,  0.2671, -0.2102,\n",
      "         -0.4692, -0.3033, -0.0888,  0.1352, -0.1219,  0.2907,  0.1419,  0.2986],\n",
      "        [-0.4156,  0.1790, -0.1572,  0.3132,  0.1497, -0.3757, -0.3237,  0.0733,\n",
      "         -0.1786, -0.0769, -0.1552, -0.2539, -0.3796, -0.3270,  0.0506, -0.2052,\n",
      "         -0.0535,  0.0741,  0.4252, -0.2110,  0.1130,  0.4561, -0.0444,  0.3839,\n",
      "         -0.2257, -0.2363, -0.4075, -0.4276, -0.0408, -0.4145,  0.4590, -0.1103]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wq spliced: torch.Size([2, 64])\n",
      "tensor([[ 1.8937e-01, -2.8415e-01, -4.6238e-01, -4.0144e-01, -4.4552e-01,\n",
      "          1.9680e-01,  4.4328e-02,  4.4592e-01, -2.1865e-01,  2.1341e-01,\n",
      "         -3.0257e-03, -4.0493e-01,  1.3867e-01,  2.9379e-01,  3.5597e-01,\n",
      "          4.7860e-01,  1.1832e-01,  1.9844e-01,  2.2294e-01,  2.2098e-01,\n",
      "          3.4316e-01,  4.0263e-01,  1.8223e-01, -3.1401e-01,  1.4572e-01,\n",
      "          2.5463e-01,  1.4850e-01,  4.2974e-01,  3.9195e-01, -4.2508e-01,\n",
      "          1.2758e-01,  4.7005e-01, -2.8637e-01, -4.6897e-01,  3.3208e-01,\n",
      "          3.4030e-02,  4.2027e-01,  1.1778e-01, -2.8841e-01,  3.7903e-01,\n",
      "         -9.4648e-02, -5.8460e-02, -2.6424e-01,  2.9487e-01,  5.8353e-02,\n",
      "         -1.1010e-01, -3.4630e-01,  2.7946e-01, -2.5414e-01, -1.1344e-02,\n",
      "          1.7114e-01, -3.4630e-01,  3.0182e-01, -1.7840e-01, -4.4955e-02,\n",
      "          4.7685e-01,  2.3429e-01, -2.0993e-04, -3.1590e-01,  1.4933e-01,\n",
      "         -3.4794e-01,  4.0296e-02, -5.2708e-02, -3.9691e-01],\n",
      "        [ 2.8393e-02, -2.1507e-01,  7.3449e-02, -3.5425e-02,  8.4971e-03,\n",
      "          6.4568e-02, -4.1707e-01,  4.9329e-01,  6.2499e-02, -3.1489e-01,\n",
      "         -2.6528e-01, -2.0642e-01,  2.8555e-01,  2.0611e-01, -2.4231e-01,\n",
      "          1.9236e-01,  1.1493e-01,  2.4035e-02,  4.2536e-01, -2.7699e-01,\n",
      "         -2.6960e-01,  4.5004e-01,  1.0166e-01,  4.5971e-01, -4.6756e-01,\n",
      "          2.6671e-01,  6.4181e-02,  3.1845e-01, -4.9001e-01,  1.8032e-01,\n",
      "          1.0662e-01, -4.9253e-01, -4.9122e-01, -4.0513e-01,  2.0755e-01,\n",
      "          1.1163e-01, -4.9098e-01,  4.1759e-01,  1.6288e-01,  6.8331e-04,\n",
      "         -3.7547e-01,  1.7793e-01, -1.0203e-01, -3.3526e-01,  3.0938e-01,\n",
      "          3.5360e-01, -4.9011e-01, -3.8699e-01,  3.5966e-01,  4.6147e-01,\n",
      "          2.8881e-01,  1.0856e-02,  3.1861e-01,  3.0779e-02,  2.5312e-02,\n",
      "         -1.1220e-01,  4.2442e-01,  8.0456e-02, -4.1062e-01,  4.2825e-01,\n",
      "          1.9142e-01,  1.2905e-01, -4.1203e-01, -4.0837e-01]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Wk spliced: torch.Size([2, 16])\n",
      "tensor([[-2.3904e-01,  4.2611e-01, -2.9801e-01,  8.4876e-02,  4.0464e-01,\n",
      "         -1.8968e-01, -3.6074e-01, -3.8140e-03, -3.8703e-01, -1.0132e-01,\n",
      "         -4.1207e-01, -1.3494e-04,  3.9459e-01,  2.9657e-01,  2.0455e-01,\n",
      "         -1.1478e-01],\n",
      "        [-1.9805e-01,  1.3532e-02, -7.3870e-02, -2.5023e-01,  4.2612e-01,\n",
      "          1.9641e-01,  4.4966e-01,  3.5824e-01, -3.8031e-01,  6.2354e-02,\n",
      "          2.4223e-01, -2.8999e-01, -2.9022e-01,  1.9836e-01,  2.7220e-01,\n",
      "          3.3876e-01]], grad_fn=<CatBackward0>)\n",
      "Wv spliced: torch.Size([2, 16])\n",
      "tensor([[-0.3861, -0.0047, -0.3063, -0.3145,  0.3639,  0.3937, -0.4880, -0.2338,\n",
      "         -0.2982, -0.3085,  0.2963,  0.0734, -0.0123,  0.1854, -0.0419, -0.4036],\n",
      "        [ 0.1613,  0.0348, -0.2421,  0.0833,  0.4053, -0.2057,  0.1263, -0.1411,\n",
      "         -0.1383, -0.1747,  0.2693,  0.1726, -0.3674, -0.2290,  0.0477, -0.3855]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Wqkv_spliced: torch.Size([2, 96])\n",
      "tensor([[ 1.8937e-01, -2.8415e-01, -4.6238e-01, -4.0144e-01, -4.4552e-01,\n",
      "          1.9680e-01,  4.4328e-02,  4.4592e-01, -2.1865e-01,  2.1341e-01,\n",
      "         -3.0257e-03, -4.0493e-01,  1.3867e-01,  2.9379e-01,  3.5597e-01,\n",
      "          4.7860e-01,  1.1832e-01,  1.9844e-01,  2.2294e-01,  2.2098e-01,\n",
      "          3.4316e-01,  4.0263e-01,  1.8223e-01, -3.1401e-01,  1.4572e-01,\n",
      "          2.5463e-01,  1.4850e-01,  4.2974e-01,  3.9195e-01, -4.2508e-01,\n",
      "          1.2758e-01,  4.7005e-01, -2.8637e-01, -4.6897e-01,  3.3208e-01,\n",
      "          3.4030e-02,  4.2027e-01,  1.1778e-01, -2.8841e-01,  3.7903e-01,\n",
      "         -9.4648e-02, -5.8460e-02, -2.6424e-01,  2.9487e-01,  5.8353e-02,\n",
      "         -1.1010e-01, -3.4630e-01,  2.7946e-01, -2.5414e-01, -1.1344e-02,\n",
      "          1.7114e-01, -3.4630e-01,  3.0182e-01, -1.7840e-01, -4.4955e-02,\n",
      "          4.7685e-01,  2.3429e-01, -2.0993e-04, -3.1590e-01,  1.4933e-01,\n",
      "         -3.4794e-01,  4.0296e-02, -5.2708e-02, -3.9691e-01, -2.3904e-01,\n",
      "          4.2611e-01, -2.9801e-01,  8.4876e-02,  4.0464e-01, -1.8968e-01,\n",
      "         -3.6074e-01, -3.8140e-03, -3.8703e-01, -1.0132e-01, -4.1207e-01,\n",
      "         -1.3494e-04,  3.9459e-01,  2.9657e-01,  2.0455e-01, -1.1478e-01,\n",
      "         -3.8608e-01, -4.6630e-03, -3.0631e-01, -3.1452e-01,  3.6388e-01,\n",
      "          3.9371e-01, -4.8803e-01, -2.3385e-01, -2.9818e-01, -3.0848e-01,\n",
      "          2.9630e-01,  7.3386e-02, -1.2276e-02,  1.8542e-01, -4.1924e-02,\n",
      "         -4.0357e-01],\n",
      "        [ 2.8393e-02, -2.1507e-01,  7.3449e-02, -3.5425e-02,  8.4971e-03,\n",
      "          6.4568e-02, -4.1707e-01,  4.9329e-01,  6.2499e-02, -3.1489e-01,\n",
      "         -2.6528e-01, -2.0642e-01,  2.8555e-01,  2.0611e-01, -2.4231e-01,\n",
      "          1.9236e-01,  1.1493e-01,  2.4035e-02,  4.2536e-01, -2.7699e-01,\n",
      "         -2.6960e-01,  4.5004e-01,  1.0166e-01,  4.5971e-01, -4.6756e-01,\n",
      "          2.6671e-01,  6.4181e-02,  3.1845e-01, -4.9001e-01,  1.8032e-01,\n",
      "          1.0662e-01, -4.9253e-01, -4.9122e-01, -4.0513e-01,  2.0755e-01,\n",
      "          1.1163e-01, -4.9098e-01,  4.1759e-01,  1.6288e-01,  6.8331e-04,\n",
      "         -3.7547e-01,  1.7793e-01, -1.0203e-01, -3.3526e-01,  3.0938e-01,\n",
      "          3.5360e-01, -4.9011e-01, -3.8699e-01,  3.5966e-01,  4.6147e-01,\n",
      "          2.8881e-01,  1.0856e-02,  3.1861e-01,  3.0779e-02,  2.5312e-02,\n",
      "         -1.1220e-01,  4.2442e-01,  8.0456e-02, -4.1062e-01,  4.2825e-01,\n",
      "          1.9142e-01,  1.2905e-01, -4.1203e-01, -4.0837e-01, -1.9805e-01,\n",
      "          1.3532e-02, -7.3870e-02, -2.5023e-01,  4.2612e-01,  1.9641e-01,\n",
      "          4.4966e-01,  3.5824e-01, -3.8031e-01,  6.2354e-02,  2.4223e-01,\n",
      "         -2.8999e-01, -2.9022e-01,  1.9836e-01,  2.7220e-01,  3.3876e-01,\n",
      "          1.6128e-01,  3.4837e-02, -2.4213e-01,  8.3348e-02,  4.0535e-01,\n",
      "         -2.0574e-01,  1.2630e-01, -1.4109e-01, -1.3830e-01, -1.7473e-01,\n",
      "          2.6931e-01,  1.7263e-01, -3.6738e-01, -2.2899e-01,  4.7750e-02,\n",
      "         -3.8553e-01]], grad_fn=<CatBackward0>)\n",
      "xqkv: torch.Size([1, 3, 96])\n",
      "tensor([[[ 6.5680e-02, -3.4133e-01,  4.0613e-02, -1.0437e-01, -4.8529e-02,\n",
      "           1.1761e-01, -5.8108e-01,  7.5494e-01,  5.8290e-02, -4.1429e-01,\n",
      "          -3.7384e-01, -3.4555e-01,  4.2079e-01,  3.3003e-01, -2.9277e-01,\n",
      "           3.3576e-01,  1.7784e-01,  6.0776e-02,  6.2904e-01, -3.5992e-01,\n",
      "          -3.3292e-01,  6.8818e-01,  1.6784e-01,  6.0449e-01, -6.3839e-01,\n",
      "           4.1002e-01,  1.1051e-01,  5.0662e-01, -6.3656e-01,  1.9612e-01,\n",
      "           1.6741e-01, -6.2951e-01, -7.3037e-01, -6.3397e-01,  3.3725e-01,\n",
      "           1.6176e-01, -6.3409e-01,  6.0382e-01,  1.9013e-01,  5.2425e-02,\n",
      "          -5.4140e-01,  2.4253e-01, -1.7950e-01, -4.3191e-01,  4.4343e-01,\n",
      "           4.8281e-01, -7.3694e-01, -5.0681e-01,  4.7179e-01,  6.4806e-01,\n",
      "           4.2978e-01, -3.1738e-02,  4.8948e-01,  1.9104e-02,  2.9528e-02,\n",
      "          -9.3201e-02,  6.2925e-01,  1.1323e-01, -6.2091e-01,  6.2311e-01,\n",
      "           2.2222e-01,  1.8714e-01, -5.8717e-01, -6.2874e-01, -3.1125e-01,\n",
      "           7.6903e-02, -1.4445e-01, -3.4071e-01,  6.5478e-01,  2.5073e-01,\n",
      "           5.8400e-01,  5.0377e-01, -5.8790e-01,  7.4018e-02,  2.8504e-01,\n",
      "          -4.0823e-01, -3.5496e-01,  3.1949e-01,  4.1095e-01,  4.6128e-01,\n",
      "           1.7461e-01,  4.8406e-02, -3.8243e-01,  7.4623e-02,  6.2000e-01,\n",
      "          -2.3616e-01,  1.1152e-01, -2.3036e-01, -2.3517e-01, -2.8785e-01,\n",
      "           4.1934e-01,  2.5297e-01, -5.1882e-01, -2.9717e-01,  6.1524e-02,\n",
      "          -5.9750e-01],\n",
      "         [-2.2336e-01,  2.2859e-01,  6.3359e-01,  4.8880e-01,  5.7200e-01,\n",
      "          -2.1046e-01, -3.1406e-01, -2.6250e-01,  3.1677e-01, -4.6603e-01,\n",
      "          -1.6005e-01,  3.8759e-01,  1.1838e-05, -2.4639e-01, -6.0254e-01,\n",
      "          -4.9000e-01, -7.9518e-02, -2.3759e-01, -2.0813e-02, -4.5224e-01,\n",
      "          -6.0310e-01, -2.3415e-01, -1.6901e-01,  6.8348e-01, -4.7424e-01,\n",
      "          -1.5914e-01, -1.4926e-01, -3.4994e-01, -8.0135e-01,  6.5216e-01,\n",
      "          -9.6428e-02, -9.0225e-01,  6.0806e-02,  3.4629e-01, -2.9422e-01,\n",
      "           2.5677e-02, -8.3797e-01,  1.0816e-01,  4.6752e-01, -4.8175e-01,\n",
      "          -1.1157e-01,  1.8430e-01,  2.7311e-01, -5.8224e-01,  1.1691e-01,\n",
      "           3.5852e-01,  1.3774e-01, -5.9460e-01,  5.4551e-01,  2.9953e-01,\n",
      "          -3.9277e-02,  4.4725e-01, -1.8711e-01,  2.4597e-01,  7.2826e-02,\n",
      "          -6.7594e-01, -3.5827e-02,  4.9974e-02,  1.4817e-01,  7.4611e-02,\n",
      "           5.6089e-01,  2.8470e-02, -1.8751e-01,  2.5262e-01,  1.8173e-01,\n",
      "          -5.3370e-01,  3.3347e-01, -2.6257e-01, -2.5149e-01,  3.6264e-01,\n",
      "           7.3672e-01,  2.2618e-01,  2.5738e-01,  1.6742e-01,  6.7386e-01,\n",
      "          -1.7899e-01, -6.8127e-01, -2.5473e-01, -9.2043e-02,  3.5530e-01,\n",
      "           5.9078e-01,  2.7455e-02,  2.4008e-01,  4.5160e-01, -2.1246e-01,\n",
      "          -6.2796e-01,  6.9886e-01,  2.1032e-01,  2.9388e-01,  2.8447e-01,\n",
      "          -2.1054e-01,  1.3297e-02, -2.1136e-01, -3.7736e-01,  8.2834e-02,\n",
      "           2.7521e-01],\n",
      "         [-2.6995e-01,  4.6024e-01,  6.1345e-01,  5.6434e-01,  6.1100e-01,\n",
      "          -2.9176e-01,  7.2384e-02, -7.7211e-01,  2.8120e-01, -1.9323e-01,\n",
      "           8.9049e-02,  6.2385e-01, -2.8239e-01, -4.7065e-01, -4.1282e-01,\n",
      "          -7.2084e-01, -1.9977e-01, -2.8105e-01, -4.4321e-01, -2.1577e-01,\n",
      "          -3.8645e-01, -6.9864e-01, -2.8355e-01,  2.8547e-01, -5.1128e-02,\n",
      "          -4.3610e-01, -2.2510e-01, -6.9387e-01, -3.8314e-01,  5.2786e-01,\n",
      "          -2.0987e-01, -4.8991e-01,  5.5166e-01,  7.7566e-01, -5.2386e-01,\n",
      "          -8.2595e-02, -4.2183e-01, -2.9587e-01,  3.4518e-01, -5.2235e-01,\n",
      "           2.5052e-01,  2.3600e-02,  3.9664e-01, -2.9892e-01, -1.7938e-01,\n",
      "           3.8519e-02,  6.3387e-01, -2.6114e-01,  2.3501e-01, -1.3203e-01,\n",
      "          -3.2816e-01,  4.7357e-01, -5.1772e-01,  2.3591e-01,  5.3827e-02,\n",
      "          -6.2098e-01, -4.5854e-01, -2.5454e-02,  5.6654e-01, -3.4273e-01,\n",
      "           4.1805e-01, -9.6803e-02,  2.0444e-01,  6.7742e-01,  3.9266e-01,\n",
      "          -5.9131e-01,  4.3416e-01, -3.6856e-02, -6.9375e-01,  1.9844e-01,\n",
      "           3.5306e-01, -1.0937e-01,  6.5483e-01,  1.1962e-01,  4.9014e-01,\n",
      "           9.2974e-02, -4.5071e-01, -4.7200e-01, -3.6887e-01,  4.9717e-02,\n",
      "           4.8024e-01, -4.7233e-03,  4.9943e-01,  4.0659e-01, -6.3095e-01,\n",
      "          -4.7652e-01,  6.3186e-01,  3.6728e-01,  4.5501e-01,  4.8084e-01,\n",
      "          -4.9434e-01, -1.5633e-01,  1.3446e-01, -1.8216e-01,  4.2474e-02,\n",
      "           6.7930e-01]]], grad_fn=<UnsafeViewBackward0>)\n",
      "xq: torch.Size([1, 3, 64])\n",
      "tensor([[[ 6.5680e-02, -3.4133e-01,  4.0613e-02, -1.0437e-01, -4.8529e-02,\n",
      "           1.1761e-01, -5.8108e-01,  7.5494e-01,  5.8290e-02, -4.1429e-01,\n",
      "          -3.7384e-01, -3.4555e-01,  4.2079e-01,  3.3003e-01, -2.9277e-01,\n",
      "           3.3576e-01,  1.7784e-01,  6.0776e-02,  6.2904e-01, -3.5992e-01,\n",
      "          -3.3292e-01,  6.8818e-01,  1.6784e-01,  6.0449e-01, -6.3839e-01,\n",
      "           4.1002e-01,  1.1051e-01,  5.0662e-01, -6.3656e-01,  1.9612e-01,\n",
      "           1.6741e-01, -6.2951e-01, -7.3037e-01, -6.3397e-01,  3.3725e-01,\n",
      "           1.6176e-01, -6.3409e-01,  6.0382e-01,  1.9013e-01,  5.2425e-02,\n",
      "          -5.4140e-01,  2.4253e-01, -1.7950e-01, -4.3191e-01,  4.4343e-01,\n",
      "           4.8281e-01, -7.3694e-01, -5.0681e-01,  4.7179e-01,  6.4806e-01,\n",
      "           4.2978e-01, -3.1738e-02,  4.8948e-01,  1.9104e-02,  2.9528e-02,\n",
      "          -9.3201e-02,  6.2925e-01,  1.1323e-01, -6.2091e-01,  6.2311e-01,\n",
      "           2.2222e-01,  1.8714e-01, -5.8717e-01, -6.2874e-01],\n",
      "         [-2.2336e-01,  2.2859e-01,  6.3359e-01,  4.8880e-01,  5.7200e-01,\n",
      "          -2.1046e-01, -3.1406e-01, -2.6250e-01,  3.1677e-01, -4.6603e-01,\n",
      "          -1.6005e-01,  3.8759e-01,  1.1838e-05, -2.4639e-01, -6.0254e-01,\n",
      "          -4.9000e-01, -7.9518e-02, -2.3759e-01, -2.0813e-02, -4.5224e-01,\n",
      "          -6.0310e-01, -2.3415e-01, -1.6901e-01,  6.8348e-01, -4.7424e-01,\n",
      "          -1.5914e-01, -1.4926e-01, -3.4994e-01, -8.0135e-01,  6.5216e-01,\n",
      "          -9.6428e-02, -9.0225e-01,  6.0806e-02,  3.4629e-01, -2.9422e-01,\n",
      "           2.5677e-02, -8.3797e-01,  1.0816e-01,  4.6752e-01, -4.8175e-01,\n",
      "          -1.1157e-01,  1.8430e-01,  2.7311e-01, -5.8224e-01,  1.1691e-01,\n",
      "           3.5852e-01,  1.3774e-01, -5.9460e-01,  5.4551e-01,  2.9953e-01,\n",
      "          -3.9277e-02,  4.4725e-01, -1.8711e-01,  2.4597e-01,  7.2826e-02,\n",
      "          -6.7594e-01, -3.5827e-02,  4.9974e-02,  1.4817e-01,  7.4611e-02,\n",
      "           5.6089e-01,  2.8470e-02, -1.8751e-01,  2.5262e-01],\n",
      "         [-2.6995e-01,  4.6024e-01,  6.1345e-01,  5.6434e-01,  6.1100e-01,\n",
      "          -2.9176e-01,  7.2384e-02, -7.7211e-01,  2.8120e-01, -1.9323e-01,\n",
      "           8.9049e-02,  6.2385e-01, -2.8239e-01, -4.7065e-01, -4.1282e-01,\n",
      "          -7.2084e-01, -1.9977e-01, -2.8105e-01, -4.4321e-01, -2.1577e-01,\n",
      "          -3.8645e-01, -6.9864e-01, -2.8355e-01,  2.8547e-01, -5.1128e-02,\n",
      "          -4.3610e-01, -2.2510e-01, -6.9387e-01, -3.8314e-01,  5.2786e-01,\n",
      "          -2.0987e-01, -4.8991e-01,  5.5166e-01,  7.7566e-01, -5.2386e-01,\n",
      "          -8.2595e-02, -4.2183e-01, -2.9587e-01,  3.4518e-01, -5.2235e-01,\n",
      "           2.5052e-01,  2.3600e-02,  3.9664e-01, -2.9892e-01, -1.7938e-01,\n",
      "           3.8519e-02,  6.3387e-01, -2.6114e-01,  2.3501e-01, -1.3203e-01,\n",
      "          -3.2816e-01,  4.7357e-01, -5.1772e-01,  2.3591e-01,  5.3827e-02,\n",
      "          -6.2098e-01, -4.5854e-01, -2.5454e-02,  5.6654e-01, -3.4273e-01,\n",
      "           4.1805e-01, -9.6803e-02,  2.0444e-01,  6.7742e-01]]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "xk: torch.Size([1, 3, 16])\n",
      "tensor([[[-0.3112,  0.0769, -0.1444, -0.3407,  0.6548,  0.2507,  0.5840,\n",
      "           0.5038, -0.5879,  0.0740,  0.2850, -0.4082, -0.3550,  0.3195,\n",
      "           0.4109,  0.4613],\n",
      "         [ 0.1817, -0.5337,  0.3335, -0.2626, -0.2515,  0.3626,  0.7367,\n",
      "           0.2262,  0.2574,  0.1674,  0.6739, -0.1790, -0.6813, -0.2547,\n",
      "          -0.0920,  0.3553],\n",
      "         [ 0.3927, -0.5913,  0.4342, -0.0369, -0.6937,  0.1984,  0.3531,\n",
      "          -0.1094,  0.6548,  0.1196,  0.4901,  0.0930, -0.4507, -0.4720,\n",
      "          -0.3689,  0.0497]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xv: torch.Size([1, 3, 16])\n",
      "tensor([[[ 0.1746,  0.0484, -0.3824,  0.0746,  0.6200, -0.2362,  0.1115,\n",
      "          -0.2304, -0.2352, -0.2878,  0.4193,  0.2530, -0.5188, -0.2972,\n",
      "           0.0615, -0.5975],\n",
      "         [ 0.5908,  0.0275,  0.2401,  0.4516, -0.2125, -0.6280,  0.6989,\n",
      "           0.2103,  0.2939,  0.2845, -0.2105,  0.0133, -0.2114, -0.3774,\n",
      "           0.0828,  0.2752],\n",
      "         [ 0.4802, -0.0047,  0.4994,  0.4066, -0.6309, -0.4765,  0.6319,\n",
      "           0.3673,  0.4550,  0.4808, -0.4943, -0.1563,  0.1345, -0.1822,\n",
      "           0.0425,  0.6793]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xq reshaped: torch.Size([1, 3, 4, 16])\n",
      "tensor([[[[ 6.5680e-02, -3.4133e-01,  4.0613e-02, -1.0437e-01, -4.8529e-02,\n",
      "            1.1761e-01, -5.8108e-01,  7.5494e-01,  5.8290e-02, -4.1429e-01,\n",
      "           -3.7384e-01, -3.4555e-01,  4.2079e-01,  3.3003e-01, -2.9277e-01,\n",
      "            3.3576e-01],\n",
      "          [ 1.7784e-01,  6.0776e-02,  6.2904e-01, -3.5992e-01, -3.3292e-01,\n",
      "            6.8818e-01,  1.6784e-01,  6.0449e-01, -6.3839e-01,  4.1002e-01,\n",
      "            1.1051e-01,  5.0662e-01, -6.3656e-01,  1.9612e-01,  1.6741e-01,\n",
      "           -6.2951e-01],\n",
      "          [-7.3037e-01, -6.3397e-01,  3.3725e-01,  1.6176e-01, -6.3409e-01,\n",
      "            6.0382e-01,  1.9013e-01,  5.2425e-02, -5.4140e-01,  2.4253e-01,\n",
      "           -1.7950e-01, -4.3191e-01,  4.4343e-01,  4.8281e-01, -7.3694e-01,\n",
      "           -5.0681e-01],\n",
      "          [ 4.7179e-01,  6.4806e-01,  4.2978e-01, -3.1738e-02,  4.8948e-01,\n",
      "            1.9104e-02,  2.9528e-02, -9.3201e-02,  6.2925e-01,  1.1323e-01,\n",
      "           -6.2091e-01,  6.2311e-01,  2.2222e-01,  1.8714e-01, -5.8717e-01,\n",
      "           -6.2874e-01]],\n",
      "\n",
      "         [[-2.2336e-01,  2.2859e-01,  6.3359e-01,  4.8880e-01,  5.7200e-01,\n",
      "           -2.1046e-01, -3.1406e-01, -2.6250e-01,  3.1677e-01, -4.6603e-01,\n",
      "           -1.6005e-01,  3.8759e-01,  1.1838e-05, -2.4639e-01, -6.0254e-01,\n",
      "           -4.9000e-01],\n",
      "          [-7.9518e-02, -2.3759e-01, -2.0813e-02, -4.5224e-01, -6.0310e-01,\n",
      "           -2.3415e-01, -1.6901e-01,  6.8348e-01, -4.7424e-01, -1.5914e-01,\n",
      "           -1.4926e-01, -3.4994e-01, -8.0135e-01,  6.5216e-01, -9.6428e-02,\n",
      "           -9.0225e-01],\n",
      "          [ 6.0806e-02,  3.4629e-01, -2.9422e-01,  2.5677e-02, -8.3797e-01,\n",
      "            1.0816e-01,  4.6752e-01, -4.8175e-01, -1.1157e-01,  1.8430e-01,\n",
      "            2.7311e-01, -5.8224e-01,  1.1691e-01,  3.5852e-01,  1.3774e-01,\n",
      "           -5.9460e-01],\n",
      "          [ 5.4551e-01,  2.9953e-01, -3.9277e-02,  4.4725e-01, -1.8711e-01,\n",
      "            2.4597e-01,  7.2826e-02, -6.7594e-01, -3.5827e-02,  4.9974e-02,\n",
      "            1.4817e-01,  7.4611e-02,  5.6089e-01,  2.8470e-02, -1.8751e-01,\n",
      "            2.5262e-01]],\n",
      "\n",
      "         [[-2.6995e-01,  4.6024e-01,  6.1345e-01,  5.6434e-01,  6.1100e-01,\n",
      "           -2.9176e-01,  7.2384e-02, -7.7211e-01,  2.8120e-01, -1.9323e-01,\n",
      "            8.9049e-02,  6.2385e-01, -2.8239e-01, -4.7065e-01, -4.1282e-01,\n",
      "           -7.2084e-01],\n",
      "          [-1.9977e-01, -2.8105e-01, -4.4321e-01, -2.1577e-01, -3.8645e-01,\n",
      "           -6.9864e-01, -2.8355e-01,  2.8547e-01, -5.1128e-02, -4.3610e-01,\n",
      "           -2.2510e-01, -6.9387e-01, -3.8314e-01,  5.2786e-01, -2.0987e-01,\n",
      "           -4.8991e-01],\n",
      "          [ 5.5166e-01,  7.7566e-01, -5.2386e-01, -8.2595e-02, -4.2183e-01,\n",
      "           -2.9587e-01,  3.4518e-01, -5.2235e-01,  2.5052e-01,  2.3600e-02,\n",
      "            3.9664e-01, -2.9892e-01, -1.7938e-01,  3.8519e-02,  6.3387e-01,\n",
      "           -2.6114e-01],\n",
      "          [ 2.3501e-01, -1.3203e-01, -3.2816e-01,  4.7357e-01, -5.1772e-01,\n",
      "            2.3591e-01,  5.3827e-02, -6.2098e-01, -4.5854e-01, -2.5454e-02,\n",
      "            5.6654e-01, -3.4273e-01,  4.1805e-01, -9.6803e-02,  2.0444e-01,\n",
      "            6.7742e-01]]]], grad_fn=<ViewBackward0>)\n",
      "xk reshaped: torch.Size([1, 3, 1, 16])\n",
      "tensor([[[[-0.3112,  0.0769, -0.1444, -0.3407,  0.6548,  0.2507,  0.5840,\n",
      "            0.5038, -0.5879,  0.0740,  0.2850, -0.4082, -0.3550,  0.3195,\n",
      "            0.4109,  0.4613]],\n",
      "\n",
      "         [[ 0.1817, -0.5337,  0.3335, -0.2626, -0.2515,  0.3626,  0.7367,\n",
      "            0.2262,  0.2574,  0.1674,  0.6739, -0.1790, -0.6813, -0.2547,\n",
      "           -0.0920,  0.3553]],\n",
      "\n",
      "         [[ 0.3927, -0.5913,  0.4342, -0.0369, -0.6937,  0.1984,  0.3531,\n",
      "           -0.1094,  0.6548,  0.1196,  0.4901,  0.0930, -0.4507, -0.4720,\n",
      "           -0.3689,  0.0497]]]], grad_fn=<ViewBackward0>)\n",
      "xv reshaped: torch.Size([1, 3, 1, 16])\n",
      "tensor([[[[ 0.1746,  0.0484, -0.3824,  0.0746,  0.6200, -0.2362,  0.1115,\n",
      "           -0.2304, -0.2352, -0.2878,  0.4193,  0.2530, -0.5188, -0.2972,\n",
      "            0.0615, -0.5975]],\n",
      "\n",
      "         [[ 0.5908,  0.0275,  0.2401,  0.4516, -0.2125, -0.6280,  0.6989,\n",
      "            0.2103,  0.2939,  0.2845, -0.2105,  0.0133, -0.2114, -0.3774,\n",
      "            0.0828,  0.2752]],\n",
      "\n",
      "         [[ 0.4802, -0.0047,  0.4994,  0.4066, -0.6309, -0.4765,  0.6319,\n",
      "            0.3673,  0.4550,  0.4808, -0.4943, -0.1563,  0.1345, -0.1822,\n",
      "            0.0425,  0.6793]]]], grad_fn=<ViewBackward0>)\n",
      "rotated xq: torch.Size([1, 3, 4, 16])\n",
      "tensor([[[[ 0.0657, -0.3413,  0.0406, -0.1044, -0.0485,  0.1176, -0.5811,\n",
      "            0.7549,  0.0583, -0.4143, -0.3738, -0.3455,  0.4208,  0.3300,\n",
      "           -0.2928,  0.3358],\n",
      "          [ 0.1778,  0.0608,  0.6290, -0.3599, -0.3329,  0.6882,  0.1678,\n",
      "            0.6045, -0.6384,  0.4100,  0.1105,  0.5066, -0.6366,  0.1961,\n",
      "            0.1674, -0.6295],\n",
      "          [-0.7304, -0.6340,  0.3372,  0.1618, -0.6341,  0.6038,  0.1901,\n",
      "            0.0524, -0.5414,  0.2425, -0.1795, -0.4319,  0.4434,  0.4828,\n",
      "           -0.7369, -0.5068],\n",
      "          [ 0.4718,  0.6481,  0.4298, -0.0317,  0.4895,  0.0191,  0.0295,\n",
      "           -0.0932,  0.6293,  0.1132, -0.6209,  0.6231,  0.2222,  0.1871,\n",
      "           -0.5872, -0.6287]],\n",
      "\n",
      "         [[-0.3872,  0.4419,  0.6519,  0.4125,  0.5691, -0.1963, -0.2949,\n",
      "           -0.2537, -0.0168, -0.2724,  0.0449,  0.4679,  0.0571, -0.2578,\n",
      "           -0.6122, -0.4946],\n",
      "          [ 0.3561, -0.1162,  0.0266, -0.3832, -0.5201, -0.2704, -0.1659,\n",
      "            0.6994, -0.3231, -0.2613, -0.1483, -0.4244, -0.8576,  0.6380,\n",
      "           -0.1017, -0.8900],\n",
      "          [ 0.1267,  0.1947, -0.3646,  0.1283, -0.8455,  0.0878,  0.4629,\n",
      "           -0.4711, -0.0091,  0.3405,  0.1681, -0.5685,  0.0327,  0.3640,\n",
      "            0.1525, -0.6031],\n",
      "          [ 0.3249,  0.2268, -0.0834,  0.4270, -0.2422,  0.2440,  0.0787,\n",
      "           -0.6803,  0.4397,  0.2020,  0.1286,  0.1525,  0.5394,  0.0422,\n",
      "           -0.1851,  0.2406]],\n",
      "\n",
      "         [[-0.1434,  0.3729,  0.4422,  0.3118,  0.6549, -0.2371,  0.0983,\n",
      "           -0.7460, -0.3625,  0.3318,  0.4345,  0.7813, -0.1554, -0.5004,\n",
      "           -0.4074, -0.7478],\n",
      "          [ 0.1296,  0.2722, -0.2244,  0.0393, -0.3026, -0.7535, -0.2697,\n",
      "            0.3027, -0.1604, -0.4417, -0.4436, -0.7256, -0.4523,  0.4461,\n",
      "           -0.2274, -0.4795],\n",
      "          [-0.4574,  0.3134, -0.6570,  0.0267, -0.3778, -0.2983,  0.3044,\n",
      "           -0.5127,  0.3974,  0.7099,  0.0103, -0.3090, -0.2596,  0.0051,\n",
      "            0.6544, -0.2796],\n",
      "          [ 0.3192, -0.0340, -0.5996,  0.5633, -0.5904,  0.2453,  0.0408,\n",
      "           -0.6447,  0.4045, -0.1301,  0.2630, -0.1564,  0.3069, -0.0697,\n",
      "            0.2074,  0.6549]]]], grad_fn=<TransposeBackward0>)\n",
      "rotated xk: torch.Size([1, 3, 1, 16])\n",
      "tensor([[[[-0.3112,  0.0769, -0.1444, -0.3407,  0.6548,  0.2507,  0.5840,\n",
      "            0.5038, -0.5879,  0.0740,  0.2850, -0.4082, -0.3550,  0.3195,\n",
      "            0.4109,  0.4613]],\n",
      "\n",
      "         [[-0.1184, -0.5408,  0.1074, -0.2268, -0.1822,  0.3764,  0.7393,\n",
      "            0.2198,  0.2920, -0.1429,  0.7442, -0.2226, -0.7030, -0.2339,\n",
      "           -0.0687,  0.3593]],\n",
      "\n",
      "         [[-0.7588, -0.3630,  0.0605, -0.0669, -0.5904,  0.2502,  0.3757,\n",
      "           -0.1111,  0.0845, -0.4818,  0.6520,  0.0743, -0.5795, -0.4468,\n",
      "           -0.3458,  0.0458]]]], grad_fn=<TransposeBackward0>)\n",
      "repeat_interleaved xk: torch.Size([1, 3, 4, 16])\n",
      "tensor([[[[-0.3112,  0.0769, -0.1444, -0.3407,  0.6548,  0.2507,  0.5840,\n",
      "            0.5038, -0.5879,  0.0740,  0.2850, -0.4082, -0.3550,  0.3195,\n",
      "            0.4109,  0.4613],\n",
      "          [-0.3112,  0.0769, -0.1444, -0.3407,  0.6548,  0.2507,  0.5840,\n",
      "            0.5038, -0.5879,  0.0740,  0.2850, -0.4082, -0.3550,  0.3195,\n",
      "            0.4109,  0.4613],\n",
      "          [-0.3112,  0.0769, -0.1444, -0.3407,  0.6548,  0.2507,  0.5840,\n",
      "            0.5038, -0.5879,  0.0740,  0.2850, -0.4082, -0.3550,  0.3195,\n",
      "            0.4109,  0.4613],\n",
      "          [-0.3112,  0.0769, -0.1444, -0.3407,  0.6548,  0.2507,  0.5840,\n",
      "            0.5038, -0.5879,  0.0740,  0.2850, -0.4082, -0.3550,  0.3195,\n",
      "            0.4109,  0.4613]],\n",
      "\n",
      "         [[-0.1184, -0.5408,  0.1074, -0.2268, -0.1822,  0.3764,  0.7393,\n",
      "            0.2198,  0.2920, -0.1429,  0.7442, -0.2226, -0.7030, -0.2339,\n",
      "           -0.0687,  0.3593],\n",
      "          [-0.1184, -0.5408,  0.1074, -0.2268, -0.1822,  0.3764,  0.7393,\n",
      "            0.2198,  0.2920, -0.1429,  0.7442, -0.2226, -0.7030, -0.2339,\n",
      "           -0.0687,  0.3593],\n",
      "          [-0.1184, -0.5408,  0.1074, -0.2268, -0.1822,  0.3764,  0.7393,\n",
      "            0.2198,  0.2920, -0.1429,  0.7442, -0.2226, -0.7030, -0.2339,\n",
      "           -0.0687,  0.3593],\n",
      "          [-0.1184, -0.5408,  0.1074, -0.2268, -0.1822,  0.3764,  0.7393,\n",
      "            0.2198,  0.2920, -0.1429,  0.7442, -0.2226, -0.7030, -0.2339,\n",
      "           -0.0687,  0.3593]],\n",
      "\n",
      "         [[-0.7588, -0.3630,  0.0605, -0.0669, -0.5904,  0.2502,  0.3757,\n",
      "           -0.1111,  0.0845, -0.4818,  0.6520,  0.0743, -0.5795, -0.4468,\n",
      "           -0.3458,  0.0458],\n",
      "          [-0.7588, -0.3630,  0.0605, -0.0669, -0.5904,  0.2502,  0.3757,\n",
      "           -0.1111,  0.0845, -0.4818,  0.6520,  0.0743, -0.5795, -0.4468,\n",
      "           -0.3458,  0.0458],\n",
      "          [-0.7588, -0.3630,  0.0605, -0.0669, -0.5904,  0.2502,  0.3757,\n",
      "           -0.1111,  0.0845, -0.4818,  0.6520,  0.0743, -0.5795, -0.4468,\n",
      "           -0.3458,  0.0458],\n",
      "          [-0.7588, -0.3630,  0.0605, -0.0669, -0.5904,  0.2502,  0.3757,\n",
      "           -0.1111,  0.0845, -0.4818,  0.6520,  0.0743, -0.5795, -0.4468,\n",
      "           -0.3458,  0.0458]]]], grad_fn=<ViewBackward0>)\n",
      "repeat_interleaved xv: torch.Size([1, 3, 4, 16])\n",
      "tensor([[[[ 0.1746,  0.0484, -0.3824,  0.0746,  0.6200, -0.2362,  0.1115,\n",
      "           -0.2304, -0.2352, -0.2878,  0.4193,  0.2530, -0.5188, -0.2972,\n",
      "            0.0615, -0.5975],\n",
      "          [ 0.1746,  0.0484, -0.3824,  0.0746,  0.6200, -0.2362,  0.1115,\n",
      "           -0.2304, -0.2352, -0.2878,  0.4193,  0.2530, -0.5188, -0.2972,\n",
      "            0.0615, -0.5975],\n",
      "          [ 0.1746,  0.0484, -0.3824,  0.0746,  0.6200, -0.2362,  0.1115,\n",
      "           -0.2304, -0.2352, -0.2878,  0.4193,  0.2530, -0.5188, -0.2972,\n",
      "            0.0615, -0.5975],\n",
      "          [ 0.1746,  0.0484, -0.3824,  0.0746,  0.6200, -0.2362,  0.1115,\n",
      "           -0.2304, -0.2352, -0.2878,  0.4193,  0.2530, -0.5188, -0.2972,\n",
      "            0.0615, -0.5975]],\n",
      "\n",
      "         [[ 0.5908,  0.0275,  0.2401,  0.4516, -0.2125, -0.6280,  0.6989,\n",
      "            0.2103,  0.2939,  0.2845, -0.2105,  0.0133, -0.2114, -0.3774,\n",
      "            0.0828,  0.2752],\n",
      "          [ 0.5908,  0.0275,  0.2401,  0.4516, -0.2125, -0.6280,  0.6989,\n",
      "            0.2103,  0.2939,  0.2845, -0.2105,  0.0133, -0.2114, -0.3774,\n",
      "            0.0828,  0.2752],\n",
      "          [ 0.5908,  0.0275,  0.2401,  0.4516, -0.2125, -0.6280,  0.6989,\n",
      "            0.2103,  0.2939,  0.2845, -0.2105,  0.0133, -0.2114, -0.3774,\n",
      "            0.0828,  0.2752],\n",
      "          [ 0.5908,  0.0275,  0.2401,  0.4516, -0.2125, -0.6280,  0.6989,\n",
      "            0.2103,  0.2939,  0.2845, -0.2105,  0.0133, -0.2114, -0.3774,\n",
      "            0.0828,  0.2752]],\n",
      "\n",
      "         [[ 0.4802, -0.0047,  0.4994,  0.4066, -0.6309, -0.4765,  0.6319,\n",
      "            0.3673,  0.4550,  0.4808, -0.4943, -0.1563,  0.1345, -0.1822,\n",
      "            0.0425,  0.6793],\n",
      "          [ 0.4802, -0.0047,  0.4994,  0.4066, -0.6309, -0.4765,  0.6319,\n",
      "            0.3673,  0.4550,  0.4808, -0.4943, -0.1563,  0.1345, -0.1822,\n",
      "            0.0425,  0.6793],\n",
      "          [ 0.4802, -0.0047,  0.4994,  0.4066, -0.6309, -0.4765,  0.6319,\n",
      "            0.3673,  0.4550,  0.4808, -0.4943, -0.1563,  0.1345, -0.1822,\n",
      "            0.0425,  0.6793],\n",
      "          [ 0.4802, -0.0047,  0.4994,  0.4066, -0.6309, -0.4765,  0.6319,\n",
      "            0.3673,  0.4550,  0.4808, -0.4943, -0.1563,  0.1345, -0.1822,\n",
      "            0.0425,  0.6793]]]], grad_fn=<ViewBackward0>)\n",
      "transposed xq: torch.Size([1, 4, 3, 16])\n",
      "tensor([[[[ 0.0657, -0.3413,  0.0406, -0.1044, -0.0485,  0.1176, -0.5811,\n",
      "            0.7549,  0.0583, -0.4143, -0.3738, -0.3455,  0.4208,  0.3300,\n",
      "           -0.2928,  0.3358],\n",
      "          [-0.3872,  0.4419,  0.6519,  0.4125,  0.5691, -0.1963, -0.2949,\n",
      "           -0.2537, -0.0168, -0.2724,  0.0449,  0.4679,  0.0571, -0.2578,\n",
      "           -0.6122, -0.4946],\n",
      "          [-0.1434,  0.3729,  0.4422,  0.3118,  0.6549, -0.2371,  0.0983,\n",
      "           -0.7460, -0.3625,  0.3318,  0.4345,  0.7813, -0.1554, -0.5004,\n",
      "           -0.4074, -0.7478]],\n",
      "\n",
      "         [[ 0.1778,  0.0608,  0.6290, -0.3599, -0.3329,  0.6882,  0.1678,\n",
      "            0.6045, -0.6384,  0.4100,  0.1105,  0.5066, -0.6366,  0.1961,\n",
      "            0.1674, -0.6295],\n",
      "          [ 0.3561, -0.1162,  0.0266, -0.3832, -0.5201, -0.2704, -0.1659,\n",
      "            0.6994, -0.3231, -0.2613, -0.1483, -0.4244, -0.8576,  0.6380,\n",
      "           -0.1017, -0.8900],\n",
      "          [ 0.1296,  0.2722, -0.2244,  0.0393, -0.3026, -0.7535, -0.2697,\n",
      "            0.3027, -0.1604, -0.4417, -0.4436, -0.7256, -0.4523,  0.4461,\n",
      "           -0.2274, -0.4795]],\n",
      "\n",
      "         [[-0.7304, -0.6340,  0.3372,  0.1618, -0.6341,  0.6038,  0.1901,\n",
      "            0.0524, -0.5414,  0.2425, -0.1795, -0.4319,  0.4434,  0.4828,\n",
      "           -0.7369, -0.5068],\n",
      "          [ 0.1267,  0.1947, -0.3646,  0.1283, -0.8455,  0.0878,  0.4629,\n",
      "           -0.4711, -0.0091,  0.3405,  0.1681, -0.5685,  0.0327,  0.3640,\n",
      "            0.1525, -0.6031],\n",
      "          [-0.4574,  0.3134, -0.6570,  0.0267, -0.3778, -0.2983,  0.3044,\n",
      "           -0.5127,  0.3974,  0.7099,  0.0103, -0.3090, -0.2596,  0.0051,\n",
      "            0.6544, -0.2796]],\n",
      "\n",
      "         [[ 0.4718,  0.6481,  0.4298, -0.0317,  0.4895,  0.0191,  0.0295,\n",
      "           -0.0932,  0.6293,  0.1132, -0.6209,  0.6231,  0.2222,  0.1871,\n",
      "           -0.5872, -0.6287],\n",
      "          [ 0.3249,  0.2268, -0.0834,  0.4270, -0.2422,  0.2440,  0.0787,\n",
      "           -0.6803,  0.4397,  0.2020,  0.1286,  0.1525,  0.5394,  0.0422,\n",
      "           -0.1851,  0.2406],\n",
      "          [ 0.3192, -0.0340, -0.5996,  0.5633, -0.5904,  0.2453,  0.0408,\n",
      "           -0.6447,  0.4045, -0.1301,  0.2630, -0.1564,  0.3069, -0.0697,\n",
      "            0.2074,  0.6549]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xk: torch.Size([1, 4, 3, 16])\n",
      "tensor([[[[-0.3112,  0.0769, -0.1444, -0.3407,  0.6548,  0.2507,  0.5840,\n",
      "            0.5038, -0.5879,  0.0740,  0.2850, -0.4082, -0.3550,  0.3195,\n",
      "            0.4109,  0.4613],\n",
      "          [-0.1184, -0.5408,  0.1074, -0.2268, -0.1822,  0.3764,  0.7393,\n",
      "            0.2198,  0.2920, -0.1429,  0.7442, -0.2226, -0.7030, -0.2339,\n",
      "           -0.0687,  0.3593],\n",
      "          [-0.7588, -0.3630,  0.0605, -0.0669, -0.5904,  0.2502,  0.3757,\n",
      "           -0.1111,  0.0845, -0.4818,  0.6520,  0.0743, -0.5795, -0.4468,\n",
      "           -0.3458,  0.0458]],\n",
      "\n",
      "         [[-0.3112,  0.0769, -0.1444, -0.3407,  0.6548,  0.2507,  0.5840,\n",
      "            0.5038, -0.5879,  0.0740,  0.2850, -0.4082, -0.3550,  0.3195,\n",
      "            0.4109,  0.4613],\n",
      "          [-0.1184, -0.5408,  0.1074, -0.2268, -0.1822,  0.3764,  0.7393,\n",
      "            0.2198,  0.2920, -0.1429,  0.7442, -0.2226, -0.7030, -0.2339,\n",
      "           -0.0687,  0.3593],\n",
      "          [-0.7588, -0.3630,  0.0605, -0.0669, -0.5904,  0.2502,  0.3757,\n",
      "           -0.1111,  0.0845, -0.4818,  0.6520,  0.0743, -0.5795, -0.4468,\n",
      "           -0.3458,  0.0458]],\n",
      "\n",
      "         [[-0.3112,  0.0769, -0.1444, -0.3407,  0.6548,  0.2507,  0.5840,\n",
      "            0.5038, -0.5879,  0.0740,  0.2850, -0.4082, -0.3550,  0.3195,\n",
      "            0.4109,  0.4613],\n",
      "          [-0.1184, -0.5408,  0.1074, -0.2268, -0.1822,  0.3764,  0.7393,\n",
      "            0.2198,  0.2920, -0.1429,  0.7442, -0.2226, -0.7030, -0.2339,\n",
      "           -0.0687,  0.3593],\n",
      "          [-0.7588, -0.3630,  0.0605, -0.0669, -0.5904,  0.2502,  0.3757,\n",
      "           -0.1111,  0.0845, -0.4818,  0.6520,  0.0743, -0.5795, -0.4468,\n",
      "           -0.3458,  0.0458]],\n",
      "\n",
      "         [[-0.3112,  0.0769, -0.1444, -0.3407,  0.6548,  0.2507,  0.5840,\n",
      "            0.5038, -0.5879,  0.0740,  0.2850, -0.4082, -0.3550,  0.3195,\n",
      "            0.4109,  0.4613],\n",
      "          [-0.1184, -0.5408,  0.1074, -0.2268, -0.1822,  0.3764,  0.7393,\n",
      "            0.2198,  0.2920, -0.1429,  0.7442, -0.2226, -0.7030, -0.2339,\n",
      "           -0.0687,  0.3593],\n",
      "          [-0.7588, -0.3630,  0.0605, -0.0669, -0.5904,  0.2502,  0.3757,\n",
      "           -0.1111,  0.0845, -0.4818,  0.6520,  0.0743, -0.5795, -0.4468,\n",
      "           -0.3458,  0.0458]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xv: torch.Size([1, 4, 3, 16])\n",
      "tensor([[[[ 0.1746,  0.0484, -0.3824,  0.0746,  0.6200, -0.2362,  0.1115,\n",
      "           -0.2304, -0.2352, -0.2878,  0.4193,  0.2530, -0.5188, -0.2972,\n",
      "            0.0615, -0.5975],\n",
      "          [ 0.5908,  0.0275,  0.2401,  0.4516, -0.2125, -0.6280,  0.6989,\n",
      "            0.2103,  0.2939,  0.2845, -0.2105,  0.0133, -0.2114, -0.3774,\n",
      "            0.0828,  0.2752],\n",
      "          [ 0.4802, -0.0047,  0.4994,  0.4066, -0.6309, -0.4765,  0.6319,\n",
      "            0.3673,  0.4550,  0.4808, -0.4943, -0.1563,  0.1345, -0.1822,\n",
      "            0.0425,  0.6793]],\n",
      "\n",
      "         [[ 0.1746,  0.0484, -0.3824,  0.0746,  0.6200, -0.2362,  0.1115,\n",
      "           -0.2304, -0.2352, -0.2878,  0.4193,  0.2530, -0.5188, -0.2972,\n",
      "            0.0615, -0.5975],\n",
      "          [ 0.5908,  0.0275,  0.2401,  0.4516, -0.2125, -0.6280,  0.6989,\n",
      "            0.2103,  0.2939,  0.2845, -0.2105,  0.0133, -0.2114, -0.3774,\n",
      "            0.0828,  0.2752],\n",
      "          [ 0.4802, -0.0047,  0.4994,  0.4066, -0.6309, -0.4765,  0.6319,\n",
      "            0.3673,  0.4550,  0.4808, -0.4943, -0.1563,  0.1345, -0.1822,\n",
      "            0.0425,  0.6793]],\n",
      "\n",
      "         [[ 0.1746,  0.0484, -0.3824,  0.0746,  0.6200, -0.2362,  0.1115,\n",
      "           -0.2304, -0.2352, -0.2878,  0.4193,  0.2530, -0.5188, -0.2972,\n",
      "            0.0615, -0.5975],\n",
      "          [ 0.5908,  0.0275,  0.2401,  0.4516, -0.2125, -0.6280,  0.6989,\n",
      "            0.2103,  0.2939,  0.2845, -0.2105,  0.0133, -0.2114, -0.3774,\n",
      "            0.0828,  0.2752],\n",
      "          [ 0.4802, -0.0047,  0.4994,  0.4066, -0.6309, -0.4765,  0.6319,\n",
      "            0.3673,  0.4550,  0.4808, -0.4943, -0.1563,  0.1345, -0.1822,\n",
      "            0.0425,  0.6793]],\n",
      "\n",
      "         [[ 0.1746,  0.0484, -0.3824,  0.0746,  0.6200, -0.2362,  0.1115,\n",
      "           -0.2304, -0.2352, -0.2878,  0.4193,  0.2530, -0.5188, -0.2972,\n",
      "            0.0615, -0.5975],\n",
      "          [ 0.5908,  0.0275,  0.2401,  0.4516, -0.2125, -0.6280,  0.6989,\n",
      "            0.2103,  0.2939,  0.2845, -0.2105,  0.0133, -0.2114, -0.3774,\n",
      "            0.0828,  0.2752],\n",
      "          [ 0.4802, -0.0047,  0.4994,  0.4066, -0.6309, -0.4765,  0.6319,\n",
      "            0.3673,  0.4550,  0.4808, -0.4943, -0.1563,  0.1345, -0.1822,\n",
      "            0.0425,  0.6793]]]], grad_fn=<TransposeBackward0>)\n",
      "scores: torch.Size([1, 4, 3, 3])\n",
      "tensor([[[[-0.0045, -0.0907, -0.1250],\n",
      "          [-0.2069, -0.2050,  0.0356],\n",
      "          [-0.1551, -0.1315,  0.0560]],\n",
      "\n",
      "         [[ 0.1589,  0.1401,  0.0806],\n",
      "          [ 0.0529,  0.0506,  0.0188],\n",
      "          [-0.0432, -0.1630, -0.0956]],\n",
      "\n",
      "         [[-0.0325,  0.0344,  0.2080],\n",
      "          [-0.0786,  0.0256,  0.0476],\n",
      "          [ 0.0078,  0.0010,  0.0240]],\n",
      "\n",
      "         [[-0.2990, -0.3131, -0.3065],\n",
      "          [-0.2559, -0.0877, -0.0687],\n",
      "          [-0.1715,  0.0664,  0.0653]]]], grad_fn=<MulBackward0>)\n",
      "mask: torch.Size([1, 1, 3, 3])\n",
      "tensor([[[[ 0.0000e+00, -2.3820e+38, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]]]])\n",
      "masked scores: torch.Size([1, 4, 3, 3])\n",
      "tensor([[[[-4.5250e-03, -2.3820e+38, -2.3820e+38],\n",
      "          [-2.0692e-01, -2.0504e-01, -2.3820e+38],\n",
      "          [-1.5509e-01, -1.3152e-01,  5.6041e-02]],\n",
      "\n",
      "         [[ 1.5889e-01, -2.3820e+38, -2.3820e+38],\n",
      "          [ 5.2900e-02,  5.0617e-02, -2.3820e+38],\n",
      "          [-4.3163e-02, -1.6304e-01, -9.5626e-02]],\n",
      "\n",
      "         [[-3.2494e-02, -2.3820e+38, -2.3820e+38],\n",
      "          [-7.8579e-02,  2.5572e-02, -2.3820e+38],\n",
      "          [ 7.8321e-03,  9.6859e-04,  2.3962e-02]],\n",
      "\n",
      "         [[-2.9900e-01, -2.3820e+38, -2.3820e+38],\n",
      "          [-2.5587e-01, -8.7740e-02, -2.3820e+38],\n",
      "          [-1.7145e-01,  6.6420e-02,  6.5342e-02]]]], grad_fn=<AddBackward0>)\n",
      "softmaxed scores: torch.Size([1, 4, 3, 3])\n",
      "tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [0.4995, 0.5005, 0.0000],\n",
      "          [0.3068, 0.3142, 0.3790]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.5006, 0.4994, 0.0000],\n",
      "          [0.3526, 0.3128, 0.3346]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.4740, 0.5260, 0.0000],\n",
      "          [0.3323, 0.3300, 0.3377]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.4581, 0.5419, 0.0000],\n",
      "          [0.2828, 0.3588, 0.3584]]]], grad_fn=<SoftmaxBackward0>)\n",
      "attention: torch.Size([1, 4, 3, 16])\n",
      "tensor([[[[ 0.1746,  0.0484, -0.3824,  0.0746,  0.6200, -0.2362,  0.1115,\n",
      "           -0.2304, -0.2352, -0.2878,  0.4193,  0.2530, -0.5188, -0.2972,\n",
      "            0.0615, -0.5975],\n",
      "          [ 0.3829,  0.0379, -0.0709,  0.2633,  0.2034, -0.4322,  0.4055,\n",
      "           -0.0098,  0.0296, -0.0014,  0.1041,  0.1330, -0.3649, -0.3373,\n",
      "            0.0722, -0.1607],\n",
      "          [ 0.4212,  0.0217,  0.1474,  0.3189, -0.1156, -0.4503,  0.4932,\n",
      "            0.1346,  0.1926,  0.1833, -0.1248,  0.0226, -0.1746, -0.2788,\n",
      "            0.0610,  0.1606]],\n",
      "\n",
      "         [[ 0.1746,  0.0484, -0.3824,  0.0746,  0.6200, -0.2362,  0.1115,\n",
      "           -0.2304, -0.2352, -0.2878,  0.4193,  0.2530, -0.5188, -0.2972,\n",
      "            0.0615, -0.5975],\n",
      "          [ 0.3825,  0.0379, -0.0715,  0.2629,  0.2042, -0.4318,  0.4049,\n",
      "           -0.0103,  0.0291, -0.0020,  0.1048,  0.1333, -0.3653, -0.3372,\n",
      "            0.0722, -0.1616],\n",
      "          [ 0.4070,  0.0241,  0.1073,  0.3036, -0.0589, -0.4391,  0.4693,\n",
      "            0.1074,  0.1612,  0.1484, -0.0834,  0.0411, -0.2041, -0.2838,\n",
      "            0.0618,  0.1027]],\n",
      "\n",
      "         [[ 0.1746,  0.0484, -0.3824,  0.0746,  0.6200, -0.2362,  0.1115,\n",
      "           -0.2304, -0.2352, -0.2878,  0.4193,  0.2530, -0.5188, -0.2972,\n",
      "            0.0615, -0.5975],\n",
      "          [ 0.3935,  0.0374, -0.0550,  0.2729,  0.1821, -0.4423,  0.4205,\n",
      "            0.0014,  0.0431,  0.0132,  0.0880,  0.1269, -0.3571, -0.3393,\n",
      "            0.0727, -0.1384],\n",
      "          [ 0.4152,  0.0236,  0.1208,  0.3111, -0.0772, -0.4466,  0.4811,\n",
      "            0.1169,  0.1725,  0.1606, -0.0971,  0.0357, -0.1967, -0.2848,\n",
      "            0.0621,  0.1217]],\n",
      "\n",
      "         [[ 0.1746,  0.0484, -0.3824,  0.0746,  0.6200, -0.2362,  0.1115,\n",
      "           -0.2304, -0.2352, -0.2878,  0.4193,  0.2530, -0.5188, -0.2972,\n",
      "            0.0615, -0.5975],\n",
      "          [ 0.4001,  0.0371, -0.0451,  0.2789,  0.1689, -0.4485,  0.4298,\n",
      "            0.0085,  0.0515,  0.0223,  0.0780,  0.1231, -0.3522, -0.3406,\n",
      "            0.0731, -0.1245],\n",
      "          [ 0.4335,  0.0218,  0.1570,  0.3289, -0.1270, -0.4629,  0.5087,\n",
      "            0.1419,  0.2020,  0.1930, -0.1341,  0.0203, -0.1744, -0.2847,\n",
      "            0.0623,  0.1732]]]], grad_fn=<UnsafeViewBackward0>)\n",
      "reshaped attention: torch.Size([1, 3, 64])\n",
      "tensor([[[ 0.1746,  0.0484, -0.3824,  0.0746,  0.6200, -0.2362,  0.1115,\n",
      "          -0.2304, -0.2352, -0.2878,  0.4193,  0.2530, -0.5188, -0.2972,\n",
      "           0.0615, -0.5975,  0.1746,  0.0484, -0.3824,  0.0746,  0.6200,\n",
      "          -0.2362,  0.1115, -0.2304, -0.2352, -0.2878,  0.4193,  0.2530,\n",
      "          -0.5188, -0.2972,  0.0615, -0.5975,  0.1746,  0.0484, -0.3824,\n",
      "           0.0746,  0.6200, -0.2362,  0.1115, -0.2304, -0.2352, -0.2878,\n",
      "           0.4193,  0.2530, -0.5188, -0.2972,  0.0615, -0.5975,  0.1746,\n",
      "           0.0484, -0.3824,  0.0746,  0.6200, -0.2362,  0.1115, -0.2304,\n",
      "          -0.2352, -0.2878,  0.4193,  0.2530, -0.5188, -0.2972,  0.0615,\n",
      "          -0.5975],\n",
      "         [ 0.3829,  0.0379, -0.0709,  0.2633,  0.2034, -0.4322,  0.4055,\n",
      "          -0.0098,  0.0296, -0.0014,  0.1041,  0.1330, -0.3649, -0.3373,\n",
      "           0.0722, -0.1607,  0.3825,  0.0379, -0.0715,  0.2629,  0.2042,\n",
      "          -0.4318,  0.4049, -0.0103,  0.0291, -0.0020,  0.1048,  0.1333,\n",
      "          -0.3653, -0.3372,  0.0722, -0.1616,  0.3935,  0.0374, -0.0550,\n",
      "           0.2729,  0.1821, -0.4423,  0.4205,  0.0014,  0.0431,  0.0132,\n",
      "           0.0880,  0.1269, -0.3571, -0.3393,  0.0727, -0.1384,  0.4001,\n",
      "           0.0371, -0.0451,  0.2789,  0.1689, -0.4485,  0.4298,  0.0085,\n",
      "           0.0515,  0.0223,  0.0780,  0.1231, -0.3522, -0.3406,  0.0731,\n",
      "          -0.1245],\n",
      "         [ 0.4212,  0.0217,  0.1474,  0.3189, -0.1156, -0.4503,  0.4932,\n",
      "           0.1346,  0.1926,  0.1833, -0.1248,  0.0226, -0.1746, -0.2788,\n",
      "           0.0610,  0.1606,  0.4070,  0.0241,  0.1073,  0.3036, -0.0589,\n",
      "          -0.4391,  0.4693,  0.1074,  0.1612,  0.1484, -0.0834,  0.0411,\n",
      "          -0.2041, -0.2838,  0.0618,  0.1027,  0.4152,  0.0236,  0.1208,\n",
      "           0.3111, -0.0772, -0.4466,  0.4811,  0.1169,  0.1725,  0.1606,\n",
      "          -0.0971,  0.0357, -0.1967, -0.2848,  0.0621,  0.1217,  0.4335,\n",
      "           0.0218,  0.1570,  0.3289, -0.1270, -0.4629,  0.5087,  0.1419,\n",
      "           0.2020,  0.1930, -0.1341,  0.0203, -0.1744, -0.2847,  0.0623,\n",
      "           0.1732]]], grad_fn=<ViewBackward0>)\n",
      "self.Wo: torch.Size([128, 4])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0514,  0.0743,  0.0537,  0.0693],\n",
      "        [-0.0094, -0.0845, -0.0542, -0.0136],\n",
      "        [ 0.0229,  0.0264, -0.0093, -0.0026],\n",
      "        [-0.0316,  0.0430,  0.0125, -0.0616],\n",
      "        [ 0.0464,  0.0413,  0.0481,  0.0531],\n",
      "        [-0.0105, -0.0405,  0.0001, -0.0071],\n",
      "        [-0.0035, -0.0689, -0.0252, -0.0542],\n",
      "        [ 0.0835,  0.0865,  0.0467, -0.0720],\n",
      "        [-0.0548,  0.0473,  0.0302,  0.0020],\n",
      "        [ 0.0081, -0.0360,  0.0452, -0.0402],\n",
      "        [-0.0376, -0.0440,  0.0329, -0.0872],\n",
      "        [-0.0355, -0.0404, -0.0046,  0.0003],\n",
      "        [ 0.0848, -0.0524,  0.0606, -0.0696],\n",
      "        [-0.0693,  0.0483, -0.0576,  0.0117],\n",
      "        [-0.0017, -0.0049,  0.0201,  0.0202],\n",
      "        [-0.0724, -0.0261,  0.0836, -0.0099],\n",
      "        [-0.0679, -0.0854, -0.0672,  0.0221],\n",
      "        [-0.0875, -0.0013,  0.0315, -0.0219],\n",
      "        [ 0.0605,  0.0836, -0.0371, -0.0282],\n",
      "        [-0.0303,  0.0743, -0.0527, -0.0353],\n",
      "        [ 0.0282,  0.0691, -0.0066,  0.0627],\n",
      "        [-0.0791, -0.0368, -0.0722,  0.0095],\n",
      "        [ 0.0809, -0.0143, -0.0701, -0.0357],\n",
      "        [-0.0454,  0.0016, -0.0115,  0.0131],\n",
      "        [ 0.0266,  0.0383, -0.0735,  0.0114],\n",
      "        [-0.0617, -0.0287,  0.0471,  0.0024],\n",
      "        [ 0.0362,  0.0466,  0.0281, -0.0051],\n",
      "        [ 0.0618,  0.0556, -0.0811,  0.0084],\n",
      "        [-0.0073,  0.0854,  0.0814,  0.0430],\n",
      "        [-0.0234, -0.0501, -0.0794,  0.0883],\n",
      "        [ 0.0510,  0.0175,  0.0640,  0.0476],\n",
      "        [-0.0776, -0.0752, -0.0081, -0.0527],\n",
      "        [-0.0840, -0.0065,  0.0413, -0.0724],\n",
      "        [ 0.0094,  0.0614, -0.0162, -0.0418],\n",
      "        [ 0.0833,  0.0642,  0.0744,  0.0247],\n",
      "        [ 0.0155,  0.0746,  0.0227, -0.0774],\n",
      "        [-0.0599,  0.0262, -0.0539,  0.0297],\n",
      "        [-0.0088, -0.0238,  0.0177, -0.0479],\n",
      "        [ 0.0257, -0.0519,  0.0176,  0.0664],\n",
      "        [-0.0519,  0.0628, -0.0223, -0.0024],\n",
      "        [-0.0426,  0.0583, -0.0716,  0.0199],\n",
      "        [ 0.0391,  0.0258,  0.0776,  0.0818],\n",
      "        [-0.0625,  0.0565, -0.0284, -0.0667],\n",
      "        [ 0.0728,  0.0282,  0.0354, -0.0383],\n",
      "        [-0.0742,  0.0047,  0.0576,  0.0267],\n",
      "        [-0.0635, -0.0323, -0.0053, -0.0564],\n",
      "        [ 0.0178, -0.0213,  0.0196, -0.0009],\n",
      "        [-0.0677, -0.0137, -0.0435,  0.0596],\n",
      "        [-0.0611, -0.0526,  0.0458, -0.0519],\n",
      "        [-0.0120,  0.0083, -0.0181, -0.0460],\n",
      "        [ 0.0733, -0.0850,  0.0844, -0.0710],\n",
      "        [-0.0707,  0.0651,  0.0544, -0.0751],\n",
      "        [ 0.0044,  0.0845, -0.0804,  0.0732],\n",
      "        [-0.0220,  0.0340, -0.0049, -0.0684],\n",
      "        [-0.0223, -0.0597, -0.0738, -0.0661],\n",
      "        [ 0.0663,  0.0443,  0.0142,  0.0746],\n",
      "        [-0.0266,  0.0484,  0.0818, -0.0387],\n",
      "        [ 0.0486, -0.0462,  0.0570,  0.0004],\n",
      "        [-0.0711, -0.0846, -0.0152, -0.0648],\n",
      "        [ 0.0782, -0.0624,  0.0493,  0.0408],\n",
      "        [ 0.0517,  0.0578, -0.0004, -0.0706],\n",
      "        [ 0.0635,  0.0529,  0.0730,  0.0173],\n",
      "        [-0.0615, -0.0505,  0.0230, -0.0285],\n",
      "        [ 0.0780, -0.0163,  0.0638,  0.0295],\n",
      "        [-0.0351, -0.0091, -0.0593, -0.0044],\n",
      "        [ 0.0792,  0.0473,  0.0735, -0.0741],\n",
      "        [ 0.0105,  0.0763, -0.0420, -0.0469],\n",
      "        [ 0.0652,  0.0307, -0.0339, -0.0420],\n",
      "        [ 0.0687, -0.0324,  0.0191,  0.0743],\n",
      "        [ 0.0727, -0.0414,  0.0630,  0.0561],\n",
      "        [-0.0774,  0.0852,  0.0413,  0.0461],\n",
      "        [-0.0510, -0.0647, -0.0310,  0.0168],\n",
      "        [ 0.0367, -0.0794,  0.0471,  0.0470],\n",
      "        [-0.0112,  0.0241, -0.0717, -0.0274],\n",
      "        [-0.0728,  0.0839,  0.0003, -0.0810],\n",
      "        [ 0.0222, -0.0643,  0.0136, -0.0075],\n",
      "        [-0.0366,  0.0549,  0.0628, -0.0377],\n",
      "        [ 0.0760, -0.0411,  0.0003,  0.0797],\n",
      "        [ 0.0150, -0.0266, -0.0145,  0.0551],\n",
      "        [-0.0400, -0.0881,  0.0034, -0.0795],\n",
      "        [ 0.0611, -0.0265, -0.0585, -0.0675],\n",
      "        [-0.0521, -0.0122, -0.0485, -0.0295],\n",
      "        [ 0.0822,  0.0452, -0.0568, -0.0710],\n",
      "        [ 0.0450,  0.0645, -0.0702, -0.0811],\n",
      "        [-0.0225, -0.0715, -0.0430, -0.0110],\n",
      "        [ 0.0735, -0.0227,  0.0861, -0.0029],\n",
      "        [ 0.0307, -0.0863,  0.0438, -0.0778],\n",
      "        [ 0.0206, -0.0404,  0.0440, -0.0200],\n",
      "        [-0.0879,  0.0735, -0.0069,  0.0122],\n",
      "        [-0.0054,  0.0238, -0.0209, -0.0291],\n",
      "        [ 0.0112, -0.0083, -0.0622,  0.0861],\n",
      "        [ 0.0630, -0.0599,  0.0275, -0.0171],\n",
      "        [-0.0425,  0.0575, -0.0010,  0.0880],\n",
      "        [ 0.0031,  0.0430,  0.0718,  0.0703],\n",
      "        [ 0.0070,  0.0678, -0.0493, -0.0748],\n",
      "        [-0.0038,  0.0665,  0.0458, -0.0291],\n",
      "        [ 0.0548,  0.0803, -0.0577,  0.0824],\n",
      "        [-0.0199, -0.0854,  0.0256,  0.0154],\n",
      "        [ 0.0358, -0.0863, -0.0115,  0.0538],\n",
      "        [ 0.0343,  0.0434, -0.0784,  0.0476],\n",
      "        [-0.0750, -0.0764, -0.0024, -0.0658],\n",
      "        [ 0.0400, -0.0731, -0.0679, -0.0725],\n",
      "        [-0.0722, -0.0172, -0.0615,  0.0616],\n",
      "        [-0.0544, -0.0756,  0.0504, -0.0241],\n",
      "        [-0.0493,  0.0617,  0.0499,  0.0277],\n",
      "        [ 0.0236,  0.0027,  0.0016, -0.0365],\n",
      "        [-0.0200, -0.0603, -0.0211, -0.0676],\n",
      "        [ 0.0158, -0.0035, -0.0131,  0.0160],\n",
      "        [-0.0304, -0.0590, -0.0502,  0.0389],\n",
      "        [ 0.0793, -0.0155, -0.0462, -0.0514],\n",
      "        [-0.0697, -0.0389, -0.0095, -0.0075],\n",
      "        [ 0.0466,  0.0835, -0.0150, -0.0661],\n",
      "        [-0.0175,  0.0384,  0.0659,  0.0800],\n",
      "        [ 0.0198, -0.0148,  0.0801,  0.0679],\n",
      "        [ 0.0548,  0.0441, -0.0773,  0.0059],\n",
      "        [-0.0003,  0.0879,  0.0650,  0.0100],\n",
      "        [ 0.0017,  0.0040,  0.0177, -0.0550],\n",
      "        [-0.0189, -0.0250, -0.0162, -0.0413],\n",
      "        [-0.0369, -0.0653,  0.0759,  0.0452],\n",
      "        [-0.0196, -0.0653, -0.0633,  0.0211],\n",
      "        [-0.0218,  0.0105, -0.0293, -0.0554],\n",
      "        [-0.0503,  0.0465,  0.0322, -0.0693],\n",
      "        [-0.0623, -0.0767,  0.0390, -0.0007],\n",
      "        [ 0.0549, -0.0156,  0.0739,  0.0425],\n",
      "        [ 0.0659,  0.0031, -0.0220, -0.0762],\n",
      "        [ 0.0374,  0.0430,  0.0135, -0.0675],\n",
      "        [-0.0469, -0.0155, -0.0748,  0.0680],\n",
      "        [ 0.0117,  0.0004,  0.0271,  0.0802]], requires_grad=True)\n",
      "spliced Wo: torch.Size([64, 2])\n",
      "tensor([[ 0.0514,  0.0743],\n",
      "        [-0.0094, -0.0845],\n",
      "        [ 0.0229,  0.0264],\n",
      "        [-0.0316,  0.0430],\n",
      "        [ 0.0464,  0.0413],\n",
      "        [-0.0105, -0.0405],\n",
      "        [-0.0035, -0.0689],\n",
      "        [ 0.0835,  0.0865],\n",
      "        [-0.0548,  0.0473],\n",
      "        [ 0.0081, -0.0360],\n",
      "        [-0.0376, -0.0440],\n",
      "        [-0.0355, -0.0404],\n",
      "        [ 0.0848, -0.0524],\n",
      "        [-0.0693,  0.0483],\n",
      "        [-0.0017, -0.0049],\n",
      "        [-0.0724, -0.0261],\n",
      "        [-0.0840, -0.0065],\n",
      "        [ 0.0094,  0.0614],\n",
      "        [ 0.0833,  0.0642],\n",
      "        [ 0.0155,  0.0746],\n",
      "        [-0.0599,  0.0262],\n",
      "        [-0.0088, -0.0238],\n",
      "        [ 0.0257, -0.0519],\n",
      "        [-0.0519,  0.0628],\n",
      "        [-0.0426,  0.0583],\n",
      "        [ 0.0391,  0.0258],\n",
      "        [-0.0625,  0.0565],\n",
      "        [ 0.0728,  0.0282],\n",
      "        [-0.0742,  0.0047],\n",
      "        [-0.0635, -0.0323],\n",
      "        [ 0.0178, -0.0213],\n",
      "        [-0.0677, -0.0137],\n",
      "        [-0.0351, -0.0091],\n",
      "        [ 0.0792,  0.0473],\n",
      "        [ 0.0105,  0.0763],\n",
      "        [ 0.0652,  0.0307],\n",
      "        [ 0.0687, -0.0324],\n",
      "        [ 0.0727, -0.0414],\n",
      "        [-0.0774,  0.0852],\n",
      "        [-0.0510, -0.0647],\n",
      "        [ 0.0367, -0.0794],\n",
      "        [-0.0112,  0.0241],\n",
      "        [-0.0728,  0.0839],\n",
      "        [ 0.0222, -0.0643],\n",
      "        [-0.0366,  0.0549],\n",
      "        [ 0.0760, -0.0411],\n",
      "        [ 0.0150, -0.0266],\n",
      "        [-0.0400, -0.0881],\n",
      "        [ 0.0548,  0.0803],\n",
      "        [-0.0199, -0.0854],\n",
      "        [ 0.0358, -0.0863],\n",
      "        [ 0.0343,  0.0434],\n",
      "        [-0.0750, -0.0764],\n",
      "        [ 0.0400, -0.0731],\n",
      "        [-0.0722, -0.0172],\n",
      "        [-0.0544, -0.0756],\n",
      "        [-0.0493,  0.0617],\n",
      "        [ 0.0236,  0.0027],\n",
      "        [-0.0200, -0.0603],\n",
      "        [ 0.0158, -0.0035],\n",
      "        [-0.0304, -0.0590],\n",
      "        [ 0.0793, -0.0155],\n",
      "        [-0.0697, -0.0389],\n",
      "        [ 0.0466,  0.0835]], grad_fn=<CatBackward0>)\n",
      "projected output: torch.Size([1, 3, 2])\n",
      "tensor([[[-0.0362,  0.0426],\n",
      "         [-0.0697,  0.1789],\n",
      "         [-0.0680,  0.2155]]], grad_fn=<UnsafeViewBackward0>)\n",
      "----------------- END MultiQueryAttention.forwardTensor() --------------------\n",
      "x in layer after MQA & resid connection and before MLP:\n",
      "tensor([[[ 0.1053,  1.5096],\n",
      "         [-0.9862,  0.6240],\n",
      "         [-0.6045,  0.0909]]], grad_fn=<AddBackward0>)\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 3, 2])\n",
      "tensor([[[ 0.1053,  1.5096],\n",
      "         [-0.9862,  0.6240],\n",
      "         [-0.6045,  0.0909]]], grad_fn=<AddBackward0>)\n",
      "normed x: torch.Size([1, 3, 2])\n",
      "tensor([[[ 0.0984,  1.4108],\n",
      "         [-1.1951,  0.7562],\n",
      "         [-1.3985,  0.2104]]], grad_fn=<MulBackward0>)\n",
      "dim: 2\n",
      "skip: 0\n",
      "spliced scale: torch.Size([2])\n",
      "tensor([0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 3, 2])\n",
      "tensor([[[ 0.0984,  1.4108],\n",
      "         [-1.1951,  0.7562],\n",
      "         [-1.3985,  0.2104]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "---------- MLP Input: torch.Tensor ------------\n",
      "------------- MLP.forwardTensor() ------------\n",
      "x: torch.Size([1, 3, 2])\n",
      "tensor([[[ 0.0984,  1.4108],\n",
      "         [-1.1951,  0.7562],\n",
      "         [-1.3985,  0.2104]]], grad_fn=<MulBackward0>)\n",
      "d_dim: 2\n",
      "d_skip: 0\n",
      "i_dim: 8\n",
      "i_skip: 0\n",
      "Wgate: torch.Size([4, 16])\n",
      "Parameter containing:\n",
      "tensor([[ 0.3095,  0.3247, -0.2348, -0.2684,  0.1481, -0.0336,  0.2918,  0.4104,\n",
      "          0.4233, -0.2854, -0.4600, -0.0114,  0.0436,  0.3290, -0.4695,  0.1966],\n",
      "        [-0.4550, -0.3347,  0.1588, -0.0416, -0.1350, -0.4122,  0.0433, -0.0343,\n",
      "          0.1841,  0.2340, -0.0984,  0.2009, -0.4429, -0.4785,  0.0231, -0.0917],\n",
      "        [-0.1875,  0.4671,  0.2435, -0.4029, -0.2097,  0.4350,  0.1203,  0.4133,\n",
      "          0.2601, -0.1608, -0.2288, -0.2892, -0.4386, -0.3339,  0.4059,  0.3157],\n",
      "        [-0.4021,  0.0384, -0.3442,  0.3066, -0.4698, -0.0893,  0.1899, -0.2196,\n",
      "          0.1504,  0.3536, -0.0917,  0.4688, -0.2037,  0.0865,  0.1056, -0.1437]],\n",
      "       requires_grad=True)\n",
      "Wgate spliced: torch.Size([2, 8])\n",
      "tensor([[ 0.3095,  0.3247, -0.2348, -0.2684,  0.1481, -0.0336,  0.2918,  0.4104],\n",
      "        [-0.4550, -0.3347,  0.1588, -0.0416, -0.1350, -0.4122,  0.0433, -0.0343]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Bgate: torch.Size([16])\n",
      "Parameter containing:\n",
      "tensor([-0.3869, -0.0935, -0.0742,  0.3095, -0.0956,  0.3838, -0.1328, -0.3449,\n",
      "        -0.4261,  0.3397,  0.2771,  0.2628, -0.2119, -0.4128, -0.2219,  0.1706],\n",
      "       requires_grad=True)\n",
      "Bgate spliced: torch.Size([8])\n",
      "tensor([-0.3869, -0.0935, -0.0742,  0.3095, -0.0956,  0.3838, -0.1328, -0.3449],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xgate: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.9984, -0.5338,  0.1267,  0.2244, -0.2715, -0.2011, -0.0430,\n",
      "          -0.3530],\n",
      "         [-1.1009, -0.7347,  0.3264,  0.5988, -0.3747,  0.1122, -0.4487,\n",
      "          -0.8614],\n",
      "         [-0.9155, -0.6181,  0.2875,  0.6760, -0.3312,  0.3440, -0.5317,\n",
      "          -0.9261]]], grad_fn=<AddBackward0>)\n",
      "GeLU'ed Xgate: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.1588, -0.1584,  0.0697,  0.1321, -0.1067, -0.0845, -0.0208,\n",
      "          -0.1278],\n",
      "         [-0.1491, -0.1699,  0.2050,  0.4343, -0.1326,  0.0611, -0.1467,\n",
      "          -0.1676],\n",
      "         [-0.1648, -0.1658,  0.1763,  0.5073, -0.1226,  0.2183, -0.1582,\n",
      "          -0.1641]]], grad_fn=<GeluBackward0>)\n",
      "Wup: torch.Size([4, 16])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0488, -0.1314, -0.0796, -0.4639,  0.2573,  0.1704,  0.0070,  0.3364,\n",
      "         -0.4475,  0.2511,  0.1227,  0.0093,  0.3071, -0.4281, -0.3575, -0.1387],\n",
      "        [-0.2920, -0.1945, -0.1729, -0.1217,  0.0128,  0.4984,  0.2747, -0.0651,\n",
      "         -0.2933,  0.0877, -0.3934, -0.1303, -0.4999, -0.2135,  0.0632,  0.2846],\n",
      "        [ 0.4273, -0.4888, -0.3618,  0.3541,  0.4532,  0.2270, -0.0352, -0.1797,\n",
      "          0.2048,  0.0527, -0.3789,  0.0368, -0.1809,  0.1246,  0.2698, -0.0715],\n",
      "        [-0.3377, -0.4910, -0.0681, -0.4828, -0.0743, -0.0438, -0.3095, -0.1333,\n",
      "          0.0544,  0.0970, -0.0450, -0.4722,  0.2919, -0.2446,  0.2606,  0.0314]],\n",
      "       requires_grad=True)\n",
      "Wup spliced: torch.Size([2, 8])\n",
      "tensor([[ 0.0488, -0.1314, -0.0796, -0.4639,  0.2573,  0.1704,  0.0070,  0.3364],\n",
      "        [-0.2920, -0.1945, -0.1729, -0.1217,  0.0128,  0.4984,  0.2747, -0.0651]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Bup: torch.Size([16])\n",
      "Parameter containing:\n",
      "tensor([-0.1737,  0.4697,  0.0072, -0.1991,  0.3706, -0.0403, -0.2663,  0.3325,\n",
      "        -0.3660,  0.3473, -0.3763, -0.3744,  0.2100,  0.1556, -0.0674,  0.4764],\n",
      "       requires_grad=True)\n",
      "Bup spliced: torch.Size([8])\n",
      "tensor([-0.1737,  0.4697,  0.0072, -0.1991,  0.3706, -0.0403, -0.2663,  0.3325],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xup: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.5809,  0.1824, -0.2446, -0.4165,  0.4140,  0.6797,  0.1219,\n",
      "           0.2738],\n",
      "         [-0.4529,  0.4796, -0.0285,  0.2632,  0.0728,  0.1329, -0.0669,\n",
      "          -0.1188],\n",
      "         [-0.3034,  0.6125,  0.0821,  0.4240,  0.0135, -0.1738, -0.2182,\n",
      "          -0.1517]]], grad_fn=<AddBackward0>)\n",
      "Xfuse: torch.Size([1, 3, 8])\n",
      "tensor([[[ 0.0922, -0.0289, -0.0171, -0.0550, -0.0442, -0.0575, -0.0025,\n",
      "          -0.0350],\n",
      "         [ 0.0675, -0.0815, -0.0058,  0.1143, -0.0097,  0.0081,  0.0098,\n",
      "           0.0199],\n",
      "         [ 0.0500, -0.1016,  0.0145,  0.2151, -0.0017, -0.0379,  0.0345,\n",
      "           0.0249]]], grad_fn=<MulBackward0>)\n",
      "Wdown: torch.Size([16, 4])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0028,  0.1688,  0.1252, -0.2132],\n",
      "        [ 0.2146,  0.2054, -0.1145,  0.1425],\n",
      "        [ 0.2420, -0.1323,  0.0254,  0.0988],\n",
      "        [-0.2279, -0.2429,  0.0742,  0.1267],\n",
      "        [ 0.1254,  0.0361,  0.2388, -0.2003],\n",
      "        [-0.0737, -0.1731, -0.2045, -0.1040],\n",
      "        [ 0.0186, -0.1040, -0.0075,  0.1881],\n",
      "        [ 0.0218, -0.1834,  0.1468, -0.1363],\n",
      "        [ 0.0217,  0.2413,  0.1934, -0.0885],\n",
      "        [-0.0613,  0.1515, -0.0892,  0.0997],\n",
      "        [ 0.1085,  0.1849, -0.1100,  0.0362],\n",
      "        [-0.0083,  0.1678, -0.1493,  0.0104],\n",
      "        [ 0.2329, -0.2497, -0.1154,  0.0158],\n",
      "        [ 0.2373,  0.1807,  0.2185,  0.2416],\n",
      "        [-0.0733,  0.1733,  0.0052,  0.2215],\n",
      "        [ 0.1725, -0.0791,  0.0407, -0.1026]], requires_grad=True)\n",
      "Wdown spliced: torch.Size([8, 2])\n",
      "tensor([[ 0.0028,  0.1688],\n",
      "        [ 0.2146,  0.2054],\n",
      "        [ 0.2420, -0.1323],\n",
      "        [-0.2279, -0.2429],\n",
      "        [ 0.1254,  0.0361],\n",
      "        [-0.0737, -0.1731],\n",
      "        [ 0.0186, -0.1040],\n",
      "        [ 0.0218, -0.1834]], grad_fn=<SliceBackward0>)\n",
      "Bdown: torch.Size([4])\n",
      "Parameter containing:\n",
      "tensor([-0.1299, -0.0603, -0.1047, -0.2112], requires_grad=True)\n",
      "Bdown spliced: torch.Size([2])\n",
      "tensor([-0.1299, -0.0603], grad_fn=<SliceBackward0>)\n",
      "outputs: torch.Size([1, 3, 2])\n",
      "tensor([[[-0.1295, -0.0200],\n",
      "         [-0.1758, -0.0990],\n",
      "         [-0.1933, -0.1285]]], grad_fn=<AddBackward0>)\n",
      "------------- END MLP.forwardTensor() ------------\n",
      "layer's final residual state:\n",
      "tensor([[[-0.0242,  1.4896],\n",
      "         [-1.1620,  0.5250],\n",
      "         [-0.7977, -0.0376]]], grad_fn=<AddBackward0>)\n",
      "----------------- END Layer.forwardTensor() --------------------\n",
      "forwardTensor() output: torch.Size([1, 3, 2])\n",
      "tensor([[[-0.0242,  1.4896],\n",
      "         [-1.1620,  0.5250],\n",
      "         [-0.7977, -0.0376]]], grad_fn=<AddBackward0>)\n",
      "Model 1 from range(2)\n",
      "----------------- Layer.forwardTensor() --------------------\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 3, 2])\n",
      "tensor([[[ 0.8669,  0.1827],\n",
      "         [-1.2019,  0.5401],\n",
      "         [-0.9241,  0.2901]]])\n",
      "normed x: torch.Size([1, 3, 2])\n",
      "tensor([[[ 1.3838,  0.2917],\n",
      "         [-1.2900,  0.5797],\n",
      "         [-1.3493,  0.4236]]])\n",
      "dim: 2\n",
      "skip: 2\n",
      "spliced scale: torch.Size([2])\n",
      "tensor([0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 3, 2])\n",
      "tensor([[[ 1.3838,  0.2917],\n",
      "         [-1.2900,  0.5797],\n",
      "         [-1.3493,  0.4236]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "---------- Attention Input: torch.Tensor ------------\n",
      "----------------- MultiQueryAttention.forwardTensor() --------------------\n",
      "x shape: torch.Size([1, 3, 2])\n",
      "d_skip: 2\n",
      "models_in_this_level: 2\n",
      "h_dim: 16\n",
      "h_skip: 16\n",
      "self.Wqkv: torch.Size([4, 192])\n",
      "Parameter containing:\n",
      "tensor([[ 1.8937e-01, -2.8415e-01, -4.6238e-01, -4.0144e-01, -4.4552e-01,\n",
      "          1.9680e-01,  4.4328e-02,  4.4592e-01, -2.1865e-01,  2.1341e-01,\n",
      "         -3.0257e-03, -4.0493e-01,  1.3867e-01,  2.9379e-01,  3.5597e-01,\n",
      "          4.7860e-01, -3.5840e-01,  8.7989e-02, -2.9445e-01, -9.1477e-02,\n",
      "         -3.5326e-02, -1.1366e-01,  1.4490e-01, -2.6402e-03,  6.1200e-02,\n",
      "          6.7722e-02, -3.2172e-01, -3.8489e-01,  2.5546e-01, -2.6750e-01,\n",
      "         -1.2889e-01, -4.3475e-01,  1.1832e-01,  1.9844e-01,  2.2294e-01,\n",
      "          2.2098e-01,  3.4316e-01,  4.0263e-01,  1.8223e-01, -3.1401e-01,\n",
      "          1.4572e-01,  2.5463e-01,  1.4850e-01,  4.2974e-01,  3.9195e-01,\n",
      "         -4.2508e-01,  1.2758e-01,  4.7005e-01,  2.9028e-01,  2.7730e-01,\n",
      "          2.4163e-01,  4.1844e-01, -3.1530e-01,  3.5747e-01,  2.3997e-02,\n",
      "          2.9750e-01,  4.4569e-01, -3.0182e-01, -1.2491e-01,  2.3174e-01,\n",
      "          4.1144e-01, -1.1239e-01, -3.2242e-01,  2.1355e-01, -2.8637e-01,\n",
      "         -4.6897e-01,  3.3208e-01,  3.4030e-02,  4.2027e-01,  1.1778e-01,\n",
      "         -2.8841e-01,  3.7903e-01, -9.4648e-02, -5.8460e-02, -2.6424e-01,\n",
      "          2.9487e-01,  5.8353e-02, -1.1010e-01, -3.4630e-01,  2.7946e-01,\n",
      "          3.0002e-01,  2.4929e-01, -4.9700e-01, -5.6904e-02, -2.3638e-01,\n",
      "          2.2197e-01, -3.2396e-01,  1.0468e-01, -1.2194e-01,  2.9137e-01,\n",
      "         -2.7048e-01, -2.6034e-02,  4.3632e-01,  2.0930e-01,  1.6822e-01,\n",
      "         -2.1106e-02, -2.5414e-01, -1.1344e-02,  1.7114e-01, -3.4630e-01,\n",
      "          3.0182e-01, -1.7840e-01, -4.4955e-02,  4.7685e-01,  2.3429e-01,\n",
      "         -2.0993e-04, -3.1590e-01,  1.4933e-01, -3.4794e-01,  4.0296e-02,\n",
      "         -5.2708e-02, -3.9691e-01,  3.8710e-02, -7.3864e-02,  3.9559e-01,\n",
      "         -2.1087e-01,  4.0306e-01, -3.9320e-01, -9.2826e-02,  4.8890e-01,\n",
      "          4.7009e-01, -4.8984e-01,  1.8414e-02,  4.3571e-01,  3.3965e-02,\n",
      "         -3.1877e-01,  2.5281e-01, -3.5005e-01, -2.3904e-01,  4.2611e-01,\n",
      "         -2.9801e-01,  8.4876e-02,  4.0464e-01, -1.8968e-01, -3.6074e-01,\n",
      "         -3.8140e-03, -3.8703e-01, -1.0132e-01, -4.1207e-01, -1.3494e-04,\n",
      "          3.9459e-01,  2.9657e-01,  2.0455e-01, -1.1478e-01,  4.8224e-01,\n",
      "         -2.4734e-01, -1.0230e-01,  9.2335e-02,  2.4343e-02, -3.5300e-01,\n",
      "          2.0875e-01,  4.3911e-01, -4.2997e-01, -1.1652e-01,  2.6931e-01,\n",
      "          1.5951e-01,  4.8167e-01, -1.0938e-01,  6.8503e-02,  9.3351e-02,\n",
      "         -3.8608e-01, -4.6630e-03, -3.0631e-01, -3.1452e-01,  3.6388e-01,\n",
      "          3.9371e-01, -4.8803e-01, -2.3385e-01, -2.9818e-01, -3.0848e-01,\n",
      "          2.9630e-01,  7.3386e-02, -1.2276e-02,  1.8542e-01, -4.1924e-02,\n",
      "         -4.0357e-01, -1.0265e-01,  2.8177e-01, -4.9081e-01,  2.1151e-01,\n",
      "          3.6830e-01,  3.0946e-01,  1.5720e-01,  1.4172e-01, -4.2283e-01,\n",
      "         -4.7915e-01,  3.8314e-01, -3.4119e-01, -2.9776e-01, -1.5916e-01,\n",
      "          3.1954e-01,  3.8582e-01],\n",
      "        [ 2.8393e-02, -2.1507e-01,  7.3449e-02, -3.5425e-02,  8.4971e-03,\n",
      "          6.4568e-02, -4.1707e-01,  4.9329e-01,  6.2499e-02, -3.1489e-01,\n",
      "         -2.6528e-01, -2.0642e-01,  2.8555e-01,  2.0611e-01, -2.4231e-01,\n",
      "          1.9236e-01, -2.4110e-01,  3.8176e-01, -3.1023e-01,  5.9767e-02,\n",
      "         -3.1687e-01, -4.0727e-01, -2.7802e-01, -4.0448e-01, -2.9654e-01,\n",
      "          4.2772e-01, -2.7135e-02, -9.2047e-02, -1.7364e-01, -3.5465e-02,\n",
      "          4.5284e-02, -1.1347e-01,  1.1493e-01,  2.4035e-02,  4.2536e-01,\n",
      "         -2.7699e-01, -2.6960e-01,  4.5004e-01,  1.0166e-01,  4.5971e-01,\n",
      "         -4.6756e-01,  2.6671e-01,  6.4181e-02,  3.1845e-01, -4.9001e-01,\n",
      "          1.8032e-01,  1.0662e-01, -4.9253e-01, -8.3386e-02, -3.9306e-01,\n",
      "         -4.4291e-01, -7.9380e-02,  5.9529e-03,  1.9958e-02, -2.0477e-01,\n",
      "         -3.1541e-01, -1.0251e-01, -3.1129e-01, -2.1621e-02, -1.0289e-02,\n",
      "         -4.2008e-01, -4.8188e-02,  3.7436e-01, -1.1315e-01, -4.9122e-01,\n",
      "         -4.0513e-01,  2.0755e-01,  1.1163e-01, -4.9098e-01,  4.1759e-01,\n",
      "          1.6288e-01,  6.8331e-04, -3.7547e-01,  1.7793e-01, -1.0203e-01,\n",
      "         -3.3526e-01,  3.0938e-01,  3.5360e-01, -4.9011e-01, -3.8699e-01,\n",
      "          2.1129e-01, -4.3664e-01, -2.1207e-01, -3.9717e-01,  3.2691e-01,\n",
      "         -1.7923e-01,  3.1414e-01,  4.3294e-01,  4.4856e-01,  4.4316e-01,\n",
      "          2.7930e-01, -3.8357e-01,  3.5021e-01,  3.0501e-01,  3.6156e-01,\n",
      "         -3.5105e-01,  3.5966e-01,  4.6147e-01,  2.8881e-01,  1.0856e-02,\n",
      "          3.1861e-01,  3.0779e-02,  2.5312e-02, -1.1220e-01,  4.2442e-01,\n",
      "          8.0456e-02, -4.1062e-01,  4.2825e-01,  1.9142e-01,  1.2905e-01,\n",
      "         -4.1203e-01, -4.0837e-01,  1.0011e-01,  1.0266e-01,  1.1884e-01,\n",
      "         -2.9486e-01, -1.7023e-01, -3.9583e-01,  4.3081e-01, -4.8520e-01,\n",
      "          8.0888e-03,  1.3026e-02,  3.0812e-02,  3.1113e-01,  5.4653e-02,\n",
      "          3.3968e-01, -2.7318e-02, -1.7378e-01, -1.9805e-01,  1.3532e-02,\n",
      "         -7.3870e-02, -2.5023e-01,  4.2612e-01,  1.9641e-01,  4.4966e-01,\n",
      "          3.5824e-01, -3.8031e-01,  6.2354e-02,  2.4223e-01, -2.8999e-01,\n",
      "         -2.9022e-01,  1.9836e-01,  2.7220e-01,  3.3876e-01,  2.3070e-01,\n",
      "         -3.7776e-01,  3.4646e-01,  3.5032e-01, -2.7519e-01, -3.7461e-01,\n",
      "         -1.4845e-01, -4.0748e-01,  4.1486e-01,  4.7666e-01,  2.3587e-01,\n",
      "          3.7843e-01,  4.7986e-01,  3.1608e-01,  4.0587e-01,  4.5123e-01,\n",
      "          1.6128e-01,  3.4837e-02, -2.4213e-01,  8.3348e-02,  4.0535e-01,\n",
      "         -2.0574e-01,  1.2630e-01, -1.4109e-01, -1.3830e-01, -1.7473e-01,\n",
      "          2.6931e-01,  1.7263e-01, -3.6738e-01, -2.2899e-01,  4.7750e-02,\n",
      "         -3.8553e-01, -3.5645e-01,  1.2255e-01,  2.6394e-01,  1.4087e-01,\n",
      "          4.8568e-01,  3.9760e-01, -4.6300e-01, -4.0858e-01, -1.0156e-02,\n",
      "          2.2234e-01, -1.7810e-02,  2.4384e-01, -1.7978e-01, -4.6589e-01,\n",
      "         -4.8086e-01,  4.0185e-01],\n",
      "        [ 2.4316e-01,  5.1094e-02,  5.5459e-02, -4.2938e-01, -1.7976e-01,\n",
      "          3.1237e-01, -3.2596e-02, -2.8321e-01,  2.7905e-01, -2.7461e-01,\n",
      "          1.0299e-01,  1.8728e-01, -1.6608e-01,  1.3264e-01,  2.7119e-01,\n",
      "          6.4278e-02,  3.1660e-01,  2.2600e-01, -4.8537e-01,  4.2158e-01,\n",
      "          4.8353e-01, -1.8455e-01,  8.0709e-02, -1.7443e-01,  4.0629e-01,\n",
      "          4.3725e-01, -1.5402e-01, -1.8739e-03,  4.4505e-01, -3.8958e-01,\n",
      "          1.8676e-01,  4.1720e-01, -4.8323e-01,  1.4650e-01,  3.3617e-01,\n",
      "          4.6191e-01,  4.8709e-01,  4.2077e-01,  2.7973e-01,  1.4151e-01,\n",
      "          4.8756e-01, -9.2613e-02, -8.1956e-02,  3.6287e-01, -4.3972e-01,\n",
      "         -3.8881e-01, -5.7135e-02,  3.9335e-01,  4.2260e-01, -4.6757e-01,\n",
      "         -2.9373e-01, -3.9142e-01, -4.4682e-01,  3.2180e-01,  3.9332e-01,\n",
      "          5.9391e-02, -4.0696e-01,  4.0756e-01, -3.7403e-01, -3.4418e-01,\n",
      "         -6.7751e-02,  3.9745e-01, -3.6439e-01,  3.8382e-01, -7.6625e-02,\n",
      "         -3.1299e-01,  4.9208e-02, -9.0591e-02,  2.8831e-01, -4.6503e-01,\n",
      "         -7.9162e-02, -1.3593e-01,  2.0823e-01,  1.3763e-01, -2.5408e-01,\n",
      "          1.9507e-01,  3.0950e-01, -4.6143e-01,  4.1417e-02, -3.1686e-02,\n",
      "         -8.1151e-02, -7.1416e-02,  1.4370e-01, -4.2339e-01, -3.9791e-01,\n",
      "          1.2989e-02,  9.9221e-02,  6.6198e-02,  2.0977e-01,  7.8000e-02,\n",
      "         -1.7535e-01,  4.6218e-01, -2.5272e-01, -4.7835e-01,  1.2324e-01,\n",
      "         -2.4548e-01,  1.5080e-01, -5.3550e-02,  1.2076e-01,  3.8934e-01,\n",
      "          3.8205e-01, -4.0994e-01, -1.1160e-01,  2.9541e-01, -3.4564e-01,\n",
      "          2.6261e-01,  5.1345e-02, -3.3273e-02,  3.0493e-01, -7.7605e-02,\n",
      "         -1.5532e-01,  1.4562e-01, -2.4403e-01,  2.2133e-01, -4.4685e-01,\n",
      "         -9.4684e-02,  2.7953e-01, -4.0533e-01,  2.5822e-01,  4.4835e-01,\n",
      "          3.3890e-01,  1.3830e-01, -2.2497e-01,  9.8689e-02,  2.8423e-01,\n",
      "         -2.7537e-01, -2.9027e-02,  3.8509e-01, -1.0216e-01,  8.3434e-02,\n",
      "         -4.5676e-01, -3.2859e-01, -1.8846e-01, -3.8350e-01,  4.8841e-01,\n",
      "          7.9258e-02, -3.2171e-01, -4.3615e-02, -3.8480e-01, -6.7773e-02,\n",
      "         -4.7649e-01, -4.3129e-01, -4.9387e-01,  4.9946e-01,  2.1872e-01,\n",
      "          1.6125e-01,  4.2134e-01,  4.6237e-01, -2.6883e-02, -6.6878e-02,\n",
      "         -3.0271e-01,  4.1654e-01,  3.9426e-01,  6.2786e-03,  3.8204e-01,\n",
      "         -2.3100e-01,  1.6674e-01,  1.9269e-02, -1.5630e-01,  1.3106e-01,\n",
      "          3.4213e-01, -2.8111e-01, -4.6205e-01,  3.2671e-01, -1.4839e-01,\n",
      "         -3.5897e-01, -4.2942e-01,  3.7445e-01,  2.6903e-01, -2.6800e-01,\n",
      "          4.6427e-01, -4.1747e-01,  3.9074e-01, -2.8527e-01,  2.7827e-01,\n",
      "         -1.4761e-02, -7.7906e-02, -2.4888e-01,  4.9965e-01,  3.1182e-01,\n",
      "          4.2581e-01, -3.5637e-02,  2.6706e-01, -2.1018e-01, -4.6920e-01,\n",
      "         -3.0328e-01, -8.8848e-02,  1.3523e-01, -1.2193e-01,  2.9067e-01,\n",
      "          1.4192e-01,  2.9863e-01],\n",
      "        [ 2.1939e-02, -2.1620e-01, -1.3989e-01,  2.0001e-01, -2.6691e-01,\n",
      "         -2.2999e-01,  3.5089e-01,  4.2780e-01, -2.5216e-01,  2.5742e-01,\n",
      "         -4.2682e-01, -2.3400e-01,  2.5624e-01,  5.2858e-02, -4.4735e-02,\n",
      "         -2.7120e-01,  2.7121e-01, -1.1131e-02, -2.0003e-01,  3.6435e-01,\n",
      "         -1.3997e-01, -3.0246e-01,  2.5170e-01, -1.9140e-01, -4.2226e-01,\n",
      "         -2.8699e-01,  4.4266e-02,  6.2823e-02,  1.7323e-01, -8.4048e-02,\n",
      "          2.8983e-02,  7.1588e-02, -4.9993e-01,  4.3573e-01, -1.6429e-01,\n",
      "         -2.9448e-01, -4.0873e-01,  2.7543e-01, -3.6739e-01,  1.9690e-01,\n",
      "         -2.6216e-01,  2.5825e-01, -1.5555e-01, -1.9827e-01,  3.1932e-01,\n",
      "         -2.0889e-01, -2.8329e-01, -1.9644e-01,  7.6186e-02, -4.7227e-01,\n",
      "          1.4629e-01,  3.2163e-01,  4.3594e-01, -2.9314e-01,  2.6589e-01,\n",
      "         -6.6499e-02,  2.4181e-01,  4.1164e-01,  4.9924e-01,  9.3671e-02,\n",
      "         -4.7423e-01, -3.0037e-01, -3.6135e-01, -4.7988e-01,  4.1612e-01,\n",
      "          7.5813e-02, -4.3853e-01, -1.3715e-01,  2.6466e-01, -4.1169e-01,\n",
      "         -2.6106e-01,  2.4077e-01,  2.4765e-01,  2.0474e-01, -4.8767e-01,\n",
      "          3.4312e-01,  3.0089e-01, -2.6358e-01, -2.3430e-01,  3.1284e-01,\n",
      "          4.9953e-01, -3.5684e-02,  1.9416e-01,  4.6613e-01,  4.7486e-01,\n",
      "          7.1292e-02, -1.6498e-01,  2.6788e-01,  3.3629e-01,  4.4775e-01,\n",
      "          4.7669e-01, -4.7820e-01, -4.4081e-01, -4.0668e-01,  2.4537e-01,\n",
      "         -2.8373e-01,  2.6604e-01, -1.5824e-01, -2.7501e-01,  4.6280e-01,\n",
      "          4.3243e-01, -6.9161e-03,  3.4847e-01, -4.3560e-01, -3.1419e-01,\n",
      "         -5.0649e-02, -1.4905e-01, -2.1218e-01,  4.0496e-01,  9.8498e-02,\n",
      "          2.1212e-02, -2.1080e-01,  3.0203e-01, -3.2247e-02, -3.5137e-01,\n",
      "         -4.0825e-01,  3.6829e-01,  7.6268e-02, -4.5142e-01, -7.0428e-02,\n",
      "         -3.8538e-02, -4.0860e-01, -2.3721e-01, -2.6784e-01,  2.6255e-01,\n",
      "          4.1158e-01, -1.2125e-01,  4.8033e-01, -3.1060e-01,  4.8208e-04,\n",
      "         -1.2071e-01,  3.9326e-02,  3.2534e-01,  1.0235e-01,  2.3560e-01,\n",
      "          3.7913e-01, -4.0633e-01, -3.9607e-01,  4.2034e-01,  9.0152e-02,\n",
      "         -3.9707e-01,  2.2478e-01,  4.7630e-01,  8.5905e-02, -3.4227e-01,\n",
      "         -2.6248e-01,  2.0666e-01,  2.5354e-01,  4.9642e-01, -3.3560e-01,\n",
      "         -2.5921e-01, -1.5643e-01,  2.4026e-01,  2.8152e-01,  7.2462e-02,\n",
      "          5.1272e-02,  4.5968e-01,  2.5359e-02, -3.2823e-01, -9.0428e-02,\n",
      "         -4.1559e-01,  1.7903e-01, -1.5719e-01,  3.1318e-01,  1.4970e-01,\n",
      "         -3.7570e-01, -3.2366e-01,  7.3289e-02, -1.7857e-01, -7.6912e-02,\n",
      "         -1.5517e-01, -2.5391e-01, -3.7961e-01, -3.2698e-01,  5.0610e-02,\n",
      "         -2.0522e-01, -5.3520e-02,  7.4136e-02,  4.2519e-01, -2.1097e-01,\n",
      "          1.1299e-01,  4.5611e-01, -4.4404e-02,  3.8389e-01, -2.2574e-01,\n",
      "         -2.3627e-01, -4.0755e-01, -4.2760e-01, -4.0825e-02, -4.1447e-01,\n",
      "          4.5897e-01, -1.1031e-01]], requires_grad=True)\n",
      "Wq: torch.Size([4, 128])\n",
      "tensor([[ 1.8937e-01, -2.8415e-01, -4.6238e-01, -4.0144e-01, -4.4552e-01,\n",
      "          1.9680e-01,  4.4328e-02,  4.4592e-01, -2.1865e-01,  2.1341e-01,\n",
      "         -3.0257e-03, -4.0493e-01,  1.3867e-01,  2.9379e-01,  3.5597e-01,\n",
      "          4.7860e-01, -3.5840e-01,  8.7989e-02, -2.9445e-01, -9.1477e-02,\n",
      "         -3.5326e-02, -1.1366e-01,  1.4490e-01, -2.6402e-03,  6.1200e-02,\n",
      "          6.7722e-02, -3.2172e-01, -3.8489e-01,  2.5546e-01, -2.6750e-01,\n",
      "         -1.2889e-01, -4.3475e-01,  1.1832e-01,  1.9844e-01,  2.2294e-01,\n",
      "          2.2098e-01,  3.4316e-01,  4.0263e-01,  1.8223e-01, -3.1401e-01,\n",
      "          1.4572e-01,  2.5463e-01,  1.4850e-01,  4.2974e-01,  3.9195e-01,\n",
      "         -4.2508e-01,  1.2758e-01,  4.7005e-01,  2.9028e-01,  2.7730e-01,\n",
      "          2.4163e-01,  4.1844e-01, -3.1530e-01,  3.5747e-01,  2.3997e-02,\n",
      "          2.9750e-01,  4.4569e-01, -3.0182e-01, -1.2491e-01,  2.3174e-01,\n",
      "          4.1144e-01, -1.1239e-01, -3.2242e-01,  2.1355e-01, -2.8637e-01,\n",
      "         -4.6897e-01,  3.3208e-01,  3.4030e-02,  4.2027e-01,  1.1778e-01,\n",
      "         -2.8841e-01,  3.7903e-01, -9.4648e-02, -5.8460e-02, -2.6424e-01,\n",
      "          2.9487e-01,  5.8353e-02, -1.1010e-01, -3.4630e-01,  2.7946e-01,\n",
      "          3.0002e-01,  2.4929e-01, -4.9700e-01, -5.6904e-02, -2.3638e-01,\n",
      "          2.2197e-01, -3.2396e-01,  1.0468e-01, -1.2194e-01,  2.9137e-01,\n",
      "         -2.7048e-01, -2.6034e-02,  4.3632e-01,  2.0930e-01,  1.6822e-01,\n",
      "         -2.1106e-02, -2.5414e-01, -1.1344e-02,  1.7114e-01, -3.4630e-01,\n",
      "          3.0182e-01, -1.7840e-01, -4.4955e-02,  4.7685e-01,  2.3429e-01,\n",
      "         -2.0993e-04, -3.1590e-01,  1.4933e-01, -3.4794e-01,  4.0296e-02,\n",
      "         -5.2708e-02, -3.9691e-01,  3.8710e-02, -7.3864e-02,  3.9559e-01,\n",
      "         -2.1087e-01,  4.0306e-01, -3.9320e-01, -9.2826e-02,  4.8890e-01,\n",
      "          4.7009e-01, -4.8984e-01,  1.8414e-02,  4.3571e-01,  3.3965e-02,\n",
      "         -3.1877e-01,  2.5281e-01, -3.5005e-01],\n",
      "        [ 2.8393e-02, -2.1507e-01,  7.3449e-02, -3.5425e-02,  8.4971e-03,\n",
      "          6.4568e-02, -4.1707e-01,  4.9329e-01,  6.2499e-02, -3.1489e-01,\n",
      "         -2.6528e-01, -2.0642e-01,  2.8555e-01,  2.0611e-01, -2.4231e-01,\n",
      "          1.9236e-01, -2.4110e-01,  3.8176e-01, -3.1023e-01,  5.9767e-02,\n",
      "         -3.1687e-01, -4.0727e-01, -2.7802e-01, -4.0448e-01, -2.9654e-01,\n",
      "          4.2772e-01, -2.7135e-02, -9.2047e-02, -1.7364e-01, -3.5465e-02,\n",
      "          4.5284e-02, -1.1347e-01,  1.1493e-01,  2.4035e-02,  4.2536e-01,\n",
      "         -2.7699e-01, -2.6960e-01,  4.5004e-01,  1.0166e-01,  4.5971e-01,\n",
      "         -4.6756e-01,  2.6671e-01,  6.4181e-02,  3.1845e-01, -4.9001e-01,\n",
      "          1.8032e-01,  1.0662e-01, -4.9253e-01, -8.3386e-02, -3.9306e-01,\n",
      "         -4.4291e-01, -7.9380e-02,  5.9529e-03,  1.9958e-02, -2.0477e-01,\n",
      "         -3.1541e-01, -1.0251e-01, -3.1129e-01, -2.1621e-02, -1.0289e-02,\n",
      "         -4.2008e-01, -4.8188e-02,  3.7436e-01, -1.1315e-01, -4.9122e-01,\n",
      "         -4.0513e-01,  2.0755e-01,  1.1163e-01, -4.9098e-01,  4.1759e-01,\n",
      "          1.6288e-01,  6.8331e-04, -3.7547e-01,  1.7793e-01, -1.0203e-01,\n",
      "         -3.3526e-01,  3.0938e-01,  3.5360e-01, -4.9011e-01, -3.8699e-01,\n",
      "          2.1129e-01, -4.3664e-01, -2.1207e-01, -3.9717e-01,  3.2691e-01,\n",
      "         -1.7923e-01,  3.1414e-01,  4.3294e-01,  4.4856e-01,  4.4316e-01,\n",
      "          2.7930e-01, -3.8357e-01,  3.5021e-01,  3.0501e-01,  3.6156e-01,\n",
      "         -3.5105e-01,  3.5966e-01,  4.6147e-01,  2.8881e-01,  1.0856e-02,\n",
      "          3.1861e-01,  3.0779e-02,  2.5312e-02, -1.1220e-01,  4.2442e-01,\n",
      "          8.0456e-02, -4.1062e-01,  4.2825e-01,  1.9142e-01,  1.2905e-01,\n",
      "         -4.1203e-01, -4.0837e-01,  1.0011e-01,  1.0266e-01,  1.1884e-01,\n",
      "         -2.9486e-01, -1.7023e-01, -3.9583e-01,  4.3081e-01, -4.8520e-01,\n",
      "          8.0888e-03,  1.3026e-02,  3.0812e-02,  3.1113e-01,  5.4653e-02,\n",
      "          3.3968e-01, -2.7318e-02, -1.7378e-01],\n",
      "        [ 2.4316e-01,  5.1094e-02,  5.5459e-02, -4.2938e-01, -1.7976e-01,\n",
      "          3.1237e-01, -3.2596e-02, -2.8321e-01,  2.7905e-01, -2.7461e-01,\n",
      "          1.0299e-01,  1.8728e-01, -1.6608e-01,  1.3264e-01,  2.7119e-01,\n",
      "          6.4278e-02,  3.1660e-01,  2.2600e-01, -4.8537e-01,  4.2158e-01,\n",
      "          4.8353e-01, -1.8455e-01,  8.0709e-02, -1.7443e-01,  4.0629e-01,\n",
      "          4.3725e-01, -1.5402e-01, -1.8739e-03,  4.4505e-01, -3.8958e-01,\n",
      "          1.8676e-01,  4.1720e-01, -4.8323e-01,  1.4650e-01,  3.3617e-01,\n",
      "          4.6191e-01,  4.8709e-01,  4.2077e-01,  2.7973e-01,  1.4151e-01,\n",
      "          4.8756e-01, -9.2613e-02, -8.1956e-02,  3.6287e-01, -4.3972e-01,\n",
      "         -3.8881e-01, -5.7135e-02,  3.9335e-01,  4.2260e-01, -4.6757e-01,\n",
      "         -2.9373e-01, -3.9142e-01, -4.4682e-01,  3.2180e-01,  3.9332e-01,\n",
      "          5.9391e-02, -4.0696e-01,  4.0756e-01, -3.7403e-01, -3.4418e-01,\n",
      "         -6.7751e-02,  3.9745e-01, -3.6439e-01,  3.8382e-01, -7.6625e-02,\n",
      "         -3.1299e-01,  4.9208e-02, -9.0591e-02,  2.8831e-01, -4.6503e-01,\n",
      "         -7.9162e-02, -1.3593e-01,  2.0823e-01,  1.3763e-01, -2.5408e-01,\n",
      "          1.9507e-01,  3.0950e-01, -4.6143e-01,  4.1417e-02, -3.1686e-02,\n",
      "         -8.1151e-02, -7.1416e-02,  1.4370e-01, -4.2339e-01, -3.9791e-01,\n",
      "          1.2989e-02,  9.9221e-02,  6.6198e-02,  2.0977e-01,  7.8000e-02,\n",
      "         -1.7535e-01,  4.6218e-01, -2.5272e-01, -4.7835e-01,  1.2324e-01,\n",
      "         -2.4548e-01,  1.5080e-01, -5.3550e-02,  1.2076e-01,  3.8934e-01,\n",
      "          3.8205e-01, -4.0994e-01, -1.1160e-01,  2.9541e-01, -3.4564e-01,\n",
      "          2.6261e-01,  5.1345e-02, -3.3273e-02,  3.0493e-01, -7.7605e-02,\n",
      "         -1.5532e-01,  1.4562e-01, -2.4403e-01,  2.2133e-01, -4.4685e-01,\n",
      "         -9.4684e-02,  2.7953e-01, -4.0533e-01,  2.5822e-01,  4.4835e-01,\n",
      "          3.3890e-01,  1.3830e-01, -2.2497e-01,  9.8689e-02,  2.8423e-01,\n",
      "         -2.7537e-01, -2.9027e-02,  3.8509e-01],\n",
      "        [ 2.1939e-02, -2.1620e-01, -1.3989e-01,  2.0001e-01, -2.6691e-01,\n",
      "         -2.2999e-01,  3.5089e-01,  4.2780e-01, -2.5216e-01,  2.5742e-01,\n",
      "         -4.2682e-01, -2.3400e-01,  2.5624e-01,  5.2858e-02, -4.4735e-02,\n",
      "         -2.7120e-01,  2.7121e-01, -1.1131e-02, -2.0003e-01,  3.6435e-01,\n",
      "         -1.3997e-01, -3.0246e-01,  2.5170e-01, -1.9140e-01, -4.2226e-01,\n",
      "         -2.8699e-01,  4.4266e-02,  6.2823e-02,  1.7323e-01, -8.4048e-02,\n",
      "          2.8983e-02,  7.1588e-02, -4.9993e-01,  4.3573e-01, -1.6429e-01,\n",
      "         -2.9448e-01, -4.0873e-01,  2.7543e-01, -3.6739e-01,  1.9690e-01,\n",
      "         -2.6216e-01,  2.5825e-01, -1.5555e-01, -1.9827e-01,  3.1932e-01,\n",
      "         -2.0889e-01, -2.8329e-01, -1.9644e-01,  7.6186e-02, -4.7227e-01,\n",
      "          1.4629e-01,  3.2163e-01,  4.3594e-01, -2.9314e-01,  2.6589e-01,\n",
      "         -6.6499e-02,  2.4181e-01,  4.1164e-01,  4.9924e-01,  9.3671e-02,\n",
      "         -4.7423e-01, -3.0037e-01, -3.6135e-01, -4.7988e-01,  4.1612e-01,\n",
      "          7.5813e-02, -4.3853e-01, -1.3715e-01,  2.6466e-01, -4.1169e-01,\n",
      "         -2.6106e-01,  2.4077e-01,  2.4765e-01,  2.0474e-01, -4.8767e-01,\n",
      "          3.4312e-01,  3.0089e-01, -2.6358e-01, -2.3430e-01,  3.1284e-01,\n",
      "          4.9953e-01, -3.5684e-02,  1.9416e-01,  4.6613e-01,  4.7486e-01,\n",
      "          7.1292e-02, -1.6498e-01,  2.6788e-01,  3.3629e-01,  4.4775e-01,\n",
      "          4.7669e-01, -4.7820e-01, -4.4081e-01, -4.0668e-01,  2.4537e-01,\n",
      "         -2.8373e-01,  2.6604e-01, -1.5824e-01, -2.7501e-01,  4.6280e-01,\n",
      "          4.3243e-01, -6.9161e-03,  3.4847e-01, -4.3560e-01, -3.1419e-01,\n",
      "         -5.0649e-02, -1.4905e-01, -2.1218e-01,  4.0496e-01,  9.8498e-02,\n",
      "          2.1212e-02, -2.1080e-01,  3.0203e-01, -3.2247e-02, -3.5137e-01,\n",
      "         -4.0825e-01,  3.6829e-01,  7.6268e-02, -4.5142e-01, -7.0428e-02,\n",
      "         -3.8538e-02, -4.0860e-01, -2.3721e-01, -2.6784e-01,  2.6255e-01,\n",
      "          4.1158e-01, -1.2125e-01,  4.8033e-01]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wk: torch.Size([4, 32])\n",
      "tensor([[-2.3904e-01,  4.2611e-01, -2.9801e-01,  8.4876e-02,  4.0464e-01,\n",
      "         -1.8968e-01, -3.6074e-01, -3.8140e-03, -3.8703e-01, -1.0132e-01,\n",
      "         -4.1207e-01, -1.3494e-04,  3.9459e-01,  2.9657e-01,  2.0455e-01,\n",
      "         -1.1478e-01,  4.8224e-01, -2.4734e-01, -1.0230e-01,  9.2335e-02,\n",
      "          2.4343e-02, -3.5300e-01,  2.0875e-01,  4.3911e-01, -4.2997e-01,\n",
      "         -1.1652e-01,  2.6931e-01,  1.5951e-01,  4.8167e-01, -1.0938e-01,\n",
      "          6.8503e-02,  9.3351e-02],\n",
      "        [-1.9805e-01,  1.3532e-02, -7.3870e-02, -2.5023e-01,  4.2612e-01,\n",
      "          1.9641e-01,  4.4966e-01,  3.5824e-01, -3.8031e-01,  6.2354e-02,\n",
      "          2.4223e-01, -2.8999e-01, -2.9022e-01,  1.9836e-01,  2.7220e-01,\n",
      "          3.3876e-01,  2.3070e-01, -3.7776e-01,  3.4646e-01,  3.5032e-01,\n",
      "         -2.7519e-01, -3.7461e-01, -1.4845e-01, -4.0748e-01,  4.1486e-01,\n",
      "          4.7666e-01,  2.3587e-01,  3.7843e-01,  4.7986e-01,  3.1608e-01,\n",
      "          4.0587e-01,  4.5123e-01],\n",
      "        [-1.0216e-01,  8.3434e-02, -4.5676e-01, -3.2859e-01, -1.8846e-01,\n",
      "         -3.8350e-01,  4.8841e-01,  7.9258e-02, -3.2171e-01, -4.3615e-02,\n",
      "         -3.8480e-01, -6.7773e-02, -4.7649e-01, -4.3129e-01, -4.9387e-01,\n",
      "          4.9946e-01,  2.1872e-01,  1.6125e-01,  4.2134e-01,  4.6237e-01,\n",
      "         -2.6883e-02, -6.6878e-02, -3.0271e-01,  4.1654e-01,  3.9426e-01,\n",
      "          6.2786e-03,  3.8204e-01, -2.3100e-01,  1.6674e-01,  1.9269e-02,\n",
      "         -1.5630e-01,  1.3106e-01],\n",
      "        [-3.1060e-01,  4.8208e-04, -1.2071e-01,  3.9326e-02,  3.2534e-01,\n",
      "          1.0235e-01,  2.3560e-01,  3.7913e-01, -4.0633e-01, -3.9607e-01,\n",
      "          4.2034e-01,  9.0152e-02, -3.9707e-01,  2.2478e-01,  4.7630e-01,\n",
      "          8.5905e-02, -3.4227e-01, -2.6248e-01,  2.0666e-01,  2.5354e-01,\n",
      "          4.9642e-01, -3.3560e-01, -2.5921e-01, -1.5643e-01,  2.4026e-01,\n",
      "          2.8152e-01,  7.2462e-02,  5.1272e-02,  4.5968e-01,  2.5359e-02,\n",
      "         -3.2823e-01, -9.0428e-02]], grad_fn=<SplitWithSizesBackward0>)\n",
      "Wv: torch.Size([4, 32])\n",
      "tensor([[-0.3861, -0.0047, -0.3063, -0.3145,  0.3639,  0.3937, -0.4880, -0.2338,\n",
      "         -0.2982, -0.3085,  0.2963,  0.0734, -0.0123,  0.1854, -0.0419, -0.4036,\n",
      "         -0.1026,  0.2818, -0.4908,  0.2115,  0.3683,  0.3095,  0.1572,  0.1417,\n",
      "         -0.4228, -0.4791,  0.3831, -0.3412, -0.2978, -0.1592,  0.3195,  0.3858],\n",
      "        [ 0.1613,  0.0348, -0.2421,  0.0833,  0.4053, -0.2057,  0.1263, -0.1411,\n",
      "         -0.1383, -0.1747,  0.2693,  0.1726, -0.3674, -0.2290,  0.0477, -0.3855,\n",
      "         -0.3565,  0.1225,  0.2639,  0.1409,  0.4857,  0.3976, -0.4630, -0.4086,\n",
      "         -0.0102,  0.2223, -0.0178,  0.2438, -0.1798, -0.4659, -0.4809,  0.4019],\n",
      "        [ 0.3421, -0.2811, -0.4621,  0.3267, -0.1484, -0.3590, -0.4294,  0.3745,\n",
      "          0.2690, -0.2680,  0.4643, -0.4175,  0.3907, -0.2853,  0.2783, -0.0148,\n",
      "         -0.0779, -0.2489,  0.4996,  0.3118,  0.4258, -0.0356,  0.2671, -0.2102,\n",
      "         -0.4692, -0.3033, -0.0888,  0.1352, -0.1219,  0.2907,  0.1419,  0.2986],\n",
      "        [-0.4156,  0.1790, -0.1572,  0.3132,  0.1497, -0.3757, -0.3237,  0.0733,\n",
      "         -0.1786, -0.0769, -0.1552, -0.2539, -0.3796, -0.3270,  0.0506, -0.2052,\n",
      "         -0.0535,  0.0741,  0.4252, -0.2110,  0.1130,  0.4561, -0.0444,  0.3839,\n",
      "         -0.2257, -0.2363, -0.4075, -0.4276, -0.0408, -0.4145,  0.4590, -0.1103]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wq spliced: torch.Size([2, 64])\n",
      "tensor([[ 0.3166,  0.2260, -0.4854,  0.4216,  0.4835, -0.1846,  0.0807, -0.1744,\n",
      "          0.4063,  0.4373, -0.1540, -0.0019,  0.4450, -0.3896,  0.1868,  0.4172,\n",
      "          0.4226, -0.4676, -0.2937, -0.3914, -0.4468,  0.3218,  0.3933,  0.0594,\n",
      "         -0.4070,  0.4076, -0.3740, -0.3442, -0.0678,  0.3974, -0.3644,  0.3838,\n",
      "         -0.0812, -0.0714,  0.1437, -0.4234, -0.3979,  0.0130,  0.0992,  0.0662,\n",
      "          0.2098,  0.0780, -0.1753,  0.4622, -0.2527, -0.4783,  0.1232, -0.2455,\n",
      "         -0.2440,  0.2213, -0.4468, -0.0947,  0.2795, -0.4053,  0.2582,  0.4484,\n",
      "          0.3389,  0.1383, -0.2250,  0.0987,  0.2842, -0.2754, -0.0290,  0.3851],\n",
      "        [ 0.2712, -0.0111, -0.2000,  0.3644, -0.1400, -0.3025,  0.2517, -0.1914,\n",
      "         -0.4223, -0.2870,  0.0443,  0.0628,  0.1732, -0.0840,  0.0290,  0.0716,\n",
      "          0.0762, -0.4723,  0.1463,  0.3216,  0.4359, -0.2931,  0.2659, -0.0665,\n",
      "          0.2418,  0.4116,  0.4992,  0.0937, -0.4742, -0.3004, -0.3614, -0.4799,\n",
      "          0.4995, -0.0357,  0.1942,  0.4661,  0.4749,  0.0713, -0.1650,  0.2679,\n",
      "          0.3363,  0.4477,  0.4767, -0.4782, -0.4408, -0.4067,  0.2454, -0.2837,\n",
      "          0.3020, -0.0322, -0.3514, -0.4083,  0.3683,  0.0763, -0.4514, -0.0704,\n",
      "         -0.0385, -0.4086, -0.2372, -0.2678,  0.2625,  0.4116, -0.1213,  0.4803]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Wk spliced: torch.Size([2, 16])\n",
      "tensor([[ 0.2187,  0.1613,  0.4213,  0.4624, -0.0269, -0.0669, -0.3027,  0.4165,\n",
      "          0.3943,  0.0063,  0.3820, -0.2310,  0.1667,  0.0193, -0.1563,  0.1311],\n",
      "        [-0.3423, -0.2625,  0.2067,  0.2535,  0.4964, -0.3356, -0.2592, -0.1564,\n",
      "          0.2403,  0.2815,  0.0725,  0.0513,  0.4597,  0.0254, -0.3282, -0.0904]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Wv spliced: torch.Size([2, 16])\n",
      "tensor([[-0.0779, -0.2489,  0.4996,  0.3118,  0.4258, -0.0356,  0.2671, -0.2102,\n",
      "         -0.4692, -0.3033, -0.0888,  0.1352, -0.1219,  0.2907,  0.1419,  0.2986],\n",
      "        [-0.0535,  0.0741,  0.4252, -0.2110,  0.1130,  0.4561, -0.0444,  0.3839,\n",
      "         -0.2257, -0.2363, -0.4075, -0.4276, -0.0408, -0.4145,  0.4590, -0.1103]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Wqkv_spliced: torch.Size([2, 96])\n",
      "tensor([[ 0.3166,  0.2260, -0.4854,  0.4216,  0.4835, -0.1846,  0.0807, -0.1744,\n",
      "          0.4063,  0.4373, -0.1540, -0.0019,  0.4450, -0.3896,  0.1868,  0.4172,\n",
      "          0.4226, -0.4676, -0.2937, -0.3914, -0.4468,  0.3218,  0.3933,  0.0594,\n",
      "         -0.4070,  0.4076, -0.3740, -0.3442, -0.0678,  0.3974, -0.3644,  0.3838,\n",
      "         -0.0812, -0.0714,  0.1437, -0.4234, -0.3979,  0.0130,  0.0992,  0.0662,\n",
      "          0.2098,  0.0780, -0.1753,  0.4622, -0.2527, -0.4783,  0.1232, -0.2455,\n",
      "         -0.2440,  0.2213, -0.4468, -0.0947,  0.2795, -0.4053,  0.2582,  0.4484,\n",
      "          0.3389,  0.1383, -0.2250,  0.0987,  0.2842, -0.2754, -0.0290,  0.3851,\n",
      "          0.2187,  0.1613,  0.4213,  0.4624, -0.0269, -0.0669, -0.3027,  0.4165,\n",
      "          0.3943,  0.0063,  0.3820, -0.2310,  0.1667,  0.0193, -0.1563,  0.1311,\n",
      "         -0.0779, -0.2489,  0.4996,  0.3118,  0.4258, -0.0356,  0.2671, -0.2102,\n",
      "         -0.4692, -0.3033, -0.0888,  0.1352, -0.1219,  0.2907,  0.1419,  0.2986],\n",
      "        [ 0.2712, -0.0111, -0.2000,  0.3644, -0.1400, -0.3025,  0.2517, -0.1914,\n",
      "         -0.4223, -0.2870,  0.0443,  0.0628,  0.1732, -0.0840,  0.0290,  0.0716,\n",
      "          0.0762, -0.4723,  0.1463,  0.3216,  0.4359, -0.2931,  0.2659, -0.0665,\n",
      "          0.2418,  0.4116,  0.4992,  0.0937, -0.4742, -0.3004, -0.3614, -0.4799,\n",
      "          0.4995, -0.0357,  0.1942,  0.4661,  0.4749,  0.0713, -0.1650,  0.2679,\n",
      "          0.3363,  0.4477,  0.4767, -0.4782, -0.4408, -0.4067,  0.2454, -0.2837,\n",
      "          0.3020, -0.0322, -0.3514, -0.4083,  0.3683,  0.0763, -0.4514, -0.0704,\n",
      "         -0.0385, -0.4086, -0.2372, -0.2678,  0.2625,  0.4116, -0.1213,  0.4803,\n",
      "         -0.3423, -0.2625,  0.2067,  0.2535,  0.4964, -0.3356, -0.2592, -0.1564,\n",
      "          0.2403,  0.2815,  0.0725,  0.0513,  0.4597,  0.0254, -0.3282, -0.0904,\n",
      "         -0.0535,  0.0741,  0.4252, -0.2110,  0.1130,  0.4561, -0.0444,  0.3839,\n",
      "         -0.2257, -0.2363, -0.4075, -0.4276, -0.0408, -0.4145,  0.4590, -0.1103]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "xqkv: torch.Size([1, 3, 96])\n",
      "tensor([[[ 0.5172,  0.3095, -0.7300,  0.6897,  0.6283, -0.3436,  0.1851,\n",
      "          -0.2972,  0.4391,  0.5214, -0.2002,  0.0157,  0.6664, -0.5636,\n",
      "           0.2669,  0.5982,  0.6070, -0.7848, -0.3638, -0.4478, -0.4912,\n",
      "           0.3598,  0.6218,  0.0628, -0.4926,  0.6841, -0.3720, -0.4490,\n",
      "          -0.2321,  0.4624, -0.6096,  0.3912,  0.0334, -0.1092,  0.2555,\n",
      "          -0.4499, -0.4121,  0.0388,  0.0892,  0.1697,  0.3884,  0.2385,\n",
      "          -0.1036,  0.5001, -0.4783, -0.7806,  0.2421, -0.4225, -0.2496,\n",
      "           0.2969, -0.7208, -0.2501,  0.4942, -0.5387,  0.2257,  0.5999,\n",
      "           0.4577,  0.0722, -0.3805,  0.0584,  0.4699, -0.2610, -0.0755,\n",
      "           0.6730,  0.2028,  0.1466,  0.6433,  0.7138,  0.1076, -0.1904,\n",
      "          -0.4945,  0.5308,  0.6157,  0.0908,  0.5498, -0.3047,  0.3648,\n",
      "           0.0341, -0.3120,  0.1550, -0.1234, -0.3228,  0.8154,  0.3700,\n",
      "           0.6222,  0.0837,  0.3566, -0.1789, -0.7151, -0.4886, -0.2418,\n",
      "           0.0624, -0.1806,  0.2813,  0.3303,  0.3811],\n",
      "         [-0.2512, -0.2980,  0.5102, -0.3326, -0.7049,  0.0627,  0.0418,\n",
      "           0.1141, -0.7689, -0.7304,  0.2243,  0.0388, -0.4737,  0.4538,\n",
      "          -0.2241, -0.4967, -0.5010,  0.3294,  0.4637,  0.6913,  0.8291,\n",
      "          -0.5850, -0.3532, -0.1152,  0.6651, -0.2871,  0.7719,  0.4983,\n",
      "          -0.1875, -0.6868,  0.2606, -0.7733,  0.3942,  0.0714, -0.0728,\n",
      "           0.8163,  0.7885,  0.0246, -0.2236,  0.0699, -0.0757,  0.1589,\n",
      "           0.5025, -0.8734,  0.0705,  0.3813, -0.0167,  0.1522,  0.4899,\n",
      "          -0.3042,  0.3727, -0.1145, -0.1471,  0.5671, -0.5948, -0.6192,\n",
      "          -0.4595, -0.4153,  0.1527, -0.2826, -0.2145,  0.5938, -0.0328,\n",
      "          -0.2183, -0.4805, -0.3602, -0.4237, -0.4495,  0.3224, -0.1083,\n",
      "           0.2402, -0.6280, -0.3693,  0.1551, -0.4508,  0.3277,  0.0514,\n",
      "          -0.0102,  0.0114, -0.2215,  0.0695,  0.3640, -0.3981, -0.5245,\n",
      "          -0.4838,  0.3104, -0.3702,  0.4936,  0.4744,  0.2543, -0.1216,\n",
      "          -0.4223,  0.1336, -0.6152,  0.0830, -0.4492],\n",
      "         [-0.3123, -0.3096,  0.5702, -0.4145, -0.7117,  0.1209, -0.0023,\n",
      "           0.1543, -0.7271, -0.7115,  0.2266,  0.0291, -0.5271,  0.4900,\n",
      "          -0.2397, -0.5326, -0.5379,  0.4308,  0.4583,  0.6644,  0.7875,\n",
      "          -0.5584, -0.4181, -0.1083,  0.6515, -0.3755,  0.7162,  0.5041,\n",
      "          -0.1095, -0.6635,  0.3386, -0.7212,  0.3211,  0.0812, -0.1116,\n",
      "           0.7687,  0.7380,  0.0127, -0.2038,  0.0242, -0.1406,  0.0844,\n",
      "           0.4385, -0.8262,  0.1543,  0.4731, -0.0623,  0.2110,  0.4572,\n",
      "          -0.3123,  0.4541, -0.0452, -0.2211,  0.5792, -0.5396, -0.6348,\n",
      "          -0.4736, -0.3597,  0.2031, -0.2466, -0.2723,  0.5459, -0.0122,\n",
      "          -0.3161, -0.4401, -0.3288, -0.4810, -0.5165,  0.2466, -0.0519,\n",
      "           0.2986, -0.6283, -0.4302,  0.1108, -0.4848,  0.3334, -0.0302,\n",
      "          -0.0153,  0.0718, -0.2151,  0.0824,  0.3672, -0.4940, -0.5101,\n",
      "          -0.5267,  0.2413, -0.3791,  0.4462,  0.5375,  0.3091, -0.0528,\n",
      "          -0.3636,  0.1472, -0.5678,  0.0029, -0.4497]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "xq: torch.Size([1, 3, 64])\n",
      "tensor([[[ 0.5172,  0.3095, -0.7300,  0.6897,  0.6283, -0.3436,  0.1851,\n",
      "          -0.2972,  0.4391,  0.5214, -0.2002,  0.0157,  0.6664, -0.5636,\n",
      "           0.2669,  0.5982,  0.6070, -0.7848, -0.3638, -0.4478, -0.4912,\n",
      "           0.3598,  0.6218,  0.0628, -0.4926,  0.6841, -0.3720, -0.4490,\n",
      "          -0.2321,  0.4624, -0.6096,  0.3912,  0.0334, -0.1092,  0.2555,\n",
      "          -0.4499, -0.4121,  0.0388,  0.0892,  0.1697,  0.3884,  0.2385,\n",
      "          -0.1036,  0.5001, -0.4783, -0.7806,  0.2421, -0.4225, -0.2496,\n",
      "           0.2969, -0.7208, -0.2501,  0.4942, -0.5387,  0.2257,  0.5999,\n",
      "           0.4577,  0.0722, -0.3805,  0.0584,  0.4699, -0.2610, -0.0755,\n",
      "           0.6730],\n",
      "         [-0.2512, -0.2980,  0.5102, -0.3326, -0.7049,  0.0627,  0.0418,\n",
      "           0.1141, -0.7689, -0.7304,  0.2243,  0.0388, -0.4737,  0.4538,\n",
      "          -0.2241, -0.4967, -0.5010,  0.3294,  0.4637,  0.6913,  0.8291,\n",
      "          -0.5850, -0.3532, -0.1152,  0.6651, -0.2871,  0.7719,  0.4983,\n",
      "          -0.1875, -0.6868,  0.2606, -0.7733,  0.3942,  0.0714, -0.0728,\n",
      "           0.8163,  0.7885,  0.0246, -0.2236,  0.0699, -0.0757,  0.1589,\n",
      "           0.5025, -0.8734,  0.0705,  0.3813, -0.0167,  0.1522,  0.4899,\n",
      "          -0.3042,  0.3727, -0.1145, -0.1471,  0.5671, -0.5948, -0.6192,\n",
      "          -0.4595, -0.4153,  0.1527, -0.2826, -0.2145,  0.5938, -0.0328,\n",
      "          -0.2183],\n",
      "         [-0.3123, -0.3096,  0.5702, -0.4145, -0.7117,  0.1209, -0.0023,\n",
      "           0.1543, -0.7271, -0.7115,  0.2266,  0.0291, -0.5271,  0.4900,\n",
      "          -0.2397, -0.5326, -0.5379,  0.4308,  0.4583,  0.6644,  0.7875,\n",
      "          -0.5584, -0.4181, -0.1083,  0.6515, -0.3755,  0.7162,  0.5041,\n",
      "          -0.1095, -0.6635,  0.3386, -0.7212,  0.3211,  0.0812, -0.1116,\n",
      "           0.7687,  0.7380,  0.0127, -0.2038,  0.0242, -0.1406,  0.0844,\n",
      "           0.4385, -0.8262,  0.1543,  0.4731, -0.0623,  0.2110,  0.4572,\n",
      "          -0.3123,  0.4541, -0.0452, -0.2211,  0.5792, -0.5396, -0.6348,\n",
      "          -0.4736, -0.3597,  0.2031, -0.2466, -0.2723,  0.5459, -0.0122,\n",
      "          -0.3161]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xk: torch.Size([1, 3, 16])\n",
      "tensor([[[ 0.2028,  0.1466,  0.6433,  0.7138,  0.1076, -0.1904, -0.4945,\n",
      "           0.5308,  0.6157,  0.0908,  0.5498, -0.3047,  0.3648,  0.0341,\n",
      "          -0.3120,  0.1550],\n",
      "         [-0.4805, -0.3602, -0.4237, -0.4495,  0.3224, -0.1083,  0.2402,\n",
      "          -0.6280, -0.3693,  0.1551, -0.4508,  0.3277,  0.0514, -0.0102,\n",
      "           0.0114, -0.2215],\n",
      "         [-0.4401, -0.3288, -0.4810, -0.5165,  0.2466, -0.0519,  0.2986,\n",
      "          -0.6283, -0.4302,  0.1108, -0.4848,  0.3334, -0.0302, -0.0153,\n",
      "           0.0718, -0.2151]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xv: torch.Size([1, 3, 16])\n",
      "tensor([[[-0.1234, -0.3228,  0.8154,  0.3700,  0.6222,  0.0837,  0.3566,\n",
      "          -0.1789, -0.7151, -0.4886, -0.2418,  0.0624, -0.1806,  0.2813,\n",
      "           0.3303,  0.3811],\n",
      "         [ 0.0695,  0.3640, -0.3981, -0.5245, -0.4838,  0.3104, -0.3702,\n",
      "           0.4936,  0.4744,  0.2543, -0.1216, -0.4223,  0.1336, -0.6152,\n",
      "           0.0830, -0.4492],\n",
      "         [ 0.0824,  0.3672, -0.4940, -0.5101, -0.5267,  0.2413, -0.3791,\n",
      "           0.4462,  0.5375,  0.3091, -0.0528, -0.3636,  0.1472, -0.5678,\n",
      "           0.0029, -0.4497]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xq reshaped: torch.Size([1, 3, 4, 16])\n",
      "tensor([[[[ 0.5172,  0.3095, -0.7300,  0.6897,  0.6283, -0.3436,  0.1851,\n",
      "           -0.2972,  0.4391,  0.5214, -0.2002,  0.0157,  0.6664, -0.5636,\n",
      "            0.2669,  0.5982],\n",
      "          [ 0.6070, -0.7848, -0.3638, -0.4478, -0.4912,  0.3598,  0.6218,\n",
      "            0.0628, -0.4926,  0.6841, -0.3720, -0.4490, -0.2321,  0.4624,\n",
      "           -0.6096,  0.3912],\n",
      "          [ 0.0334, -0.1092,  0.2555, -0.4499, -0.4121,  0.0388,  0.0892,\n",
      "            0.1697,  0.3884,  0.2385, -0.1036,  0.5001, -0.4783, -0.7806,\n",
      "            0.2421, -0.4225],\n",
      "          [-0.2496,  0.2969, -0.7208, -0.2501,  0.4942, -0.5387,  0.2257,\n",
      "            0.5999,  0.4577,  0.0722, -0.3805,  0.0584,  0.4699, -0.2610,\n",
      "           -0.0755,  0.6730]],\n",
      "\n",
      "         [[-0.2512, -0.2980,  0.5102, -0.3326, -0.7049,  0.0627,  0.0418,\n",
      "            0.1141, -0.7689, -0.7304,  0.2243,  0.0388, -0.4737,  0.4538,\n",
      "           -0.2241, -0.4967],\n",
      "          [-0.5010,  0.3294,  0.4637,  0.6913,  0.8291, -0.5850, -0.3532,\n",
      "           -0.1152,  0.6651, -0.2871,  0.7719,  0.4983, -0.1875, -0.6868,\n",
      "            0.2606, -0.7733],\n",
      "          [ 0.3942,  0.0714, -0.0728,  0.8163,  0.7885,  0.0246, -0.2236,\n",
      "            0.0699, -0.0757,  0.1589,  0.5025, -0.8734,  0.0705,  0.3813,\n",
      "           -0.0167,  0.1522],\n",
      "          [ 0.4899, -0.3042,  0.3727, -0.1145, -0.1471,  0.5671, -0.5948,\n",
      "           -0.6192, -0.4595, -0.4153,  0.1527, -0.2826, -0.2145,  0.5938,\n",
      "           -0.0328, -0.2183]],\n",
      "\n",
      "         [[-0.3123, -0.3096,  0.5702, -0.4145, -0.7117,  0.1209, -0.0023,\n",
      "            0.1543, -0.7271, -0.7115,  0.2266,  0.0291, -0.5271,  0.4900,\n",
      "           -0.2397, -0.5326],\n",
      "          [-0.5379,  0.4308,  0.4583,  0.6644,  0.7875, -0.5584, -0.4181,\n",
      "           -0.1083,  0.6515, -0.3755,  0.7162,  0.5041, -0.1095, -0.6635,\n",
      "            0.3386, -0.7212],\n",
      "          [ 0.3211,  0.0812, -0.1116,  0.7687,  0.7380,  0.0127, -0.2038,\n",
      "            0.0242, -0.1406,  0.0844,  0.4385, -0.8262,  0.1543,  0.4731,\n",
      "           -0.0623,  0.2110],\n",
      "          [ 0.4572, -0.3123,  0.4541, -0.0452, -0.2211,  0.5792, -0.5396,\n",
      "           -0.6348, -0.4736, -0.3597,  0.2031, -0.2466, -0.2723,  0.5459,\n",
      "           -0.0122, -0.3161]]]], grad_fn=<ViewBackward0>)\n",
      "xk reshaped: torch.Size([1, 3, 1, 16])\n",
      "tensor([[[[ 0.2028,  0.1466,  0.6433,  0.7138,  0.1076, -0.1904, -0.4945,\n",
      "            0.5308,  0.6157,  0.0908,  0.5498, -0.3047,  0.3648,  0.0341,\n",
      "           -0.3120,  0.1550]],\n",
      "\n",
      "         [[-0.4805, -0.3602, -0.4237, -0.4495,  0.3224, -0.1083,  0.2402,\n",
      "           -0.6280, -0.3693,  0.1551, -0.4508,  0.3277,  0.0514, -0.0102,\n",
      "            0.0114, -0.2215]],\n",
      "\n",
      "         [[-0.4401, -0.3288, -0.4810, -0.5165,  0.2466, -0.0519,  0.2986,\n",
      "           -0.6283, -0.4302,  0.1108, -0.4848,  0.3334, -0.0302, -0.0153,\n",
      "            0.0718, -0.2151]]]], grad_fn=<ViewBackward0>)\n",
      "xv reshaped: torch.Size([1, 3, 1, 16])\n",
      "tensor([[[[-0.1234, -0.3228,  0.8154,  0.3700,  0.6222,  0.0837,  0.3566,\n",
      "           -0.1789, -0.7151, -0.4886, -0.2418,  0.0624, -0.1806,  0.2813,\n",
      "            0.3303,  0.3811]],\n",
      "\n",
      "         [[ 0.0695,  0.3640, -0.3981, -0.5245, -0.4838,  0.3104, -0.3702,\n",
      "            0.4936,  0.4744,  0.2543, -0.1216, -0.4223,  0.1336, -0.6152,\n",
      "            0.0830, -0.4492]],\n",
      "\n",
      "         [[ 0.0824,  0.3672, -0.4940, -0.5101, -0.5267,  0.2413, -0.3791,\n",
      "            0.4462,  0.5375,  0.3091, -0.0528, -0.3636,  0.1472, -0.5678,\n",
      "            0.0029, -0.4497]]]], grad_fn=<ViewBackward0>)\n",
      "rotated xq: torch.Size([1, 3, 4, 16])\n",
      "tensor([[[[ 0.5172,  0.3095, -0.7300,  0.6897,  0.6283, -0.3436,  0.1851,\n",
      "           -0.2972,  0.4391,  0.5214, -0.2002,  0.0157,  0.6664, -0.5636,\n",
      "            0.2669,  0.5982],\n",
      "          [ 0.6070, -0.7848, -0.3638, -0.4478, -0.4912,  0.3598,  0.6218,\n",
      "            0.0628, -0.4926,  0.6841, -0.3720, -0.4490, -0.2321,  0.4624,\n",
      "           -0.6096,  0.3912],\n",
      "          [ 0.0334, -0.1092,  0.2555, -0.4499, -0.4121,  0.0388,  0.0892,\n",
      "            0.1697,  0.3884,  0.2385, -0.1036,  0.5001, -0.4783, -0.7806,\n",
      "            0.2421, -0.4225],\n",
      "          [-0.2496,  0.2969, -0.7208, -0.2501,  0.4942, -0.5387,  0.2257,\n",
      "            0.5999,  0.4577,  0.0722, -0.3805,  0.0584,  0.4699, -0.2610,\n",
      "           -0.0755,  0.6730]],\n",
      "\n",
      "         [[ 0.5113,  0.1373,  0.4151, -0.3342, -0.6541,  0.0371,  0.0489,\n",
      "            0.1229, -0.6268, -0.7768,  0.3719, -0.0206, -0.5417,  0.4566,\n",
      "           -0.2227, -0.4946],\n",
      "          [-0.8304,  0.4317,  0.2007,  0.5923,  0.8436, -0.5455, -0.3613,\n",
      "           -0.1014, -0.0622, -0.0673,  0.8778,  0.6127, -0.1038, -0.7186,\n",
      "            0.2493, -0.7752],\n",
      "          [ 0.2767, -0.0243, -0.2255,  0.9580,  0.7776,  0.0031, -0.2230,\n",
      "            0.0672,  0.2909,  0.1725,  0.4549, -0.7152,  0.1488,  0.3821,\n",
      "           -0.0238,  0.1534],\n",
      "          [ 0.6513, -0.0360,  0.3068, -0.0627, -0.1249,  0.5328, -0.5934,\n",
      "           -0.6152,  0.1639, -0.5135,  0.2610, -0.2984, -0.2281,  0.6247,\n",
      "           -0.0516, -0.2293]],\n",
      "\n",
      "         [[ 0.7911,  0.5083,  0.3259, -0.3987, -0.5928,  0.0651,  0.0129,\n",
      "            0.1731,  0.0186, -0.5864,  0.5198, -0.1170, -0.6580,  0.5005,\n",
      "           -0.2394, -0.5268],\n",
      "          [-0.3686,  0.5247, -0.0537,  0.4473,  0.7936, -0.4804, -0.4386,\n",
      "           -0.0826, -0.7603,  0.2266,  0.8485,  0.7039,  0.0492, -0.7220,\n",
      "            0.3115, -0.7246],\n",
      "          [-0.0058, -0.0411, -0.3493,  1.0083,  0.6927, -0.0405, -0.1994,\n",
      "            0.0166,  0.3505,  0.1097,  0.2877, -0.5068,  0.2978,  0.4716,\n",
      "           -0.0751,  0.2118],\n",
      "          [ 0.2404,  0.1898,  0.2462,  0.0435, -0.1626,  0.5143, -0.5378,\n",
      "           -0.6231,  0.6128, -0.4369,  0.4322, -0.2469, -0.3108,  0.6075,\n",
      "           -0.0463, -0.3385]]]], grad_fn=<TransposeBackward0>)\n",
      "rotated xk: torch.Size([1, 3, 1, 16])\n",
      "tensor([[[[ 0.2028,  0.1466,  0.6433,  0.7138,  0.1076, -0.1904, -0.4945,\n",
      "            0.5308,  0.6157,  0.0908,  0.5498, -0.3047,  0.3648,  0.0341,\n",
      "           -0.3120,  0.1550]],\n",
      "\n",
      "         [[ 0.0511, -0.3874, -0.2625, -0.5004,  0.3157, -0.1075,  0.2398,\n",
      "           -0.6240, -0.6039, -0.0608, -0.5602,  0.2430,  0.0833, -0.0162,\n",
      "            0.0189, -0.2326]],\n",
      "\n",
      "         [[ 0.5743, -0.2418, -0.1014, -0.6002,  0.2477, -0.0499,  0.2935,\n",
      "           -0.6202, -0.2212, -0.2488, -0.6753,  0.1327,  0.0193, -0.0210,\n",
      "            0.0906, -0.2374]]]], grad_fn=<TransposeBackward0>)\n",
      "repeat_interleaved xk: torch.Size([1, 3, 4, 16])\n",
      "tensor([[[[ 0.2028,  0.1466,  0.6433,  0.7138,  0.1076, -0.1904, -0.4945,\n",
      "            0.5308,  0.6157,  0.0908,  0.5498, -0.3047,  0.3648,  0.0341,\n",
      "           -0.3120,  0.1550],\n",
      "          [ 0.2028,  0.1466,  0.6433,  0.7138,  0.1076, -0.1904, -0.4945,\n",
      "            0.5308,  0.6157,  0.0908,  0.5498, -0.3047,  0.3648,  0.0341,\n",
      "           -0.3120,  0.1550],\n",
      "          [ 0.2028,  0.1466,  0.6433,  0.7138,  0.1076, -0.1904, -0.4945,\n",
      "            0.5308,  0.6157,  0.0908,  0.5498, -0.3047,  0.3648,  0.0341,\n",
      "           -0.3120,  0.1550],\n",
      "          [ 0.2028,  0.1466,  0.6433,  0.7138,  0.1076, -0.1904, -0.4945,\n",
      "            0.5308,  0.6157,  0.0908,  0.5498, -0.3047,  0.3648,  0.0341,\n",
      "           -0.3120,  0.1550]],\n",
      "\n",
      "         [[ 0.0511, -0.3874, -0.2625, -0.5004,  0.3157, -0.1075,  0.2398,\n",
      "           -0.6240, -0.6039, -0.0608, -0.5602,  0.2430,  0.0833, -0.0162,\n",
      "            0.0189, -0.2326],\n",
      "          [ 0.0511, -0.3874, -0.2625, -0.5004,  0.3157, -0.1075,  0.2398,\n",
      "           -0.6240, -0.6039, -0.0608, -0.5602,  0.2430,  0.0833, -0.0162,\n",
      "            0.0189, -0.2326],\n",
      "          [ 0.0511, -0.3874, -0.2625, -0.5004,  0.3157, -0.1075,  0.2398,\n",
      "           -0.6240, -0.6039, -0.0608, -0.5602,  0.2430,  0.0833, -0.0162,\n",
      "            0.0189, -0.2326],\n",
      "          [ 0.0511, -0.3874, -0.2625, -0.5004,  0.3157, -0.1075,  0.2398,\n",
      "           -0.6240, -0.6039, -0.0608, -0.5602,  0.2430,  0.0833, -0.0162,\n",
      "            0.0189, -0.2326]],\n",
      "\n",
      "         [[ 0.5743, -0.2418, -0.1014, -0.6002,  0.2477, -0.0499,  0.2935,\n",
      "           -0.6202, -0.2212, -0.2488, -0.6753,  0.1327,  0.0193, -0.0210,\n",
      "            0.0906, -0.2374],\n",
      "          [ 0.5743, -0.2418, -0.1014, -0.6002,  0.2477, -0.0499,  0.2935,\n",
      "           -0.6202, -0.2212, -0.2488, -0.6753,  0.1327,  0.0193, -0.0210,\n",
      "            0.0906, -0.2374],\n",
      "          [ 0.5743, -0.2418, -0.1014, -0.6002,  0.2477, -0.0499,  0.2935,\n",
      "           -0.6202, -0.2212, -0.2488, -0.6753,  0.1327,  0.0193, -0.0210,\n",
      "            0.0906, -0.2374],\n",
      "          [ 0.5743, -0.2418, -0.1014, -0.6002,  0.2477, -0.0499,  0.2935,\n",
      "           -0.6202, -0.2212, -0.2488, -0.6753,  0.1327,  0.0193, -0.0210,\n",
      "            0.0906, -0.2374]]]], grad_fn=<ViewBackward0>)\n",
      "repeat_interleaved xv: torch.Size([1, 3, 4, 16])\n",
      "tensor([[[[-0.1234, -0.3228,  0.8154,  0.3700,  0.6222,  0.0837,  0.3566,\n",
      "           -0.1789, -0.7151, -0.4886, -0.2418,  0.0624, -0.1806,  0.2813,\n",
      "            0.3303,  0.3811],\n",
      "          [-0.1234, -0.3228,  0.8154,  0.3700,  0.6222,  0.0837,  0.3566,\n",
      "           -0.1789, -0.7151, -0.4886, -0.2418,  0.0624, -0.1806,  0.2813,\n",
      "            0.3303,  0.3811],\n",
      "          [-0.1234, -0.3228,  0.8154,  0.3700,  0.6222,  0.0837,  0.3566,\n",
      "           -0.1789, -0.7151, -0.4886, -0.2418,  0.0624, -0.1806,  0.2813,\n",
      "            0.3303,  0.3811],\n",
      "          [-0.1234, -0.3228,  0.8154,  0.3700,  0.6222,  0.0837,  0.3566,\n",
      "           -0.1789, -0.7151, -0.4886, -0.2418,  0.0624, -0.1806,  0.2813,\n",
      "            0.3303,  0.3811]],\n",
      "\n",
      "         [[ 0.0695,  0.3640, -0.3981, -0.5245, -0.4838,  0.3104, -0.3702,\n",
      "            0.4936,  0.4744,  0.2543, -0.1216, -0.4223,  0.1336, -0.6152,\n",
      "            0.0830, -0.4492],\n",
      "          [ 0.0695,  0.3640, -0.3981, -0.5245, -0.4838,  0.3104, -0.3702,\n",
      "            0.4936,  0.4744,  0.2543, -0.1216, -0.4223,  0.1336, -0.6152,\n",
      "            0.0830, -0.4492],\n",
      "          [ 0.0695,  0.3640, -0.3981, -0.5245, -0.4838,  0.3104, -0.3702,\n",
      "            0.4936,  0.4744,  0.2543, -0.1216, -0.4223,  0.1336, -0.6152,\n",
      "            0.0830, -0.4492],\n",
      "          [ 0.0695,  0.3640, -0.3981, -0.5245, -0.4838,  0.3104, -0.3702,\n",
      "            0.4936,  0.4744,  0.2543, -0.1216, -0.4223,  0.1336, -0.6152,\n",
      "            0.0830, -0.4492]],\n",
      "\n",
      "         [[ 0.0824,  0.3672, -0.4940, -0.5101, -0.5267,  0.2413, -0.3791,\n",
      "            0.4462,  0.5375,  0.3091, -0.0528, -0.3636,  0.1472, -0.5678,\n",
      "            0.0029, -0.4497],\n",
      "          [ 0.0824,  0.3672, -0.4940, -0.5101, -0.5267,  0.2413, -0.3791,\n",
      "            0.4462,  0.5375,  0.3091, -0.0528, -0.3636,  0.1472, -0.5678,\n",
      "            0.0029, -0.4497],\n",
      "          [ 0.0824,  0.3672, -0.4940, -0.5101, -0.5267,  0.2413, -0.3791,\n",
      "            0.4462,  0.5375,  0.3091, -0.0528, -0.3636,  0.1472, -0.5678,\n",
      "            0.0029, -0.4497],\n",
      "          [ 0.0824,  0.3672, -0.4940, -0.5101, -0.5267,  0.2413, -0.3791,\n",
      "            0.4462,  0.5375,  0.3091, -0.0528, -0.3636,  0.1472, -0.5678,\n",
      "            0.0029, -0.4497]]]], grad_fn=<ViewBackward0>)\n",
      "transposed xq: torch.Size([1, 4, 3, 16])\n",
      "tensor([[[[ 0.5172,  0.3095, -0.7300,  0.6897,  0.6283, -0.3436,  0.1851,\n",
      "           -0.2972,  0.4391,  0.5214, -0.2002,  0.0157,  0.6664, -0.5636,\n",
      "            0.2669,  0.5982],\n",
      "          [ 0.5113,  0.1373,  0.4151, -0.3342, -0.6541,  0.0371,  0.0489,\n",
      "            0.1229, -0.6268, -0.7768,  0.3719, -0.0206, -0.5417,  0.4566,\n",
      "           -0.2227, -0.4946],\n",
      "          [ 0.7911,  0.5083,  0.3259, -0.3987, -0.5928,  0.0651,  0.0129,\n",
      "            0.1731,  0.0186, -0.5864,  0.5198, -0.1170, -0.6580,  0.5005,\n",
      "           -0.2394, -0.5268]],\n",
      "\n",
      "         [[ 0.6070, -0.7848, -0.3638, -0.4478, -0.4912,  0.3598,  0.6218,\n",
      "            0.0628, -0.4926,  0.6841, -0.3720, -0.4490, -0.2321,  0.4624,\n",
      "           -0.6096,  0.3912],\n",
      "          [-0.8304,  0.4317,  0.2007,  0.5923,  0.8436, -0.5455, -0.3613,\n",
      "           -0.1014, -0.0622, -0.0673,  0.8778,  0.6127, -0.1038, -0.7186,\n",
      "            0.2493, -0.7752],\n",
      "          [-0.3686,  0.5247, -0.0537,  0.4473,  0.7936, -0.4804, -0.4386,\n",
      "           -0.0826, -0.7603,  0.2266,  0.8485,  0.7039,  0.0492, -0.7220,\n",
      "            0.3115, -0.7246]],\n",
      "\n",
      "         [[ 0.0334, -0.1092,  0.2555, -0.4499, -0.4121,  0.0388,  0.0892,\n",
      "            0.1697,  0.3884,  0.2385, -0.1036,  0.5001, -0.4783, -0.7806,\n",
      "            0.2421, -0.4225],\n",
      "          [ 0.2767, -0.0243, -0.2255,  0.9580,  0.7776,  0.0031, -0.2230,\n",
      "            0.0672,  0.2909,  0.1725,  0.4549, -0.7152,  0.1488,  0.3821,\n",
      "           -0.0238,  0.1534],\n",
      "          [-0.0058, -0.0411, -0.3493,  1.0083,  0.6927, -0.0405, -0.1994,\n",
      "            0.0166,  0.3505,  0.1097,  0.2877, -0.5068,  0.2978,  0.4716,\n",
      "           -0.0751,  0.2118]],\n",
      "\n",
      "         [[-0.2496,  0.2969, -0.7208, -0.2501,  0.4942, -0.5387,  0.2257,\n",
      "            0.5999,  0.4577,  0.0722, -0.3805,  0.0584,  0.4699, -0.2610,\n",
      "           -0.0755,  0.6730],\n",
      "          [ 0.6513, -0.0360,  0.3068, -0.0627, -0.1249,  0.5328, -0.5934,\n",
      "           -0.6152,  0.1639, -0.5135,  0.2610, -0.2984, -0.2281,  0.6247,\n",
      "           -0.0516, -0.2293],\n",
      "          [ 0.2404,  0.1898,  0.2462,  0.0435, -0.1626,  0.5143, -0.5378,\n",
      "           -0.6231,  0.6128, -0.4369,  0.4322, -0.2469, -0.3108,  0.6075,\n",
      "           -0.0463, -0.3385]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xk: torch.Size([1, 4, 3, 16])\n",
      "tensor([[[[ 0.2028,  0.1466,  0.6433,  0.7138,  0.1076, -0.1904, -0.4945,\n",
      "            0.5308,  0.6157,  0.0908,  0.5498, -0.3047,  0.3648,  0.0341,\n",
      "           -0.3120,  0.1550],\n",
      "          [ 0.0511, -0.3874, -0.2625, -0.5004,  0.3157, -0.1075,  0.2398,\n",
      "           -0.6240, -0.6039, -0.0608, -0.5602,  0.2430,  0.0833, -0.0162,\n",
      "            0.0189, -0.2326],\n",
      "          [ 0.5743, -0.2418, -0.1014, -0.6002,  0.2477, -0.0499,  0.2935,\n",
      "           -0.6202, -0.2212, -0.2488, -0.6753,  0.1327,  0.0193, -0.0210,\n",
      "            0.0906, -0.2374]],\n",
      "\n",
      "         [[ 0.2028,  0.1466,  0.6433,  0.7138,  0.1076, -0.1904, -0.4945,\n",
      "            0.5308,  0.6157,  0.0908,  0.5498, -0.3047,  0.3648,  0.0341,\n",
      "           -0.3120,  0.1550],\n",
      "          [ 0.0511, -0.3874, -0.2625, -0.5004,  0.3157, -0.1075,  0.2398,\n",
      "           -0.6240, -0.6039, -0.0608, -0.5602,  0.2430,  0.0833, -0.0162,\n",
      "            0.0189, -0.2326],\n",
      "          [ 0.5743, -0.2418, -0.1014, -0.6002,  0.2477, -0.0499,  0.2935,\n",
      "           -0.6202, -0.2212, -0.2488, -0.6753,  0.1327,  0.0193, -0.0210,\n",
      "            0.0906, -0.2374]],\n",
      "\n",
      "         [[ 0.2028,  0.1466,  0.6433,  0.7138,  0.1076, -0.1904, -0.4945,\n",
      "            0.5308,  0.6157,  0.0908,  0.5498, -0.3047,  0.3648,  0.0341,\n",
      "           -0.3120,  0.1550],\n",
      "          [ 0.0511, -0.3874, -0.2625, -0.5004,  0.3157, -0.1075,  0.2398,\n",
      "           -0.6240, -0.6039, -0.0608, -0.5602,  0.2430,  0.0833, -0.0162,\n",
      "            0.0189, -0.2326],\n",
      "          [ 0.5743, -0.2418, -0.1014, -0.6002,  0.2477, -0.0499,  0.2935,\n",
      "           -0.6202, -0.2212, -0.2488, -0.6753,  0.1327,  0.0193, -0.0210,\n",
      "            0.0906, -0.2374]],\n",
      "\n",
      "         [[ 0.2028,  0.1466,  0.6433,  0.7138,  0.1076, -0.1904, -0.4945,\n",
      "            0.5308,  0.6157,  0.0908,  0.5498, -0.3047,  0.3648,  0.0341,\n",
      "           -0.3120,  0.1550],\n",
      "          [ 0.0511, -0.3874, -0.2625, -0.5004,  0.3157, -0.1075,  0.2398,\n",
      "           -0.6240, -0.6039, -0.0608, -0.5602,  0.2430,  0.0833, -0.0162,\n",
      "            0.0189, -0.2326],\n",
      "          [ 0.5743, -0.2418, -0.1014, -0.6002,  0.2477, -0.0499,  0.2935,\n",
      "           -0.6202, -0.2212, -0.2488, -0.6753,  0.1327,  0.0193, -0.0210,\n",
      "            0.0906, -0.2374]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xv: torch.Size([1, 4, 3, 16])\n",
      "tensor([[[[-0.1234, -0.3228,  0.8154,  0.3700,  0.6222,  0.0837,  0.3566,\n",
      "           -0.1789, -0.7151, -0.4886, -0.2418,  0.0624, -0.1806,  0.2813,\n",
      "            0.3303,  0.3811],\n",
      "          [ 0.0695,  0.3640, -0.3981, -0.5245, -0.4838,  0.3104, -0.3702,\n",
      "            0.4936,  0.4744,  0.2543, -0.1216, -0.4223,  0.1336, -0.6152,\n",
      "            0.0830, -0.4492],\n",
      "          [ 0.0824,  0.3672, -0.4940, -0.5101, -0.5267,  0.2413, -0.3791,\n",
      "            0.4462,  0.5375,  0.3091, -0.0528, -0.3636,  0.1472, -0.5678,\n",
      "            0.0029, -0.4497]],\n",
      "\n",
      "         [[-0.1234, -0.3228,  0.8154,  0.3700,  0.6222,  0.0837,  0.3566,\n",
      "           -0.1789, -0.7151, -0.4886, -0.2418,  0.0624, -0.1806,  0.2813,\n",
      "            0.3303,  0.3811],\n",
      "          [ 0.0695,  0.3640, -0.3981, -0.5245, -0.4838,  0.3104, -0.3702,\n",
      "            0.4936,  0.4744,  0.2543, -0.1216, -0.4223,  0.1336, -0.6152,\n",
      "            0.0830, -0.4492],\n",
      "          [ 0.0824,  0.3672, -0.4940, -0.5101, -0.5267,  0.2413, -0.3791,\n",
      "            0.4462,  0.5375,  0.3091, -0.0528, -0.3636,  0.1472, -0.5678,\n",
      "            0.0029, -0.4497]],\n",
      "\n",
      "         [[-0.1234, -0.3228,  0.8154,  0.3700,  0.6222,  0.0837,  0.3566,\n",
      "           -0.1789, -0.7151, -0.4886, -0.2418,  0.0624, -0.1806,  0.2813,\n",
      "            0.3303,  0.3811],\n",
      "          [ 0.0695,  0.3640, -0.3981, -0.5245, -0.4838,  0.3104, -0.3702,\n",
      "            0.4936,  0.4744,  0.2543, -0.1216, -0.4223,  0.1336, -0.6152,\n",
      "            0.0830, -0.4492],\n",
      "          [ 0.0824,  0.3672, -0.4940, -0.5101, -0.5267,  0.2413, -0.3791,\n",
      "            0.4462,  0.5375,  0.3091, -0.0528, -0.3636,  0.1472, -0.5678,\n",
      "            0.0029, -0.4497]],\n",
      "\n",
      "         [[-0.1234, -0.3228,  0.8154,  0.3700,  0.6222,  0.0837,  0.3566,\n",
      "           -0.1789, -0.7151, -0.4886, -0.2418,  0.0624, -0.1806,  0.2813,\n",
      "            0.3303,  0.3811],\n",
      "          [ 0.0695,  0.3640, -0.3981, -0.5245, -0.4838,  0.3104, -0.3702,\n",
      "            0.4936,  0.4744,  0.2543, -0.1216, -0.4223,  0.1336, -0.6152,\n",
      "            0.0830, -0.4492],\n",
      "          [ 0.0824,  0.3672, -0.4940, -0.5101, -0.5267,  0.2413, -0.3791,\n",
      "            0.4462,  0.5375,  0.3091, -0.0528, -0.3636,  0.1472, -0.5678,\n",
      "            0.0029, -0.4497]]]], grad_fn=<TransposeBackward0>)\n",
      "scores: torch.Size([1, 4, 3, 3])\n",
      "tensor([[[[ 0.1232, -0.0080,  0.0278],\n",
      "          [-0.0798,  0.0066,  0.0871],\n",
      "          [ 0.0548, -0.1454,  0.0348]],\n",
      "\n",
      "         [[-0.2670,  0.1991,  0.2040],\n",
      "          [ 0.1894, -0.0926, -0.2564],\n",
      "          [ 0.0563,  0.0370, -0.1525]],\n",
      "\n",
      "         [[-0.1156, -0.0026,  0.0568],\n",
      "          [ 0.3954, -0.2229, -0.2138],\n",
      "          [ 0.3492, -0.1921, -0.2215]],\n",
      "\n",
      "         [[ 0.0162, -0.0219, -0.0388],\n",
      "          [ 0.0847, -0.0301,  0.1118],\n",
      "          [ 0.1514, -0.1495, -0.0226]]]], grad_fn=<MulBackward0>)\n",
      "mask: torch.Size([1, 1, 3, 3])\n",
      "tensor([[[[ 0.0000e+00, -2.3820e+38, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]]]])\n",
      "masked scores: torch.Size([1, 4, 3, 3])\n",
      "tensor([[[[ 1.2320e-01, -2.3820e+38, -2.3820e+38],\n",
      "          [-7.9750e-02,  6.6254e-03, -2.3820e+38],\n",
      "          [ 5.4779e-02, -1.4544e-01,  3.4846e-02]],\n",
      "\n",
      "         [[-2.6702e-01, -2.3820e+38, -2.3820e+38],\n",
      "          [ 1.8937e-01, -9.2571e-02, -2.3820e+38],\n",
      "          [ 5.6303e-02,  3.7009e-02, -1.5254e-01]],\n",
      "\n",
      "         [[-1.1561e-01, -2.3820e+38, -2.3820e+38],\n",
      "          [ 3.9540e-01, -2.2289e-01, -2.3820e+38],\n",
      "          [ 3.4920e-01, -1.9214e-01, -2.2151e-01]],\n",
      "\n",
      "         [[ 1.6245e-02, -2.3820e+38, -2.3820e+38],\n",
      "          [ 8.4705e-02, -3.0091e-02, -2.3820e+38],\n",
      "          [ 1.5139e-01, -1.4948e-01, -2.2565e-02]]]], grad_fn=<AddBackward0>)\n",
      "softmaxed scores: torch.Size([1, 4, 3, 3])\n",
      "tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [0.4784, 0.5216, 0.0000],\n",
      "          [0.3573, 0.2925, 0.3502]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.5700, 0.4300, 0.0000],\n",
      "          [0.3581, 0.3513, 0.2906]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.6498, 0.3502, 0.0000],\n",
      "          [0.4657, 0.2711, 0.2632]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.5287, 0.4713, 0.0000],\n",
      "          [0.3875, 0.2868, 0.3256]]]], grad_fn=<SoftmaxBackward0>)\n",
      "attention: torch.Size([1, 4, 3, 16])\n",
      "tensor([[[[-0.1234, -0.3228,  0.8154,  0.3700,  0.6222,  0.0837,  0.3566,\n",
      "           -0.1789, -0.7151, -0.4886, -0.2418,  0.0624, -0.1806,  0.2813,\n",
      "            0.3303,  0.3811],\n",
      "          [-0.0228,  0.0354,  0.1825, -0.0966,  0.0453,  0.2019, -0.0225,\n",
      "            0.1719, -0.0947, -0.1011, -0.1791, -0.1904, -0.0167, -0.1863,\n",
      "            0.2013, -0.0520],\n",
      "          [ 0.0051,  0.1197,  0.0019, -0.1999, -0.1036,  0.2052, -0.1137,\n",
      "            0.2367,  0.0715,  0.0081, -0.1405, -0.2286,  0.0261, -0.2783,\n",
      "            0.1433, -0.1527]],\n",
      "\n",
      "         [[-0.1234, -0.3228,  0.8154,  0.3700,  0.6222,  0.0837,  0.3566,\n",
      "           -0.1789, -0.7151, -0.4886, -0.2418,  0.0624, -0.1806,  0.2813,\n",
      "            0.3303,  0.3811],\n",
      "          [-0.0405, -0.0275,  0.2937, -0.0146,  0.1466,  0.1812,  0.0441,\n",
      "            0.1103, -0.2037, -0.1692, -0.1901, -0.1460, -0.0455, -0.1042,\n",
      "            0.2239,  0.0241],\n",
      "          [ 0.0042,  0.1190,  0.0086, -0.2000, -0.1002,  0.2091, -0.1125,\n",
      "            0.2390,  0.0667,  0.0042, -0.1447, -0.2317,  0.0250, -0.2804,\n",
      "            0.1483, -0.1520]],\n",
      "\n",
      "         [[-0.1234, -0.3228,  0.8154,  0.3700,  0.6222,  0.0837,  0.3566,\n",
      "           -0.1789, -0.7151, -0.4886, -0.2418,  0.0624, -0.1806,  0.2813,\n",
      "            0.3303,  0.3811],\n",
      "          [-0.0559, -0.0823,  0.3905,  0.0567,  0.2349,  0.1631,  0.1021,\n",
      "            0.0566, -0.2986, -0.2285, -0.1997, -0.1073, -0.0706, -0.0326,\n",
      "            0.2437,  0.0903],\n",
      "          [-0.0170,  0.0450,  0.1419, -0.1041,  0.0200,  0.1866, -0.0341,\n",
      "            0.1679, -0.0630, -0.0773, -0.1595, -0.1811, -0.0092, -0.1852,\n",
      "            0.1771, -0.0626]],\n",
      "\n",
      "         [[-0.1234, -0.3228,  0.8154,  0.3700,  0.6222,  0.0837,  0.3566,\n",
      "           -0.1789, -0.7151, -0.4886, -0.2418,  0.0624, -0.1806,  0.2813,\n",
      "            0.3303,  0.3811],\n",
      "          [-0.0325,  0.0009,  0.2435, -0.0516,  0.1009,  0.1905,  0.0140,\n",
      "            0.1381, -0.1545, -0.1385, -0.1852, -0.1660, -0.0325, -0.1412,\n",
      "            0.2137, -0.0102],\n",
      "          [-0.0011,  0.0989,  0.0409, -0.1732, -0.0692,  0.2000, -0.0915,\n",
      "            0.2176,  0.0340, -0.0157, -0.1458, -0.2153,  0.0163, -0.2523,\n",
      "            0.1527, -0.1276]]]], grad_fn=<UnsafeViewBackward0>)\n",
      "reshaped attention: torch.Size([1, 3, 64])\n",
      "tensor([[[-0.1234, -0.3228,  0.8154,  0.3700,  0.6222,  0.0837,  0.3566,\n",
      "          -0.1789, -0.7151, -0.4886, -0.2418,  0.0624, -0.1806,  0.2813,\n",
      "           0.3303,  0.3811, -0.1234, -0.3228,  0.8154,  0.3700,  0.6222,\n",
      "           0.0837,  0.3566, -0.1789, -0.7151, -0.4886, -0.2418,  0.0624,\n",
      "          -0.1806,  0.2813,  0.3303,  0.3811, -0.1234, -0.3228,  0.8154,\n",
      "           0.3700,  0.6222,  0.0837,  0.3566, -0.1789, -0.7151, -0.4886,\n",
      "          -0.2418,  0.0624, -0.1806,  0.2813,  0.3303,  0.3811, -0.1234,\n",
      "          -0.3228,  0.8154,  0.3700,  0.6222,  0.0837,  0.3566, -0.1789,\n",
      "          -0.7151, -0.4886, -0.2418,  0.0624, -0.1806,  0.2813,  0.3303,\n",
      "           0.3811],\n",
      "         [-0.0228,  0.0354,  0.1825, -0.0966,  0.0453,  0.2019, -0.0225,\n",
      "           0.1719, -0.0947, -0.1011, -0.1791, -0.1904, -0.0167, -0.1863,\n",
      "           0.2013, -0.0520, -0.0405, -0.0275,  0.2937, -0.0146,  0.1466,\n",
      "           0.1812,  0.0441,  0.1103, -0.2037, -0.1692, -0.1901, -0.1460,\n",
      "          -0.0455, -0.1042,  0.2239,  0.0241, -0.0559, -0.0823,  0.3905,\n",
      "           0.0567,  0.2349,  0.1631,  0.1021,  0.0566, -0.2986, -0.2285,\n",
      "          -0.1997, -0.1073, -0.0706, -0.0326,  0.2437,  0.0903, -0.0325,\n",
      "           0.0009,  0.2435, -0.0516,  0.1009,  0.1905,  0.0140,  0.1381,\n",
      "          -0.1545, -0.1385, -0.1852, -0.1660, -0.0325, -0.1412,  0.2137,\n",
      "          -0.0102],\n",
      "         [ 0.0051,  0.1197,  0.0019, -0.1999, -0.1036,  0.2052, -0.1137,\n",
      "           0.2367,  0.0715,  0.0081, -0.1405, -0.2286,  0.0261, -0.2783,\n",
      "           0.1433, -0.1527,  0.0042,  0.1190,  0.0086, -0.2000, -0.1002,\n",
      "           0.2091, -0.1125,  0.2390,  0.0667,  0.0042, -0.1447, -0.2317,\n",
      "           0.0250, -0.2804,  0.1483, -0.1520, -0.0170,  0.0450,  0.1419,\n",
      "          -0.1041,  0.0200,  0.1866, -0.0341,  0.1679, -0.0630, -0.0773,\n",
      "          -0.1595, -0.1811, -0.0092, -0.1852,  0.1771, -0.0626, -0.0011,\n",
      "           0.0989,  0.0409, -0.1732, -0.0692,  0.2000, -0.0915,  0.2176,\n",
      "           0.0340, -0.0157, -0.1458, -0.2153,  0.0163, -0.2523,  0.1527,\n",
      "          -0.1276]]], grad_fn=<ViewBackward0>)\n",
      "self.Wo: torch.Size([128, 4])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0514,  0.0743,  0.0537,  0.0693],\n",
      "        [-0.0094, -0.0845, -0.0542, -0.0136],\n",
      "        [ 0.0229,  0.0264, -0.0093, -0.0026],\n",
      "        [-0.0316,  0.0430,  0.0125, -0.0616],\n",
      "        [ 0.0464,  0.0413,  0.0481,  0.0531],\n",
      "        [-0.0105, -0.0405,  0.0001, -0.0071],\n",
      "        [-0.0035, -0.0689, -0.0252, -0.0542],\n",
      "        [ 0.0835,  0.0865,  0.0467, -0.0720],\n",
      "        [-0.0548,  0.0473,  0.0302,  0.0020],\n",
      "        [ 0.0081, -0.0360,  0.0452, -0.0402],\n",
      "        [-0.0376, -0.0440,  0.0329, -0.0872],\n",
      "        [-0.0355, -0.0404, -0.0046,  0.0003],\n",
      "        [ 0.0848, -0.0524,  0.0606, -0.0696],\n",
      "        [-0.0693,  0.0483, -0.0576,  0.0117],\n",
      "        [-0.0017, -0.0049,  0.0201,  0.0202],\n",
      "        [-0.0724, -0.0261,  0.0836, -0.0099],\n",
      "        [-0.0679, -0.0854, -0.0672,  0.0221],\n",
      "        [-0.0875, -0.0013,  0.0315, -0.0219],\n",
      "        [ 0.0605,  0.0836, -0.0371, -0.0282],\n",
      "        [-0.0303,  0.0743, -0.0527, -0.0353],\n",
      "        [ 0.0282,  0.0691, -0.0066,  0.0627],\n",
      "        [-0.0791, -0.0368, -0.0722,  0.0095],\n",
      "        [ 0.0809, -0.0143, -0.0701, -0.0357],\n",
      "        [-0.0454,  0.0016, -0.0115,  0.0131],\n",
      "        [ 0.0266,  0.0383, -0.0735,  0.0114],\n",
      "        [-0.0617, -0.0287,  0.0471,  0.0024],\n",
      "        [ 0.0362,  0.0466,  0.0281, -0.0051],\n",
      "        [ 0.0618,  0.0556, -0.0811,  0.0084],\n",
      "        [-0.0073,  0.0854,  0.0814,  0.0430],\n",
      "        [-0.0234, -0.0501, -0.0794,  0.0883],\n",
      "        [ 0.0510,  0.0175,  0.0640,  0.0476],\n",
      "        [-0.0776, -0.0752, -0.0081, -0.0527],\n",
      "        [-0.0840, -0.0065,  0.0413, -0.0724],\n",
      "        [ 0.0094,  0.0614, -0.0162, -0.0418],\n",
      "        [ 0.0833,  0.0642,  0.0744,  0.0247],\n",
      "        [ 0.0155,  0.0746,  0.0227, -0.0774],\n",
      "        [-0.0599,  0.0262, -0.0539,  0.0297],\n",
      "        [-0.0088, -0.0238,  0.0177, -0.0479],\n",
      "        [ 0.0257, -0.0519,  0.0176,  0.0664],\n",
      "        [-0.0519,  0.0628, -0.0223, -0.0024],\n",
      "        [-0.0426,  0.0583, -0.0716,  0.0199],\n",
      "        [ 0.0391,  0.0258,  0.0776,  0.0818],\n",
      "        [-0.0625,  0.0565, -0.0284, -0.0667],\n",
      "        [ 0.0728,  0.0282,  0.0354, -0.0383],\n",
      "        [-0.0742,  0.0047,  0.0576,  0.0267],\n",
      "        [-0.0635, -0.0323, -0.0053, -0.0564],\n",
      "        [ 0.0178, -0.0213,  0.0196, -0.0009],\n",
      "        [-0.0677, -0.0137, -0.0435,  0.0596],\n",
      "        [-0.0611, -0.0526,  0.0458, -0.0519],\n",
      "        [-0.0120,  0.0083, -0.0181, -0.0460],\n",
      "        [ 0.0733, -0.0850,  0.0844, -0.0710],\n",
      "        [-0.0707,  0.0651,  0.0544, -0.0751],\n",
      "        [ 0.0044,  0.0845, -0.0804,  0.0732],\n",
      "        [-0.0220,  0.0340, -0.0049, -0.0684],\n",
      "        [-0.0223, -0.0597, -0.0738, -0.0661],\n",
      "        [ 0.0663,  0.0443,  0.0142,  0.0746],\n",
      "        [-0.0266,  0.0484,  0.0818, -0.0387],\n",
      "        [ 0.0486, -0.0462,  0.0570,  0.0004],\n",
      "        [-0.0711, -0.0846, -0.0152, -0.0648],\n",
      "        [ 0.0782, -0.0624,  0.0493,  0.0408],\n",
      "        [ 0.0517,  0.0578, -0.0004, -0.0706],\n",
      "        [ 0.0635,  0.0529,  0.0730,  0.0173],\n",
      "        [-0.0615, -0.0505,  0.0230, -0.0285],\n",
      "        [ 0.0780, -0.0163,  0.0638,  0.0295],\n",
      "        [-0.0351, -0.0091, -0.0593, -0.0044],\n",
      "        [ 0.0792,  0.0473,  0.0735, -0.0741],\n",
      "        [ 0.0105,  0.0763, -0.0420, -0.0469],\n",
      "        [ 0.0652,  0.0307, -0.0339, -0.0420],\n",
      "        [ 0.0687, -0.0324,  0.0191,  0.0743],\n",
      "        [ 0.0727, -0.0414,  0.0630,  0.0561],\n",
      "        [-0.0774,  0.0852,  0.0413,  0.0461],\n",
      "        [-0.0510, -0.0647, -0.0310,  0.0168],\n",
      "        [ 0.0367, -0.0794,  0.0471,  0.0470],\n",
      "        [-0.0112,  0.0241, -0.0717, -0.0274],\n",
      "        [-0.0728,  0.0839,  0.0003, -0.0810],\n",
      "        [ 0.0222, -0.0643,  0.0136, -0.0075],\n",
      "        [-0.0366,  0.0549,  0.0628, -0.0377],\n",
      "        [ 0.0760, -0.0411,  0.0003,  0.0797],\n",
      "        [ 0.0150, -0.0266, -0.0145,  0.0551],\n",
      "        [-0.0400, -0.0881,  0.0034, -0.0795],\n",
      "        [ 0.0611, -0.0265, -0.0585, -0.0675],\n",
      "        [-0.0521, -0.0122, -0.0485, -0.0295],\n",
      "        [ 0.0822,  0.0452, -0.0568, -0.0710],\n",
      "        [ 0.0450,  0.0645, -0.0702, -0.0811],\n",
      "        [-0.0225, -0.0715, -0.0430, -0.0110],\n",
      "        [ 0.0735, -0.0227,  0.0861, -0.0029],\n",
      "        [ 0.0307, -0.0863,  0.0438, -0.0778],\n",
      "        [ 0.0206, -0.0404,  0.0440, -0.0200],\n",
      "        [-0.0879,  0.0735, -0.0069,  0.0122],\n",
      "        [-0.0054,  0.0238, -0.0209, -0.0291],\n",
      "        [ 0.0112, -0.0083, -0.0622,  0.0861],\n",
      "        [ 0.0630, -0.0599,  0.0275, -0.0171],\n",
      "        [-0.0425,  0.0575, -0.0010,  0.0880],\n",
      "        [ 0.0031,  0.0430,  0.0718,  0.0703],\n",
      "        [ 0.0070,  0.0678, -0.0493, -0.0748],\n",
      "        [-0.0038,  0.0665,  0.0458, -0.0291],\n",
      "        [ 0.0548,  0.0803, -0.0577,  0.0824],\n",
      "        [-0.0199, -0.0854,  0.0256,  0.0154],\n",
      "        [ 0.0358, -0.0863, -0.0115,  0.0538],\n",
      "        [ 0.0343,  0.0434, -0.0784,  0.0476],\n",
      "        [-0.0750, -0.0764, -0.0024, -0.0658],\n",
      "        [ 0.0400, -0.0731, -0.0679, -0.0725],\n",
      "        [-0.0722, -0.0172, -0.0615,  0.0616],\n",
      "        [-0.0544, -0.0756,  0.0504, -0.0241],\n",
      "        [-0.0493,  0.0617,  0.0499,  0.0277],\n",
      "        [ 0.0236,  0.0027,  0.0016, -0.0365],\n",
      "        [-0.0200, -0.0603, -0.0211, -0.0676],\n",
      "        [ 0.0158, -0.0035, -0.0131,  0.0160],\n",
      "        [-0.0304, -0.0590, -0.0502,  0.0389],\n",
      "        [ 0.0793, -0.0155, -0.0462, -0.0514],\n",
      "        [-0.0697, -0.0389, -0.0095, -0.0075],\n",
      "        [ 0.0466,  0.0835, -0.0150, -0.0661],\n",
      "        [-0.0175,  0.0384,  0.0659,  0.0800],\n",
      "        [ 0.0198, -0.0148,  0.0801,  0.0679],\n",
      "        [ 0.0548,  0.0441, -0.0773,  0.0059],\n",
      "        [-0.0003,  0.0879,  0.0650,  0.0100],\n",
      "        [ 0.0017,  0.0040,  0.0177, -0.0550],\n",
      "        [-0.0189, -0.0250, -0.0162, -0.0413],\n",
      "        [-0.0369, -0.0653,  0.0759,  0.0452],\n",
      "        [-0.0196, -0.0653, -0.0633,  0.0211],\n",
      "        [-0.0218,  0.0105, -0.0293, -0.0554],\n",
      "        [-0.0503,  0.0465,  0.0322, -0.0693],\n",
      "        [-0.0623, -0.0767,  0.0390, -0.0007],\n",
      "        [ 0.0549, -0.0156,  0.0739,  0.0425],\n",
      "        [ 0.0659,  0.0031, -0.0220, -0.0762],\n",
      "        [ 0.0374,  0.0430,  0.0135, -0.0675],\n",
      "        [-0.0469, -0.0155, -0.0748,  0.0680],\n",
      "        [ 0.0117,  0.0004,  0.0271,  0.0802]], requires_grad=True)\n",
      "spliced Wo: torch.Size([64, 2])\n",
      "tensor([[-0.0672,  0.0221],\n",
      "        [ 0.0315, -0.0219],\n",
      "        [-0.0371, -0.0282],\n",
      "        [-0.0527, -0.0353],\n",
      "        [-0.0066,  0.0627],\n",
      "        [-0.0722,  0.0095],\n",
      "        [-0.0701, -0.0357],\n",
      "        [-0.0115,  0.0131],\n",
      "        [-0.0735,  0.0114],\n",
      "        [ 0.0471,  0.0024],\n",
      "        [ 0.0281, -0.0051],\n",
      "        [-0.0811,  0.0084],\n",
      "        [ 0.0814,  0.0430],\n",
      "        [-0.0794,  0.0883],\n",
      "        [ 0.0640,  0.0476],\n",
      "        [-0.0081, -0.0527],\n",
      "        [ 0.0458, -0.0519],\n",
      "        [-0.0181, -0.0460],\n",
      "        [ 0.0844, -0.0710],\n",
      "        [ 0.0544, -0.0751],\n",
      "        [-0.0804,  0.0732],\n",
      "        [-0.0049, -0.0684],\n",
      "        [-0.0738, -0.0661],\n",
      "        [ 0.0142,  0.0746],\n",
      "        [ 0.0818, -0.0387],\n",
      "        [ 0.0570,  0.0004],\n",
      "        [-0.0152, -0.0648],\n",
      "        [ 0.0493,  0.0408],\n",
      "        [-0.0004, -0.0706],\n",
      "        [ 0.0730,  0.0173],\n",
      "        [ 0.0230, -0.0285],\n",
      "        [ 0.0638,  0.0295],\n",
      "        [-0.0585, -0.0675],\n",
      "        [-0.0485, -0.0295],\n",
      "        [-0.0568, -0.0710],\n",
      "        [-0.0702, -0.0811],\n",
      "        [-0.0430, -0.0110],\n",
      "        [ 0.0861, -0.0029],\n",
      "        [ 0.0438, -0.0778],\n",
      "        [ 0.0440, -0.0200],\n",
      "        [-0.0069,  0.0122],\n",
      "        [-0.0209, -0.0291],\n",
      "        [-0.0622,  0.0861],\n",
      "        [ 0.0275, -0.0171],\n",
      "        [-0.0010,  0.0880],\n",
      "        [ 0.0718,  0.0703],\n",
      "        [-0.0493, -0.0748],\n",
      "        [ 0.0458, -0.0291],\n",
      "        [ 0.0659,  0.0800],\n",
      "        [ 0.0801,  0.0679],\n",
      "        [-0.0773,  0.0059],\n",
      "        [ 0.0650,  0.0100],\n",
      "        [ 0.0177, -0.0550],\n",
      "        [-0.0162, -0.0413],\n",
      "        [ 0.0759,  0.0452],\n",
      "        [-0.0633,  0.0211],\n",
      "        [-0.0293, -0.0554],\n",
      "        [ 0.0322, -0.0693],\n",
      "        [ 0.0390, -0.0007],\n",
      "        [ 0.0739,  0.0425],\n",
      "        [-0.0220, -0.0762],\n",
      "        [ 0.0135, -0.0675],\n",
      "        [-0.0748,  0.0680],\n",
      "        [ 0.0271,  0.0802]], grad_fn=<CatBackward0>)\n",
      "projected output: torch.Size([1, 3, 2])\n",
      "tensor([[[-0.1419, -0.0725],\n",
      "         [-0.0702, -0.0594],\n",
      "         [-0.0466, -0.0379]]], grad_fn=<UnsafeViewBackward0>)\n",
      "----------------- END MultiQueryAttention.forwardTensor() --------------------\n",
      "x in layer after MQA & resid connection and before MLP:\n",
      "tensor([[[ 0.7250,  0.1102],\n",
      "         [-1.2721,  0.4807],\n",
      "         [-0.9707,  0.2522]]], grad_fn=<AddBackward0>)\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 3, 2])\n",
      "tensor([[[ 0.7250,  0.1102],\n",
      "         [-1.2721,  0.4807],\n",
      "         [-0.9707,  0.2522]]], grad_fn=<AddBackward0>)\n",
      "normed x: torch.Size([1, 3, 2])\n",
      "tensor([[[ 1.3981,  0.2126],\n",
      "         [-1.3229,  0.4999],\n",
      "         [-1.3688,  0.3556]]], grad_fn=<MulBackward0>)\n",
      "dim: 2\n",
      "skip: 2\n",
      "spliced scale: torch.Size([2])\n",
      "tensor([0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 3, 2])\n",
      "tensor([[[ 1.3981,  0.2126],\n",
      "         [-1.3229,  0.4999],\n",
      "         [-1.3688,  0.3556]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "---------- MLP Input: torch.Tensor ------------\n",
      "------------- MLP.forwardTensor() ------------\n",
      "x: torch.Size([1, 3, 2])\n",
      "tensor([[[ 1.3981,  0.2126],\n",
      "         [-1.3229,  0.4999],\n",
      "         [-1.3688,  0.3556]]], grad_fn=<MulBackward0>)\n",
      "d_dim: 2\n",
      "d_skip: 2\n",
      "i_dim: 8\n",
      "i_skip: 8\n",
      "Wgate: torch.Size([4, 16])\n",
      "Parameter containing:\n",
      "tensor([[ 0.3095,  0.3247, -0.2348, -0.2684,  0.1481, -0.0336,  0.2918,  0.4104,\n",
      "          0.4233, -0.2854, -0.4600, -0.0114,  0.0436,  0.3290, -0.4695,  0.1966],\n",
      "        [-0.4550, -0.3347,  0.1588, -0.0416, -0.1350, -0.4122,  0.0433, -0.0343,\n",
      "          0.1841,  0.2340, -0.0984,  0.2009, -0.4429, -0.4785,  0.0231, -0.0917],\n",
      "        [-0.1875,  0.4671,  0.2435, -0.4029, -0.2097,  0.4350,  0.1203,  0.4133,\n",
      "          0.2601, -0.1608, -0.2288, -0.2892, -0.4386, -0.3339,  0.4059,  0.3157],\n",
      "        [-0.4021,  0.0384, -0.3442,  0.3066, -0.4698, -0.0893,  0.1899, -0.2196,\n",
      "          0.1504,  0.3536, -0.0917,  0.4688, -0.2037,  0.0865,  0.1056, -0.1437]],\n",
      "       requires_grad=True)\n",
      "Wgate spliced: torch.Size([2, 8])\n",
      "tensor([[ 0.2601, -0.1608, -0.2288, -0.2892, -0.4386, -0.3339,  0.4059,  0.3157],\n",
      "        [ 0.1504,  0.3536, -0.0917,  0.4688, -0.2037,  0.0865,  0.1056, -0.1437]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Bgate: torch.Size([16])\n",
      "Parameter containing:\n",
      "tensor([-0.3869, -0.0935, -0.0742,  0.3095, -0.0956,  0.3838, -0.1328, -0.3449,\n",
      "        -0.4261,  0.3397,  0.2771,  0.2628, -0.2119, -0.4128, -0.2219,  0.1706],\n",
      "       requires_grad=True)\n",
      "Bgate spliced: torch.Size([8])\n",
      "tensor([-0.4261,  0.3397,  0.2771,  0.2628, -0.2119, -0.4128, -0.2219,  0.1706],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xgate: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.0305,  0.1900, -0.0623, -0.0419, -0.8683, -0.8612,  0.3681,\n",
      "           0.5815],\n",
      "         [-0.6949,  0.7292,  0.5339,  0.8797,  0.2665,  0.0721, -0.7061,\n",
      "          -0.3189],\n",
      "         [-0.7286,  0.6856,  0.5576,  0.8253,  0.3160,  0.0750, -0.7399,\n",
      "          -0.3126]]], grad_fn=<AddBackward0>)\n",
      "GeLU'ed Xgate: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.0149,  0.1093, -0.0296, -0.0203, -0.1672, -0.1676,  0.2369,\n",
      "           0.4185],\n",
      "         [-0.1693,  0.5593,  0.3755,  0.7130,  0.1613,  0.0381, -0.1695,\n",
      "          -0.1195],\n",
      "         [-0.1699,  0.5166,  0.3967,  0.6565,  0.1972,  0.0397, -0.1699,\n",
      "          -0.1179]]], grad_fn=<GeluBackward0>)\n",
      "Wup: torch.Size([4, 16])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0488, -0.1314, -0.0796, -0.4639,  0.2573,  0.1704,  0.0070,  0.3364,\n",
      "         -0.4475,  0.2511,  0.1227,  0.0093,  0.3071, -0.4281, -0.3575, -0.1387],\n",
      "        [-0.2920, -0.1945, -0.1729, -0.1217,  0.0128,  0.4984,  0.2747, -0.0651,\n",
      "         -0.2933,  0.0877, -0.3934, -0.1303, -0.4999, -0.2135,  0.0632,  0.2846],\n",
      "        [ 0.4273, -0.4888, -0.3618,  0.3541,  0.4532,  0.2270, -0.0352, -0.1797,\n",
      "          0.2048,  0.0527, -0.3789,  0.0368, -0.1809,  0.1246,  0.2698, -0.0715],\n",
      "        [-0.3377, -0.4910, -0.0681, -0.4828, -0.0743, -0.0438, -0.3095, -0.1333,\n",
      "          0.0544,  0.0970, -0.0450, -0.4722,  0.2919, -0.2446,  0.2606,  0.0314]],\n",
      "       requires_grad=True)\n",
      "Wup spliced: torch.Size([2, 8])\n",
      "tensor([[ 0.2048,  0.0527, -0.3789,  0.0368, -0.1809,  0.1246,  0.2698, -0.0715],\n",
      "        [ 0.0544,  0.0970, -0.0450, -0.4722,  0.2919, -0.2446,  0.2606,  0.0314]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Bup: torch.Size([16])\n",
      "Parameter containing:\n",
      "tensor([-0.1737,  0.4697,  0.0072, -0.1991,  0.3706, -0.0403, -0.2663,  0.3325,\n",
      "        -0.3660,  0.3473, -0.3763, -0.3744,  0.2100,  0.1556, -0.0674,  0.4764],\n",
      "       requires_grad=True)\n",
      "Bup spliced: torch.Size([8])\n",
      "tensor([-0.3660,  0.3473, -0.3763, -0.3744,  0.2100,  0.1556, -0.0674,  0.4764],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xup: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.0681,  0.4416, -0.9156, -0.4234,  0.0192,  0.2778,  0.3652,\n",
      "           0.3831],\n",
      "         [-0.6097,  0.3261,  0.1024, -0.6591,  0.5952, -0.1316, -0.2941,\n",
      "           0.5867],\n",
      "         [-0.6269,  0.3097,  0.1263, -0.5927,  0.5614, -0.1020, -0.3440,\n",
      "           0.5854]]], grad_fn=<AddBackward0>)\n",
      "Xfuse: torch.Size([1, 3, 8])\n",
      "tensor([[[ 0.0010,  0.0483,  0.0271,  0.0086, -0.0032, -0.0465,  0.0865,\n",
      "           0.1603],\n",
      "         [ 0.1032,  0.1824,  0.0385, -0.4700,  0.0960, -0.0050,  0.0498,\n",
      "          -0.0701],\n",
      "         [ 0.1065,  0.1600,  0.0501, -0.3891,  0.1107, -0.0041,  0.0585,\n",
      "          -0.0691]]], grad_fn=<MulBackward0>)\n",
      "Wdown: torch.Size([16, 4])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0028,  0.1688,  0.1252, -0.2132],\n",
      "        [ 0.2146,  0.2054, -0.1145,  0.1425],\n",
      "        [ 0.2420, -0.1323,  0.0254,  0.0988],\n",
      "        [-0.2279, -0.2429,  0.0742,  0.1267],\n",
      "        [ 0.1254,  0.0361,  0.2388, -0.2003],\n",
      "        [-0.0737, -0.1731, -0.2045, -0.1040],\n",
      "        [ 0.0186, -0.1040, -0.0075,  0.1881],\n",
      "        [ 0.0218, -0.1834,  0.1468, -0.1363],\n",
      "        [ 0.0217,  0.2413,  0.1934, -0.0885],\n",
      "        [-0.0613,  0.1515, -0.0892,  0.0997],\n",
      "        [ 0.1085,  0.1849, -0.1100,  0.0362],\n",
      "        [-0.0083,  0.1678, -0.1493,  0.0104],\n",
      "        [ 0.2329, -0.2497, -0.1154,  0.0158],\n",
      "        [ 0.2373,  0.1807,  0.2185,  0.2416],\n",
      "        [-0.0733,  0.1733,  0.0052,  0.2215],\n",
      "        [ 0.1725, -0.0791,  0.0407, -0.1026]], requires_grad=True)\n",
      "Wdown spliced: torch.Size([8, 2])\n",
      "tensor([[ 0.1934, -0.0885],\n",
      "        [-0.0892,  0.0997],\n",
      "        [-0.1100,  0.0362],\n",
      "        [-0.1493,  0.0104],\n",
      "        [-0.1154,  0.0158],\n",
      "        [ 0.2185,  0.2416],\n",
      "        [ 0.0052,  0.2215],\n",
      "        [ 0.0407, -0.1026]], grad_fn=<SliceBackward0>)\n",
      "Bdown: torch.Size([4])\n",
      "Parameter containing:\n",
      "tensor([-0.1299, -0.0603, -0.1047, -0.2112], requires_grad=True)\n",
      "Bdown spliced: torch.Size([2])\n",
      "tensor([-0.1047, -0.2112], grad_fn=<SliceBackward0>)\n",
      "outputs: torch.Size([1, 3, 2])\n",
      "tensor([[[-0.1159, -0.2140],\n",
      "         [-0.0498, -0.1871],\n",
      "         [-0.0619, -0.1861]]], grad_fn=<AddBackward0>)\n",
      "------------- END MLP.forwardTensor() ------------\n",
      "layer's final residual state:\n",
      "tensor([[[ 0.6091, -0.1038],\n",
      "         [-1.3219,  0.2936],\n",
      "         [-1.0326,  0.0661]]], grad_fn=<AddBackward0>)\n",
      "----------------- END Layer.forwardTensor() --------------------\n",
      "forwardTensor() output: torch.Size([1, 3, 2])\n",
      "tensor([[[ 0.6091, -0.1038],\n",
      "         [-1.3219,  0.2936],\n",
      "         [-1.0326,  0.0661]]], grad_fn=<AddBackward0>)\n",
      "final output: ((tensor([[[-0.5520,  0.1924,  1.1748, -1.7492],\n",
      "         [-0.9243,  0.8913, -2.0857,  0.2210],\n",
      "         [ 0.1333,  0.0720,  0.6909, -1.9409]]], grad_fn=<AddBackward0>),), (tensor([[[-0.0242,  1.4896],\n",
      "         [-1.1620,  0.5250],\n",
      "         [-0.7977, -0.0376]]], grad_fn=<AddBackward0>), tensor([[[ 0.6091, -0.1038],\n",
      "         [-1.3219,  0.2936],\n",
      "         [-1.0326,  0.0661]]], grad_fn=<AddBackward0>)))\n",
      "------------- END Layer.forwardTuple() ------------\n",
      "out: ((tensor([[[-0.5520,  0.1924,  1.1748, -1.7492],\n",
      "         [-0.9243,  0.8913, -2.0857,  0.2210],\n",
      "         [ 0.1333,  0.0720,  0.6909, -1.9409]]], grad_fn=<AddBackward0>),), (tensor([[[-0.0242,  1.4896],\n",
      "         [-1.1620,  0.5250],\n",
      "         [-0.7977, -0.0376]]], grad_fn=<AddBackward0>), tensor([[[ 0.6091, -0.1038],\n",
      "         [-1.3219,  0.2936],\n",
      "         [-1.0326,  0.0661]]], grad_fn=<AddBackward0>)))\n",
      "---------- RESET CONFIG --------\n",
      "model_count:  [1, 2, 4]\n",
      "model_dim_list:  [128, 64, 32]\n"
     ]
    }
   ],
   "source": [
    "# Testing our Layer's forwardTuple()\n",
    "verbose = True\n",
    "\n",
    "print(\"--------- Micro Hyperparameters -------\")\n",
    "hold1, hold2, hold3, hold4 = config.hidden_size, config.levels, config.max_position_embeddings, config.hidden_size\n",
    "config.hidden_size = 4\n",
    "config.levels = 2\n",
    "config.max_position_embeddings = 3\n",
    "config.vocab_size = 5\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "\n",
    "embedding = torch.randn(config.vocab_size, config.hidden_size)\n",
    "print(f\"embedding: {embedding.shape}\\n{embedding}\")\n",
    "\n",
    "layer = Layer(config)\n",
    "# we need to make sure to send in a tuple of the expected size. above we set hidden_size=4 and levels=2\n",
    "x = ((torch.randn((1,3,config.hidden_size)),),\n",
    "     (torch.randn((1,3,config.hidden_size//config.split)),torch.randn((1,3,config.hidden_size//config.split))))\n",
    "print(f\"x: {x}\")\n",
    "out = layer(x)\n",
    "print(f\"out: {out}\")\n",
    "\n",
    "verbose = False\n",
    "print(\"---------- RESET CONFIG --------\")\n",
    "config.hidden_size = hold1\n",
    "config.levels = hold2\n",
    "config.max_position_embeddings = hold3\n",
    "config.vocab_size = hold4\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec942733",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09e1d472",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FractalLoss(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        \"\"\"\n",
    "        input: \n",
    "            - logits are a tuple of tuples of tensors each of shape [batch_size, max_seq_len, vocab_size]\n",
    "            - target is a shape [batch_size, max_seq_len] tensor of the integer indices of the correct tokens\n",
    "        output: a tensor containing a single float of the loss value\n",
    "        \"\"\"\n",
    "        global verbose\n",
    "        if verbose: \n",
    "            print(\"------------- FractalLoss.forward() ------------\")\n",
    "            print(f\"logits:\\n{logits}\")\n",
    "            \n",
    "        assert type(x) == tuple # since this function should only be used during training\n",
    "            \n",
    "        # should only be used during training, so we assert input_len == max_position_embeddings\n",
    "        b,t,v = logits[0][0].shape\n",
    "        if verbose: print(f\"b:{b}, t:{t}, v:{v}, b*t:{b*t}\")\n",
    "        assert t == config.max_position_embeddings\n",
    "        \n",
    "        # Calculate losses for each output and stack them. \n",
    "        # i apologize for the weird format instead of regular for loops, but it feels better in my head\n",
    "        loss = torch.stack([ # stacks across levels\n",
    "                            torch.stack( # stacks across models in level\n",
    "                                        [self.criterion(logits_ij.view(b*t, v), # reshapes for CELoss\n",
    "                                                        target.view(b*t)) \n",
    "                                         for logits_ij in logits[i]] # iterates across models in level\n",
    "                            ).sum() # sums across models in level\n",
    "                            for i in range(len(logits))] # iterates across levels\n",
    "                          ).sum() # sums across levels\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"final loss: {loss}\")\n",
    "            print(\"------------- END FractalLoss.forward() ------------\")\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "36754b4f-9dd4-4a61-a626-36b03d5d2217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Micro Hyperparameters -------\n",
      "model_count:  [1, 2]\n",
      "model_dim_list:  [4, 2]\n",
      "embedding: torch.Size([5, 4])\n",
      "tensor([[-1.2442, -0.2054,  0.4328, -0.6742],\n",
      "        [ 0.4481,  0.3009, -0.4987, -0.2391],\n",
      "        [-1.2810, -0.6251, -0.2957, -0.3404],\n",
      "        [ 1.9168,  0.2815,  0.4247, -0.0595],\n",
      "        [-0.5346, -1.8139, -0.5375,  0.3907]])\n",
      "logits: ((tensor([[[ 0.3822,  0.0105,  0.5044, -0.2861,  0.1556],\n",
      "         [-1.0049,  0.5298, -0.6397,  2.2576,  1.3787],\n",
      "         [ 0.2819,  0.0857, -0.2324, -0.8041,  0.2229]],\n",
      "\n",
      "        [[-0.7764,  0.0303, -0.9326,  1.1717,  0.5954],\n",
      "         [-0.7568,  1.0427, -0.8230,  0.2476, -1.5933],\n",
      "         [-0.4977, -0.0927,  1.8751, -0.7547,  0.2503]]]),), (tensor([[[-0.8271,  0.5912,  0.1501, -0.1874,  0.7931],\n",
      "         [-0.3611, -1.0671, -0.1335, -1.7038,  1.2444],\n",
      "         [-0.2208, -1.4560, -0.0983,  0.7279, -0.8795]],\n",
      "\n",
      "        [[-2.0097,  0.2164, -1.0551,  0.3478, -0.1330],\n",
      "         [-0.9583, -1.1940, -0.9532, -0.1890, -1.1287],\n",
      "         [-0.2459, -0.1444,  0.5404,  0.4526, -0.4160]]]), tensor([[[ 0.0741,  0.3123,  1.5325, -0.8124,  0.2009],\n",
      "         [-0.7934, -0.8889,  1.8271, -0.7021,  1.1229],\n",
      "         [ 1.1388, -0.5550,  0.2435, -0.6505, -1.3991]],\n",
      "\n",
      "        [[-0.3258,  0.8816,  0.7005, -0.2641, -0.3478],\n",
      "         [ 0.7193,  1.6464, -0.6238, -1.2664,  0.5664],\n",
      "         [ 0.5673,  0.1961, -1.1300, -1.3256,  0.3379]]])))\n",
      "target: tensor([[[4, 2, 4],\n",
      "         [1, 0, 3]]])\n",
      "------------- FractalLoss.forward() ------------\n",
      "logits:\n",
      "((tensor([[[ 0.3822,  0.0105,  0.5044, -0.2861,  0.1556],\n",
      "         [-1.0049,  0.5298, -0.6397,  2.2576,  1.3787],\n",
      "         [ 0.2819,  0.0857, -0.2324, -0.8041,  0.2229]],\n",
      "\n",
      "        [[-0.7764,  0.0303, -0.9326,  1.1717,  0.5954],\n",
      "         [-0.7568,  1.0427, -0.8230,  0.2476, -1.5933],\n",
      "         [-0.4977, -0.0927,  1.8751, -0.7547,  0.2503]]]),), (tensor([[[-0.8271,  0.5912,  0.1501, -0.1874,  0.7931],\n",
      "         [-0.3611, -1.0671, -0.1335, -1.7038,  1.2444],\n",
      "         [-0.2208, -1.4560, -0.0983,  0.7279, -0.8795]],\n",
      "\n",
      "        [[-2.0097,  0.2164, -1.0551,  0.3478, -0.1330],\n",
      "         [-0.9583, -1.1940, -0.9532, -0.1890, -1.1287],\n",
      "         [-0.2459, -0.1444,  0.5404,  0.4526, -0.4160]]]), tensor([[[ 0.0741,  0.3123,  1.5325, -0.8124,  0.2009],\n",
      "         [-0.7934, -0.8889,  1.8271, -0.7021,  1.1229],\n",
      "         [ 1.1388, -0.5550,  0.2435, -0.6505, -1.3991]],\n",
      "\n",
      "        [[-0.3258,  0.8816,  0.7005, -0.2641, -0.3478],\n",
      "         [ 0.7193,  1.6464, -0.6238, -1.2664,  0.5664],\n",
      "         [ 0.5673,  0.1961, -1.1300, -1.3256,  0.3379]]])))\n",
      "b:2, t:3, v:5, b*t:6\n",
      "final loss: 5.738158226013184\n",
      "------------- END FractalLoss.forward() ------------\n",
      "out: 5.738158226013184\n",
      "---------- RESET CONFIG --------\n",
      "model_count:  [1, 2, 4]\n",
      "model_dim_list:  [128, 64, 32]\n"
     ]
    }
   ],
   "source": [
    "# Testing our FractalLoss\n",
    "verbose = True\n",
    "\n",
    "print(\"--------- Micro Hyperparameters -------\")\n",
    "hold1, hold2, hold3, hold4 = config.hidden_size, config.levels, config.max_position_embeddings, config.hidden_size\n",
    "config.hidden_size = 4\n",
    "config.levels = 2\n",
    "config.max_position_embeddings = 3\n",
    "config.vocab_size = 5\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "\n",
    "embedding = torch.randn(config.vocab_size, config.hidden_size)\n",
    "print(f\"embedding: {embedding.shape}\\n{embedding}\")\n",
    "\n",
    "loss = FractalLoss(config)\n",
    "# we need to make sure to send in a tuple of the expected size. above we set hidden_size=4 and levels=2\n",
    "logits = ((torch.randn((2,3,config.vocab_size)),),\n",
    "     (torch.randn((2,3,config.vocab_size)),torch.randn((2,3,config.vocab_size))))\n",
    "print(f\"logits: {logits}\")\n",
    "target = torch.randint(config.vocab_size, (2,3)).unsqueeze(0)\n",
    "print(f\"target: {target}\")\n",
    "out = loss(logits, target)\n",
    "print(f\"out: {out}\")\n",
    "\n",
    "verbose = False\n",
    "print(\"---------- RESET CONFIG --------\")\n",
    "config.hidden_size = hold1\n",
    "config.levels = hold2\n",
    "config.max_position_embeddings = hold3\n",
    "config.vocab_size = hold4\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4383399d",
   "metadata": {},
   "source": [
    "# The Model itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0018879",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mFractalFormer_base\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config: Config, tokenizer: tokenizer):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class FractalFormer_base(nn.Module):\n",
    "    def __init__(self, config: Config, tokenizer: tokenizer):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        # hyperparameters\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.max_seq_len = config.max_position_embeddings\n",
    "        self.head_dim = config.head_dim\n",
    "        self.vocab_size = config.vocab_size\n",
    "\n",
    "        ### FractalFormer-specific hyperparameters\n",
    "        self.num_levels = config.levels # the number of levels for sub-models to exist on\n",
    "        self.split = config.split # the number of splits to make at a given level\n",
    "        self.model_count = config.model_count # list of number of models at a given level\n",
    "        self.model_dim_list = config.model_dim_list # list of hidden dimensions corresponding to each given level\n",
    "        self.head_dim_list = config.head_dim_list # list of attention head dimensions corresponding to each given level    \n",
    "\n",
    "        # the embedding matrix. for converting tokens to the first residual state, and the last residual state to logits\n",
    "        self.embedder = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "\n",
    "        # for normalizing the initial embeddings\n",
    "        self.embedder_norm = RMSNorm(config.hidden_size)\n",
    "\n",
    "        # Initialize a sequence of DecoderLayer instances as specified by the number of hidden layers in the config\n",
    "        self.layers = nn.ModuleList(Layer(config) for _ in range(config.num_hidden_layers))\n",
    "\n",
    "        # initializing output layer\n",
    "        self.output_layer = OutputLayer(self.embedder.weight, config)\n",
    "        # i think i need to do this bc in the above version you can't use `self.` inside the init\n",
    "        #@property \n",
    "        #def output_layer(self):\n",
    "            #return OutputLayer(self.embedder.weight, config)\n",
    "\n",
    "        # the loss function\n",
    "        self.criterion = FractalLoss(config)\n",
    "\n",
    "    def forwardTensor(self,\n",
    "                      input_token_ids: torch.Tensor,\n",
    "                      level: int = 0, # integer designating the level of model to use. 0 is largest model, -1 is smallest\n",
    "                      model: int = 0, # integer designating the model in that level to use. 0 is top-left, -1 is bottom right\n",
    "                     ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        inputs: \n",
    "            - input_token_ids (torch.Tensor): a tensor of integers size (batch_size, sequence_length)\n",
    "            - level: integer designating the level of model to use. 0 is largest model, -1 is smallest\n",
    "            - model: integer designating the model in that level to use. 0 is top-left, -1 is bottom right\n",
    "        output: a torch.Tensor shape (batch_size, sequence_length, vocab_size)\n",
    "        \"\"\"\n",
    "        global verbose\n",
    "        if verbose: \n",
    "            print(\"------------- FractalFormer.forwardTensor() ------------\")\n",
    "            print(f\"input_token_ids: {input_token_ids.shape}\\n{input_token_ids}\")\n",
    "        \n",
    "        # adjusting everything to the specified level & model\n",
    "        d_dim = self.hidden_size // (2**level)\n",
    "        d_skip = model * d_dim\n",
    "        if verbose:\n",
    "            print(f\"d_dim: {d_dim}\")\n",
    "            print(f\"d_skip: {d_skip}\")\n",
    "        \n",
    "        # turn the input tokens into the first residual state using the embedding matrix\n",
    "        # (batch_size, input_len) & (vocab_size, hidden_size) -> (batch_size, input_len, hidden_size) -> (batch_size, input_len, d_dim)\n",
    "        x = self.embedder(input_token_ids)\n",
    "        if verbose: print(f\"x0: {x.shape}\\n{x}\")\n",
    "\n",
    "        x = x[:,:, d_skip:d_skip + d_dim]\n",
    "        if verbose: print(f\"spliced x0: {x0.shape}\\n{x0}\")\n",
    "        \n",
    "        # Gemma normalizes the embedding by sqrt(hidden_size)\n",
    "        # the question is, should I do this with the full sized hidden_size or do it at the splice size????\n",
    "        # imma do it at the splice size and change it later if i think the models aren't learning well\n",
    "        #x = x * (d_dim**0.5)\n",
    "        # alternatively i could just switch to doing a regular RMSNorm which would be more like me\n",
    "        # if i figure out this different sizes of hyperspheres thing it'd be more in line with that\n",
    "        x = self.embedder_norm(x, model)\n",
    "        if verbose: print(f\"normalized initial x: {x.shape}\\n{x}\")\n",
    "\n",
    "        # Iteratively process the input through each Layer\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if verbose: print(f\"begin layer {i}\")\n",
    "            x = layer(x, model)\n",
    "            if verbose: print(f\"output of layer {i}: {x.shape}\\n{x}\")\n",
    "\n",
    "        logits = self.output_layer(x, model)\n",
    "        if verbose: \n",
    "            print(f\"output logits: {logits.shape}\\n{logits}\")\n",
    "            print(\"------------- END FractalFormer.forwardTensor() ------------\")\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def forwardTuple(self,\n",
    "                     input_token_ids: torch.Tensor,\n",
    "                     target_token_ids: torch.Tensor,\n",
    "                    ) -> torch.Tensor:\n",
    "        global verbose\n",
    "        if verbose: \n",
    "            print(\"------------- FractalFormer.forwardTuple() ------------\")\n",
    "            print(f\"input_token_ids: {input_token_ids.shape}\\n{input_token_ids}\")\n",
    "            print(f\"target_token_ids: {target_token_ids.shape}\\n{target_token_ids}\")\n",
    "        \n",
    "        # use the embedding matrix to turn the input tokens into the first residual state of the largest model\n",
    "        # (batch_size, input_len) & (vocab_size, hidden_size) -> (batch_size, input_len, hidden_size)\n",
    "        x0 = self.embedder(input_token_ids)\n",
    "        if verbose: print(f\"initial x: {x.shape}\\n{x}\")\n",
    "\n",
    "        # create the first fractal tuple of residual states\n",
    "        x = ()\n",
    "        for i, models_in_level in enumerate(config.model_count):\n",
    "            if verbose: print(f\"i: {i}, models_in_level: {models_in_level}, iterating over {config.model_count}\")\n",
    "            \n",
    "            x_lvl = ()\n",
    "            for j, d_dim in enumerate(config.model_dim_list):\n",
    "                if verbose: print(f\"j: {j}, d_dim: {d_dim}, iterating over {config.model_dim_list}\")\n",
    "\n",
    "                skip = j * d_dim\n",
    "                if verbose: print(f\"skip: {skip}\")\n",
    "                \n",
    "                x_ij_spliced = x0[:,:,skip:skip + d_dim]\n",
    "                if verbose: print(f\"initial x[{i}][{j}] spliced: {x_ij_spliced.shape}\\n{x_ij_spliced}\")\n",
    "                    \n",
    "                x_ij_spliced_normed = self.embedder_norm(x_ij_spliced, model=j) # * (d_dim**0.5) # if i want to do Gemma normalization instead\n",
    "                if verbose: print(f\"initial x[{i}][{j}] spliced & normed: {x_ij_spliced_normed.shape}\\n{x_ij_spliced_normed}\")\n",
    "                \n",
    "                x_lvl += (x_ij_spliced_normed,)  \n",
    "            x += (x_lvl,)\n",
    "        if verbose: print(f\"full tuple initial x: {x0}\")\n",
    "\n",
    "        # Iteratively process the input through each Layer\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if verbose: print(f\"begin layer {i}\")\n",
    "            \n",
    "            x = layer(x)\n",
    "            if verbose: print(f\"output of layer {i}: {x}\")\n",
    "\n",
    "        logits = self.output_layer(x)\n",
    "        if verbose: \n",
    "            print(f\"output logits: {logits}\")\n",
    "            print(\"------------- END FractalFormer.forwardTuple() ------------\")\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def forward(self,\n",
    "                input_token_ids: torch.Tensor, # a shape (batch_size, input_seq_len OR max_seq_len)list of integer token ids\n",
    "                target_token_ids: torch.Tensor = None, # a shape (batch_size, max_seq_len) list of token ids to train on\n",
    "                level: int = 0, # integer designating the level of model to use. 0 is largest model\n",
    "                model: int = 0, # integer designating the model in that level to use. 0 is top-left model in level\n",
    "                ):\n",
    "        global verbose\n",
    "        if verbose: \n",
    "            print(\"------------- FractalFormer.forward() ------------\")\n",
    "            print(f\"input_token_ids: {input_token_ids.shape}\\n{input_token_ids}\")\n",
    "            print(f\"target_token_ids: {target_token_ids}\")\n",
    "            print(f\"level: {level}\")\n",
    "            print(f\"model: {model}\")\n",
    "        \n",
    "        if target_token_ids is None: # if we're not training, then we don't need to calculate loss\n",
    "            logits = self.forwardTensor(input_token_ids, level, model)\n",
    "            loss = None\n",
    "        else:\n",
    "            # if we are training\n",
    "            # training uses a tuple of tuples of tensors\n",
    "            logits = self.forwardTuple(input_token_ids, target_token_ids) # -> Tuple[Tuple[Tensor shape (batch_size, max_seq_len, vocab_size)]]\n",
    "            \n",
    "            # custom Fractal CELoss function\n",
    "            loss = self.criterion(logits, target_token_ids) \n",
    "        \n",
    "        if verbose: \n",
    "            print(f\"logits: {logits}\")\n",
    "            print(f\"loss: {loss}\")\n",
    "            print(\"------------- END FractalFormer.forward() ------------\")\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad() # no need to keep track of gradients during inference\n",
    "    def Sampler(\n",
    "        self,\n",
    "        logits: torch.Tensor, # shape (batch_size, input_len, vocab_size)\n",
    "        temperature: float, # controls how boring vs random the outputs should be\n",
    "        top_p: float, # the maximum cumulative probability of output options we're willing to consider\n",
    "        top_k: int, # the maximum number of output options we're willing to consider\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        The Sampler function is responsible for generating token predictions from Gemma's output.\n",
    "        It supports temperature scaling, top-p (nucleus) sampling, and top-k sampling \n",
    "        The class operates as follows:\n",
    "    \n",
    "        1. Selects the last hidden state for each sequence in the batch\n",
    "    \n",
    "        2. Computes logits by multiplying the selected hidden states with the transposed embedding matrix. \n",
    "    \n",
    "        3. Temperature is used to scale the logits, making the distribution over tokens sharper (lower temperature) \n",
    "        or flatter (higher temperature), which affects the randomness of the sampling (flatter -> more random)\n",
    "    \n",
    "        4. The softmax function is applied to the scaled logits to obtain a probability distribution over the vocabulary.\n",
    "    \n",
    "        5. For top-p sampling, the function computes the cumulative sum of the sorted probabilities and masks out tokens until the \n",
    "        cumulative probability exceeds the threshold defined by `top_ps`. This allows the model to focus on a subset of the most \n",
    "        probable tokens while ignoring the long tail of less likely tokens. \n",
    "        We to ignore long tail probabilities to avoid nonsensical output\n",
    "    \n",
    "        7. For top-k sampling, the function masks out all tokens except the `k` most likely ones, as specified by `top_ks`. \n",
    "        This ensures that the model only considers a fixed number of the most probable tokens for the next token prediction.\n",
    "    \n",
    "        8. After applying both the top-p and top-k masks, the probabilities are re-normalized so that they sum up to 1\n",
    "    \n",
    "        9. The function then samples from the re-normalized probability distribution to select the next token. \n",
    "        \"\"\"\n",
    "        if config.verbose['Sampler']:\n",
    "            print(\"----------------- FractalFormer.Sampler() --------------\")\n",
    "            print(f\"temperature: {temperature}, top_p: {top_p}, top_k: {top_k}\")\n",
    "            \n",
    "        # Select the last element for each sequence.\n",
    "        # (batch_size, input_len, vocab_size) -> (batch_size, vocab_size)\n",
    "        logits = logits[:,-1,:]\n",
    "        if config.verbose['Sampler']: print(f\"logits: {logits.shape}\\n{logits}\")\n",
    "        \n",
    "        # Apply temperature scaling\n",
    "        # (batch_size, vocab_size) / float -> (batch_size, vocab_size)\n",
    "        logits.clone().div_(temperature) # the clone() is because i didn't properly prevent gradient tracking and i'm too lazy to fix the issue at its cause\n",
    "        if config.verbose['Sampler']: print(f\"logits w temperature: {logits.shape}\\n{logits}\")\n",
    "\n",
    "        # Calculate probabilities with softmax.\n",
    "        probs = torch.softmax(logits, dim=-1, dtype=torch.float) # dim=-1 is the vocab_size dimension that we calculate along\n",
    "        if config.verbose['Sampler']: print(f\"probs: {probs.shape}\\n{probs}\")\n",
    "\n",
    "        # sort the probabilities to for use in top-p & top-k\n",
    "        # both are (batch_size, vocab_size)\n",
    "        probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n",
    "        # probs_sort contains float probabilities while probs_idx contains integer indices\n",
    "        if config.verbose['Sampler']: \n",
    "            print(f\"probs_sort: {probs_sort.shape}\\n{probs_sort}\")\n",
    "            print(f\"probs_idx: {probs_idx.shape}\\n{probs_idx}\")\n",
    "\n",
    "        # calculating top-p\n",
    "        # creates same-size tensor of cumulatve probabilities instead of indivdiual probs\n",
    "        probs_sum = torch.cumsum(probs_sort, dim=-1) \n",
    "        if config.verbose['Sampler']: print(f\"probs_sum: {probs_sum.shape}\\n{probs_sum}\")\n",
    "        # mask where 0's are top-p selections & 1's are to be excluded\n",
    "        top_ps_mask = (probs_sum - probs_sort) > top_p\n",
    "        if config.verbose['Sampler']: print(f\"top_ps_mask: {top_ps_mask.shape}\\n{top_ps_mask}\")\n",
    "        # the original probabilities with excluded tokens changed to 0.0\n",
    "        probs_sort = torch.where(top_ps_mask, 0, probs_sort) \n",
    "        if config.verbose['Sampler']: print(f\"probs_sort: {probs_sort.shape}\\n{probs_sort}\")\n",
    "\n",
    "        # calculating top_k\n",
    "        # create a shape (vocab_size) tensor that just iterates up by 1's\n",
    "        top_ks_mask = torch.arange(probs_idx.shape[-1], device=probs_idx.device) \n",
    "        if config.verbose['Sampler']: print(f\"top_ks_mask: {top_ks_mask.shape}\\n{top_ks_mask}\")\n",
    "        # expand our mask along the batch_size dimension to become size (batch_size, vocab_size)\n",
    "        # \"expand\" means copy the original into this new size, so each length vocab_size row is the same\n",
    "        top_ks_mask = top_ks_mask.expand(probs_idx.shape[0], -1)\n",
    "        if config.verbose['Sampler']: print(f\"top_ks_mask: {top_ks_mask.shape}\\n{top_ks_mask}\")\n",
    "        # top_ks is a list of integers. we keep whichever entries in top_ks_mask are greater than their corresponding entries in top_ks\n",
    "        top_ks_mask = top_ks_mask >= top_k\n",
    "        if config.verbose['Sampler']: print(f\"top_ks_mask: {top_ks_mask.shape}\\n{top_ks_mask}\")\n",
    "\n",
    "        # we'll be combining top-p with top-k and using whichever gives us fewer tokens. a very conservative approach\n",
    "        # this trims probs_sort to also fit within our top_k requirement\n",
    "        probs_sort = torch.where(top_ks_mask, 0, probs_sort)\n",
    "        if config.verbose['Sampler']: print(f\"probs_sort: {probs_sort.shape}\\n{probs_sort}\")\n",
    "\n",
    "        # Re-normalization so that total probabilities add up to 1\n",
    "        probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n",
    "        if config.verbose['Sampler']: print(f\"probs_sort: {probs_sort.shape}\\n{probs_sort}\")\n",
    "        \n",
    "        # now we rearrange the modified probabilities in probs_sort back to their original order according to probs_idx\n",
    "        probs = torch.gather(probs_sort,\n",
    "                             dim=-1,\n",
    "                             index=torch.argsort(probs_idx, dim=-1))\n",
    "        if config.verbose['Sampler']: print(f\"probs: {probs.shape}\\n{probs}\")\n",
    "        \n",
    "        # samples from the distribution\n",
    "        next_token_id = torch.multinomial(probs, num_samples=1)\n",
    "        if config.verbose['Sampler']: print(f\"next_token_id: {next_token_id.shape}\\n{next_token_id}\")\n",
    "        \n",
    "        return next_token_id # returns the predicted token\n",
    "        \n",
    "    def generate(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        output_len: int = 100, # the model will output 100 tokens\n",
    "        temperature: float = 0.7, # 0.95 is pretty close to not even using temperature at all (1.0 would be no effect)\n",
    "        top_p: float = 1.0, # defaulting to 1 means we essentially don't use top-p\n",
    "        top_k: int = config.vocab_size, # setting top_k = vocab_size means we're effectively not using top_k at all\n",
    "        level: int = 0, # which size model we want to perform inference with\n",
    "        model: int = 0, # which model in that level we want to perform inference with\n",
    "    ) -> str: \n",
    "        \n",
    "        # encoding the prompt into token indices\n",
    "        tokens = self.tokenizer.encode(prompt)\n",
    "\n",
    "        # turning it into the right tensor shape\n",
    "        tokens = torch.tensor(tokens, device=config.device).unsqueeze(0)\n",
    "        \n",
    "        # we wouldn't want to go past the maximum context length we trained on\n",
    "        assert len(tokens) + output_len <= self.config.max_position_embeddings\n",
    "\n",
    "        for i in range(output_len):\n",
    "            # get the model's output logits and ignore the loss, which would be a NoneType object\n",
    "            logits, _ = self(tokens[:,:self.max_seq_len], level=level, model=model)\n",
    "            \n",
    "            next_token = self.Sampler(\n",
    "                logits = logits, # the actual output of the model\n",
    "                temperature = temperature,\n",
    "                top_p = top_p,\n",
    "                top_k = top_k\n",
    "            )\n",
    "            print(next_token)\n",
    "\n",
    "            # add our new token to the sequence\n",
    "            tokens = torch.cat((tokens, next_token), dim=1)\n",
    "\n",
    "        # decode our list of tokens to an actual string\n",
    "        output = self.tokenizer.decode(tokens.squeeze(0).tolist())\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565e80c4",
   "metadata": {},
   "source": [
    "# Training-related Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "866442d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test splits\n",
    "data = torch.tensor(tokenizer.encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be our training dataset, the rest for validation\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ae69c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading for training which generates a small batch of data of inputs x and targets y\n",
    "def get_batch(split, batch_size):\n",
    "    # whether we grab from our training or validation dataset\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - config.max_position_embeddings, (batch_size,))\n",
    "    x = torch.stack([data[i:i+config.max_position_embeddings] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+config.max_position_embeddings+1] for i in ix])\n",
    "    x, y = x.to(config.device), y.to(config.device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2cd7dab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 26,  15,  17,  26,  32,  21,  27,  71,  32, 102,  57,   1,  90,  56,\n",
      "           1,  46,  59,  56,  58,  57,   1, 102,  51,   1,  90,  56,   1,  54,\n",
      "         115,  44,  96,  57,   1,  88,   1,  39,   1,  48,  53,  58,  11,   0,\n",
      "          18,  73,  98,  77,   1,  96,   1,  72,  93, 105,  43, 125,  45,  47,\n",
      "          95,   1,  88,  56,   1,  41,  39,  59,  91,   1,  84,   1,  87,  39,\n",
      "          95,  52,   8,   0,  25,  77,  49,   1,  61,  92,  58,   1,  21,   1,\n",
      "          57, 106,  66,  61, 102, 108,   1,  88,   1,  57,  92,  81,   1,  44,\n",
      "          69,  42,   0,  14,  63,   1,  43,  95,  56,  63,   1,  57,  63,  81,\n",
      "          39,  40,  99,   1,  39,   1,  44,  39,  96,  46,  44,  59,  50,   1,\n",
      "          95,  56,  96,  63,  71,  32,  87,   1,  42,  59, 124,   1,  41,  53,\n",
      "          83,  57,   1,  46,  53,  83,   1,  84,   7,  51,  73, 115,  61, 125,\n",
      "          52, 106,  66,  42,  56,  63,   1,  88,  56,   1,  43,  63,  89,  11,\n",
      "           0,  27,  52,  43,   1,  94,   1, 127,   1,  41,  79,  95,  52,  58,\n",
      "          66,  86,   1, 102,  57,   1,  41,  79,  44,  89,  57,  73,  82,  19,\n",
      "          47,  95,  57,   1,  83,   1,  65,  74,   1,  69,  80,  70, 104,  10,\n",
      "           1,  39,  50,  93,  39,  42,  63,   1,  87,   1,  92,  65,   1,  41,\n",
      "          77,  56,  47, 113,   0,  26,  53,  58,  47, 104,   1,  84,   1,  17,\n",
      "          57,  41,  39,  50,  59,  57,   1,  86,   1,  13,  52,  45,  43,  50,\n",
      "          53,  82,  35,  46]])\n",
      "NCENTIO:\n",
      "This nor hurts him nor profits you a jot;\n",
      "Forbear it therefore; give your cause to heaven.\n",
      "Mark what I say, which you shall find\n",
      "By every syllable a faithful verity:\n",
      "The duke comes home to-morrow; nay, dry your eyes;\n",
      "One of our convent, and his confessor,\n",
      "Gives me this instance: already he hath carried\n",
      "Notice to Escalus and Angelo,\n",
      "Wh\n",
      "-------\n",
      "tensor([[ 15,  17,  26,  32,  21,  27,  71,  32, 102,  57,   1,  90,  56,   1,\n",
      "          46,  59,  56,  58,  57,   1, 102,  51,   1,  90,  56,   1,  54, 115,\n",
      "          44,  96,  57,   1,  88,   1,  39,   1,  48,  53,  58,  11,   0,  18,\n",
      "          73,  98,  77,   1,  96,   1,  72,  93, 105,  43, 125,  45,  47,  95,\n",
      "           1,  88,  56,   1,  41,  39,  59,  91,   1,  84,   1,  87,  39,  95,\n",
      "          52,   8,   0,  25,  77,  49,   1,  61,  92,  58,   1,  21,   1,  57,\n",
      "         106,  66,  61, 102, 108,   1,  88,   1,  57,  92,  81,   1,  44,  69,\n",
      "          42,   0,  14,  63,   1,  43,  95,  56,  63,   1,  57,  63,  81,  39,\n",
      "          40,  99,   1,  39,   1,  44,  39,  96,  46,  44,  59,  50,   1,  95,\n",
      "          56,  96,  63,  71,  32,  87,   1,  42,  59, 124,   1,  41,  53,  83,\n",
      "          57,   1,  46,  53,  83,   1,  84,   7,  51,  73, 115,  61, 125,  52,\n",
      "         106,  66,  42,  56,  63,   1,  88,  56,   1,  43,  63,  89,  11,   0,\n",
      "          27,  52,  43,   1,  94,   1, 127,   1,  41,  79,  95,  52,  58,  66,\n",
      "          86,   1, 102,  57,   1,  41,  79,  44,  89,  57,  73,  82,  19,  47,\n",
      "          95,  57,   1,  83,   1,  65,  74,   1,  69,  80,  70, 104,  10,   1,\n",
      "          39,  50,  93,  39,  42,  63,   1,  87,   1,  92,  65,   1,  41,  77,\n",
      "          56,  47, 113,   0,  26,  53,  58,  47, 104,   1,  84,   1,  17,  57,\n",
      "          41,  39,  50,  59,  57,   1,  86,   1,  13,  52,  45,  43,  50,  53,\n",
      "          82,  35,  46,  53]])\n",
      "CENTIO:\n",
      "This nor hurts him nor profits you a jot;\n",
      "Forbear it therefore; give your cause to heaven.\n",
      "Mark what I say, which you shall find\n",
      "By every syllable a faithful verity:\n",
      "The duke comes home to-morrow; nay, dry your eyes;\n",
      "One of our convent, and his confessor,\n",
      "Gives me this instance: already he hath carried\n",
      "Notice to Escalus and Angelo,\n",
      "Who\n"
     ]
    }
   ],
   "source": [
    "# a demonstration of what a batch with batch_size=1 looks like. Notice the one-token offset in characters\n",
    "xb, yb = get_batch('train', 1)\n",
    "print(xb)\n",
    "print(tokenizer.decode(xb.squeeze(0).tolist()))\n",
    "print(\"-------\")\n",
    "print(yb)\n",
    "print(tokenizer.decode(yb.squeeze(0).tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "01205572",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, batch_size, eval_iters = 10): # to estimate loss during the training loop\n",
    "    out = {}\n",
    "    model.eval() # sets model to eval mode\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split, batch_size)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train() # just resets to training mode\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9254fc45",
   "metadata": {},
   "source": [
    "# Instantiating a brand new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee5ba532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "972.672 K parameters\n",
      "FractalFormer(\n",
      "  (embedder): Embedding(128, 128)\n",
      "  (embedder_norm): RMSNorm()\n",
      "  (layers): ModuleList(\n",
      "    (0-3): 4 x Layer(\n",
      "      (self_attn): MultiQueryAttention(\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (input_layernorm): RMSNorm()\n",
      "      (post_attention_layernorm): RMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (output_layer): OutputLayer(\n",
      "    (embedding_norm): RMSNorm()\n",
      "    (final_norm): RMSNorm()\n",
      "  )\n",
      "  (criterion): FractalLoss(\n",
      "    (criterion): CrossEntropyLoss()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = FractalFormer_base(config, tokenizer).to(config.device)\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e3, 'K parameters')\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c01e3de",
   "metadata": {},
   "source": [
    "# Load a Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6bfcc9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "964.352 K parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "minGemma(\n",
       "  (embedder): Embedding(65, 128)\n",
       "  (model): GemmaBody(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaAttention(\n",
       "          (qkv_proj): Linear(in_features=128, out_features=192, bias=False)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=False)\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (up_proj): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (down_proj): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): RMSNorm()\n",
       "  )\n",
       "  (criterion): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize a blank model\n",
    "model = FractalFormer_base(config, tokenizer).to(config.device)  \n",
    "\n",
    "# here's the path to a minGemma model that i've trained with roughly 1m parameters\n",
    "path = 'models/.pth'\n",
    "\n",
    "# Load the saved state dictionary\n",
    "model.load_state_dict(torch.load(path))\n",
    "# REMEMBER TO CHANGE VALUES IN CONFIG TO MATCH THE MODEL YOU'VE LOADED\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e3, 'K parameters')\n",
    "\n",
    "# If you only plan to do inference, switch to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# If you plan to continue training the model, switch to training mode\n",
    "#model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5afc88",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "709c2430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "# this is not what they used, but this learning rate & weight decay work for our tiny minGemma\n",
    "learning_rate = 3e-4\n",
    "weight_decay = 0.01\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# how long we want to train for\n",
    "max_iters = 10000\n",
    "\n",
    "# how often we want to check & see how our loss is doing\n",
    "eval_interval = 250\n",
    "\n",
    "# batch size to use\n",
    "batch_size = 32#\n",
    "\n",
    "# if you want to do debugging\n",
    "config.verbose['RMSNorm'] = False\n",
    "config.verbose['MLP'] = False\n",
    "config.verbose['MQA'] = False\n",
    "config.verbose['Layer'] = False\n",
    "config.verbose['OutputLayer'] = False\n",
    "config.verbose['FractalLoss'] = False\n",
    "config.verbose['FractalFormer'] = False\n",
    "config.verbose['Sampler'] = True\n",
    "config.verbose['Generate'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fb1d77",
   "metadata": {},
   "source": [
    "# ------------ BOOKMARK ----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "db20fc3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 640.2181, val loss 641.0156, time elapsed: 5.72 seconds\n",
      "step 10: train loss 445.6562, val loss 450.6684, time elapsed: 104.72 seconds\n",
      "step 20: train loss 320.2246, val loss 323.1044, time elapsed: 195.74 seconds\n",
      "step 30: train loss 235.7205, val loss 240.3692, time elapsed: 275.47 seconds\n",
      "step 40: train loss 177.5212, val loss 180.2356, time elapsed: 365.52 seconds\n",
      "step 50: train loss 148.6345, val loss 151.0631, time elapsed: 458.36 seconds\n",
      "step 60: train loss 122.5238, val loss 125.2075, time elapsed: 538.75 seconds\n",
      "step 70: train loss 97.4091, val loss 99.2961, time elapsed: 621.33 seconds\n",
      "step 80: train loss 81.3953, val loss 82.7814, time elapsed: 699.72 seconds\n",
      "step 90: train loss 74.2637, val loss 75.4228, time elapsed: 778.76 seconds\n",
      "step 99: train loss 70.1484, val loss 70.5660, time elapsed: 857.93 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Enable anomaly detection. uncomment these lines if you need to do extensive debugging\n",
    "#torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train', batch_size)\n",
    "    \n",
    "    # train\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        current_time = time.time()\n",
    "        elapsed_time = current_time - start_time\n",
    "        losses = estimate_loss(model, batch_size)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, time elapsed: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Disable anomaly detection after the training loop\n",
    "#torch.autograd.set_detect_anomaly(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3386f092",
   "metadata": {},
   "source": [
    "# Saving your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61635f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model currently held in memory\n",
    "# the filename specifies the model's class, hyperparameters, and date/time it was saved\n",
    "torch.save(model.state_dict(),\n",
    "           f'models/{model.__class__.__name__}'\n",
    "           f'-vocab_size{config.vocab_size}'\n",
    "           f'-max_position_embeddings{config.max_position_embeddings}'\n",
    "           f'-num_hidden_layers{config.num_hidden_layers}'\n",
    "           f'-num_attention_heads{config.num_attention_heads}'\n",
    "           f'-num_key_value_heads{config.num_key_value_heads}'\n",
    "           f'-hidden_size{config.hidden_size}'\n",
    "           f'-intermediate_size{config.intermediate_size}'\n",
    "           f'-head_dim{config.head_dim}'\n",
    "           f'-rms_norm_eps{config.rms_norm_eps}'\n",
    "           f'-rope_theta{config.rope_theta}'\n",
    "            f'-levels{config.levels}'\n",
    "            f'-split{config.split}'\n",
    "           f'--{time.strftime(\"%Y-%m-%d|%H-%M-%S\")}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e2a6cb",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "18529d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- FractalFormer.Sampler() --------------\n",
      "temperature: 0.5, top_p: 0.8, top_k: 5\n",
      "logits: torch.Size([1, 128])\n",
      "tensor([[  8.4632,   9.3730,   7.2354,   3.7065,   5.8596,   7.6923, -12.2389,\n",
      "         -10.4475,   9.1219,   1.8713,  -9.7986,   7.0615,   7.7828,   4.0747,\n",
      "           4.2298, -19.7567,   8.9482,  -7.2145,   7.7869,  -2.8423,  -2.5234,\n",
      "           9.7612, -19.9382, -15.9604,  -2.2166, -11.7809, -10.6705,   8.5758,\n",
      "         -12.0705,  -4.7288,   9.5486,  -0.5235,   0.4926,   5.6694,  -5.6895,\n",
      "           2.1589,   3.6490,  -8.7978,   7.1795,  10.9023,  10.2642,   9.3096,\n",
      "           9.1435,   9.3280,   9.8984,  10.7102,   9.1300,   9.1159, -24.8913,\n",
      "           9.2140,   9.7885,   9.8180,   8.8303,   9.7628,   9.7268,  -9.8703,\n",
      "          10.0528,  10.4413,   9.7633,   2.2645,   4.1024,   9.5707,   6.4610,\n",
      "           8.2783,  -5.2987,  10.0392,   8.7876,   9.0320,   4.5639,   9.0329,\n",
      "           2.6898,   4.8141,   9.7006,   7.9726,   3.7522,   8.3538,   8.1183,\n",
      "           9.9042,   8.0901,   7.9974,   9.2473,   8.9077,   8.1196,  -0.2450,\n",
      "          -2.3305,   9.4770,  -0.8647,  10.1330,   8.7895,   8.1872,   8.7766,\n",
      "           8.7701,   9.3764,  10.4870,   9.4014,   8.5298,  -1.5715,   7.2749,\n",
      "          -9.2057, -10.6783,   9.7342,   8.8884,   7.9047,   8.7138,   9.0588,\n",
      "           8.8620,   2.8524,   8.6769,   9.5168,  -8.3437,   7.3653,  -1.0022,\n",
      "           9.1290,   0.2944,   5.5666,   2.5569,  -0.4830,   7.8492,  -5.0336,\n",
      "           2.5126,   0.7207,   5.5780,   6.8390,   7.9779,   8.7838, -12.5675,\n",
      "           9.8041,   8.6540]], requires_grad=True)\n",
      "logits w temperature: torch.Size([1, 128])\n",
      "tensor([[  8.4632,   9.3730,   7.2354,   3.7065,   5.8596,   7.6923, -12.2389,\n",
      "         -10.4475,   9.1219,   1.8713,  -9.7986,   7.0615,   7.7828,   4.0747,\n",
      "           4.2298, -19.7567,   8.9482,  -7.2145,   7.7869,  -2.8423,  -2.5234,\n",
      "           9.7612, -19.9382, -15.9604,  -2.2166, -11.7809, -10.6705,   8.5758,\n",
      "         -12.0705,  -4.7288,   9.5486,  -0.5235,   0.4926,   5.6694,  -5.6895,\n",
      "           2.1589,   3.6490,  -8.7978,   7.1795,  10.9023,  10.2642,   9.3096,\n",
      "           9.1435,   9.3280,   9.8984,  10.7102,   9.1300,   9.1159, -24.8913,\n",
      "           9.2140,   9.7885,   9.8180,   8.8303,   9.7628,   9.7268,  -9.8703,\n",
      "          10.0528,  10.4413,   9.7633,   2.2645,   4.1024,   9.5707,   6.4610,\n",
      "           8.2783,  -5.2987,  10.0392,   8.7876,   9.0320,   4.5639,   9.0329,\n",
      "           2.6898,   4.8141,   9.7006,   7.9726,   3.7522,   8.3538,   8.1183,\n",
      "           9.9042,   8.0901,   7.9974,   9.2473,   8.9077,   8.1196,  -0.2450,\n",
      "          -2.3305,   9.4770,  -0.8647,  10.1330,   8.7895,   8.1872,   8.7766,\n",
      "           8.7701,   9.3764,  10.4870,   9.4014,   8.5298,  -1.5715,   7.2749,\n",
      "          -9.2057, -10.6783,   9.7342,   8.8884,   7.9047,   8.7138,   9.0588,\n",
      "           8.8620,   2.8524,   8.6769,   9.5168,  -8.3437,   7.3653,  -1.0022,\n",
      "           9.1290,   0.2944,   5.5666,   2.5569,  -0.4830,   7.8492,  -5.0336,\n",
      "           2.5126,   0.7207,   5.5780,   6.8390,   7.9779,   8.7838, -12.5675,\n",
      "           9.8041,   8.6540]], requires_grad=True)\n",
      "probs: torch.Size([1, 128])\n",
      "tensor([[5.7428e-03, 1.4264e-02, 1.6824e-03, 4.9352e-05, 4.2500e-04, 2.6567e-03,\n",
      "         5.8657e-12, 3.5181e-11, 1.1097e-02, 7.8760e-06, 6.7320e-11, 1.4137e-03,\n",
      "         2.9082e-03, 7.1321e-05, 8.3290e-05, 3.1870e-15, 9.3272e-03, 8.9201e-10,\n",
      "         2.9202e-03, 7.0664e-08, 9.7205e-08, 2.1029e-02, 2.6579e-15, 1.4194e-13,\n",
      "         1.3212e-07, 9.2726e-12, 2.8150e-11, 6.4273e-03, 6.9415e-12, 1.0713e-08,\n",
      "         1.7003e-02, 7.1821e-07, 1.9839e-06, 3.5138e-04, 4.0992e-09, 1.0501e-05,\n",
      "         4.6597e-05, 1.8314e-10, 1.5907e-03, 6.5831e-02, 3.4777e-02, 1.3388e-02,\n",
      "         1.1339e-02, 1.3637e-02, 2.4122e-02, 5.4320e-02, 1.1187e-02, 1.1030e-02,\n",
      "         1.8769e-17, 1.2167e-02, 2.1611e-02, 2.2260e-02, 8.2896e-03, 2.1063e-02,\n",
      "         2.0320e-02, 6.2656e-11, 2.8150e-02, 4.1513e-02, 2.1075e-02, 1.1670e-05,\n",
      "         7.3329e-05, 1.7383e-02, 7.7548e-04, 4.7731e-03, 6.0591e-09, 2.7771e-02,\n",
      "         7.9436e-03, 1.0143e-02, 1.1633e-04, 1.0151e-02, 1.7856e-05, 1.4940e-04,\n",
      "         1.9793e-02, 3.5160e-03, 5.1659e-05, 5.1475e-03, 4.0677e-03, 2.4262e-02,\n",
      "         3.9545e-03, 3.6045e-03, 1.2579e-02, 8.9570e-03, 4.0730e-03, 9.4884e-07,\n",
      "         1.1789e-07, 1.5827e-02, 5.1060e-07, 3.0501e-02, 7.9588e-03, 4.3577e-03,\n",
      "         7.8566e-03, 7.8053e-03, 1.4312e-02, 4.3458e-02, 1.4675e-02, 6.1386e-03,\n",
      "         2.5182e-07, 1.7500e-03, 1.2180e-10, 2.7931e-11, 2.0471e-02, 8.7862e-03,\n",
      "         3.2854e-03, 7.3782e-03, 1.0418e-02, 8.5570e-03, 2.1009e-05, 7.1114e-03,\n",
      "         1.6470e-02, 2.8840e-10, 1.9156e-03, 4.4501e-07, 1.1176e-02, 1.6273e-06,\n",
      "         3.1707e-04, 1.5634e-05, 7.4787e-07, 3.1079e-03, 7.8988e-09, 1.4956e-05,\n",
      "         2.4923e-06, 3.2071e-04, 1.1318e-03, 3.5348e-03, 7.9136e-03, 4.2229e-12,\n",
      "         2.1951e-02, 6.9502e-03]])\n",
      "probs_sort: torch.Size([1, 128])\n",
      "tensor([[6.5831e-02, 5.4320e-02, 4.3458e-02, 4.1513e-02, 3.4777e-02, 3.0501e-02,\n",
      "         2.8150e-02, 2.7771e-02, 2.4262e-02, 2.4122e-02, 2.2260e-02, 2.1951e-02,\n",
      "         2.1611e-02, 2.1075e-02, 2.1063e-02, 2.1029e-02, 2.0471e-02, 2.0320e-02,\n",
      "         1.9793e-02, 1.7383e-02, 1.7003e-02, 1.6470e-02, 1.5827e-02, 1.4675e-02,\n",
      "         1.4312e-02, 1.4264e-02, 1.3637e-02, 1.3388e-02, 1.2579e-02, 1.2167e-02,\n",
      "         1.1339e-02, 1.1187e-02, 1.1176e-02, 1.1097e-02, 1.1030e-02, 1.0418e-02,\n",
      "         1.0151e-02, 1.0143e-02, 9.3272e-03, 8.9570e-03, 8.7862e-03, 8.5570e-03,\n",
      "         8.2896e-03, 7.9588e-03, 7.9436e-03, 7.9136e-03, 7.8566e-03, 7.8053e-03,\n",
      "         7.3782e-03, 7.1114e-03, 6.9502e-03, 6.4273e-03, 6.1386e-03, 5.7428e-03,\n",
      "         5.1475e-03, 4.7731e-03, 4.3577e-03, 4.0730e-03, 4.0677e-03, 3.9545e-03,\n",
      "         3.6045e-03, 3.5348e-03, 3.5160e-03, 3.2854e-03, 3.1079e-03, 2.9202e-03,\n",
      "         2.9082e-03, 2.6567e-03, 1.9156e-03, 1.7500e-03, 1.6824e-03, 1.5907e-03,\n",
      "         1.4137e-03, 1.1318e-03, 7.7548e-04, 4.2500e-04, 3.5138e-04, 3.2071e-04,\n",
      "         3.1707e-04, 1.4940e-04, 1.1633e-04, 8.3290e-05, 7.3329e-05, 7.1321e-05,\n",
      "         5.1659e-05, 4.9352e-05, 4.6597e-05, 2.1009e-05, 1.7856e-05, 1.5634e-05,\n",
      "         1.4956e-05, 1.1670e-05, 1.0501e-05, 7.8760e-06, 2.4923e-06, 1.9839e-06,\n",
      "         1.6273e-06, 9.4884e-07, 7.4787e-07, 7.1821e-07, 5.1060e-07, 4.4501e-07,\n",
      "         2.5182e-07, 1.3212e-07, 1.1789e-07, 9.7205e-08, 7.0664e-08, 1.0713e-08,\n",
      "         7.8988e-09, 6.0591e-09, 4.0992e-09, 8.9201e-10, 2.8840e-10, 1.8314e-10,\n",
      "         1.2180e-10, 6.7320e-11, 6.2656e-11, 3.5181e-11, 2.8150e-11, 2.7931e-11,\n",
      "         9.2726e-12, 6.9415e-12, 5.8657e-12, 4.2229e-12, 1.4194e-13, 3.1870e-15,\n",
      "         2.6579e-15, 1.8769e-17]])\n",
      "probs_idx: torch.Size([1, 128])\n",
      "tensor([[ 39,  45,  93,  57,  40,  87,  56,  65,  77,  44,  51, 126,  50,  58,\n",
      "          53,  21, 100,  54,  72,  61,  30, 108,  85,  94,  92,   1,  43,  41,\n",
      "          80,  49,  42,  46, 112,   8,  47, 104,  69,  67,  16,  81, 101, 105,\n",
      "          52,  88,  66, 124,  90,  91, 103, 107, 127,  27,  95,   0,  75,  63,\n",
      "          89,  82,  76,  78,  79, 123,  73, 102, 117,  18,  12,   5, 110,  97,\n",
      "           2,  38,  11, 122,  62,   4,  33, 121, 114,  71,  68,  14,  60,  13,\n",
      "          74,   3,  36, 106,  70, 115, 119,  59,  35,   9, 120,  32, 113,  83,\n",
      "         116,  31,  86, 111,  96,  24,  84,  20,  19,  29, 118,  64,  34,  17,\n",
      "         109,  37,  98,  10,  55,   7,  26,  99,  25,  28,   6, 125,  23,  15,\n",
      "          22,  48]])\n",
      "probs_sum: torch.Size([1, 128])\n",
      "tensor([[0.0658, 0.1202, 0.1636, 0.2051, 0.2399, 0.2704, 0.2986, 0.3263, 0.3506,\n",
      "         0.3747, 0.3970, 0.4189, 0.4405, 0.4616, 0.4827, 0.5037, 0.5242, 0.5445,\n",
      "         0.5643, 0.5817, 0.5987, 0.6151, 0.6310, 0.6456, 0.6599, 0.6742, 0.6878,\n",
      "         0.7012, 0.7138, 0.7260, 0.7373, 0.7485, 0.7597, 0.7708, 0.7818, 0.7922,\n",
      "         0.8024, 0.8125, 0.8219, 0.8308, 0.8396, 0.8482, 0.8564, 0.8644, 0.8723,\n",
      "         0.8803, 0.8881, 0.8959, 0.9033, 0.9104, 0.9174, 0.9238, 0.9299, 0.9357,\n",
      "         0.9408, 0.9456, 0.9499, 0.9540, 0.9581, 0.9620, 0.9656, 0.9692, 0.9727,\n",
      "         0.9760, 0.9791, 0.9820, 0.9849, 0.9876, 0.9895, 0.9912, 0.9929, 0.9945,\n",
      "         0.9959, 0.9971, 0.9978, 0.9983, 0.9986, 0.9989, 0.9992, 0.9994, 0.9995,\n",
      "         0.9996, 0.9997, 0.9997, 0.9998, 0.9998, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000]])\n",
      "top_ps_mask: torch.Size([1, 128])\n",
      "tensor([[False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True]])\n",
      "probs_sort: torch.Size([1, 128])\n",
      "tensor([[0.0658, 0.0543, 0.0435, 0.0415, 0.0348, 0.0305, 0.0281, 0.0278, 0.0243,\n",
      "         0.0241, 0.0223, 0.0220, 0.0216, 0.0211, 0.0211, 0.0210, 0.0205, 0.0203,\n",
      "         0.0198, 0.0174, 0.0170, 0.0165, 0.0158, 0.0147, 0.0143, 0.0143, 0.0136,\n",
      "         0.0134, 0.0126, 0.0122, 0.0113, 0.0112, 0.0112, 0.0111, 0.0110, 0.0104,\n",
      "         0.0102, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000]])\n",
      "top_ks_mask: torch.Size([128])\n",
      "tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127])\n",
      "top_ks_mask: torch.Size([1, 128])\n",
      "tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "         126, 127]])\n",
      "top_ks_mask: torch.Size([1, 128])\n",
      "tensor([[False, False, False, False, False,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True]])\n",
      "probs_sort: torch.Size([1, 128])\n",
      "tensor([[0.0658, 0.0543, 0.0435, 0.0415, 0.0348, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000]])\n",
      "probs_sort: torch.Size([1, 128])\n",
      "tensor([[0.2744, 0.2264, 0.1812, 0.1730, 0.1450, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000]])\n",
      "probs: torch.Size([1, 128])\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.2744, 0.1450, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.2264, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.1730, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.1812, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000]])\n",
      "next_token_id: torch.Size([1, 1])\n",
      "tensor([[93]])\n",
      "tensor([[93]])\n",
      "----------------- FractalFormer.Sampler() --------------\n",
      "temperature: 0.5, top_p: 0.8, top_k: 5\n",
      "logits: torch.Size([1, 128])\n",
      "tensor([[  9.6693,  12.3050,   7.8162,   4.3359,   6.1799,   7.8276, -10.5183,\n",
      "          -9.9660,   9.4245,   2.4784, -11.3493,   6.6677,   7.7094,   4.5402,\n",
      "           5.2740, -19.6818,   9.3740,  -8.6170,   7.5427,  -2.5119,  -3.2068,\n",
      "           8.2714, -18.2616, -16.1391,  -2.1694, -12.4589, -10.3000,   8.7770,\n",
      "         -14.2597,  -4.4706,   9.7419,   1.5092,   0.8910,   5.3498,  -5.4288,\n",
      "           2.7254,   4.3991,  -9.3266,   6.9716,  10.5831,   9.1881,   8.9154,\n",
      "           9.4397,   9.7929,   9.3858,   9.2613,   9.5046,   9.7957, -25.8407,\n",
      "           8.4334,   9.2408,   9.9878,   8.7733,  10.3222,   8.8025, -10.5067,\n",
      "           9.1663,  10.5799,  10.7952,   2.5645,   3.6149,   8.3903,   6.1385,\n",
      "           9.4845,  -4.8137,   9.4866,   9.0392,   8.1203,   4.2491,   9.1644,\n",
      "           3.0956,   5.6194,   8.5529,   7.9301,   3.9524,   8.7423,   7.8045,\n",
      "           8.6850,   9.4844,   8.5436,   8.7125,   9.0445,   8.0684,   0.4782,\n",
      "          -4.5355,   9.3981,  -1.6692,   9.1145,   8.4677,   8.1643,   8.5262,\n",
      "           8.8596,   8.8247,  13.4447,   7.4851,   8.6237,  -1.4269,   7.2360,\n",
      "         -11.0635,  -9.7367,   9.3559,   7.1706,   8.5502,   9.3638,   8.2004,\n",
      "           8.5446,   3.8562,   8.7059,   8.6084,  -8.9653,   7.7313,  -0.9161,\n",
      "           8.8973,   0.5338,   5.8332,   3.5902,  -0.4893,   8.3550,  -4.7076,\n",
      "           2.0032,   1.5624,   5.6788,   6.8665,   7.7738,   7.1770, -13.1662,\n",
      "           8.7607,   8.9138]], requires_grad=True)\n",
      "logits w temperature: torch.Size([1, 128])\n",
      "tensor([[  9.6693,  12.3050,   7.8162,   4.3359,   6.1799,   7.8276, -10.5183,\n",
      "          -9.9660,   9.4245,   2.4784, -11.3493,   6.6677,   7.7094,   4.5402,\n",
      "           5.2740, -19.6818,   9.3740,  -8.6170,   7.5427,  -2.5119,  -3.2068,\n",
      "           8.2714, -18.2616, -16.1391,  -2.1694, -12.4589, -10.3000,   8.7770,\n",
      "         -14.2597,  -4.4706,   9.7419,   1.5092,   0.8910,   5.3498,  -5.4288,\n",
      "           2.7254,   4.3991,  -9.3266,   6.9716,  10.5831,   9.1881,   8.9154,\n",
      "           9.4397,   9.7929,   9.3858,   9.2613,   9.5046,   9.7957, -25.8407,\n",
      "           8.4334,   9.2408,   9.9878,   8.7733,  10.3222,   8.8025, -10.5067,\n",
      "           9.1663,  10.5799,  10.7952,   2.5645,   3.6149,   8.3903,   6.1385,\n",
      "           9.4845,  -4.8137,   9.4866,   9.0392,   8.1203,   4.2491,   9.1644,\n",
      "           3.0956,   5.6194,   8.5529,   7.9301,   3.9524,   8.7423,   7.8045,\n",
      "           8.6850,   9.4844,   8.5436,   8.7125,   9.0445,   8.0684,   0.4782,\n",
      "          -4.5355,   9.3981,  -1.6692,   9.1145,   8.4677,   8.1643,   8.5262,\n",
      "           8.8596,   8.8247,  13.4447,   7.4851,   8.6237,  -1.4269,   7.2360,\n",
      "         -11.0635,  -9.7367,   9.3559,   7.1706,   8.5502,   9.3638,   8.2004,\n",
      "           8.5446,   3.8562,   8.7059,   8.6084,  -8.9653,   7.7313,  -0.9161,\n",
      "           8.8973,   0.5338,   5.8332,   3.5902,  -0.4893,   8.3550,  -4.7076,\n",
      "           2.0032,   1.5624,   5.6788,   6.8665,   7.7738,   7.1770, -13.1662,\n",
      "           8.7607,   8.9138]], requires_grad=True)\n",
      "probs: torch.Size([1, 128])\n",
      "tensor([[1.0137e-02, 1.4144e-01, 1.5889e-03, 4.8936e-05, 3.0935e-04, 1.6072e-03,\n",
      "         1.7320e-11, 3.0089e-11, 7.9356e-03, 7.6369e-06, 7.5445e-12, 5.0389e-04,\n",
      "         1.4279e-03, 6.0031e-05, 1.2503e-04, 1.8151e-15, 7.5445e-03, 1.1595e-10,\n",
      "         1.2087e-03, 5.1962e-08, 2.5935e-08, 2.5050e-03, 7.5100e-15, 6.2729e-14,\n",
      "         7.3185e-08, 2.4874e-12, 2.1544e-11, 4.1532e-03, 4.1081e-13, 7.3282e-09,\n",
      "         1.0900e-02, 2.8973e-06, 1.5614e-06, 1.3488e-04, 2.8110e-09, 9.7771e-06,\n",
      "         5.2131e-05, 5.7026e-11, 6.8283e-04, 2.5278e-02, 6.2646e-03, 4.7697e-03,\n",
      "         8.0575e-03, 1.1470e-02, 7.6347e-03, 6.7406e-03, 8.5976e-03, 1.1503e-02,\n",
      "         3.8381e-18, 2.9455e-03, 6.6039e-03, 1.3939e-02, 4.1379e-03, 1.9475e-02,\n",
      "         4.2604e-03, 1.7521e-11, 6.1295e-03, 2.5197e-02, 3.1251e-02, 8.3235e-06,\n",
      "         2.3795e-05, 2.8212e-03, 2.9682e-04, 8.4262e-03, 5.2001e-09, 8.4444e-03,\n",
      "         5.3982e-03, 2.1537e-03, 4.4866e-05, 6.1182e-03, 1.4157e-05, 1.7662e-04,\n",
      "         3.3192e-03, 1.7807e-03, 3.3350e-05, 4.0117e-03, 1.5705e-03, 3.7882e-03,\n",
      "         8.4254e-03, 3.2886e-03, 3.8936e-03, 5.4271e-03, 2.0448e-03, 1.0334e-06,\n",
      "         6.8677e-09, 7.7292e-03, 1.2068e-07, 5.8206e-03, 3.0483e-03, 2.2505e-03,\n",
      "         3.2318e-03, 4.5106e-03, 4.3562e-03, 4.4212e-01, 1.1411e-03, 3.5630e-03,\n",
      "         1.5377e-07, 8.8948e-04, 1.0040e-11, 3.7843e-11, 7.4093e-03, 8.3311e-04,\n",
      "         3.3103e-03, 7.4684e-03, 2.3332e-03, 3.2921e-03, 3.0290e-05, 3.8681e-03,\n",
      "         3.5086e-03, 8.1844e-11, 1.4596e-03, 2.5629e-07, 4.6838e-03, 1.0925e-06,\n",
      "         2.1872e-04, 2.3215e-05, 3.9269e-07, 2.7233e-03, 5.7824e-09, 4.7485e-06,\n",
      "         3.0556e-06, 1.8743e-04, 6.1470e-04, 1.5230e-03, 8.3853e-04, 1.2263e-12,\n",
      "         4.0861e-03, 4.7620e-03]])\n",
      "probs_sort: torch.Size([1, 128])\n",
      "tensor([[4.4212e-01, 1.4144e-01, 3.1251e-02, 2.5278e-02, 2.5197e-02, 1.9475e-02,\n",
      "         1.3939e-02, 1.1503e-02, 1.1470e-02, 1.0900e-02, 1.0137e-02, 8.5976e-03,\n",
      "         8.4444e-03, 8.4262e-03, 8.4254e-03, 8.0575e-03, 7.9356e-03, 7.7292e-03,\n",
      "         7.6347e-03, 7.5445e-03, 7.4684e-03, 7.4093e-03, 6.7406e-03, 6.6039e-03,\n",
      "         6.2646e-03, 6.1295e-03, 6.1182e-03, 5.8206e-03, 5.4271e-03, 5.3982e-03,\n",
      "         4.7697e-03, 4.7620e-03, 4.6838e-03, 4.5106e-03, 4.3562e-03, 4.2604e-03,\n",
      "         4.1532e-03, 4.1379e-03, 4.0861e-03, 4.0117e-03, 3.8936e-03, 3.8681e-03,\n",
      "         3.7882e-03, 3.5630e-03, 3.5086e-03, 3.3192e-03, 3.3103e-03, 3.2921e-03,\n",
      "         3.2886e-03, 3.2318e-03, 3.0483e-03, 2.9455e-03, 2.8212e-03, 2.7233e-03,\n",
      "         2.5050e-03, 2.3332e-03, 2.2505e-03, 2.1537e-03, 2.0448e-03, 1.7807e-03,\n",
      "         1.6072e-03, 1.5889e-03, 1.5705e-03, 1.5230e-03, 1.4596e-03, 1.4279e-03,\n",
      "         1.2087e-03, 1.1411e-03, 8.8948e-04, 8.3853e-04, 8.3311e-04, 6.8283e-04,\n",
      "         6.1470e-04, 5.0389e-04, 3.0935e-04, 2.9682e-04, 2.1872e-04, 1.8743e-04,\n",
      "         1.7662e-04, 1.3488e-04, 1.2503e-04, 6.0031e-05, 5.2131e-05, 4.8936e-05,\n",
      "         4.4866e-05, 3.3350e-05, 3.0290e-05, 2.3795e-05, 2.3215e-05, 1.4157e-05,\n",
      "         9.7771e-06, 8.3235e-06, 7.6369e-06, 4.7485e-06, 3.0556e-06, 2.8973e-06,\n",
      "         1.5614e-06, 1.0925e-06, 1.0334e-06, 3.9269e-07, 2.5629e-07, 1.5377e-07,\n",
      "         1.2068e-07, 7.3185e-08, 5.1962e-08, 2.5935e-08, 7.3282e-09, 6.8677e-09,\n",
      "         5.7824e-09, 5.2001e-09, 2.8110e-09, 1.1595e-10, 8.1844e-11, 5.7026e-11,\n",
      "         3.7843e-11, 3.0089e-11, 2.1544e-11, 1.7521e-11, 1.7320e-11, 1.0040e-11,\n",
      "         7.5445e-12, 2.4874e-12, 1.2263e-12, 4.1081e-13, 6.2729e-14, 7.5100e-15,\n",
      "         1.8151e-15, 3.8381e-18]])\n",
      "probs_idx: torch.Size([1, 128])\n",
      "tensor([[ 93,   1,  58,  39,  57,  53,  51,  47,  43,  30,   0,  46,  65,  63,\n",
      "          78,  42,   8,  85,  44,  16, 103, 100,  45,  50,  40,  56,  69,  87,\n",
      "          81,  66,  41, 127, 112,  91,  92,  54,  27,  52, 126,  75,  80, 107,\n",
      "          77,  95, 108,  72, 102, 105,  79,  90,  88,  49,  61, 117,  21, 104,\n",
      "          89,  67,  82,  73,   5,   2,  76, 123, 110,  12,  18,  94,  97, 124,\n",
      "         101,  38, 122,  11,   4,  62, 114, 121,  71,  33,  14,  13,  36,   3,\n",
      "          68,  74, 106,  60, 115,  70,  35,  59,   9, 119, 120,  31,  32, 113,\n",
      "          83, 116, 111,  96,  86,  24,  19,  20,  29,  84, 118,  64,  34,  17,\n",
      "         109,  37,  99,   7,  26,  55,   6,  98,  10,  25, 125,  28,  23,  22,\n",
      "          15,  48]])\n",
      "probs_sum: torch.Size([1, 128])\n",
      "tensor([[0.4421, 0.5836, 0.6148, 0.6401, 0.6653, 0.6848, 0.6987, 0.7102, 0.7217,\n",
      "         0.7326, 0.7427, 0.7513, 0.7598, 0.7682, 0.7766, 0.7847, 0.7926, 0.8003,\n",
      "         0.8080, 0.8155, 0.8230, 0.8304, 0.8371, 0.8437, 0.8500, 0.8561, 0.8622,\n",
      "         0.8681, 0.8735, 0.8789, 0.8837, 0.8884, 0.8931, 0.8976, 0.9020, 0.9062,\n",
      "         0.9104, 0.9145, 0.9186, 0.9226, 0.9265, 0.9304, 0.9342, 0.9377, 0.9412,\n",
      "         0.9446, 0.9479, 0.9512, 0.9545, 0.9577, 0.9607, 0.9637, 0.9665, 0.9692,\n",
      "         0.9717, 0.9741, 0.9763, 0.9785, 0.9805, 0.9823, 0.9839, 0.9855, 0.9871,\n",
      "         0.9886, 0.9900, 0.9915, 0.9927, 0.9938, 0.9947, 0.9955, 0.9964, 0.9971,\n",
      "         0.9977, 0.9982, 0.9985, 0.9988, 0.9990, 0.9992, 0.9994, 0.9995, 0.9996,\n",
      "         0.9997, 0.9997, 0.9998, 0.9998, 0.9999, 0.9999, 0.9999, 0.9999, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000]])\n",
      "top_ps_mask: torch.Size([1, 128])\n",
      "tensor([[False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True]])\n",
      "probs_sort: torch.Size([1, 128])\n",
      "tensor([[0.4421, 0.1414, 0.0313, 0.0253, 0.0252, 0.0195, 0.0139, 0.0115, 0.0115,\n",
      "         0.0109, 0.0101, 0.0086, 0.0084, 0.0084, 0.0084, 0.0081, 0.0079, 0.0077,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000]])\n",
      "top_ks_mask: torch.Size([128])\n",
      "tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127])\n",
      "top_ks_mask: torch.Size([1, 128])\n",
      "tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "         126, 127]])\n",
      "top_ks_mask: torch.Size([1, 128])\n",
      "tensor([[False, False, False, False, False,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True]])\n",
      "probs_sort: torch.Size([1, 128])\n",
      "tensor([[0.4421, 0.1414, 0.0313, 0.0253, 0.0252, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000]])\n",
      "probs_sort: torch.Size([1, 128])\n",
      "tensor([[0.6646, 0.2126, 0.0470, 0.0380, 0.0379, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000]])\n",
      "probs: torch.Size([1, 128])\n",
      "tensor([[0.0000, 0.2126, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0380, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0379, 0.0470, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.6646, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000]])\n",
      "next_token_id: torch.Size([1, 1])\n",
      "tensor([[1]])\n",
      "tensor([[1]])\n",
      "----------------- FractalFormer.Sampler() --------------\n",
      "temperature: 0.5, top_p: 0.8, top_k: 5\n",
      "logits: torch.Size([1, 128])\n",
      "tensor([[  8.4715,   9.3291,   7.2329,   3.6941,   5.8468,   7.6798, -12.2401,\n",
      "         -10.4234,   9.1291,   1.8670,  -9.7873,   7.0723,   7.7727,   4.0809,\n",
      "           4.2158, -19.7676,   8.9462,  -7.2163,   7.7442,  -2.8171,  -2.5129,\n",
      "           9.7559, -19.9318, -15.9721,  -2.2170, -11.7696, -10.6717,   8.5713,\n",
      "         -12.0638,  -4.7353,   9.5315,  -0.5475,   0.5104,   5.6757,  -5.7075,\n",
      "           2.1571,   3.6509,  -8.8342,   7.1765,  10.8974,  10.2562,   9.3012,\n",
      "           9.1329,   9.2955,   9.9081,  10.7350,   9.1484,   9.1146, -24.8870,\n",
      "           9.1902,   9.8001,   9.7903,   8.8497,   9.7558,   9.7299,  -9.8513,\n",
      "          10.0391,  10.4359,   9.7569,   2.2762,   4.1229,   9.5618,   6.4676,\n",
      "           8.2918,  -5.3017,  10.0512,   8.7757,   9.0638,   4.5695,   9.0255,\n",
      "           2.6836,   4.7912,   9.6915,   7.9884,   3.7396,   8.3767,   8.1192,\n",
      "           9.8985,   8.0895,   7.9677,   9.2321,   8.8728,   8.1273,  -0.2612,\n",
      "          -2.3189,   9.4835,  -0.8315,  10.1298,   8.7939,   8.1835,   8.7741,\n",
      "           8.7693,   9.3816,  10.5233,   9.4161,   8.5228,  -1.5601,   7.2612,\n",
      "          -9.1759, -10.6974,   9.7322,   8.8779,   7.8779,   8.7112,   9.0455,\n",
      "           8.8684,   2.8295,   8.6773,   9.4846,  -8.3272,   7.3342,  -1.0246,\n",
      "           9.1050,   0.2960,   5.5244,   2.5560,  -0.4843,   7.8534,  -5.0235,\n",
      "           2.5213,   0.7156,   5.5349,   6.8475,   7.9831,   8.7760, -12.5875,\n",
      "           9.8039,   8.6552]], requires_grad=True)\n",
      "logits w temperature: torch.Size([1, 128])\n",
      "tensor([[  8.4715,   9.3291,   7.2329,   3.6941,   5.8468,   7.6798, -12.2401,\n",
      "         -10.4234,   9.1291,   1.8670,  -9.7873,   7.0723,   7.7727,   4.0809,\n",
      "           4.2158, -19.7676,   8.9462,  -7.2163,   7.7442,  -2.8171,  -2.5129,\n",
      "           9.7559, -19.9318, -15.9721,  -2.2170, -11.7696, -10.6717,   8.5713,\n",
      "         -12.0638,  -4.7353,   9.5315,  -0.5475,   0.5104,   5.6757,  -5.7075,\n",
      "           2.1571,   3.6509,  -8.8342,   7.1765,  10.8974,  10.2562,   9.3012,\n",
      "           9.1329,   9.2955,   9.9081,  10.7350,   9.1484,   9.1146, -24.8870,\n",
      "           9.1902,   9.8001,   9.7903,   8.8497,   9.7558,   9.7299,  -9.8513,\n",
      "          10.0391,  10.4359,   9.7569,   2.2762,   4.1229,   9.5618,   6.4676,\n",
      "           8.2918,  -5.3017,  10.0512,   8.7757,   9.0638,   4.5695,   9.0255,\n",
      "           2.6836,   4.7912,   9.6915,   7.9884,   3.7396,   8.3767,   8.1192,\n",
      "           9.8985,   8.0895,   7.9677,   9.2321,   8.8728,   8.1273,  -0.2612,\n",
      "          -2.3189,   9.4835,  -0.8315,  10.1298,   8.7939,   8.1835,   8.7741,\n",
      "           8.7693,   9.3816,  10.5233,   9.4161,   8.5228,  -1.5601,   7.2612,\n",
      "          -9.1759, -10.6974,   9.7322,   8.8779,   7.8779,   8.7112,   9.0455,\n",
      "           8.8684,   2.8295,   8.6773,   9.4846,  -8.3272,   7.3342,  -1.0246,\n",
      "           9.1050,   0.2960,   5.5244,   2.5560,  -0.4843,   7.8534,  -5.0235,\n",
      "           2.5213,   0.7156,   5.5349,   6.8475,   7.9831,   8.7760, -12.5875,\n",
      "           9.8039,   8.6552]], requires_grad=True)\n",
      "probs: torch.Size([1, 128])\n",
      "tensor([[5.7996e-03, 1.3673e-02, 1.6806e-03, 4.8823e-05, 4.2025e-04, 2.6278e-03,\n",
      "         5.8680e-12, 3.6095e-11, 1.1194e-02, 7.8541e-06, 6.8188e-11, 1.4313e-03,\n",
      "         2.8837e-03, 7.1877e-05, 8.2262e-05, 3.1573e-15, 9.3237e-03, 8.9183e-10,\n",
      "         2.8025e-03, 7.2580e-08, 9.8389e-08, 2.0952e-02, 2.6793e-15, 1.4051e-13,\n",
      "         1.3227e-07, 9.3929e-12, 2.8160e-11, 6.4085e-03, 6.9990e-12, 1.0661e-08,\n",
      "         1.6741e-02, 7.0225e-07, 2.0228e-06, 3.5418e-04, 4.0324e-09, 1.0498e-05,\n",
      "         4.6758e-05, 1.7686e-10, 1.5886e-03, 6.5613e-02, 3.4554e-02, 1.3296e-02,\n",
      "         1.1237e-02, 1.3221e-02, 2.4396e-02, 5.5776e-02, 1.1412e-02, 1.1034e-02,\n",
      "         1.8880e-17, 1.1899e-02, 2.1898e-02, 2.1686e-02, 8.4659e-03, 2.0950e-02,\n",
      "         2.0413e-02, 6.3964e-11, 2.7812e-02, 4.1354e-02, 2.0972e-02, 1.1826e-05,\n",
      "         7.4964e-05, 1.7256e-02, 7.8188e-04, 4.8457e-03, 6.0502e-09, 2.8151e-02,\n",
      "         7.8617e-03, 1.0487e-02, 1.1716e-04, 1.0093e-02, 1.7773e-05, 1.4625e-04,\n",
      "         1.9645e-02, 3.5778e-03, 5.1094e-05, 5.2751e-03, 4.0775e-03, 2.4164e-02,\n",
      "         3.9583e-03, 3.5042e-03, 1.2410e-02, 8.6631e-03, 4.1108e-03, 9.3510e-07,\n",
      "         1.1946e-07, 1.5955e-02, 5.2864e-07, 3.0450e-02, 8.0065e-03, 4.3485e-03,\n",
      "         7.8490e-03, 7.8119e-03, 1.4410e-02, 4.5132e-02, 1.4916e-02, 6.1049e-03,\n",
      "         2.5511e-07, 1.7289e-03, 1.2568e-10, 2.7444e-11, 2.0461e-02, 8.7079e-03,\n",
      "         3.2034e-03, 7.3704e-03, 1.0297e-02, 8.6253e-03, 2.0565e-05, 7.1252e-03,\n",
      "         1.5973e-02, 2.9364e-10, 1.8599e-03, 4.3581e-07, 1.0928e-02, 1.6324e-06,\n",
      "         3.0445e-04, 1.5644e-05, 7.4811e-07, 3.1258e-03, 7.9912e-09, 1.5110e-05,\n",
      "         2.4834e-06, 3.0765e-04, 1.1432e-03, 3.5586e-03, 7.8641e-03, 4.1456e-12,\n",
      "         2.1981e-02, 6.9694e-03]])\n",
      "probs_sort: torch.Size([1, 128])\n",
      "tensor([[6.5613e-02, 5.5776e-02, 4.5132e-02, 4.1354e-02, 3.4554e-02, 3.0450e-02,\n",
      "         2.8151e-02, 2.7812e-02, 2.4396e-02, 2.4164e-02, 2.1981e-02, 2.1898e-02,\n",
      "         2.1686e-02, 2.0972e-02, 2.0952e-02, 2.0950e-02, 2.0461e-02, 2.0413e-02,\n",
      "         1.9645e-02, 1.7256e-02, 1.6741e-02, 1.5973e-02, 1.5955e-02, 1.4916e-02,\n",
      "         1.4410e-02, 1.3673e-02, 1.3296e-02, 1.3221e-02, 1.2410e-02, 1.1899e-02,\n",
      "         1.1412e-02, 1.1237e-02, 1.1194e-02, 1.1034e-02, 1.0928e-02, 1.0487e-02,\n",
      "         1.0297e-02, 1.0093e-02, 9.3237e-03, 8.7079e-03, 8.6631e-03, 8.6253e-03,\n",
      "         8.4659e-03, 8.0065e-03, 7.8641e-03, 7.8617e-03, 7.8490e-03, 7.8119e-03,\n",
      "         7.3704e-03, 7.1252e-03, 6.9694e-03, 6.4085e-03, 6.1049e-03, 5.7996e-03,\n",
      "         5.2751e-03, 4.8457e-03, 4.3485e-03, 4.1108e-03, 4.0775e-03, 3.9583e-03,\n",
      "         3.5778e-03, 3.5586e-03, 3.5042e-03, 3.2034e-03, 3.1258e-03, 2.8837e-03,\n",
      "         2.8025e-03, 2.6278e-03, 1.8599e-03, 1.7289e-03, 1.6806e-03, 1.5886e-03,\n",
      "         1.4313e-03, 1.1432e-03, 7.8188e-04, 4.2025e-04, 3.5418e-04, 3.0765e-04,\n",
      "         3.0445e-04, 1.4625e-04, 1.1716e-04, 8.2262e-05, 7.4964e-05, 7.1877e-05,\n",
      "         5.1094e-05, 4.8823e-05, 4.6758e-05, 2.0565e-05, 1.7773e-05, 1.5644e-05,\n",
      "         1.5110e-05, 1.1826e-05, 1.0498e-05, 7.8541e-06, 2.4834e-06, 2.0228e-06,\n",
      "         1.6324e-06, 9.3510e-07, 7.4811e-07, 7.0225e-07, 5.2864e-07, 4.3581e-07,\n",
      "         2.5511e-07, 1.3227e-07, 1.1946e-07, 9.8389e-08, 7.2580e-08, 1.0661e-08,\n",
      "         7.9912e-09, 6.0502e-09, 4.0324e-09, 8.9183e-10, 2.9364e-10, 1.7686e-10,\n",
      "         1.2568e-10, 6.8188e-11, 6.3964e-11, 3.6095e-11, 2.8160e-11, 2.7444e-11,\n",
      "         9.3929e-12, 6.9990e-12, 5.8680e-12, 4.1456e-12, 1.4051e-13, 3.1573e-15,\n",
      "         2.6793e-15, 1.8880e-17]])\n",
      "probs_idx: torch.Size([1, 128])\n",
      "tensor([[ 39,  45,  93,  57,  40,  87,  65,  56,  44,  77, 126,  50,  51,  58,\n",
      "          21,  53, 100,  54,  72,  61,  30, 108,  85,  94,  92,   1,  41,  43,\n",
      "          80,  49,  46,  42,   8,  47, 112,  67, 104,  69,  16, 101,  81, 105,\n",
      "          52,  88, 124,  66,  90,  91, 103, 107, 127,  27,  95,   0,  75,  63,\n",
      "          89,  82,  76,  78,  73, 123,  79, 102, 117,  12,  18,   5, 110,  97,\n",
      "           2,  38,  11, 122,  62,   4,  33, 121, 114,  71,  68,  14,  60,  13,\n",
      "          74,   3,  36, 106,  70, 115, 119,  59,  35,   9, 120,  32, 113,  83,\n",
      "         116,  31,  86, 111,  96,  24,  84,  20,  19,  29, 118,  64,  34,  17,\n",
      "         109,  37,  98,  10,  55,   7,  26,  99,  25,  28,   6, 125,  23,  15,\n",
      "          22,  48]])\n",
      "probs_sum: torch.Size([1, 128])\n",
      "tensor([[0.0656, 0.1214, 0.1665, 0.2079, 0.2424, 0.2729, 0.3010, 0.3288, 0.3532,\n",
      "         0.3774, 0.3994, 0.4213, 0.4430, 0.4639, 0.4849, 0.5058, 0.5263, 0.5467,\n",
      "         0.5664, 0.5836, 0.6004, 0.6163, 0.6323, 0.6472, 0.6616, 0.6753, 0.6886,\n",
      "         0.7018, 0.7142, 0.7261, 0.7375, 0.7488, 0.7600, 0.7710, 0.7819, 0.7924,\n",
      "         0.8027, 0.8128, 0.8221, 0.8308, 0.8395, 0.8481, 0.8566, 0.8646, 0.8725,\n",
      "         0.8803, 0.8882, 0.8960, 0.9033, 0.9105, 0.9174, 0.9238, 0.9300, 0.9358,\n",
      "         0.9410, 0.9459, 0.9502, 0.9543, 0.9584, 0.9624, 0.9659, 0.9695, 0.9730,\n",
      "         0.9762, 0.9793, 0.9822, 0.9850, 0.9877, 0.9895, 0.9912, 0.9929, 0.9945,\n",
      "         0.9959, 0.9971, 0.9979, 0.9983, 0.9986, 0.9989, 0.9993, 0.9994, 0.9995,\n",
      "         0.9996, 0.9997, 0.9997, 0.9998, 0.9998, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000]])\n",
      "top_ps_mask: torch.Size([1, 128])\n",
      "tensor([[False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True]])\n",
      "probs_sort: torch.Size([1, 128])\n",
      "tensor([[0.0656, 0.0558, 0.0451, 0.0414, 0.0346, 0.0305, 0.0282, 0.0278, 0.0244,\n",
      "         0.0242, 0.0220, 0.0219, 0.0217, 0.0210, 0.0210, 0.0210, 0.0205, 0.0204,\n",
      "         0.0196, 0.0173, 0.0167, 0.0160, 0.0160, 0.0149, 0.0144, 0.0137, 0.0133,\n",
      "         0.0132, 0.0124, 0.0119, 0.0114, 0.0112, 0.0112, 0.0110, 0.0109, 0.0105,\n",
      "         0.0103, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000]])\n",
      "top_ks_mask: torch.Size([128])\n",
      "tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127])\n",
      "top_ks_mask: torch.Size([1, 128])\n",
      "tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "         126, 127]])\n",
      "top_ks_mask: torch.Size([1, 128])\n",
      "tensor([[False, False, False, False, False,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True]])\n",
      "probs_sort: torch.Size([1, 128])\n",
      "tensor([[0.0656, 0.0558, 0.0451, 0.0414, 0.0346, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000]])\n",
      "probs_sort: torch.Size([1, 128])\n",
      "tensor([[0.2706, 0.2301, 0.1862, 0.1706, 0.1425, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000]])\n",
      "probs: torch.Size([1, 128])\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.2706, 0.1425, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.2301, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.1706, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.1862, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000]])\n",
      "next_token_id: torch.Size([1, 1])\n",
      "tensor([[39]])\n",
      "tensor([[39]])\n",
      "JULIET:\n",
      "O Romeo, Romeo! wherefore art thou re a\n"
     ]
    }
   ],
   "source": [
    "input_str = \"JULIET:\\nO Romeo, Romeo! wherefore art thou \" # the classic line\n",
    "max_useable_output_len = config.max_position_embeddings - len(input_str)\n",
    "output = model.generate(input_str, \n",
    "                        output_len = 3,#max_useable_output_len, \n",
    "                        temperature=0.5, \n",
    "                        top_k = 5, \n",
    "                        top_p = 0.8,\n",
    "                       level = 0,\n",
    "                       model = 0)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0d0551-87da-4bd8-9029-0de79b8ea6b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
