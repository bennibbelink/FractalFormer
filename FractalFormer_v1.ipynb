{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffd66297",
   "metadata": {},
   "source": [
    "# FractaFormer\n",
    "\n",
    "this base version is going to be absurdly terribly no-good inefficient because we're taking the biggest computational issue ($O(t^2)$ attention) and making it way worse by doing MANY of them at once and then having to keep track of each parameter's gradient from MANY different perspectives. This is basically just an extension of [MatFormer+](https://github.com/evintunador/matryoshkaGPT/blob/main/MatFormer%2B.ipynb) where instead of one inner model, we have 2 (or whatever number you specify) models inside 1 at each layer\n",
    "\n",
    "# TODO\n",
    "- ~config~\n",
    "- ~RMSNorm~\n",
    "    - ~test~\n",
    "- ~mlp~\n",
    "    - ~tensor~\n",
    "    - ~tuple~\n",
    "    - ~triple-check test~\n",
    "- ~mqa~\n",
    "    - ~tensor~\n",
    "    - ~tuple~\n",
    "    - ~triple-check test~\n",
    "- ~layer~\n",
    "- ~output~\n",
    "    - ~tensor~\n",
    "    - ~tuple~\n",
    "    - triple check test\n",
    "- loss~~\n",
    "    - ~tuple~\n",
    "    - triple check test\n",
    "- model itself\n",
    "    - ~tensor~\n",
    "    - tuple\n",
    "    - triple check test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2125763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# used for the tokenizer\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Imports used for the config\n",
    "import dataclasses \n",
    "from typing import Optional\n",
    "\n",
    "# Imports used for the model\n",
    "import re\n",
    "from typing import Any, List, Sequence, Tuple, Union\n",
    "import numpy as np\n",
    "\n",
    "# used in the training loop\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbf790c",
   "metadata": {},
   "source": [
    "# The Dataset\n",
    "\n",
    "the dataset we'll be using is just TinyShakespeare for sake of simplicity & ability to do run/train locally on any computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8123fdfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n",
      "\n",
      " ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'] 65\n"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# the first 200 characters. It's just one continuous text document with all of the works of shakespeare back-to-back\n",
    "print(text[:200])\n",
    "\n",
    "# here are all the unique characters that occur in this text and how many there are\n",
    "chars = sorted(list(set(text)))\n",
    "v = len(chars)\n",
    "print('\\n', chars, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4158e7c",
   "metadata": {},
   "source": [
    "# The Tokenizer\n",
    "\n",
    "We'll be using a very simple tokenizer I previoiusly trained off of the TinyShakespeare dataset that has 128 total tokens and ignores stuff like special tokens & regex. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a1ee552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab length:  128\n",
      "Encoded: [22, 33, 24, 21, 17, 32, 71, 27, 1, 30, 53, 83, 53, 66, 30, 53, 83, 53, 2, 1, 61, 87, 93, 105, 43, 1, 77, 58, 1, 65, 67, 1, 30, 53, 83, 53, 12] 37\n",
      "Decoded: JULIET:\n",
      "O Romeo, Romeo! wherefore art thou Romeo? 49\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer data using pickle\n",
    "with open('./tokenizers/tokenizer.model', 'rb') as f:\n",
    "    loaded_tokenizer_data = pickle.load(f)\n",
    "\n",
    "# Extract the stoi mapping and merges from the loaded data\n",
    "loaded_stoi = loaded_tokenizer_data['stoi']\n",
    "loaded_merges = loaded_tokenizer_data['merges']\n",
    "\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self, stoi, merges):\n",
    "        self.stoi = stoi\n",
    "        self.merges = merges\n",
    "        self.itos = {i: s for s, i in stoi.items()}  # Inverse mapping for decoding\n",
    "\n",
    "        self.vocab_len = len(stoi) + len(merges)\n",
    "\n",
    "    def encode(self, text):\n",
    "        # Convert the text to a list of token IDs, using space for unknown characters\n",
    "        tokens = [self.stoi.get(c, self.stoi[' ']) for c in text]\n",
    "\n",
    "        # Perform merging with the possibility of nested merges\n",
    "        i = 0\n",
    "        while i < len(tokens) - 1:\n",
    "            pair = (tokens[i], tokens[i + 1])\n",
    "            if pair in self.merges:\n",
    "                # Replace the current pair with its merged token\n",
    "                merged_token = self.merges[pair]\n",
    "                tokens[i] = merged_token\n",
    "                del tokens[i + 1]\n",
    "\n",
    "                # Move back to handle possible nested merges\n",
    "                if i > 0:\n",
    "                    i -= 1\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        def expand_token(token):\n",
    "            # Base case: if the token is a direct mapping, return its character\n",
    "            if token in self.itos:\n",
    "                return self.itos[token]\n",
    "            # Recursive case: if the token is a merged token, expand its constituents\n",
    "            elif token in self.merges.values():\n",
    "                pair = next(key for key, value in self.merges.items() if value == token)\n",
    "                return ''.join(expand_token(t) for t in pair)\n",
    "            # Fallback for unknown tokens\n",
    "            else:\n",
    "                return ''\n",
    "\n",
    "        # Decode each token in the list, handling nested merges recursively\n",
    "        return ''.join(expand_token(token) for token in tokens)\n",
    "        \n",
    "# Example usage\n",
    "# Assuming loaded_stoi and loaded_merges are already loaded from the tokenizer.model file\n",
    "\n",
    "tokenizer = SimpleTokenizer(loaded_stoi, loaded_merges)\n",
    "print(\"vocab length: \", tokenizer.vocab_len)\n",
    "\n",
    "# Encoding text\n",
    "encoded_text = tokenizer.encode(\"JULIET:\\nO Romeo, Romeo! wherefore art thou Romeo?\")\n",
    "print(\"Encoded:\", encoded_text, len(encoded_text))\n",
    "\n",
    "# Decoding back\n",
    "decoded_text = tokenizer.decode(encoded_text)\n",
    "print(\"Decoded:\", decoded_text, len(decoded_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f1a588",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8da0d14c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single large model -> hierarchy of many smaller models inside\n",
      "model_count:  [1, 2, 4]\n",
      "model_dim_list:  [128, 64, 32]\n",
      "head_dim_list:  [32, 16, 8]\n"
     ]
    }
   ],
   "source": [
    "@dataclasses.dataclass # a class meant specifically to just hold data\n",
    "class Config:\n",
    "    \"\"\" \n",
    "    The default configuration & hyperparameters for FractalFormer\n",
    "    \"\"\"\n",
    "    # The number of tokens in the vocabulary.\n",
    "    vocab_size: int = tokenizer.vocab_len\n",
    "    \n",
    "    # The maximum sequence length that this model might ever be used with.\n",
    "    max_position_embeddings: int = 256\n",
    "    \n",
    "    # The number of layers in the model.\n",
    "    num_hidden_layers: int = 4\n",
    "    \n",
    "    # The number of attention heads used in the attention layers of the model.\n",
    "    num_attention_heads: int = 4\n",
    "    \n",
    "    # The number of key-value heads for implementing multi-query attention.\n",
    "    num_key_value_heads: int = 1\n",
    "    # Ensures that the number of query heads is evenly divisible by the number of KV heads.\n",
    "    assert num_attention_heads % num_key_value_heads == 0\n",
    "    \n",
    "    # The hidden size of the model, AKA the embedding dimension\n",
    "    hidden_size: int = 128\n",
    "    # the attention heads need to cleanly divide up the hidden_size of the model for MQA\n",
    "    assert hidden_size % num_attention_heads == 0\n",
    "\n",
    "    # how much larger the inner dimension of the MLP should be than the hidden size of the model\n",
    "    intermediate_multiplier = 4\n",
    "    # The inner dimension of the MLP part of the decoder layer\n",
    "    @property\n",
    "    def intermediate_size(self):\n",
    "        return self.intermediate_multiplier * self.hidden_size\n",
    "    \n",
    "    # The number of head dimensions\n",
    "    head_dim: int = 32\n",
    "    \n",
    "    # The epsilon used by the rms normalization layers.\n",
    "    rms_norm_eps: float = 1e-6 # this is to promote numerical stability & prevent dividing by 0\n",
    "    \n",
    "    # the scaling factor that determines the frequencies for the rotary positional encodings\n",
    "    rope_theta = 100.0\n",
    "    # smaller models should use a smaller theta, but I'm just guessing here. 1000 might work too. 10,000 is the usual\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # the % of neurons to dropout in the MLP\n",
    "    dropout = 0.1\n",
    "\n",
    "    ####### FractalFormer-specific hyperparameters\n",
    "\n",
    "    # the number of levels for sub-models to exist on\n",
    "    levels = 3\n",
    "    \n",
    "    # the number of splits to make at a given level\n",
    "    split = 2 # i don't recommend choosing any value other than 2\n",
    "    # needs to be divisible by 2 in order to splice cleanly\n",
    "    assert split % 2 == 0\n",
    "    # RoPE requires a head dimension of length larger than 1 in order to work\n",
    "    assert head_dim // (split * (levels-1)) > 1\n",
    "    # really though you shouldn't be getting anywhere near that small of a head dimension even at the lowest level, that'd be useless\n",
    "\n",
    "    @property\n",
    "    def model_count(self):\n",
    "        return [self.split**i for i in range(self.levels)]\n",
    "\n",
    "    @property\n",
    "    def model_dim_list(self):\n",
    "        return [self.hidden_size // (self.split**i) for i in range(self.levels)]\n",
    "\n",
    "    @property\n",
    "    def head_dim_list(self):\n",
    "        return [self.head_dim // (self.split**i) for i in range(self.levels)]\n",
    "\n",
    "config = Config()\n",
    "\n",
    "print(\"single large model -> hierarchy of many smaller models inside\")\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "print(\"head_dim_list: \", config.head_dim_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b8d06d",
   "metadata": {},
   "source": [
    "# Rotary Positional Encoding (RoPE)\n",
    "\n",
    "i don't think i need to adjust the code for this one as long as i always call it individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab56e84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotary_emb(x: torch.Tensor, dim: int, theta: float = 10000.0) -> torch.Tensor:\n",
    "    \"\"\"Applies the rotary embedding to the inputted query or key tensor\"\"\"\n",
    "    # Get sequence length\n",
    "    seq_len = x.size(1)\n",
    "    device = x.device\n",
    "    \n",
    "    # Dynamically compute frequency cis based on the input sequence length\n",
    "    # dynamic is less efficient but pre-computed was giving me trouble so whatever\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2, device=device).float() / dim))\n",
    "    t = torch.arange(seq_len, device=device)\n",
    "    freqs = torch.outer(t, freqs).float()\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "\n",
    "    # Apply rotary embeddings to the input tensor\n",
    "    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis.unsqueeze(0)).type_as(x)  # Ensure batch dimension is handled\n",
    "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "    x_out = x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "\n",
    "    return x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67bf0ee",
   "metadata": {},
   "source": [
    "# RMSNorm\n",
    "\n",
    "Layernorm is relatively simple code-wise. However, of note is the fact that during training, the entire full length vector gets normalized whereas during inference we only layernorm the sub-vector we've been given if we're not using the full model size. This is interesting because RMSNorm puts a vector of length $d$ onto a hypersphere of radius $\\sqrt{d}$ which means that while the embeddings of the largest model exist on a hypersphere of the aforementioned size, for each number of layers $i\\in\\mathbb{N}$ s.t. $0 < i \\leq$ `config.model_count` the embeddings are placed onto a hypersphere of radius $\\sqrt{\\frac{d}{s^i}}$ where $s=$`config.split`. I'm not sure yet exactly how to interpret this concatenation of vectors geometrically. When you combine the entries of two hypserspheres to make a larger hypserspheres, what happens to the feature groupings on the surface of the smaller hyperspheres? I presume there are some type of interaction effects or something. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89f37bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the RMS Normalization (Root Mean Square Normalization) layer.\n",
    "    RMSNorm is a variant of layer normalization that normalizes the activations\n",
    "    of the previous layer based on their root mean square value.\n",
    "\n",
    "    Parameters:\n",
    "    - dim (int): The dimension of the input features the normalization is applied to.\n",
    "    - eps (float): A small value added to the denominator for numerical stability. Default is 1e-6.\n",
    "    - add_unit_offset (bool): If True, adds a unit (1) to the learned scaling coefficient, effectively\n",
    "      starting with no scaling. If False, the scaling coefficient starts from zero. Default is True.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        eps: float = 1e-6,\n",
    "        #add_unit_offset: bool = True,\n",
    "    ):\n",
    "        super().__init__() \n",
    "        self.eps = eps  # Small epsilon value for numerical stability since you can't divide by 0\n",
    "        #self.add_unit_offset = add_unit_offset  # Flag to determine if a unit should be added to the weight\n",
    "        \n",
    "        # Initialize the weight parameter with zeros, which will be learned during training.\n",
    "        # The shape of the weight is [dim], meaning one weight per feature dimension.\n",
    "        self.weight = nn.Parameter(torch.zeros(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        \"\"\"\n",
    "        Private helper function to normalize the input tensor.\n",
    "\n",
    "        Parameters:\n",
    "        - x (Tensor): The input tensor to normalize.\n",
    "\n",
    "        Returns:\n",
    "        - Tensor: The normalized tensor.\n",
    "        \"\"\"\n",
    "        # Calculate the root mean square value for each feature (across the last dimension),\n",
    "        # then use reciprocal square root (rsqrt) for normalization.\n",
    "        # Add self.eps to the denominator for numerical stability.\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, model: int = 0) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the RMSNorm layer\n",
    "\n",
    "        Parameters:\n",
    "        - x (Tensor): The input tensor to normalize.\n",
    "        - model (int): the index indicating the model being used in this layer. used for splicing self.weight\n",
    "\n",
    "        Returns:\n",
    "        - output: The normalized and scaled tensor.\n",
    "        \"\"\"\n",
    "        global verbose\n",
    "        if verbose: \n",
    "            print(\"------------- RMSNorm.forward() ------------\")\n",
    "            print(f\"x: {x.shape}\\n{x}\")\n",
    "            \n",
    "        # Normalize the input tensor using the _norm function and ensure the data type matches the input.\n",
    "        x = self._norm(x.float()).type_as(x)\n",
    "        if verbose: print(f\"normed x: {x.shape}\\n{x}\")\n",
    "        \n",
    "        # grabbing x's dimension to use for splicing\n",
    "        dim = x.shape[-1]\n",
    "        \n",
    "        # calculating skip for our splice\n",
    "        skip = model * dim\n",
    "        if verbose: \n",
    "            print(f\"dim: {dim}\")\n",
    "            print(f\"skip: {skip}\")\n",
    "        \n",
    "        # scale the normalized tensor by (1 + self.weight), which effectively starts with no scaling\n",
    "        spliced_scale = self.weight[skip:skip + dim]\n",
    "        output = x * (1 + spliced_scale)\n",
    "        if verbose:\n",
    "            print(f\"spliced scale: {spliced_scale.shape}\\n{spliced_scale}\")\n",
    "            print(f\"scaled normed x: {output.shape}\\n{output}\")\n",
    "            print(\"------------- END RMSNorm.forward() ------------\")\n",
    "                          \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecec2e6",
   "metadata": {},
   "source": [
    "The following cell was designed to help you visualize what's happening with RMSNorm's splicing. With RMSNorm we'll only have to think about doing this with individual tensors, but with future methods like MLP and MQA we'll have to create an entirely separate forward method used during training that deals with tuples of tensors. The thing to pay attention to here is the size of the scale weights. scale_weights' entries are 0's because we've not yet undergone training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57fa35c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Micro Hyperparameters -------\n",
      "model_count:  [1, 2, 4]\n",
      "model_dim_list:  [4, 2, 1]\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the big model |-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 2, 4])\n",
      "tensor([[[0.5476, 0.4260, 0.2188, 0.1814],\n",
      "         [0.3320, 0.7403, 0.4972, 0.3718]]])\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 2, 4])\n",
      "tensor([[[0.5476, 0.4260, 0.2188, 0.1814],\n",
      "         [0.3320, 0.7403, 0.4972, 0.3718]]])\n",
      "normed x: torch.Size([1, 2, 4])\n",
      "tensor([[[1.4608, 1.1363, 0.5837, 0.4840],\n",
      "         [0.6500, 1.4493, 0.9733, 0.7279]]])\n",
      "dim: 4\n",
      "skip: 0\n",
      "spliced scale: torch.Size([4])\n",
      "tensor([0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 2, 4])\n",
      "tensor([[[1.4608, 1.1363, 0.5837, 0.4840],\n",
      "         [0.6500, 1.4493, 0.9733, 0.7279]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "y: torch.Size([1, 2, 4])\n",
      "tensor([[[1.4608, 1.1363, 0.5837, 0.4840],\n",
      "         [0.6500, 1.4493, 0.9733, 0.7279]]], grad_fn=<MulBackward0>)\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the first sub-model |-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 2, 2])\n",
      "tensor([[[0.8396, 0.5867],\n",
      "         [0.2925, 0.2003]]])\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 2, 2])\n",
      "tensor([[[0.8396, 0.5867],\n",
      "         [0.2925, 0.2003]]])\n",
      "normed x: torch.Size([1, 2, 2])\n",
      "tensor([[[1.1593, 0.8100],\n",
      "         [1.1669, 0.7989]]])\n",
      "dim: 2\n",
      "skip: 0\n",
      "spliced scale: torch.Size([2])\n",
      "tensor([0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 2, 2])\n",
      "tensor([[[1.1593, 0.8100],\n",
      "         [1.1669, 0.7989]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "y: torch.Size([1, 2, 2])\n",
      "tensor([[[1.1593, 0.8100],\n",
      "         [1.1669, 0.7989]]], grad_fn=<MulBackward0>)\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the second sub-model |-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 2, 2])\n",
      "tensor([[[0.8514, 0.5973],\n",
      "         [0.9607, 0.2236]]])\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 2, 2])\n",
      "tensor([[[0.8514, 0.5973],\n",
      "         [0.9607, 0.2236]]])\n",
      "normed x: torch.Size([1, 2, 2])\n",
      "tensor([[[1.1577, 0.8122],\n",
      "         [1.3774, 0.3205]]])\n",
      "dim: 2\n",
      "skip: 2\n",
      "spliced scale: torch.Size([2])\n",
      "tensor([0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 2, 2])\n",
      "tensor([[[1.1577, 0.8122],\n",
      "         [1.3774, 0.3205]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "y: torch.Size([1, 2, 2])\n",
      "tensor([[[1.1577, 0.8122],\n",
      "         [1.3774, 0.3205]]], grad_fn=<MulBackward0>)\n",
      "---------- RESET CONFIG --------\n",
      "model_count:  [1, 2, 4]\n"
     ]
    }
   ],
   "source": [
    "# Testing our RMSNorm's forward()\n",
    "verbose = True\n",
    "print(\"--------- Micro Hyperparameters -------\")\n",
    "hold = config.hidden_size\n",
    "config.hidden_size = 4\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the big model |-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,2,config.hidden_size)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "norm = RMSNorm(config.hidden_size)\n",
    "y = norm(x)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the first sub-model |-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,2,config.hidden_size//2)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "norm = RMSNorm(config.hidden_size)\n",
    "y = norm(x)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the second sub-model |-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,2,config.hidden_size//2)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "norm = RMSNorm(config.hidden_size)\n",
    "y = norm(x, model=1)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "verbose = False\n",
    "print(\"---------- RESET CONFIG --------\")\n",
    "config.hidden_size = hold\n",
    "print(\"model_count: \", config.model_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f07034e",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./images/ffwd.jpeg\" width=\"512\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c30de56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    This class implements a multi-layer perceptron with a GeGLU gating mechanism. The GeGLU\n",
    "    activation combines a standard GeLU activation with a learned gating mechanism, enabling\n",
    "    the network to control the flow of information more dynamically.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        intermediate_size: int,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the GemmaMLP module.\n",
    "\n",
    "        Parameters:\n",
    "            hidden_size (int): The size of the input and output tensors.\n",
    "            intermediate_size (int): The size of the tensor after the initial transformation\n",
    "                                     and before the gating and final projection. This is typically\n",
    "                                     larger than the hidden size to allow for a richer representation.\n",
    "            dropout (float): the dropout rate to use during training in forwardTuple()\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.intermediate_size = intermediate_size\n",
    "        assert intermediate_size % hidden_size == 0\n",
    "        self.intermediate_multiplier = intermediate_size // hidden_size\n",
    "\n",
    "        # Linear transformation for the gating mechanism, projecting input to an intermediate size.\n",
    "        self.Wgate = nn.Parameter(torch.Tensor(hidden_size, intermediate_size))\n",
    "        self.Bgate = nn.Parameter(torch.Tensor(intermediate_size))\n",
    "\n",
    "        # Linear transformation for the input tensor, also projecting to the intermediate size but\n",
    "        # intended for element-wise multiplication with the gated output.\n",
    "        self.Wup = nn.Parameter(torch.Tensor(hidden_size, intermediate_size))\n",
    "        self.Bup = nn.Parameter(torch.Tensor(intermediate_size))\n",
    "\n",
    "        # Linear transformation to project the gated and combined tensor back to the original\n",
    "        # hidden size, completing the MLP structure.\n",
    "        self.Wdown = nn.Parameter(torch.Tensor(intermediate_size, hidden_size))\n",
    "        self.Bdown = nn.Parameter(torch.Tensor(hidden_size))\n",
    "\n",
    "        # Initialize weights with uniform distribution\n",
    "        # For gate & up, where in_features is hidden_size\n",
    "        limit_gateup = 1 / np.sqrt(hidden_size)\n",
    "        nn.init.uniform_(self.Wgate, -limit_gateup, limit_gateup)\n",
    "        nn.init.uniform_(self.Bgate, -limit_gateup, limit_gateup)\n",
    "        nn.init.uniform_(self.Wup, -limit_gateup, limit_gateup)\n",
    "        nn.init.uniform_(self.Bup, -limit_gateup, limit_gateup)\n",
    "        \n",
    "        # For down, where in_features is intermediate_size\n",
    "        limit_down = 1 / np.sqrt(intermediate_size)\n",
    "        nn.init.uniform_(self.Wdown, -limit_down, limit_down)\n",
    "        nn.init.uniform_(self.Bdown, -limit_down, limit_down)\n",
    "        \n",
    "        # defining our dropout for training in forwardTuple()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forwardTensor(self, x, model:int=0):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the MLP module during inference.\n",
    "\n",
    "        Parameters:\n",
    "            x (Tensor): The input tensor to the MLP. \n",
    "                        shape (batch size, sequence length, hidden dimension) where hidden dimension changes by which model was used\n",
    "            model (int): the indicator of which model we're using. \n",
    "                        used in calculating our skip length for splicing. \n",
    "                        defaults to the equivalent of what's used in MatFormer+, meaning no skip, aka we use the top-left-most splice\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The output tensor after applying the GeGLU gating mechanism and the MLP transformations.\n",
    "        \"\"\"\n",
    "        global verbose\n",
    "        if verbose: \n",
    "            print(\"------------- MLP.forwardTensor() ------------\")\n",
    "            print(f\"x: {x.shape}\\n{x}\")\n",
    "            \n",
    "        # figuring out how we should do our splicing\n",
    "        d_dim = x.shape[-1]\n",
    "        d_skip = model * d_dim\n",
    "        i_dim = d_dim * self.intermediate_multiplier\n",
    "        i_skip = model * i_dim\n",
    "        if verbose: \n",
    "            print(f\"d_dim: {d_dim}\")\n",
    "            print(f\"d_skip: {d_skip}\")\n",
    "            print(f\"i_dim: {i_dim}\")\n",
    "            print(f\"i_skip: {i_skip}\")\n",
    "        \n",
    "        # Applies linear transformation for gating.\n",
    "        Wgate = self.Wgate[d_skip:d_skip + d_dim, i_skip:i_skip + i_dim]\n",
    "        Bgate = self.Bgate[i_skip:i_skip + i_dim]\n",
    "        Xgate = x @ Wgate + Bgate\n",
    "        if verbose: \n",
    "            print(f\"Wgate: {self.Wgate.shape}\\n{self.Wgate}\")\n",
    "            print(f\"Wgate spliced: {Wgate.shape}\\n{Wgate}\")\n",
    "            print(f\"Bgate: {self.Bgate.shape}\\n{self.Bgate}\")\n",
    "            print(f\"Bgate spliced: {Bgate.shape}\\n{Bgate}\")\n",
    "            print(f\"Xgate: {Xgate.shape}\\n{Xgate}\")\n",
    "\n",
    "        # Applies GeLU activation to the gate, introducing non-linearity and enabling the gating mechanism.\n",
    "        Xgate = F.gelu(Xgate)\n",
    "        if verbose: print(f\"GeLU'ed Xgate: {Xgate.shape}\\n{Xgate}\")\n",
    "\n",
    "        # Applies another linear transformation to the input tensor for subsequent combination with the gate.\n",
    "        Wup = self.Wup[d_skip:d_skip + d_dim, i_skip:i_skip + i_dim]\n",
    "        Bup = self.Bup[i_skip:i_skip + i_dim]\n",
    "        Xup = x @ Wup + Bup\n",
    "        if verbose: \n",
    "            print(f\"Wup: {self.Wup.shape}\\n{self.Wup}\")\n",
    "            print(f\"Wup spliced: {Wup.shape}\\n{Wup}\")\n",
    "            print(f\"Bup: {self.Bup.shape}\\n{self.Bup}\")\n",
    "            print(f\"Bup spliced: {Bup.shape}\\n{Bup}\")\n",
    "            print(f\"Xup: {Xup.shape}\\n{Xup}\")\n",
    "\n",
    "        # Element-wise multiplication of the gated tensor with the transformed input tensor, modulating\n",
    "        # the input based on the gate's activation.\n",
    "        Xfuse = Xgate * Xup\n",
    "        if verbose: print(f\"Xfuse: {Xfuse.shape}\\n{Xfuse}\")\n",
    "\n",
    "        # Applies the final linear transformation to project the modulated tensor back to the hidden size.\n",
    "        Wdown = self.Wdown[i_skip:i_skip + i_dim, d_skip:d_skip + d_dim]\n",
    "        Bdown = self.Bdown[d_skip:d_skip + d_dim]\n",
    "        outputs = Xfuse @ Wdown + Bdown\n",
    "        if verbose: \n",
    "            print(f\"Wdown: {self.Wdown.shape}\\n{self.Wdown}\")\n",
    "            print(f\"Wdown spliced: {Wdown.shape}\\n{Wdown}\")\n",
    "            print(f\"Bdown: {self.Bdown.shape}\\n{self.Bdown}\")\n",
    "            print(f\"Bdown spliced: {Bdown.shape}\\n{Bdown}\")\n",
    "            print(f\"outputs: {outputs.shape}\\n{outputs}\") \n",
    "            print(\"------------- END MLP.forwardTensor() ------------\")\n",
    "\n",
    "        # Returns the final output tensor of the MLP, after gating and modulation.\n",
    "        return outputs\n",
    "\n",
    "    def forwardTuple(self, x, drop_bool: bool = True):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the MLP module during training.\n",
    "\n",
    "        Parameters:\n",
    "            x (Tuple[Tuple[Tensor]]): \n",
    "                The input tuple of tuples of tensors to the MLP. \n",
    "                first tuple is of length config.levels and second layer of tuples have lengths of config.model_count\n",
    "                tensors are shape (batch size, sequence length, hidden dimension) where hidden dimension changes by which model was used\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Tuple[Tensor]]: \n",
    "                The output tuple of tuples of tensors after applying the GeGLU gating mechanism and the MLP transformations.\n",
    "        \"\"\"\n",
    "        global verbose\n",
    "        if verbose: \n",
    "            print(\"------------- MLP.forwardTuple() ------------\")\n",
    "            print(f\"x: {x}\")\n",
    "\n",
    "        # if we had sent through the config we could've just grabbed these values from there but too late now\n",
    "        num_levels = len(x)\n",
    "        models_per_level = [len(x[i]) for i in range(num_levels)]\n",
    "        if verbose: \n",
    "            print(f\"num_levels: {num_levels}\")\n",
    "            print(f\"models_per_level: {models_per_level}\")\n",
    "        \n",
    "        out = ()\n",
    "        for i in range(num_levels):\n",
    "            if verbose: print(f\"i: {i}\")\n",
    "            \n",
    "            out_lvl = ()\n",
    "            for j in range(models_per_level[i]):\n",
    "                if verbose: print(f\"j: {j}\")\n",
    "\n",
    "                output = self.forwardTensor(x[i][j], model=j)\n",
    "                if verbose: print(f\"forwardTensor() output: {output.shape}\\n{output}\")\n",
    "                    \n",
    "                out_lvl += (self.drop(output),) if drop_bool else (output,)\n",
    "\n",
    "            # pretty sure i have to save & store everything without overwriting to prevent in-place arguments. so annoying\n",
    "            if verbose: print(f\"out_lvl: {out_lvl}\")\n",
    "            out += (out_lvl,)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"out: {out}\")\n",
    "            print(\"------------- END MLP.forwardTuple() ------------\")\n",
    "        return out\n",
    "        \n",
    "    def forward(self, x, model=0, drop_bool = True):\n",
    "        train = True if type(x) == tuple else False\n",
    "        print(f\"---------- MLP Input: {'Tuple' if train else 'torch.Tensor'} ------------\")\n",
    "        return self.forwardTuple(x, drop_bool) if train else self.forwardTensor(x, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95fe6ab",
   "metadata": {},
   "source": [
    "The following two cells are designed to help you comprehend what's happening. If you walk through every single print statement and follow along even down to watching what happens to each weight, you'll be able to clearly see what's happening with the odd splicing behavior. In order to make this somewhat feasible, I've set very small matrices for these examples. However I will admit it is still inevitably a pain, which is why I included the drawings above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b26bd23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Micro Hyperparameters -------\n",
      "model_count:  [1, 2, 4]\n",
      "model_dim_list:  [4, 2, 1]\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the big model |-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 2, 4])\n",
      "tensor([[[0.8357, 0.6235, 0.2187, 0.8647],\n",
      "         [0.0509, 0.1876, 0.5219, 0.2856]]])\n",
      "---------- MLP Input: torch.Tensor ------------\n",
      "------------- MLP.forwardTensor() ------------\n",
      "x: torch.Size([1, 2, 4])\n",
      "tensor([[[0.8357, 0.6235, 0.2187, 0.8647],\n",
      "         [0.0509, 0.1876, 0.5219, 0.2856]]])\n",
      "d_dim: 4\n",
      "d_skip: 0\n",
      "i_dim: 8\n",
      "i_skip: 0\n",
      "Wgate: torch.Size([4, 8])\n",
      "Parameter containing:\n",
      "tensor([[-0.3891,  0.1615, -0.3803,  0.0968, -0.3116,  0.0135, -0.3844, -0.0793],\n",
      "        [-0.4522, -0.0294, -0.4864, -0.2767,  0.0902,  0.2481, -0.2995, -0.4016],\n",
      "        [-0.3055,  0.2483,  0.0267, -0.2415,  0.2368, -0.4932,  0.4063, -0.0098],\n",
      "        [ 0.3214, -0.3187,  0.0295, -0.3992, -0.1088, -0.3484, -0.0786,  0.1379]],\n",
      "       requires_grad=True)\n",
      "Wgate spliced: torch.Size([4, 8])\n",
      "tensor([[-0.3891,  0.1615, -0.3803,  0.0968, -0.3116,  0.0135, -0.3844, -0.0793],\n",
      "        [-0.4522, -0.0294, -0.4864, -0.2767,  0.0902,  0.2481, -0.2995, -0.4016],\n",
      "        [-0.3055,  0.2483,  0.0267, -0.2415,  0.2368, -0.4932,  0.4063, -0.0098],\n",
      "        [ 0.3214, -0.3187,  0.0295, -0.3992, -0.1088, -0.3484, -0.0786,  0.1379]],\n",
      "       grad_fn=<AliasBackward0>)\n",
      "Bgate: torch.Size([8])\n",
      "Parameter containing:\n",
      "tensor([ 0.2224,  0.4973, -0.0207,  0.1325, -0.4664, -0.1762, -0.1117,  0.3340],\n",
      "       requires_grad=True)\n",
      "Bgate spliced: torch.Size([8])\n",
      "tensor([ 0.2224,  0.4973, -0.0207,  0.1325, -0.4664, -0.1762, -0.1117,  0.3340],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xgate: torch.Size([1, 2, 8])\n",
      "tensor([[[-0.1736,  0.3925, -0.6104, -0.3572, -0.7128, -0.4194, -0.5988,\n",
      "           0.1344],\n",
      "         [ 0.0501,  0.5385, -0.1089, -0.1546, -0.3728, -0.4859,  0.0022,\n",
      "           0.2889]]], grad_fn=<AddBackward0>)\n",
      "GeLU'ed Xgate: torch.Size([1, 2, 8])\n",
      "tensor([[[-0.0748,  0.2562, -0.1653, -0.1288, -0.1696, -0.1415, -0.1645,\n",
      "           0.0744],\n",
      "         [ 0.0261,  0.3796, -0.0497, -0.0678, -0.1322, -0.1523,  0.0011,\n",
      "           0.1773]]], grad_fn=<GeluBackward0>)\n",
      "Wup: torch.Size([4, 8])\n",
      "Parameter containing:\n",
      "tensor([[ 0.1884, -0.1090, -0.2385,  0.3435,  0.1160, -0.0708,  0.0213,  0.0936],\n",
      "        [-0.0098, -0.3586, -0.4083,  0.1763, -0.3069,  0.3843,  0.0558, -0.3685],\n",
      "        [-0.3440, -0.3130, -0.0244, -0.2671, -0.4865, -0.1032, -0.1786, -0.4559],\n",
      "        [-0.1674, -0.1142,  0.4604, -0.4587,  0.4115,  0.4645,  0.2169,  0.1728]],\n",
      "       requires_grad=True)\n",
      "Wup spliced: torch.Size([4, 8])\n",
      "tensor([[ 0.1884, -0.1090, -0.2385,  0.3435,  0.1160, -0.0708,  0.0213,  0.0936],\n",
      "        [-0.0098, -0.3586, -0.4083,  0.1763, -0.3069,  0.3843,  0.0558, -0.3685],\n",
      "        [-0.3440, -0.3130, -0.0244, -0.2671, -0.4865, -0.1032, -0.1786, -0.4559],\n",
      "        [-0.1674, -0.1142,  0.4604, -0.4587,  0.4115,  0.4645,  0.2169,  0.1728]],\n",
      "       grad_fn=<AliasBackward0>)\n",
      "Bup: torch.Size([8])\n",
      "Parameter containing:\n",
      "tensor([-0.3094,  0.4105,  0.1707,  0.3800, -0.1242, -0.4778,  0.2081, -0.1678],\n",
      "       requires_grad=True)\n",
      "Bup spliced: torch.Size([8])\n",
      "tensor([-0.3094,  0.4105,  0.1707,  0.3800, -0.1242, -0.4778,  0.2081, -0.1678],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xup: torch.Size([1, 2, 8])\n",
      "tensor([[[-0.3780, -0.0714,  0.1096,  0.3219,  0.0308,  0.0817,  0.4091,\n",
      "          -0.2696],\n",
      "         [-0.5290,  0.1417,  0.2007,  0.1601, -0.3122, -0.3306,  0.1884,\n",
      "          -0.4207]]], grad_fn=<AddBackward0>)\n",
      "Xfuse: torch.Size([1, 2, 8])\n",
      "tensor([[[ 0.0283, -0.0183, -0.0181, -0.0415, -0.0052, -0.0116, -0.0673,\n",
      "          -0.0201],\n",
      "         [-0.0138,  0.0538, -0.0100, -0.0109,  0.0413,  0.0504,  0.0002,\n",
      "          -0.0746]]], grad_fn=<MulBackward0>)\n",
      "Wdown: torch.Size([8, 4])\n",
      "Parameter containing:\n",
      "tensor([[-0.1072, -0.1056, -0.3445, -0.2180],\n",
      "        [ 0.3398,  0.2241, -0.3478,  0.0153],\n",
      "        [-0.2967, -0.0860, -0.0154,  0.1172],\n",
      "        [-0.1199, -0.1057, -0.2220, -0.2039],\n",
      "        [-0.2114,  0.1966,  0.3148, -0.3213],\n",
      "        [ 0.3278, -0.1079,  0.0026, -0.0847],\n",
      "        [ 0.2394, -0.2573,  0.0816, -0.3254],\n",
      "        [-0.3018,  0.0552, -0.0244,  0.3433]], requires_grad=True)\n",
      "Wdown spliced: torch.Size([8, 4])\n",
      "tensor([[-0.1072, -0.1056, -0.3445, -0.2180],\n",
      "        [ 0.3398,  0.2241, -0.3478,  0.0153],\n",
      "        [-0.2967, -0.0860, -0.0154,  0.1172],\n",
      "        [-0.1199, -0.1057, -0.2220, -0.2039],\n",
      "        [-0.2114,  0.1966,  0.3148, -0.3213],\n",
      "        [ 0.3278, -0.1079,  0.0026, -0.0847],\n",
      "        [ 0.2394, -0.2573,  0.0816, -0.3254],\n",
      "        [-0.3018,  0.0552, -0.0244,  0.3433]], grad_fn=<AliasBackward0>)\n",
      "Bdown: torch.Size([4])\n",
      "Parameter containing:\n",
      "tensor([0.1795, 0.2317, 0.2478, 0.2532], requires_grad=True)\n",
      "Bdown spliced: torch.Size([4])\n",
      "tensor([0.1795, 0.2317, 0.2478, 0.2532], grad_fn=<SliceBackward0>)\n",
      "outputs: torch.Size([1, 2, 4])\n",
      "tensor([[[0.1678, 0.2470, 0.2472, 0.2708],\n",
      "         [0.2338, 0.2458, 0.2514, 0.2149]]], grad_fn=<AddBackward0>)\n",
      "------------- END MLP.forwardTensor() ------------\n",
      "y: torch.Size([1, 2, 4])\n",
      "tensor([[[0.1678, 0.2470, 0.2472, 0.2708],\n",
      "         [0.2338, 0.2458, 0.2514, 0.2149]]], grad_fn=<AddBackward0>)\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the first sub-model |-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 2, 2])\n",
      "tensor([[[0.4956, 0.4116],\n",
      "         [0.6861, 0.8913]]])\n",
      "---------- MLP Input: torch.Tensor ------------\n",
      "------------- MLP.forwardTensor() ------------\n",
      "x: torch.Size([1, 2, 2])\n",
      "tensor([[[0.4956, 0.4116],\n",
      "         [0.6861, 0.8913]]])\n",
      "d_dim: 2\n",
      "d_skip: 0\n",
      "i_dim: 4\n",
      "i_skip: 0\n",
      "Wgate: torch.Size([4, 8])\n",
      "Parameter containing:\n",
      "tensor([[-0.3032, -0.0485, -0.4001,  0.2466, -0.2568,  0.0315,  0.1295, -0.2514],\n",
      "        [-0.1162, -0.2709,  0.2860, -0.1458, -0.2656, -0.4158,  0.3740,  0.4569],\n",
      "        [ 0.4181, -0.2765,  0.1733,  0.4909, -0.0522,  0.0066, -0.4431,  0.3117],\n",
      "        [ 0.0391,  0.0826, -0.1097, -0.0674,  0.0277, -0.0454, -0.0344,  0.2668]],\n",
      "       requires_grad=True)\n",
      "Wgate spliced: torch.Size([2, 4])\n",
      "tensor([[-0.3032, -0.0485, -0.4001,  0.2466],\n",
      "        [-0.1162, -0.2709,  0.2860, -0.1458]], grad_fn=<SliceBackward0>)\n",
      "Bgate: torch.Size([8])\n",
      "Parameter containing:\n",
      "tensor([ 0.3898,  0.3483, -0.2695,  0.3839, -0.2391,  0.1373,  0.0598,  0.0215],\n",
      "       requires_grad=True)\n",
      "Bgate spliced: torch.Size([4])\n",
      "tensor([ 0.3898,  0.3483, -0.2695,  0.3839], grad_fn=<SliceBackward0>)\n",
      "Xgate: torch.Size([1, 2, 4])\n",
      "tensor([[[ 0.1917,  0.2128, -0.3501,  0.4462],\n",
      "         [ 0.0782,  0.0736, -0.2891,  0.4232]]], grad_fn=<AddBackward0>)\n",
      "GeLU'ed Xgate: torch.Size([1, 2, 4])\n",
      "tensor([[[ 0.1104,  0.1243, -0.1271,  0.2999],\n",
      "         [ 0.0415,  0.0389, -0.1117,  0.2810]]], grad_fn=<GeluBackward0>)\n",
      "Wup: torch.Size([4, 8])\n",
      "Parameter containing:\n",
      "tensor([[ 0.4905, -0.3800,  0.2665,  0.1555,  0.0869, -0.1328, -0.1558, -0.1661],\n",
      "        [-0.3470,  0.2403,  0.4735, -0.2509, -0.3341, -0.0474,  0.2639,  0.0893],\n",
      "        [-0.4615, -0.0926,  0.2014, -0.4853,  0.3262,  0.0891, -0.3307, -0.3736],\n",
      "        [ 0.3528, -0.4256, -0.2572, -0.2068, -0.3812, -0.1613, -0.3834,  0.2595]],\n",
      "       requires_grad=True)\n",
      "Wup spliced: torch.Size([2, 4])\n",
      "tensor([[ 0.4905, -0.3800,  0.2665,  0.1555],\n",
      "        [-0.3470,  0.2403,  0.4735, -0.2509]], grad_fn=<SliceBackward0>)\n",
      "Bup: torch.Size([8])\n",
      "Parameter containing:\n",
      "tensor([ 0.1284,  0.3907, -0.3069,  0.4707,  0.3908,  0.2157, -0.1696,  0.1340],\n",
      "       requires_grad=True)\n",
      "Bup spliced: torch.Size([4])\n",
      "tensor([ 0.1284,  0.3907, -0.3069,  0.4707], grad_fn=<SliceBackward0>)\n",
      "Xup: torch.Size([1, 2, 4])\n",
      "tensor([[[0.2287, 0.3012, 0.0201, 0.4445],\n",
      "         [0.1557, 0.3441, 0.2980, 0.3538]]], grad_fn=<AddBackward0>)\n",
      "Xfuse: torch.Size([1, 2, 4])\n",
      "tensor([[[ 0.0253,  0.0374, -0.0026,  0.1333],\n",
      "         [ 0.0065,  0.0134, -0.0333,  0.0994]]], grad_fn=<MulBackward0>)\n",
      "Wdown: torch.Size([8, 4])\n",
      "Parameter containing:\n",
      "tensor([[-0.3267, -0.2871, -0.0944,  0.2684],\n",
      "        [-0.0132, -0.2071,  0.3029, -0.1059],\n",
      "        [ 0.1787, -0.0357,  0.2356, -0.3334],\n",
      "        [ 0.2241,  0.2288, -0.3237,  0.1721],\n",
      "        [-0.0959,  0.0190,  0.1802,  0.1973],\n",
      "        [ 0.3082, -0.3128,  0.2202, -0.0355],\n",
      "        [-0.1005, -0.0374, -0.1857, -0.0862],\n",
      "        [-0.0168,  0.1753, -0.2483, -0.2729]], requires_grad=True)\n",
      "Wdown spliced: torch.Size([4, 2])\n",
      "tensor([[-0.3267, -0.2871],\n",
      "        [-0.0132, -0.2071],\n",
      "        [ 0.1787, -0.0357],\n",
      "        [ 0.2241,  0.2288]], grad_fn=<SliceBackward0>)\n",
      "Bdown: torch.Size([4])\n",
      "Parameter containing:\n",
      "tensor([ 0.0279,  0.1486,  0.2153, -0.0459], requires_grad=True)\n",
      "Bdown spliced: torch.Size([2])\n",
      "tensor([0.0279, 0.1486], grad_fn=<SliceBackward0>)\n",
      "outputs: torch.Size([1, 2, 2])\n",
      "tensor([[[0.0486, 0.1642],\n",
      "         [0.0419, 0.1679]]], grad_fn=<AddBackward0>)\n",
      "------------- END MLP.forwardTensor() ------------\n",
      "y: torch.Size([1, 2, 2])\n",
      "tensor([[[0.0486, 0.1642],\n",
      "         [0.0419, 0.1679]]], grad_fn=<AddBackward0>)\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the second sub-model |-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 2, 2])\n",
      "tensor([[[0.6205, 0.7483],\n",
      "         [0.0641, 0.8232]]])\n",
      "---------- MLP Input: torch.Tensor ------------\n",
      "------------- MLP.forwardTensor() ------------\n",
      "x: torch.Size([1, 2, 2])\n",
      "tensor([[[0.6205, 0.7483],\n",
      "         [0.0641, 0.8232]]])\n",
      "d_dim: 2\n",
      "d_skip: 2\n",
      "i_dim: 4\n",
      "i_skip: 4\n",
      "Wgate: torch.Size([4, 8])\n",
      "Parameter containing:\n",
      "tensor([[ 0.3843, -0.1348,  0.2053,  0.3153, -0.4068, -0.4677, -0.0730, -0.3016],\n",
      "        [ 0.2973,  0.1402, -0.2070, -0.2250,  0.4569, -0.2684, -0.1212, -0.3632],\n",
      "        [-0.4169, -0.0893,  0.1257,  0.4396,  0.0064, -0.3286,  0.4873, -0.2164],\n",
      "        [-0.2593, -0.2385,  0.0917,  0.1865, -0.4885,  0.2188,  0.1476, -0.3652]],\n",
      "       requires_grad=True)\n",
      "Wgate spliced: torch.Size([2, 4])\n",
      "tensor([[ 0.0064, -0.3286,  0.4873, -0.2164],\n",
      "        [-0.4885,  0.2188,  0.1476, -0.3652]], grad_fn=<SliceBackward0>)\n",
      "Bgate: torch.Size([8])\n",
      "Parameter containing:\n",
      "tensor([ 0.3008, -0.4024, -0.3317,  0.1185, -0.4596,  0.2753,  0.3196,  0.2271],\n",
      "       requires_grad=True)\n",
      "Bgate spliced: torch.Size([4])\n",
      "tensor([-0.4596,  0.2753,  0.3196,  0.2271], grad_fn=<SliceBackward0>)\n",
      "Xgate: torch.Size([1, 2, 4])\n",
      "tensor([[[-0.8212,  0.2352,  0.7324, -0.1804],\n",
      "         [-0.8613,  0.4344,  0.4723, -0.0874]]], grad_fn=<AddBackward0>)\n",
      "GeLU'ed Xgate: torch.Size([1, 2, 4])\n",
      "tensor([[[-0.1690,  0.1394,  0.5625, -0.0773],\n",
      "         [-0.1676,  0.2902,  0.3219, -0.0407]]], grad_fn=<GeluBackward0>)\n",
      "Wup: torch.Size([4, 8])\n",
      "Parameter containing:\n",
      "tensor([[ 0.4924,  0.4051, -0.3193,  0.3504,  0.1682, -0.1313, -0.0689,  0.1024],\n",
      "        [-0.3787, -0.4595, -0.1682, -0.4827,  0.0566, -0.3342,  0.2858,  0.1597],\n",
      "        [ 0.1596, -0.0244,  0.0750,  0.1617,  0.2362, -0.3552, -0.3443,  0.4419],\n",
      "        [ 0.0549,  0.0355,  0.1480,  0.4255,  0.2273,  0.0737,  0.0096, -0.2767]],\n",
      "       requires_grad=True)\n",
      "Wup spliced: torch.Size([2, 4])\n",
      "tensor([[ 0.2362, -0.3552, -0.3443,  0.4419],\n",
      "        [ 0.2273,  0.0737,  0.0096, -0.2767]], grad_fn=<SliceBackward0>)\n",
      "Bup: torch.Size([8])\n",
      "Parameter containing:\n",
      "tensor([-0.0179, -0.4823,  0.2727,  0.0841, -0.1779, -0.1410,  0.2653, -0.2537],\n",
      "       requires_grad=True)\n",
      "Bup spliced: torch.Size([4])\n",
      "tensor([-0.1779, -0.1410,  0.2653, -0.2537], grad_fn=<SliceBackward0>)\n",
      "Xup: torch.Size([1, 2, 4])\n",
      "tensor([[[ 0.1387, -0.3062,  0.0589, -0.1865],\n",
      "         [ 0.0243, -0.1030,  0.2512, -0.4531]]], grad_fn=<AddBackward0>)\n",
      "Xfuse: torch.Size([1, 2, 4])\n",
      "tensor([[[-0.0234, -0.0427,  0.0331,  0.0144],\n",
      "         [-0.0041, -0.0299,  0.0809,  0.0184]]], grad_fn=<MulBackward0>)\n",
      "Wdown: torch.Size([8, 4])\n",
      "Parameter containing:\n",
      "tensor([[-0.3254,  0.2938, -0.1539, -0.0105],\n",
      "        [ 0.0663,  0.2672,  0.1841,  0.0310],\n",
      "        [ 0.1728, -0.1593,  0.1430,  0.2365],\n",
      "        [ 0.2814,  0.0189, -0.1979,  0.3329],\n",
      "        [ 0.0216,  0.2229, -0.0351,  0.1877],\n",
      "        [-0.3396, -0.2578, -0.0887, -0.1607],\n",
      "        [-0.2664, -0.3398,  0.0982,  0.1970],\n",
      "        [ 0.3039,  0.1403, -0.0524, -0.1963]], requires_grad=True)\n",
      "Wdown spliced: torch.Size([4, 2])\n",
      "tensor([[-0.0351,  0.1877],\n",
      "        [-0.0887, -0.1607],\n",
      "        [ 0.0982,  0.1970],\n",
      "        [-0.0524, -0.1963]], grad_fn=<SliceBackward0>)\n",
      "Bdown: torch.Size([4])\n",
      "Parameter containing:\n",
      "tensor([-0.0346,  0.1719,  0.0083, -0.0434], requires_grad=True)\n",
      "Bdown spliced: torch.Size([2])\n",
      "tensor([ 0.0083, -0.0434], grad_fn=<SliceBackward0>)\n",
      "outputs: torch.Size([1, 2, 2])\n",
      "tensor([[[ 0.0154, -0.0373],\n",
      "         [ 0.0181, -0.0271]]], grad_fn=<AddBackward0>)\n",
      "------------- END MLP.forwardTensor() ------------\n",
      "y: torch.Size([1, 2, 2])\n",
      "tensor([[[ 0.0154, -0.0373],\n",
      "         [ 0.0181, -0.0271]]], grad_fn=<AddBackward0>)\n",
      "---------- RESET CONFIG --------\n",
      "model_count:  [1, 2, 4]\n"
     ]
    }
   ],
   "source": [
    "# Testing our MLP's forwardTensor()\n",
    "verbose = True\n",
    "print(\"--------- Micro Hyperparameters -------\")\n",
    "hold = config.hidden_size\n",
    "config.hidden_size = 4\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the big model |-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,2,4)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "mlp = MLP(4,8)\n",
    "y = mlp(x)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the first sub-model |-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,2,2)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "mlp = MLP(4,8)\n",
    "y = mlp(x)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the second sub-model |-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,2,2)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "mlp = MLP(4,8)\n",
    "y = mlp(x, model=1)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "verbose = False\n",
    "print(\"---------- RESET CONFIG --------\")\n",
    "config.hidden_size = hold\n",
    "print(\"model_count: \", config.model_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46a6916d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Micro Hyperparameters -------\n",
      "model_count:  [1, 2]\n",
      "model_dim_list:  [4, 2]\n",
      "x: ((tensor([[[ 0.2583,  0.1370, -1.1712,  1.6180],\n",
      "         [-0.6207, -0.9031,  1.2158,  0.2890]]]),), (tensor([[[-0.1530, -1.2461],\n",
      "         [ 1.2806, -0.1644]]]), tensor([[[ 0.2353, -1.0113],\n",
      "         [-1.0704, -0.7673]]])))\n",
      "---------- MLP Input: Tuple ------------\n",
      "------------- MLP.forwardTuple() ------------\n",
      "x: ((tensor([[[ 0.2583,  0.1370, -1.1712,  1.6180],\n",
      "         [-0.6207, -0.9031,  1.2158,  0.2890]]]),), (tensor([[[-0.1530, -1.2461],\n",
      "         [ 1.2806, -0.1644]]]), tensor([[[ 0.2353, -1.0113],\n",
      "         [-1.0704, -0.7673]]])))\n",
      "num_levels: 2\n",
      "models_per_level: [1, 2]\n",
      "i: 0\n",
      "j: 0\n",
      "------------- MLP.forwardTensor() ------------\n",
      "x: torch.Size([1, 2, 4])\n",
      "tensor([[[ 0.2583,  0.1370, -1.1712,  1.6180],\n",
      "         [-0.6207, -0.9031,  1.2158,  0.2890]]])\n",
      "d_dim: 4\n",
      "d_skip: 0\n",
      "i_dim: 8\n",
      "i_skip: 0\n",
      "Wgate: torch.Size([4, 8])\n",
      "Parameter containing:\n",
      "tensor([[ 0.1863, -0.2803,  0.4625, -0.4580,  0.3121,  0.0467, -0.0790, -0.1465],\n",
      "        [ 0.3797, -0.3123, -0.3688, -0.3750, -0.0446, -0.0879,  0.4098, -0.3060],\n",
      "        [-0.2256, -0.2867, -0.4558,  0.3796, -0.1439, -0.1428,  0.2303,  0.0482],\n",
      "        [ 0.1100,  0.4719, -0.1367, -0.3740,  0.0464, -0.1033, -0.0676,  0.0477]],\n",
      "       requires_grad=True)\n",
      "Wgate spliced: torch.Size([4, 8])\n",
      "tensor([[ 0.1863, -0.2803,  0.4625, -0.4580,  0.3121,  0.0467, -0.0790, -0.1465],\n",
      "        [ 0.3797, -0.3123, -0.3688, -0.3750, -0.0446, -0.0879,  0.4098, -0.3060],\n",
      "        [-0.2256, -0.2867, -0.4558,  0.3796, -0.1439, -0.1428,  0.2303,  0.0482],\n",
      "        [ 0.1100,  0.4719, -0.1367, -0.3740,  0.0464, -0.1033, -0.0676,  0.0477]],\n",
      "       grad_fn=<AliasBackward0>)\n",
      "Bgate: torch.Size([8])\n",
      "Parameter containing:\n",
      "tensor([ 0.2357, -0.2306, -0.4972,  0.4800, -0.4742,  0.3036,  0.1153,  0.1335],\n",
      "       requires_grad=True)\n",
      "Bgate spliced: torch.Size([8])\n",
      "tensor([ 0.2357, -0.2306, -0.4972,  0.4800, -0.4742,  0.3036,  0.1153,  0.1335],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xgate: torch.Size([1, 2, 8])\n",
      "tensor([[[ 0.7782,  0.7534, -0.1156, -0.7394, -0.1562,  0.3037, -0.2280,\n",
      "           0.0745],\n",
      "         [-0.4653,  0.0132, -1.0449,  1.4565, -0.7893,  0.1505,  0.0547,\n",
      "           0.5732]]], grad_fn=<AddBackward0>)\n",
      "GeLU'ed Xgate: torch.Size([1, 2, 8])\n",
      "tensor([[[ 0.6083,  0.5834, -0.0525, -0.1699, -0.0684,  0.1881, -0.0934,\n",
      "           0.0395],\n",
      "         [-0.1493,  0.0067, -0.1547,  1.3507, -0.1697,  0.0843,  0.0285,\n",
      "           0.4108]]], grad_fn=<GeluBackward0>)\n",
      "Wup: torch.Size([4, 8])\n",
      "Parameter containing:\n",
      "tensor([[-0.2685,  0.1442,  0.2358, -0.1505, -0.0661,  0.2592,  0.0171,  0.3597],\n",
      "        [ 0.0502, -0.1260,  0.4952,  0.1856, -0.4531, -0.0034, -0.1874,  0.0640],\n",
      "        [-0.2330, -0.3899, -0.3097,  0.1499, -0.1695,  0.1976, -0.3880,  0.4740],\n",
      "        [ 0.4368,  0.3926, -0.3620, -0.2235, -0.0447, -0.2876,  0.0687,  0.1953]],\n",
      "       requires_grad=True)\n",
      "Wup spliced: torch.Size([4, 8])\n",
      "tensor([[-0.2685,  0.1442,  0.2358, -0.1505, -0.0661,  0.2592,  0.0171,  0.3597],\n",
      "        [ 0.0502, -0.1260,  0.4952,  0.1856, -0.4531, -0.0034, -0.1874,  0.0640],\n",
      "        [-0.2330, -0.3899, -0.3097,  0.1499, -0.1695,  0.1976, -0.3880,  0.4740],\n",
      "        [ 0.4368,  0.3926, -0.3620, -0.2235, -0.0447, -0.2876,  0.0687,  0.1953]],\n",
      "       grad_fn=<AliasBackward0>)\n",
      "Bup: torch.Size([8])\n",
      "Parameter containing:\n",
      "tensor([ 0.1137, -0.1044, -0.3828,  0.1259, -0.4157, -0.1103,  0.4669, -0.0564],\n",
      "       requires_grad=True)\n",
      "Bup spliced: torch.Size([8])\n",
      "tensor([ 0.1137, -0.1044, -0.3828,  0.1259, -0.4157, -0.1103,  0.4669, -0.0564],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xup: torch.Size([1, 2, 8])\n",
      "tensor([[[ 1.0310,  1.0075, -0.4770, -0.4247, -0.3686, -0.7407,  1.0112,\n",
      "          -0.1939],\n",
      "         [ 0.0780, -0.4408, -1.4575,  0.1694, -0.1845, -0.1110,  0.1736,\n",
      "           0.2953]]], grad_fn=<AddBackward0>)\n",
      "Xfuse: torch.Size([1, 2, 8])\n",
      "tensor([[[ 0.6272,  0.5878,  0.0250,  0.0722,  0.0252, -0.1393, -0.0945,\n",
      "          -0.0077],\n",
      "         [-0.0116, -0.0029,  0.2255,  0.2288,  0.0313, -0.0094,  0.0050,\n",
      "           0.1213]]], grad_fn=<MulBackward0>)\n",
      "Wdown: torch.Size([8, 4])\n",
      "Parameter containing:\n",
      "tensor([[ 0.1426,  0.0410, -0.0737,  0.2459],\n",
      "        [-0.3022, -0.0239,  0.1967, -0.3077],\n",
      "        [-0.0389, -0.2365, -0.1682, -0.2535],\n",
      "        [ 0.0955,  0.1159, -0.3307,  0.1766],\n",
      "        [-0.3291, -0.2194, -0.2834,  0.0600],\n",
      "        [-0.3205,  0.3102, -0.0751, -0.2271],\n",
      "        [-0.2392, -0.1752, -0.1309,  0.0628],\n",
      "        [ 0.1046,  0.0355, -0.2765, -0.2553]], requires_grad=True)\n",
      "Wdown spliced: torch.Size([8, 4])\n",
      "tensor([[ 0.1426,  0.0410, -0.0737,  0.2459],\n",
      "        [-0.3022, -0.0239,  0.1967, -0.3077],\n",
      "        [-0.0389, -0.2365, -0.1682, -0.2535],\n",
      "        [ 0.0955,  0.1159, -0.3307,  0.1766],\n",
      "        [-0.3291, -0.2194, -0.2834,  0.0600],\n",
      "        [-0.3205,  0.3102, -0.0751, -0.2271],\n",
      "        [-0.2392, -0.1752, -0.1309,  0.0628],\n",
      "        [ 0.1046,  0.0355, -0.2765, -0.2553]], grad_fn=<AliasBackward0>)\n",
      "Bdown: torch.Size([4])\n",
      "Parameter containing:\n",
      "tensor([ 0.3467,  0.1025, -0.1755, -0.1387], requires_grad=True)\n",
      "Bdown spliced: torch.Size([4])\n",
      "tensor([ 0.3467,  0.1025, -0.1755, -0.1387], grad_fn=<SliceBackward0>)\n",
      "outputs: torch.Size([1, 2, 4])\n",
      "tensor([[[ 0.3227,  0.0841, -0.1164, -0.1298],\n",
      "         [ 0.3633,  0.0689, -0.3312, -0.1841]]], grad_fn=<AddBackward0>)\n",
      "------------- END MLP.forwardTensor() ------------\n",
      "forwardTensor() output: torch.Size([1, 2, 4])\n",
      "tensor([[[ 0.3227,  0.0841, -0.1164, -0.1298],\n",
      "         [ 0.3633,  0.0689, -0.3312, -0.1841]]], grad_fn=<AddBackward0>)\n",
      "out_lvl: (tensor([[[ 0.3585,  0.0934, -0.1293, -0.1443],\n",
      "         [ 0.4036,  0.0766, -0.3680, -0.2045]]], grad_fn=<MulBackward0>),)\n",
      "i: 1\n",
      "j: 0\n",
      "------------- MLP.forwardTensor() ------------\n",
      "x: torch.Size([1, 2, 2])\n",
      "tensor([[[-0.1530, -1.2461],\n",
      "         [ 1.2806, -0.1644]]])\n",
      "d_dim: 2\n",
      "d_skip: 0\n",
      "i_dim: 4\n",
      "i_skip: 0\n",
      "Wgate: torch.Size([4, 8])\n",
      "Parameter containing:\n",
      "tensor([[ 0.1863, -0.2803,  0.4625, -0.4580,  0.3121,  0.0467, -0.0790, -0.1465],\n",
      "        [ 0.3797, -0.3123, -0.3688, -0.3750, -0.0446, -0.0879,  0.4098, -0.3060],\n",
      "        [-0.2256, -0.2867, -0.4558,  0.3796, -0.1439, -0.1428,  0.2303,  0.0482],\n",
      "        [ 0.1100,  0.4719, -0.1367, -0.3740,  0.0464, -0.1033, -0.0676,  0.0477]],\n",
      "       requires_grad=True)\n",
      "Wgate spliced: torch.Size([2, 4])\n",
      "tensor([[ 0.1863, -0.2803,  0.4625, -0.4580],\n",
      "        [ 0.3797, -0.3123, -0.3688, -0.3750]], grad_fn=<SliceBackward0>)\n",
      "Bgate: torch.Size([8])\n",
      "Parameter containing:\n",
      "tensor([ 0.2357, -0.2306, -0.4972,  0.4800, -0.4742,  0.3036,  0.1153,  0.1335],\n",
      "       requires_grad=True)\n",
      "Bgate spliced: torch.Size([4])\n",
      "tensor([ 0.2357, -0.2306, -0.4972,  0.4800], grad_fn=<SliceBackward0>)\n",
      "Xgate: torch.Size([1, 2, 4])\n",
      "tensor([[[-0.2659,  0.2014, -0.1084,  1.0175],\n",
      "         [ 0.4119, -0.5383,  0.1557, -0.0448]]], grad_fn=<AddBackward0>)\n",
      "GeLU'ed Xgate: torch.Size([1, 2, 4])\n",
      "tensor([[[-0.1051,  0.1168, -0.0495,  0.8603],\n",
      "         [ 0.2717, -0.1589,  0.0875, -0.0216]]], grad_fn=<GeluBackward0>)\n",
      "Wup: torch.Size([4, 8])\n",
      "Parameter containing:\n",
      "tensor([[-0.2685,  0.1442,  0.2358, -0.1505, -0.0661,  0.2592,  0.0171,  0.3597],\n",
      "        [ 0.0502, -0.1260,  0.4952,  0.1856, -0.4531, -0.0034, -0.1874,  0.0640],\n",
      "        [-0.2330, -0.3899, -0.3097,  0.1499, -0.1695,  0.1976, -0.3880,  0.4740],\n",
      "        [ 0.4368,  0.3926, -0.3620, -0.2235, -0.0447, -0.2876,  0.0687,  0.1953]],\n",
      "       requires_grad=True)\n",
      "Wup spliced: torch.Size([2, 4])\n",
      "tensor([[-0.2685,  0.1442,  0.2358, -0.1505],\n",
      "        [ 0.0502, -0.1260,  0.4952,  0.1856]], grad_fn=<SliceBackward0>)\n",
      "Bup: torch.Size([8])\n",
      "Parameter containing:\n",
      "tensor([ 0.1137, -0.1044, -0.3828,  0.1259, -0.4157, -0.1103,  0.4669, -0.0564],\n",
      "       requires_grad=True)\n",
      "Bup spliced: torch.Size([4])\n",
      "tensor([ 0.1137, -0.1044, -0.3828,  0.1259], grad_fn=<SliceBackward0>)\n",
      "Xup: torch.Size([1, 2, 4])\n",
      "tensor([[[ 0.0922,  0.0305, -1.0359, -0.0824],\n",
      "         [-0.2383,  0.1010, -0.1623, -0.0973]]], grad_fn=<AddBackward0>)\n",
      "Xfuse: torch.Size([1, 2, 4])\n",
      "tensor([[[-0.0097,  0.0036,  0.0513, -0.0709],\n",
      "         [-0.0648, -0.0161, -0.0142,  0.0021]]], grad_fn=<MulBackward0>)\n",
      "Wdown: torch.Size([8, 4])\n",
      "Parameter containing:\n",
      "tensor([[ 0.1426,  0.0410, -0.0737,  0.2459],\n",
      "        [-0.3022, -0.0239,  0.1967, -0.3077],\n",
      "        [-0.0389, -0.2365, -0.1682, -0.2535],\n",
      "        [ 0.0955,  0.1159, -0.3307,  0.1766],\n",
      "        [-0.3291, -0.2194, -0.2834,  0.0600],\n",
      "        [-0.3205,  0.3102, -0.0751, -0.2271],\n",
      "        [-0.2392, -0.1752, -0.1309,  0.0628],\n",
      "        [ 0.1046,  0.0355, -0.2765, -0.2553]], requires_grad=True)\n",
      "Wdown spliced: torch.Size([4, 2])\n",
      "tensor([[ 0.1426,  0.0410],\n",
      "        [-0.3022, -0.0239],\n",
      "        [-0.0389, -0.2365],\n",
      "        [ 0.0955,  0.1159]], grad_fn=<SliceBackward0>)\n",
      "Bdown: torch.Size([4])\n",
      "Parameter containing:\n",
      "tensor([ 0.3467,  0.1025, -0.1755, -0.1387], requires_grad=True)\n",
      "Bdown spliced: torch.Size([2])\n",
      "tensor([0.3467, 0.1025], grad_fn=<SliceBackward0>)\n",
      "outputs: torch.Size([1, 2, 2])\n",
      "tensor([[[0.3355, 0.0816],\n",
      "         [0.3431, 0.1038]]], grad_fn=<AddBackward0>)\n",
      "------------- END MLP.forwardTensor() ------------\n",
      "forwardTensor() output: torch.Size([1, 2, 2])\n",
      "tensor([[[0.3355, 0.0816],\n",
      "         [0.3431, 0.1038]]], grad_fn=<AddBackward0>)\n",
      "j: 1\n",
      "------------- MLP.forwardTensor() ------------\n",
      "x: torch.Size([1, 2, 2])\n",
      "tensor([[[ 0.2353, -1.0113],\n",
      "         [-1.0704, -0.7673]]])\n",
      "d_dim: 2\n",
      "d_skip: 2\n",
      "i_dim: 4\n",
      "i_skip: 4\n",
      "Wgate: torch.Size([4, 8])\n",
      "Parameter containing:\n",
      "tensor([[ 0.1863, -0.2803,  0.4625, -0.4580,  0.3121,  0.0467, -0.0790, -0.1465],\n",
      "        [ 0.3797, -0.3123, -0.3688, -0.3750, -0.0446, -0.0879,  0.4098, -0.3060],\n",
      "        [-0.2256, -0.2867, -0.4558,  0.3796, -0.1439, -0.1428,  0.2303,  0.0482],\n",
      "        [ 0.1100,  0.4719, -0.1367, -0.3740,  0.0464, -0.1033, -0.0676,  0.0477]],\n",
      "       requires_grad=True)\n",
      "Wgate spliced: torch.Size([2, 4])\n",
      "tensor([[-0.1439, -0.1428,  0.2303,  0.0482],\n",
      "        [ 0.0464, -0.1033, -0.0676,  0.0477]], grad_fn=<SliceBackward0>)\n",
      "Bgate: torch.Size([8])\n",
      "Parameter containing:\n",
      "tensor([ 0.2357, -0.2306, -0.4972,  0.4800, -0.4742,  0.3036,  0.1153,  0.1335],\n",
      "       requires_grad=True)\n",
      "Bgate spliced: torch.Size([4])\n",
      "tensor([-0.4742,  0.3036,  0.1153,  0.1335], grad_fn=<SliceBackward0>)\n",
      "Xgate: torch.Size([1, 2, 4])\n",
      "tensor([[[-0.5550,  0.3745,  0.2378,  0.0966],\n",
      "         [-0.3558,  0.5357, -0.0794,  0.0453]]], grad_fn=<AddBackward0>)\n",
      "GeLU'ed Xgate: torch.Size([1, 2, 4])\n",
      "tensor([[[-0.1606,  0.2419,  0.1412,  0.0520],\n",
      "         [-0.1284,  0.3771, -0.0372,  0.0235]]], grad_fn=<GeluBackward0>)\n",
      "Wup: torch.Size([4, 8])\n",
      "Parameter containing:\n",
      "tensor([[-0.2685,  0.1442,  0.2358, -0.1505, -0.0661,  0.2592,  0.0171,  0.3597],\n",
      "        [ 0.0502, -0.1260,  0.4952,  0.1856, -0.4531, -0.0034, -0.1874,  0.0640],\n",
      "        [-0.2330, -0.3899, -0.3097,  0.1499, -0.1695,  0.1976, -0.3880,  0.4740],\n",
      "        [ 0.4368,  0.3926, -0.3620, -0.2235, -0.0447, -0.2876,  0.0687,  0.1953]],\n",
      "       requires_grad=True)\n",
      "Wup spliced: torch.Size([2, 4])\n",
      "tensor([[-0.1695,  0.1976, -0.3880,  0.4740],\n",
      "        [-0.0447, -0.2876,  0.0687,  0.1953]], grad_fn=<SliceBackward0>)\n",
      "Bup: torch.Size([8])\n",
      "Parameter containing:\n",
      "tensor([ 0.1137, -0.1044, -0.3828,  0.1259, -0.4157, -0.1103,  0.4669, -0.0564],\n",
      "       requires_grad=True)\n",
      "Bup spliced: torch.Size([4])\n",
      "tensor([-0.4157, -0.1103,  0.4669, -0.0564], grad_fn=<SliceBackward0>)\n",
      "Xup: torch.Size([1, 2, 4])\n",
      "tensor([[[-0.4103,  0.2271,  0.3061, -0.1424],\n",
      "         [-0.1999, -0.1011,  0.8295, -0.7136]]], grad_fn=<AddBackward0>)\n",
      "Xfuse: torch.Size([1, 2, 4])\n",
      "tensor([[[ 0.0659,  0.0549,  0.0432, -0.0074],\n",
      "         [ 0.0257, -0.0381, -0.0308, -0.0168]]], grad_fn=<MulBackward0>)\n",
      "Wdown: torch.Size([8, 4])\n",
      "Parameter containing:\n",
      "tensor([[ 0.1426,  0.0410, -0.0737,  0.2459],\n",
      "        [-0.3022, -0.0239,  0.1967, -0.3077],\n",
      "        [-0.0389, -0.2365, -0.1682, -0.2535],\n",
      "        [ 0.0955,  0.1159, -0.3307,  0.1766],\n",
      "        [-0.3291, -0.2194, -0.2834,  0.0600],\n",
      "        [-0.3205,  0.3102, -0.0751, -0.2271],\n",
      "        [-0.2392, -0.1752, -0.1309,  0.0628],\n",
      "        [ 0.1046,  0.0355, -0.2765, -0.2553]], requires_grad=True)\n",
      "Wdown spliced: torch.Size([4, 2])\n",
      "tensor([[-0.2834,  0.0600],\n",
      "        [-0.0751, -0.2271],\n",
      "        [-0.1309,  0.0628],\n",
      "        [-0.2765, -0.2553]], grad_fn=<SliceBackward0>)\n",
      "Bdown: torch.Size([4])\n",
      "Parameter containing:\n",
      "tensor([ 0.3467,  0.1025, -0.1755, -0.1387], requires_grad=True)\n",
      "Bdown spliced: torch.Size([2])\n",
      "tensor([-0.1755, -0.1387], grad_fn=<SliceBackward0>)\n",
      "outputs: torch.Size([1, 2, 2])\n",
      "tensor([[[-0.2019, -0.1427],\n",
      "         [-0.1713, -0.1262]]], grad_fn=<AddBackward0>)\n",
      "------------- END MLP.forwardTensor() ------------\n",
      "forwardTensor() output: torch.Size([1, 2, 2])\n",
      "tensor([[[-0.2019, -0.1427],\n",
      "         [-0.1713, -0.1262]]], grad_fn=<AddBackward0>)\n",
      "out_lvl: (tensor([[[0.3728, 0.0907],\n",
      "         [0.3812, 0.1153]]], grad_fn=<MulBackward0>), tensor([[[-0.2244, -0.1585],\n",
      "         [-0.1903, -0.1402]]], grad_fn=<MulBackward0>))\n",
      "out: ((tensor([[[ 0.3585,  0.0934, -0.1293, -0.1443],\n",
      "         [ 0.4036,  0.0766, -0.3680, -0.2045]]], grad_fn=<MulBackward0>),), (tensor([[[0.3728, 0.0907],\n",
      "         [0.3812, 0.1153]]], grad_fn=<MulBackward0>), tensor([[[-0.2244, -0.1585],\n",
      "         [-0.1903, -0.1402]]], grad_fn=<MulBackward0>)))\n",
      "------------- END MLP.forwardTuple() ------------\n",
      "out: ((tensor([[[ 0.3585,  0.0934, -0.1293, -0.1443],\n",
      "         [ 0.4036,  0.0766, -0.3680, -0.2045]]], grad_fn=<MulBackward0>),), (tensor([[[0.3728, 0.0907],\n",
      "         [0.3812, 0.1153]]], grad_fn=<MulBackward0>), tensor([[[-0.2244, -0.1585],\n",
      "         [-0.1903, -0.1402]]], grad_fn=<MulBackward0>)))\n",
      "---------- RESET CONFIG --------\n",
      "model_count:  [1, 2, 4]\n",
      "model_dim_list:  [128, 64, 32]\n"
     ]
    }
   ],
   "source": [
    "# Testing our MLP's forwardTuple()\n",
    "verbose = True\n",
    "print(\"--------- Micro Hyperparameters -------\")\n",
    "hold1, hold2 = config.hidden_size, config.levels\n",
    "config.hidden_size = 4\n",
    "config.levels = 2\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "\n",
    "mlp = MLP(4,8)\n",
    "x = ((torch.randn((1,2,4)),),\n",
    "     (torch.randn((1,2,2)),torch.randn((1,2,2)))\n",
    "    )\n",
    "print(f\"x: {x}\")\n",
    "out = mlp(x)\n",
    "print(f\"out: {out}\")\n",
    "\n",
    "verbose = False\n",
    "print(\"---------- RESET CONFIG --------\")\n",
    "config.hidden_size = hold1\n",
    "config.levels = hold2\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f269924",
   "metadata": {},
   "source": [
    "# Attention\n",
    "\n",
    "To subset the attention heads, we have to not only splice according to the model's embedding dimension but also take into account new smaller head sizes and how they're spaced throughout the matrix. I'm assuming you know how self-attention works well enough to look at this weight matrix and get the idea\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./images/sa.jpeg\" width=\"512\"/>\n",
    "</p>\n",
    "\n",
    "then we've gotta concatenate the outputs of each head\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./images/mha_concat.jpeg\" width=\"512\"/>\n",
    "</p>\n",
    "\n",
    "and after that linearly project them\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./images/mha_proj.jpeg\" width=\"512\"/>\n",
    "</p>\n",
    "\n",
    "this is the place where our splicing gets conceptually annoying. instead of just grabbing the matrix in the upper corner, because of the way attention head output concatenation works we actually need to skip over certain parts of the linear projection matrix and then concatenate them together in order to use them. Here's an example of what the matrix multiplication looks like. on the left is a simplified version of the concatenated attention heads where i just showed it as a matrix rather than a tensor, and then on the right is the actual projection matrix. notice how the numbers in the pink output matrix look similar to the first column of the purple output matrix with a positive number, its negative, and then a smaller positive number; that's the self-similarity in action. the yellow arrows point to the parts that get skipped over. obviously this would look a lot uglier with bigger matrices & incorporating the blue/green layer\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./images/mha_proj_matmul.jpeg\" width=\"512\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a66e2155",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiQueryAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements Multi-Query Attention which supports a distinct number of attention heads for queries and key-values (KV).\n",
    "    In the case where the same number of queries and key-values are used, this implemenation is equivalent to regular Multi-Head Attention.  \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.num_kv_heads = config.num_key_value_heads\n",
    "        \n",
    "        # Determines the number of query heads associated with each KV head.\n",
    "        self.num_queries_per_kv = self.num_heads // self.num_kv_heads\n",
    "\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.head_dim = config.head_dim\n",
    "        self.theta = config.rope_theta\n",
    "\n",
    "        # Calculates the total size for all query projections.\n",
    "        self.q_size = self.num_heads * self.head_dim\n",
    "        # Calculates the total size for all key and value projections.\n",
    "        self.kv_size = self.num_kv_heads * self.head_dim\n",
    "        \n",
    "        # Initialize our learnable matrices\n",
    "        # the linear projection layer for queries, keys, and values\n",
    "        # no real reason why we're creating one matrix instead of separate ones. cleaner model summary view?\n",
    "        self.Wqkv = nn.Parameter(torch.Tensor(self.hidden_size,\n",
    "                                              (self.num_heads + 2 * self.num_kv_heads) * self.head_dim))\n",
    "        # the output projection layer, mapping the concatenated attention outputs back to the hidden size.\n",
    "        self.Wo = nn.Parameter(torch.Tensor(self.num_heads * self.head_dim, self.hidden_size))\n",
    "        \n",
    "        # Initialize weights with uniform distribution\n",
    "        # For qkv_proj, where in_features is hidden_size\n",
    "        limit_Wqkv = 1 / np.sqrt(self.hidden_size)\n",
    "        nn.init.uniform_(self.Wqkv, -limit_Wqkv, limit_Wqkv)\n",
    "        # for o_proj, where in_features is self.num_heads * self.head_dim\n",
    "        limit_Wo = 1 / np.sqrt(self.num_heads * self.head_dim)\n",
    "        nn.init.uniform_(self.Wo, -limit_Wo, limit_Wo)\n",
    "        \n",
    "        # for our attention mask we'll use very large negative values to prevent attending to certain tokens\n",
    "        mask_negatives = torch.full((1, 1, config.max_position_embeddings, config.max_position_embeddings),\n",
    "                                 -2.3819763e38).to(torch.float)\n",
    "        # then we'll replace the lower triangular ones with 0's to allow attention to see past tokens\n",
    "        mask = torch.triu(mask_negatives, diagonal=1).to(config.device)\n",
    "        # to define self.mask as a tensor that shouldn't undergo gradient descent\n",
    "        self.register_buffer('mask', mask)\n",
    "        \n",
    "        # defining our dropout\n",
    "        self.drop = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forwardTensor(self,\n",
    "                      x: torch.Tensor,\n",
    "                      model: int = 0,\n",
    "                     ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            x (torch.Tensor): Te input tensor to the attention mechanism.\n",
    "                        shape (batch_size, input_len, hidden_size)\n",
    "            model (int): the indicator of which model we're using. \n",
    "                        used in calculating our skip length for splicing. \n",
    "                        defaults to the equivalent of what's used in MatFormer+, meaning no skip, aka we use the top-left-most splice\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: The output tensor after applying the attention mechanism\n",
    "        \"\"\"\n",
    "        global verbose\n",
    "        if verbose: print(\"----------------- MultiQueryAttention.forwardTensor() --------------------\")\n",
    "        \n",
    "        # Ensures the input tensor is 3-dimensional (batch_size, input_len, hidden_size).\n",
    "        x_shape = x.shape\n",
    "        assert len(x_shape) == 3\n",
    "        if verbose: print(f\"x shape: {x_shape}\")\n",
    "\n",
    "        # Extracts input sequence length and embedding dimension length from the hidden states tensor.\n",
    "        batch_size, input_len, d_dim = x_shape\n",
    "        \n",
    "        # figuring out how we should do our splicing\n",
    "        # first along the embedding dimension\n",
    "        d_skip = model * d_dim  # the size of our skip along the model's embedding dimension\n",
    "        if verbose: print(f\"d_skip: {d_skip}\")\n",
    "        \n",
    "        # then for splicing along the head sizes dimension\n",
    "        index = config.model_dim_list.index(d_dim)\n",
    "        models_in_this_level = config.model_count[index] # how many models are in this level\n",
    "        h_dim = config.head_dim_list[index] # the head dimension size of this model in this level\n",
    "        h_skip = model * h_dim # the size of our skip along the head dimension\n",
    "        if verbose: \n",
    "            print(f\"models_in_this_level: {models_in_this_level}\")\n",
    "            print(f\"h_dim: {h_dim}\")\n",
    "            print(f\"h_skip: {h_skip}\")\n",
    "\n",
    "        # Splits the Wqkv tensor into separate tensors for queries, keys, and values based on their respective sizes.\n",
    "        if verbose: print(f\"self.Wqkv: {self.Wqkv.shape}\\n{self.Wqkv}\")\n",
    "        Wq, Wk, Wv = self.Wqkv.split([self.q_size,\n",
    "                                      self.kv_size,\n",
    "                                      self.kv_size],dim=-1)\n",
    "        if verbose: \n",
    "            print(f\"Wq: {Wq.shape}\\n{Wq}\")\n",
    "            print(f\"Wk: {Wk.shape}\\n{Wk}\")\n",
    "            print(f\"Wv: {Wv.shape}\\n{Wv}\")\n",
    "        \n",
    "        # splicing to get our correct weight matrices for each respective head\n",
    "        # d_dim is relatively self-explanatory\n",
    "        # i*self.head_dim is bc we initialized one single q, k, and v matrix for all heads so we have to\n",
    "        # iterate through said matrix to get to the correct head\n",
    "        Wq = torch.cat([Wq[d_skip:d_skip + d_dim,\\\n",
    "                               i*self.head_dim + h_skip:i*self.head_dim + h_skip + h_dim] \\\n",
    "                               for i in range(self.num_heads)], dim=1)\n",
    "        Wk = torch.cat([Wk[d_skip:d_skip + d_dim,\\\n",
    "                               i*self.head_dim + h_skip:i*self.head_dim + h_skip + h_dim] \\\n",
    "                               for i in range(self.num_kv_heads)], dim=1)\n",
    "        Wv = torch.cat([Wv[d_skip:d_skip + d_dim,\\\n",
    "                               i*self.head_dim + h_skip:i*self.head_dim + h_skip + h_dim] \\\n",
    "                               for i in range(self.num_kv_heads)], dim=1)\n",
    "        if verbose:\n",
    "            print(f\"Wq spliced: {Wq.shape}\\n{Wq}\")\n",
    "            print(f\"Wk spliced: {Wk.shape}\\n{Wk}\")\n",
    "            print(f\"Wv spliced: {Wv.shape}\\n{Wv}\")\n",
    "        \n",
    "        # this needs to be size (d_dim, (self.num_heads + 2 * self.num_kv_heads) * h_dim) aka (32,24)\n",
    "        # recombine the spliced Wq Wk and Wv. Now they're the right size for matmul against x\n",
    "        Wqkv_spliced = torch.cat((Wq, Wk, Wv), dim=-1)\n",
    "        if verbose:\n",
    "            print(f\"Wqkv_spliced: {Wqkv_spliced.shape}\\n{Wqkv_spliced}\")\n",
    "        \n",
    "\n",
    "        # finally we can project x to get our queries, keys and values\n",
    "        xqkv = x @ Wqkv_spliced\n",
    "        if verbose: print(f\"xqkv: {xqkv.shape}\\n{xqkv}\")\n",
    "            \n",
    "        # Splits the combined Xqkv tensor into separate tensors for queries (xq), keys (xk), and values (xv) based on their respective sizes.\n",
    "        xq, xk, xv = xqkv.split([self.q_size // models_in_this_level,\n",
    "                                 self.kv_size // models_in_this_level,\n",
    "                                 self.kv_size // models_in_this_level],dim=-1)\n",
    "        if verbose:\n",
    "            print(f\"xq: {xq.shape}\\n{xq}\")\n",
    "            print(f\"xk: {xk.shape}\\n{xk}\")\n",
    "            print(f\"xv: {xv.shape}\\n{xv}\")\n",
    "\n",
    "        # Reshapes each of the Q, K, and V tensors to separate the heads and align the dimensions for attention operations.\n",
    "        xq = xq.view(batch_size, input_len, self.num_heads, h_dim)#, self.head_dim)\n",
    "        xk = xk.view(batch_size, input_len, self.num_kv_heads, h_dim)#, self.head_dim)\n",
    "        xv = xv.view(batch_size, input_len, self.num_kv_heads, h_dim)#, self.head_dim)\n",
    "        if verbose:\n",
    "            print(f\"xq reshaped: {xq.shape}\\n{xq}\")\n",
    "            print(f\"xk reshaped: {xk.shape}\\n{xk}\")\n",
    "            print(f\"xv reshaped: {xv.shape}\\n{xv}\")\n",
    "\n",
    "        # Applies rotary positional embeddings to queries and keys to incorporate positional information.\n",
    "        xq = apply_rotary_emb(xq, h_dim, self.theta)#self.head_dim\n",
    "        xk = apply_rotary_emb(xk, h_dim, self.theta)#self.head_dim\n",
    "        # is the differring head dimension going to mess with RoPE? Not sure\n",
    "        if verbose:\n",
    "            print(f\"rotated xq: {xq.shape}\\n{xq}\")\n",
    "            print(f\"rotated xk: {xk.shape}\\n{xk}\")\n",
    "\n",
    "        # If the number of KV heads is different from the number of query heads, adjusts keys and values to match the query heads count.\n",
    "        if self.num_kv_heads != self.num_heads:\n",
    "            # [batch_size, input_len, n_local_heads, head_dim]\n",
    "            xk = torch.repeat_interleave(xk, self.num_queries_per_kv, dim=2)\n",
    "            xv = torch.repeat_interleave(xv, self.num_queries_per_kv, dim=2)\n",
    "            if verbose:\n",
    "                print(f\"repeat_interleaved xk: {xk.shape}\\n{xk}\")\n",
    "                print(f\"repeat_interleaved xv: {xv.shape}\\n{xv}\")\n",
    "\n",
    "        # Transposes Q, K, and V tensors to align them for the batch matrix multiplication in attention calculation.\n",
    "        # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        q = xq.transpose(1, 2)\n",
    "        k = xk.transpose(1, 2)\n",
    "        v = xv.transpose(1, 2)\n",
    "        if verbose:\n",
    "            print(f\"transposed xq: {q.shape}\\n{q}\")\n",
    "            print(f\"transposed xk: {k.shape}\\n{k}\")\n",
    "            print(f\"transposed xv: {v.shape}\\n{v}\")\n",
    "\n",
    "        # Calculates attention scores by performing a batch matrix multiplication between queries and keys, followed by scaling.\n",
    "        # [batch_size, n_local_heads, input_len, input_len]\n",
    "        scores = torch.matmul(q, k.transpose(2, 3)) * h_dim**-0.5#self.scaling\n",
    "        if verbose: print(f\"scores: {scores.shape}\\n{scores}\")\n",
    "        \n",
    "        # Applies the lower-triangular mask to the attention scores\n",
    "        if verbose: print(f\"mask: {self.mask[...,:input_len, :input_len].shape}\\n{self.mask[...,:input_len, :input_len]}\")\n",
    "        scores = scores + self.mask[...,:input_len, :input_len] # make sure mask is the correct size. input_len <= max_seq_len\n",
    "        if verbose: print(f\"masked scores: {scores.shape}\\n{scores}\")\n",
    "\n",
    "        # Applies softmax to the scores to obtain attention probabilities\n",
    "        scores = F.softmax(scores, dim=-1)\n",
    "        if verbose: print(f\"softmaxed scores: {scores.shape}\\n{scores}\")\n",
    "        \n",
    "        # Computes the weighted sum of values based on the attention scores to obtain the output of the attention mechanism.\n",
    "        # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        attention = torch.matmul(scores, v)\n",
    "        if verbose: print(f\"attention: {attention.shape}\\n{attention}\")\n",
    "\n",
    "        # Reshapes the attention output to match the expected output dimensions, combining the heads back into the hidden dimension.\n",
    "        # [batch_size, input_len, hidden_dim]\n",
    "        attention = attention.transpose(1, 2).contiguous().view(batch_size, input_len, -1)\n",
    "        if verbose: print(f\"reshaped attention: {attention.shape}\\n{attention}\")\n",
    "\n",
    "        # Splice the output projection\n",
    "        Wo = torch.cat([self.Wo[i*self.head_dim + h_skip:i*self.head_dim + h_skip + h_dim,\\\n",
    "                                d_skip:d_skip + d_dim,\\\n",
    "                               ] for i in range(self.num_heads)], dim=0)\n",
    "        if verbose: \n",
    "            print(f\"self.Wo: {self.Wo.shape}\\n{self.Wo}\")\n",
    "            print(f\"spliced Wo: {Wo.shape}\\n{Wo}\")\n",
    "            \n",
    "        # Applies the final linear projection to the attention output, mapping it back to the hidden size dimension.\n",
    "        output = attention @ Wo\n",
    "        if verbose: \n",
    "            print(f\"projected output: {output.shape}\\n{output}\")\n",
    "            print(\"----------------- END MultiQueryAttention.forwardTensor() --------------------\")\n",
    "            \n",
    "        return output\n",
    "\n",
    "    def forwardTuple(self,\n",
    "                     x: Tuple[Tuple[torch.Tensor]],\n",
    "                     drop_bool: bool = True\n",
    "                    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the Attention module during training.\n",
    "\n",
    "        Parameters:\n",
    "            x (Tuple[Tuple[Tensor]]): \n",
    "                The input tuple of tuples of tensors \n",
    "                first tuple is of length config.levels and second layer of tuples have lengths of config.model_count\n",
    "                tensors are shape (batch size, sequence length, hidden dimension) where hidden dimension changes by which model was used\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Tuple[Tensor]]: \n",
    "                The output tuple of tuples of tensors after applying the MQA mechanism\n",
    "        \"\"\"\n",
    "        global verbose\n",
    "        if verbose: \n",
    "            print(\"------------- MultiQueryAttention.forwardTuple() ------------\")\n",
    "            print(f\"x: {x}\")\n",
    "            \n",
    "        # forwardTuple() should only be used during training, so we assert input_len == max_position_embeddings\n",
    "        input_len = x[0][0].shape[1]\n",
    "        if verbose: print(f\"input_len: {input_len}\")\n",
    "        assert input_len == config.max_position_embeddings\n",
    "\n",
    "        # we could define these from the config but this way the method is more flexible to testing\n",
    "        num_levels = len(x)\n",
    "        models_per_level = [len(x[i]) for i in range(num_levels)]\n",
    "        if verbose: \n",
    "            print(f\"num_levels: {num_levels}\")\n",
    "            print(f\"models_per_level: {models_per_level}\")\n",
    "\n",
    "        # the loop that iterates over levels, aka the different potential sizes of models\n",
    "        out = ()\n",
    "        for i in range(num_levels):\n",
    "            if verbose: print(f\"Level {i} from range({num_levels})\")\n",
    "\n",
    "            # now for the loop that iterates over models in this level\n",
    "            out_lvl = ()\n",
    "            for j in range(models_per_level[i]):\n",
    "                if verbose: print(f\"Model {j} from range({models_per_level[i]})\")\n",
    "\n",
    "                output = self.forwardTensor(x[i][j], model=j)\n",
    "                if verbose: print(f\"forwardTensor() output: {output.shape}\\n{output}\")\n",
    "                \n",
    "                out_lvl += (self.drop(output),) if drop_bool else (output,)\n",
    "            \n",
    "            out += (out_lvl,)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"final output: {out}\")\n",
    "            print(\"------------- END MultiQueryAttention.forwardTuple() ------------\")\n",
    "\n",
    "        return out\n",
    "        \n",
    "    def forward(self, x, model=0, drop_bool = True):\n",
    "        train = True if type(x) == tuple else False\n",
    "        print(f\"---------- Attention Input: {'Tuple' if train else 'torch.Tensor'} ------------\")\n",
    "        return self.forwardTuple(x, drop_bool) if train else self.forwardTensor(x, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad02319",
   "metadata": {},
   "source": [
    "And here are the detailed print statements for the attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41c59f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Micro Hyperparameters -------\n",
      "model_count:  [1, 2, 4]\n",
      "model_dim_list:  [8, 4, 2]\n",
      "head_dim_list:  [4, 2, 1]\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the big model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 3, 8])\n",
      "tensor([[[0.0562, 0.9052, 0.6449, 0.7861, 0.4129, 0.1102, 0.5653, 0.5609],\n",
      "         [0.0178, 0.8824, 0.6357, 0.0244, 0.5049, 0.4234, 0.2152, 0.0481],\n",
      "         [0.8340, 0.2033, 0.0968, 0.9891, 0.6792, 0.4305, 0.8989, 0.7066]]])\n",
      "---------- Attention Input: torch.Tensor ------------\n",
      "----------------- MultiQueryAttention.forwardTensor() --------------------\n",
      "x shape: torch.Size([1, 3, 8])\n",
      "d_skip: 0\n",
      "models_in_this_level: 1\n",
      "h_dim: 4\n",
      "h_skip: 0\n",
      "self.Wqkv: torch.Size([8, 16])\n",
      "Parameter containing:\n",
      "tensor([[-0.0521,  0.3254,  0.3317,  0.0196, -0.0625, -0.1371, -0.2930,  0.0693,\n",
      "          0.0161,  0.2415,  0.0518, -0.2021, -0.1875,  0.0807, -0.3144,  0.2513],\n",
      "        [-0.0458,  0.1770, -0.1642, -0.3496,  0.1726, -0.3313, -0.2308,  0.1149,\n",
      "          0.0125,  0.1080, -0.1251, -0.0620,  0.1101, -0.0091,  0.1960, -0.3052],\n",
      "        [-0.0313,  0.0596, -0.1604,  0.3271, -0.1371,  0.1151,  0.0203, -0.2159,\n",
      "          0.2081, -0.2036, -0.0213, -0.1026,  0.2307, -0.1824, -0.2344, -0.0052],\n",
      "        [ 0.3032,  0.1215, -0.0956, -0.0832, -0.3171, -0.1036,  0.0681, -0.0747,\n",
      "         -0.0892, -0.2176,  0.1667,  0.1092,  0.2955, -0.2188, -0.0919, -0.0346],\n",
      "        [ 0.2879, -0.0695,  0.1223, -0.2329, -0.0758,  0.2404, -0.3067, -0.0355,\n",
      "          0.2973,  0.3323, -0.0084,  0.0406, -0.0329, -0.1695,  0.0004, -0.1495],\n",
      "        [-0.2888, -0.3131,  0.1873, -0.0745, -0.1311,  0.0104, -0.3243, -0.0877,\n",
      "          0.2778, -0.1843,  0.0253,  0.0587, -0.2988, -0.0907,  0.2151,  0.0173],\n",
      "        [-0.1590, -0.2223, -0.0336, -0.0472, -0.3503,  0.2537, -0.2859,  0.3522,\n",
      "         -0.0441, -0.1335,  0.3309, -0.2301, -0.1005,  0.2174, -0.2338, -0.0798],\n",
      "        [-0.1713,  0.3114,  0.3238,  0.1326, -0.2212, -0.1649,  0.0419, -0.1568,\n",
      "         -0.2288,  0.2626, -0.1060, -0.0675, -0.0731,  0.2811,  0.0052, -0.2692]],\n",
      "       requires_grad=True)\n",
      "Wq: torch.Size([8, 8])\n",
      "tensor([[-0.0521,  0.3254,  0.3317,  0.0196, -0.0625, -0.1371, -0.2930,  0.0693],\n",
      "        [-0.0458,  0.1770, -0.1642, -0.3496,  0.1726, -0.3313, -0.2308,  0.1149],\n",
      "        [-0.0313,  0.0596, -0.1604,  0.3271, -0.1371,  0.1151,  0.0203, -0.2159],\n",
      "        [ 0.3032,  0.1215, -0.0956, -0.0832, -0.3171, -0.1036,  0.0681, -0.0747],\n",
      "        [ 0.2879, -0.0695,  0.1223, -0.2329, -0.0758,  0.2404, -0.3067, -0.0355],\n",
      "        [-0.2888, -0.3131,  0.1873, -0.0745, -0.1311,  0.0104, -0.3243, -0.0877],\n",
      "        [-0.1590, -0.2223, -0.0336, -0.0472, -0.3503,  0.2537, -0.2859,  0.3522],\n",
      "        [-0.1713,  0.3114,  0.3238,  0.1326, -0.2212, -0.1649,  0.0419, -0.1568]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wk: torch.Size([8, 4])\n",
      "tensor([[ 0.0161,  0.2415,  0.0518, -0.2021],\n",
      "        [ 0.0125,  0.1080, -0.1251, -0.0620],\n",
      "        [ 0.2081, -0.2036, -0.0213, -0.1026],\n",
      "        [-0.0892, -0.2176,  0.1667,  0.1092],\n",
      "        [ 0.2973,  0.3323, -0.0084,  0.0406],\n",
      "        [ 0.2778, -0.1843,  0.0253,  0.0587],\n",
      "        [-0.0441, -0.1335,  0.3309, -0.2301],\n",
      "        [-0.2288,  0.2626, -0.1060, -0.0675]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wv: torch.Size([8, 4])\n",
      "tensor([[-0.1875,  0.0807, -0.3144,  0.2513],\n",
      "        [ 0.1101, -0.0091,  0.1960, -0.3052],\n",
      "        [ 0.2307, -0.1824, -0.2344, -0.0052],\n",
      "        [ 0.2955, -0.2188, -0.0919, -0.0346],\n",
      "        [-0.0329, -0.1695,  0.0004, -0.1495],\n",
      "        [-0.2988, -0.0907,  0.2151,  0.0173],\n",
      "        [-0.1005,  0.2174, -0.2338, -0.0798],\n",
      "        [-0.0731,  0.2811,  0.0052, -0.2692]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wq spliced: torch.Size([8, 8])\n",
      "tensor([[-0.0521,  0.3254,  0.3317,  0.0196, -0.0625, -0.1371, -0.2930,  0.0693],\n",
      "        [-0.0458,  0.1770, -0.1642, -0.3496,  0.1726, -0.3313, -0.2308,  0.1149],\n",
      "        [-0.0313,  0.0596, -0.1604,  0.3271, -0.1371,  0.1151,  0.0203, -0.2159],\n",
      "        [ 0.3032,  0.1215, -0.0956, -0.0832, -0.3171, -0.1036,  0.0681, -0.0747],\n",
      "        [ 0.2879, -0.0695,  0.1223, -0.2329, -0.0758,  0.2404, -0.3067, -0.0355],\n",
      "        [-0.2888, -0.3131,  0.1873, -0.0745, -0.1311,  0.0104, -0.3243, -0.0877],\n",
      "        [-0.1590, -0.2223, -0.0336, -0.0472, -0.3503,  0.2537, -0.2859,  0.3522],\n",
      "        [-0.1713,  0.3114,  0.3238,  0.1326, -0.2212, -0.1649,  0.0419, -0.1568]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Wk spliced: torch.Size([8, 4])\n",
      "tensor([[ 0.0161,  0.2415,  0.0518, -0.2021],\n",
      "        [ 0.0125,  0.1080, -0.1251, -0.0620],\n",
      "        [ 0.2081, -0.2036, -0.0213, -0.1026],\n",
      "        [-0.0892, -0.2176,  0.1667,  0.1092],\n",
      "        [ 0.2973,  0.3323, -0.0084,  0.0406],\n",
      "        [ 0.2778, -0.1843,  0.0253,  0.0587],\n",
      "        [-0.0441, -0.1335,  0.3309, -0.2301],\n",
      "        [-0.2288,  0.2626, -0.1060, -0.0675]], grad_fn=<CatBackward0>)\n",
      "Wv spliced: torch.Size([8, 4])\n",
      "tensor([[-0.1875,  0.0807, -0.3144,  0.2513],\n",
      "        [ 0.1101, -0.0091,  0.1960, -0.3052],\n",
      "        [ 0.2307, -0.1824, -0.2344, -0.0052],\n",
      "        [ 0.2955, -0.2188, -0.0919, -0.0346],\n",
      "        [-0.0329, -0.1695,  0.0004, -0.1495],\n",
      "        [-0.2988, -0.0907,  0.2151,  0.0173],\n",
      "        [-0.1005,  0.2174, -0.2338, -0.0798],\n",
      "        [-0.0731,  0.2811,  0.0052, -0.2692]], grad_fn=<CatBackward0>)\n",
      "Wqkv_spliced: torch.Size([8, 16])\n",
      "tensor([[-0.0521,  0.3254,  0.3317,  0.0196, -0.0625, -0.1371, -0.2930,  0.0693,\n",
      "          0.0161,  0.2415,  0.0518, -0.2021, -0.1875,  0.0807, -0.3144,  0.2513],\n",
      "        [-0.0458,  0.1770, -0.1642, -0.3496,  0.1726, -0.3313, -0.2308,  0.1149,\n",
      "          0.0125,  0.1080, -0.1251, -0.0620,  0.1101, -0.0091,  0.1960, -0.3052],\n",
      "        [-0.0313,  0.0596, -0.1604,  0.3271, -0.1371,  0.1151,  0.0203, -0.2159,\n",
      "          0.2081, -0.2036, -0.0213, -0.1026,  0.2307, -0.1824, -0.2344, -0.0052],\n",
      "        [ 0.3032,  0.1215, -0.0956, -0.0832, -0.3171, -0.1036,  0.0681, -0.0747,\n",
      "         -0.0892, -0.2176,  0.1667,  0.1092,  0.2955, -0.2188, -0.0919, -0.0346],\n",
      "        [ 0.2879, -0.0695,  0.1223, -0.2329, -0.0758,  0.2404, -0.3067, -0.0355,\n",
      "          0.2973,  0.3323, -0.0084,  0.0406, -0.0329, -0.1695,  0.0004, -0.1495],\n",
      "        [-0.2888, -0.3131,  0.1873, -0.0745, -0.1311,  0.0104, -0.3243, -0.0877,\n",
      "          0.2778, -0.1843,  0.0253,  0.0587, -0.2988, -0.0907,  0.2151,  0.0173],\n",
      "        [-0.1590, -0.2223, -0.0336, -0.0472, -0.3503,  0.2537, -0.2859,  0.3522,\n",
      "         -0.0441, -0.1335,  0.3309, -0.2301, -0.1005,  0.2174, -0.2338, -0.0798],\n",
      "        [-0.1713,  0.3114,  0.3238,  0.1326, -0.2212, -0.1649,  0.0419, -0.1568,\n",
      "         -0.2288,  0.2626, -0.1060, -0.0675, -0.0731,  0.2811,  0.0052, -0.2692]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "xqkv: torch.Size([1, 3, 16])\n",
      "tensor([[[ 0.0748,  0.2982, -0.0748, -0.2266, -0.5528, -0.1634, -0.4593,\n",
      "          -0.0033,  0.0764, -0.0023,  0.1339, -0.1925,  0.3258, -0.0927,\n",
      "          -0.1691, -0.5486],\n",
      "         [-0.0733,  0.0023, -0.0939, -0.2552, -0.1234, -0.0517, -0.5460,\n",
      "          -0.0233,  0.3886,  0.0385, -0.0464, -0.1283,  0.0793, -0.1915,\n",
      "           0.0573, -0.3672],\n",
      "         [ 0.0514,  0.2716,  0.4954, -0.2444, -0.9231,  0.0063, -0.7973,\n",
      "           0.1302,  0.0680,  0.2003,  0.4083, -0.2848, -0.1124,  0.0713,\n",
      "          -0.4496, -0.2432]]], grad_fn=<UnsafeViewBackward0>)\n",
      "xq: torch.Size([1, 3, 8])\n",
      "tensor([[[ 0.0748,  0.2982, -0.0748, -0.2266, -0.5528, -0.1634, -0.4593,\n",
      "          -0.0033],\n",
      "         [-0.0733,  0.0023, -0.0939, -0.2552, -0.1234, -0.0517, -0.5460,\n",
      "          -0.0233],\n",
      "         [ 0.0514,  0.2716,  0.4954, -0.2444, -0.9231,  0.0063, -0.7973,\n",
      "           0.1302]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xk: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.0764, -0.0023,  0.1339, -0.1925],\n",
      "         [ 0.3886,  0.0385, -0.0464, -0.1283],\n",
      "         [ 0.0680,  0.2003,  0.4083, -0.2848]]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "xv: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.3258, -0.0927, -0.1691, -0.5486],\n",
      "         [ 0.0793, -0.1915,  0.0573, -0.3672],\n",
      "         [-0.1124,  0.0713, -0.4496, -0.2432]]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "xq reshaped: torch.Size([1, 3, 2, 4])\n",
      "tensor([[[[ 0.0748,  0.2982, -0.0748, -0.2266],\n",
      "          [-0.5528, -0.1634, -0.4593, -0.0033]],\n",
      "\n",
      "         [[-0.0733,  0.0023, -0.0939, -0.2552],\n",
      "          [-0.1234, -0.0517, -0.5460, -0.0233]],\n",
      "\n",
      "         [[ 0.0514,  0.2716,  0.4954, -0.2444],\n",
      "          [-0.9231,  0.0063, -0.7973,  0.1302]]]], grad_fn=<ViewBackward0>)\n",
      "xk reshaped: torch.Size([1, 3, 1, 4])\n",
      "tensor([[[[ 0.0764, -0.0023,  0.1339, -0.1925]],\n",
      "\n",
      "         [[ 0.3886,  0.0385, -0.0464, -0.1283]],\n",
      "\n",
      "         [[ 0.0680,  0.2003,  0.4083, -0.2848]]]], grad_fn=<ViewBackward0>)\n",
      "xv reshaped: torch.Size([1, 3, 1, 4])\n",
      "tensor([[[[ 0.3258, -0.0927, -0.1691, -0.5486]],\n",
      "\n",
      "         [[ 0.0793, -0.1915,  0.0573, -0.3672]],\n",
      "\n",
      "         [[-0.1124,  0.0713, -0.4496, -0.2432]]]], grad_fn=<ViewBackward0>)\n",
      "rotated xq: torch.Size([1, 3, 2, 4])\n",
      "tensor([[[[ 0.0748,  0.2982, -0.0748, -0.2266],\n",
      "          [-0.5528, -0.1634, -0.4593, -0.0033]],\n",
      "\n",
      "         [[ 0.0394,  0.0277, -0.1124, -0.2537],\n",
      "          [ 0.3927, -0.0491, -0.3989, -0.0283]],\n",
      "\n",
      "         [[-0.4718,  0.3147, -0.1594, -0.1856],\n",
      "          [ 1.1091, -0.0197, -0.5076,  0.1289]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "rotated xk: torch.Size([1, 3, 1, 4])\n",
      "tensor([[[[ 0.0764, -0.0023,  0.1339, -0.1925]],\n",
      "\n",
      "         [[ 0.2490,  0.0511,  0.3020, -0.1238]],\n",
      "\n",
      "         [[-0.3996,  0.2529, -0.1081, -0.2393]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "repeat_interleaved xk: torch.Size([1, 3, 2, 4])\n",
      "tensor([[[[ 0.0764, -0.0023,  0.1339, -0.1925],\n",
      "          [ 0.0764, -0.0023,  0.1339, -0.1925]],\n",
      "\n",
      "         [[ 0.2490,  0.0511,  0.3020, -0.1238],\n",
      "          [ 0.2490,  0.0511,  0.3020, -0.1238]],\n",
      "\n",
      "         [[-0.3996,  0.2529, -0.1081, -0.2393],\n",
      "          [-0.3996,  0.2529, -0.1081, -0.2393]]]], grad_fn=<ViewBackward0>)\n",
      "repeat_interleaved xv: torch.Size([1, 3, 2, 4])\n",
      "tensor([[[[ 0.3258, -0.0927, -0.1691, -0.5486],\n",
      "          [ 0.3258, -0.0927, -0.1691, -0.5486]],\n",
      "\n",
      "         [[ 0.0793, -0.1915,  0.0573, -0.3672],\n",
      "          [ 0.0793, -0.1915,  0.0573, -0.3672]],\n",
      "\n",
      "         [[-0.1124,  0.0713, -0.4496, -0.2432],\n",
      "          [-0.1124,  0.0713, -0.4496, -0.2432]]]], grad_fn=<ViewBackward0>)\n",
      "transposed xq: torch.Size([1, 2, 3, 4])\n",
      "tensor([[[[ 0.0748,  0.2982, -0.0748, -0.2266],\n",
      "          [ 0.0394,  0.0277, -0.1124, -0.2537],\n",
      "          [-0.4718,  0.3147, -0.1594, -0.1856]],\n",
      "\n",
      "         [[-0.5528, -0.1634, -0.4593, -0.0033],\n",
      "          [ 0.3927, -0.0491, -0.3989, -0.0283],\n",
      "          [ 1.1091, -0.0197, -0.5076,  0.1289]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "transposed xk: torch.Size([1, 2, 3, 4])\n",
      "tensor([[[[ 0.0764, -0.0023,  0.1339, -0.1925],\n",
      "          [ 0.2490,  0.0511,  0.3020, -0.1238],\n",
      "          [-0.3996,  0.2529, -0.1081, -0.2393]],\n",
      "\n",
      "         [[ 0.0764, -0.0023,  0.1339, -0.1925],\n",
      "          [ 0.2490,  0.0511,  0.3020, -0.1238],\n",
      "          [-0.3996,  0.2529, -0.1081, -0.2393]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "transposed xv: torch.Size([1, 2, 3, 4])\n",
      "tensor([[[[ 0.3258, -0.0927, -0.1691, -0.5486],\n",
      "          [ 0.0793, -0.1915,  0.0573, -0.3672],\n",
      "          [-0.1124,  0.0713, -0.4496, -0.2432]],\n",
      "\n",
      "         [[ 0.3258, -0.0927, -0.1691, -0.5486],\n",
      "          [ 0.0793, -0.1915,  0.0573, -0.3672],\n",
      "          [-0.1124,  0.0713, -0.4496, -0.2432]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[ 0.0193,  0.0197,  0.0539],\n",
      "          [ 0.0184,  0.0044,  0.0321],\n",
      "          [-0.0112, -0.0633,  0.1649]],\n",
      "\n",
      "         [[-0.0513, -0.1421,  0.1150],\n",
      "          [-0.0089, -0.0108, -0.0597],\n",
      "          [-0.0040,  0.0530, -0.2121]]]], grad_fn=<MulBackward0>)\n",
      "mask: torch.Size([1, 1, 3, 3])\n",
      "tensor([[[[ 0.0000e+00, -2.3820e+38, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]]]])\n",
      "masked scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[ 1.9306e-02, -2.3820e+38, -2.3820e+38],\n",
      "          [ 1.8372e-02,  4.3535e-03, -2.3820e+38],\n",
      "          [-1.1202e-02, -6.3290e-02,  1.6488e-01]],\n",
      "\n",
      "         [[-5.1343e-02, -2.3820e+38, -2.3820e+38],\n",
      "          [-8.9138e-03, -1.0832e-02, -2.3820e+38],\n",
      "          [-3.9931e-03,  5.2957e-02, -2.1207e-01]]]], grad_fn=<AddBackward0>)\n",
      "softmaxed scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [0.5035, 0.4965, 0.0000],\n",
      "          [0.3183, 0.3021, 0.3796]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.5005, 0.4995, 0.0000],\n",
      "          [0.3483, 0.3688, 0.2829]]]], grad_fn=<SoftmaxBackward0>)\n",
      "attention: torch.Size([1, 2, 3, 4])\n",
      "tensor([[[[ 0.3258, -0.0927, -0.1691, -0.5486],\n",
      "          [ 0.2034, -0.1418, -0.0567, -0.4586],\n",
      "          [ 0.0850, -0.0603, -0.2072, -0.3779]],\n",
      "\n",
      "         [[ 0.3258, -0.0927, -0.1691, -0.5486],\n",
      "          [ 0.2027, -0.1421, -0.0560, -0.4580],\n",
      "          [ 0.1110, -0.0827, -0.1650, -0.3953]]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "reshaped attention: torch.Size([1, 3, 8])\n",
      "tensor([[[ 0.3258, -0.0927, -0.1691, -0.5486,  0.3258, -0.0927, -0.1691,\n",
      "          -0.5486],\n",
      "         [ 0.2034, -0.1418, -0.0567, -0.4586,  0.2027, -0.1421, -0.0560,\n",
      "          -0.4580],\n",
      "         [ 0.0850, -0.0603, -0.2072, -0.3779,  0.1110, -0.0827, -0.1650,\n",
      "          -0.3953]]], grad_fn=<ViewBackward0>)\n",
      "self.Wo: torch.Size([8, 8])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0913, -0.2204,  0.1645,  0.0421,  0.0169,  0.3333,  0.0861,  0.0757],\n",
      "        [ 0.2463,  0.3021, -0.2094,  0.1762, -0.2742, -0.0807, -0.0561, -0.2589],\n",
      "        [ 0.1512, -0.0923,  0.0580, -0.2793,  0.0252,  0.2646,  0.2945,  0.1299],\n",
      "        [ 0.3475, -0.0023, -0.3067,  0.1180,  0.1250, -0.2708, -0.0496, -0.1917],\n",
      "        [-0.3459,  0.2803, -0.1687, -0.2851,  0.2470,  0.1467, -0.0762, -0.2329],\n",
      "        [-0.1050,  0.0585, -0.1092, -0.2751, -0.2048,  0.2488,  0.2454, -0.2834],\n",
      "        [-0.3039, -0.1479, -0.0601,  0.1835, -0.1754,  0.1023, -0.3158, -0.0380],\n",
      "        [-0.1635,  0.0054,  0.1894,  0.0554,  0.0798, -0.2067, -0.2554,  0.1431]],\n",
      "       requires_grad=True)\n",
      "spliced Wo: torch.Size([8, 8])\n",
      "tensor([[ 0.0913, -0.2204,  0.1645,  0.0421,  0.0169,  0.3333,  0.0861,  0.0757],\n",
      "        [ 0.2463,  0.3021, -0.2094,  0.1762, -0.2742, -0.0807, -0.0561, -0.2589],\n",
      "        [ 0.1512, -0.0923,  0.0580, -0.2793,  0.0252,  0.2646,  0.2945,  0.1299],\n",
      "        [ 0.3475, -0.0023, -0.3067,  0.1180,  0.1250, -0.2708, -0.0496, -0.1917],\n",
      "        [-0.3459,  0.2803, -0.1687, -0.2851,  0.2470,  0.1467, -0.0762, -0.2329],\n",
      "        [-0.1050,  0.0585, -0.1092, -0.2751, -0.2048,  0.2488,  0.2454, -0.2834],\n",
      "        [-0.3039, -0.1479, -0.0601,  0.1835, -0.1754,  0.1023, -0.3158, -0.0380],\n",
      "        [-0.1635,  0.0054,  0.1894,  0.0554,  0.0798, -0.2067, -0.2554,  0.1431]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "projected output: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.1711,  0.0250,  0.0929, -0.1489,  0.0434,  0.3407,  0.1566,\n",
      "           0.0102],\n",
      "         [-0.1475, -0.0271,  0.0984, -0.1091,  0.0360,  0.2717,  0.1159,\n",
      "           0.0623],\n",
      "         [-0.0846,  0.0315,  0.0558, -0.0548,  0.0073,  0.1412,  0.0927,\n",
      "           0.0149]]], grad_fn=<UnsafeViewBackward0>)\n",
      "----------------- END MultiQueryAttention.forwardTensor() --------------------\n",
      "y: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.1711,  0.0250,  0.0929, -0.1489,  0.0434,  0.3407,  0.1566,\n",
      "           0.0102],\n",
      "         [-0.1475, -0.0271,  0.0984, -0.1091,  0.0360,  0.2717,  0.1159,\n",
      "           0.0623],\n",
      "         [-0.0846,  0.0315,  0.0558, -0.0548,  0.0073,  0.1412,  0.0927,\n",
      "           0.0149]]], grad_fn=<UnsafeViewBackward0>)\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the first sub-model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[0.1377, 0.7229, 0.5810, 0.3093],\n",
      "         [0.6568, 0.6312, 0.8767, 0.4894],\n",
      "         [0.4381, 0.2832, 0.6342, 0.0245]]])\n",
      "---------- Attention Input: torch.Tensor ------------\n",
      "----------------- MultiQueryAttention.forwardTensor() --------------------\n",
      "x shape: torch.Size([1, 3, 4])\n",
      "d_skip: 0\n",
      "models_in_this_level: 2\n",
      "h_dim: 2\n",
      "h_skip: 0\n",
      "self.Wqkv: torch.Size([8, 16])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0513, -0.3160, -0.1424, -0.1530,  0.3500, -0.0701,  0.3394, -0.2583,\n",
      "         -0.2023,  0.1323, -0.1443, -0.2675, -0.0697, -0.3318,  0.1629, -0.1420],\n",
      "        [ 0.1148,  0.1336,  0.1448,  0.2248,  0.1108,  0.0134,  0.1007, -0.2076,\n",
      "          0.2301, -0.0246,  0.3027,  0.0900, -0.2184, -0.0431, -0.3479, -0.2972],\n",
      "        [-0.3497, -0.0750, -0.0610, -0.0181,  0.1412, -0.0222, -0.0998, -0.0103,\n",
      "          0.1112, -0.3432, -0.0906,  0.2322,  0.0086, -0.1578,  0.0525, -0.1223],\n",
      "        [-0.2295, -0.0900,  0.0452, -0.0274,  0.2309, -0.2759, -0.2668, -0.0077,\n",
      "          0.0590, -0.1741,  0.2861, -0.0694,  0.3457,  0.3166,  0.3079, -0.1514],\n",
      "        [-0.2518, -0.1613, -0.3482,  0.0321, -0.2383,  0.2420, -0.2652,  0.1662,\n",
      "          0.0509,  0.3339,  0.1742, -0.2432,  0.3394, -0.1822, -0.0720, -0.3480],\n",
      "        [ 0.3078,  0.2659,  0.0654,  0.3153,  0.1924, -0.0353,  0.1856,  0.0296,\n",
      "          0.3367, -0.2471, -0.0952, -0.1392,  0.1920,  0.2476, -0.2487,  0.3381],\n",
      "        [ 0.3143, -0.1643,  0.3169, -0.1576, -0.0943,  0.1219, -0.2540, -0.0869,\n",
      "          0.1129,  0.3419,  0.1806, -0.3461, -0.3484,  0.0748,  0.2715,  0.3346],\n",
      "        [ 0.3441, -0.1675,  0.1021, -0.2402,  0.0056, -0.1579,  0.3241,  0.3331,\n",
      "          0.0650, -0.2888,  0.3041,  0.1122, -0.0121, -0.2842, -0.2991, -0.0694]],\n",
      "       requires_grad=True)\n",
      "Wq: torch.Size([8, 8])\n",
      "tensor([[ 0.0513, -0.3160, -0.1424, -0.1530,  0.3500, -0.0701,  0.3394, -0.2583],\n",
      "        [ 0.1148,  0.1336,  0.1448,  0.2248,  0.1108,  0.0134,  0.1007, -0.2076],\n",
      "        [-0.3497, -0.0750, -0.0610, -0.0181,  0.1412, -0.0222, -0.0998, -0.0103],\n",
      "        [-0.2295, -0.0900,  0.0452, -0.0274,  0.2309, -0.2759, -0.2668, -0.0077],\n",
      "        [-0.2518, -0.1613, -0.3482,  0.0321, -0.2383,  0.2420, -0.2652,  0.1662],\n",
      "        [ 0.3078,  0.2659,  0.0654,  0.3153,  0.1924, -0.0353,  0.1856,  0.0296],\n",
      "        [ 0.3143, -0.1643,  0.3169, -0.1576, -0.0943,  0.1219, -0.2540, -0.0869],\n",
      "        [ 0.3441, -0.1675,  0.1021, -0.2402,  0.0056, -0.1579,  0.3241,  0.3331]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wk: torch.Size([8, 4])\n",
      "tensor([[-0.2023,  0.1323, -0.1443, -0.2675],\n",
      "        [ 0.2301, -0.0246,  0.3027,  0.0900],\n",
      "        [ 0.1112, -0.3432, -0.0906,  0.2322],\n",
      "        [ 0.0590, -0.1741,  0.2861, -0.0694],\n",
      "        [ 0.0509,  0.3339,  0.1742, -0.2432],\n",
      "        [ 0.3367, -0.2471, -0.0952, -0.1392],\n",
      "        [ 0.1129,  0.3419,  0.1806, -0.3461],\n",
      "        [ 0.0650, -0.2888,  0.3041,  0.1122]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wv: torch.Size([8, 4])\n",
      "tensor([[-0.0697, -0.3318,  0.1629, -0.1420],\n",
      "        [-0.2184, -0.0431, -0.3479, -0.2972],\n",
      "        [ 0.0086, -0.1578,  0.0525, -0.1223],\n",
      "        [ 0.3457,  0.3166,  0.3079, -0.1514],\n",
      "        [ 0.3394, -0.1822, -0.0720, -0.3480],\n",
      "        [ 0.1920,  0.2476, -0.2487,  0.3381],\n",
      "        [-0.3484,  0.0748,  0.2715,  0.3346],\n",
      "        [-0.0121, -0.2842, -0.2991, -0.0694]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wq spliced: torch.Size([4, 4])\n",
      "tensor([[ 0.0513, -0.3160,  0.3500, -0.0701],\n",
      "        [ 0.1148,  0.1336,  0.1108,  0.0134],\n",
      "        [-0.3497, -0.0750,  0.1412, -0.0222],\n",
      "        [-0.2295, -0.0900,  0.2309, -0.2759]], grad_fn=<CatBackward0>)\n",
      "Wk spliced: torch.Size([4, 2])\n",
      "tensor([[-0.2023,  0.1323],\n",
      "        [ 0.2301, -0.0246],\n",
      "        [ 0.1112, -0.3432],\n",
      "        [ 0.0590, -0.1741]], grad_fn=<CatBackward0>)\n",
      "Wv spliced: torch.Size([4, 2])\n",
      "tensor([[-0.0697, -0.3318],\n",
      "        [-0.2184, -0.0431],\n",
      "        [ 0.0086, -0.1578],\n",
      "        [ 0.3457,  0.3166]], grad_fn=<CatBackward0>)\n",
      "Wqkv_spliced: torch.Size([4, 8])\n",
      "tensor([[ 0.0513, -0.3160,  0.3500, -0.0701, -0.2023,  0.1323, -0.0697, -0.3318],\n",
      "        [ 0.1148,  0.1336,  0.1108,  0.0134,  0.2301, -0.0246, -0.2184, -0.0431],\n",
      "        [-0.3497, -0.0750,  0.1412, -0.0222,  0.1112, -0.3432,  0.0086, -0.1578],\n",
      "        [-0.2295, -0.0900,  0.2309, -0.2759,  0.0590, -0.1741,  0.3457,  0.3166]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "xqkv: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.1841, -0.0184,  0.2817, -0.0982,  0.2214, -0.2528, -0.0556,\n",
      "          -0.0706],\n",
      "         [-0.3128, -0.2330,  0.5366, -0.1921,  0.1388, -0.3148, -0.0070,\n",
      "          -0.2285],\n",
      "         [-0.1724, -0.1504,  0.2799, -0.0478,  0.0486, -0.1710, -0.0785,\n",
      "          -0.2499]]], grad_fn=<UnsafeViewBackward0>)\n",
      "xq: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.1841, -0.0184,  0.2817, -0.0982],\n",
      "         [-0.3128, -0.2330,  0.5366, -0.1921],\n",
      "         [-0.1724, -0.1504,  0.2799, -0.0478]]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "xk: torch.Size([1, 3, 2])\n",
      "tensor([[[ 0.2214, -0.2528],\n",
      "         [ 0.1388, -0.3148],\n",
      "         [ 0.0486, -0.1710]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xv: torch.Size([1, 3, 2])\n",
      "tensor([[[-0.0556, -0.0706],\n",
      "         [-0.0070, -0.2285],\n",
      "         [-0.0785, -0.2499]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xq reshaped: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[-0.1841, -0.0184],\n",
      "          [ 0.2817, -0.0982]],\n",
      "\n",
      "         [[-0.3128, -0.2330],\n",
      "          [ 0.5366, -0.1921]],\n",
      "\n",
      "         [[-0.1724, -0.1504],\n",
      "          [ 0.2799, -0.0478]]]], grad_fn=<ViewBackward0>)\n",
      "xk reshaped: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[ 0.2214, -0.2528]],\n",
      "\n",
      "         [[ 0.1388, -0.3148]],\n",
      "\n",
      "         [[ 0.0486, -0.1710]]]], grad_fn=<ViewBackward0>)\n",
      "xv reshaped: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[-0.0556, -0.0706]],\n",
      "\n",
      "         [[-0.0070, -0.2285]],\n",
      "\n",
      "         [[-0.0785, -0.2499]]]], grad_fn=<ViewBackward0>)\n",
      "rotated xq: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[-0.1841, -0.0184],\n",
      "          [ 0.2817, -0.0982]],\n",
      "\n",
      "         [[ 0.0271, -0.3891],\n",
      "          [ 0.4516,  0.3477]],\n",
      "\n",
      "         [[ 0.2085, -0.0942],\n",
      "          [-0.0730,  0.2744]]]], grad_fn=<TransposeBackward0>)\n",
      "rotated xk: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[ 0.2214, -0.2528]],\n",
      "\n",
      "         [[ 0.3398, -0.0533]],\n",
      "\n",
      "         [[ 0.1352,  0.1153]]]], grad_fn=<TransposeBackward0>)\n",
      "repeat_interleaved xk: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[ 0.2214, -0.2528],\n",
      "          [ 0.2214, -0.2528]],\n",
      "\n",
      "         [[ 0.3398, -0.0533],\n",
      "          [ 0.3398, -0.0533]],\n",
      "\n",
      "         [[ 0.1352,  0.1153],\n",
      "          [ 0.1352,  0.1153]]]], grad_fn=<ViewBackward0>)\n",
      "repeat_interleaved xv: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[-0.0556, -0.0706],\n",
      "          [-0.0556, -0.0706]],\n",
      "\n",
      "         [[-0.0070, -0.2285],\n",
      "          [-0.0070, -0.2285]],\n",
      "\n",
      "         [[-0.0785, -0.2499],\n",
      "          [-0.0785, -0.2499]]]], grad_fn=<ViewBackward0>)\n",
      "transposed xq: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[-0.1841, -0.0184],\n",
      "          [ 0.0271, -0.3891],\n",
      "          [ 0.2085, -0.0942]],\n",
      "\n",
      "         [[ 0.2817, -0.0982],\n",
      "          [ 0.4516,  0.3477],\n",
      "          [-0.0730,  0.2744]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xk: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[ 0.2214, -0.2528],\n",
      "          [ 0.3398, -0.0533],\n",
      "          [ 0.1352,  0.1153]],\n",
      "\n",
      "         [[ 0.2214, -0.2528],\n",
      "          [ 0.3398, -0.0533],\n",
      "          [ 0.1352,  0.1153]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xv: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[-0.0556, -0.0706],\n",
      "          [-0.0070, -0.2285],\n",
      "          [-0.0785, -0.2499]],\n",
      "\n",
      "         [[-0.0556, -0.0706],\n",
      "          [-0.0070, -0.2285],\n",
      "          [-0.0785, -0.2499]]]], grad_fn=<TransposeBackward0>)\n",
      "scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[-0.0255, -0.0436, -0.0191],\n",
      "          [ 0.0738,  0.0212, -0.0291],\n",
      "          [ 0.0495,  0.0537,  0.0123]],\n",
      "\n",
      "         [[ 0.0617,  0.0714,  0.0189],\n",
      "          [ 0.0085,  0.0954,  0.0715],\n",
      "          [-0.0605, -0.0279,  0.0154]]]], grad_fn=<MulBackward0>)\n",
      "mask: torch.Size([1, 1, 3, 3])\n",
      "tensor([[[[ 0.0000e+00, -2.3820e+38, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]]]])\n",
      "masked scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[-2.5532e-02, -2.3820e+38, -2.3820e+38],\n",
      "          [ 7.3800e-02,  2.1172e-02, -2.3820e+38],\n",
      "          [ 4.9480e-02,  5.3652e-02,  1.2256e-02]],\n",
      "\n",
      "         [[ 6.1658e-02, -2.3820e+38, -2.3820e+38],\n",
      "          [ 8.5235e-03,  9.5413e-02, -2.3820e+38],\n",
      "          [-6.0486e-02, -2.7891e-02,  1.5386e-02]]]], grad_fn=<AddBackward0>)\n",
      "softmaxed scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [0.5132, 0.4868, 0.0000],\n",
      "          [0.3370, 0.3384, 0.3247]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.4783, 0.5217, 0.0000],\n",
      "          [0.3213, 0.3320, 0.3467]]]], grad_fn=<SoftmaxBackward0>)\n",
      "attention: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[-0.0556, -0.0706],\n",
      "          [-0.0319, -0.1475],\n",
      "          [-0.0466, -0.1822]],\n",
      "\n",
      "         [[-0.0556, -0.0706],\n",
      "          [-0.0302, -0.1530],\n",
      "          [-0.0474, -0.1852]]]], grad_fn=<UnsafeViewBackward0>)\n",
      "reshaped attention: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.0556, -0.0706, -0.0556, -0.0706],\n",
      "         [-0.0319, -0.1475, -0.0302, -0.1530],\n",
      "         [-0.0466, -0.1822, -0.0474, -0.1852]]], grad_fn=<ViewBackward0>)\n",
      "self.Wo: torch.Size([8, 8])\n",
      "Parameter containing:\n",
      "tensor([[ 0.2217,  0.1160,  0.2757, -0.3168,  0.3170,  0.2646, -0.1341, -0.1811],\n",
      "        [-0.0323, -0.0581,  0.0794,  0.2936, -0.2148, -0.3303,  0.1437,  0.0609],\n",
      "        [ 0.2465,  0.2174, -0.2247,  0.3393, -0.2318, -0.1898,  0.2125,  0.0683],\n",
      "        [-0.2606,  0.1784,  0.0845, -0.0599, -0.0459, -0.1296,  0.1645,  0.3473],\n",
      "        [ 0.2893,  0.2159, -0.0692,  0.1964,  0.2852, -0.1046,  0.0556,  0.3022],\n",
      "        [-0.0938,  0.0559, -0.1110, -0.1517, -0.0770, -0.0911, -0.0507, -0.2081],\n",
      "        [-0.2835,  0.0870, -0.0801,  0.2000,  0.2043,  0.2463,  0.1029, -0.2502],\n",
      "        [-0.1466,  0.3461,  0.1823, -0.2685, -0.2831,  0.0060,  0.2121,  0.2125]],\n",
      "       requires_grad=True)\n",
      "spliced Wo: torch.Size([4, 4])\n",
      "tensor([[ 0.2217,  0.1160,  0.2757, -0.3168],\n",
      "        [-0.0323, -0.0581,  0.0794,  0.2936],\n",
      "        [ 0.2893,  0.2159, -0.0692,  0.1964],\n",
      "        [-0.0938,  0.0559, -0.1110, -0.1517]], grad_fn=<CatBackward0>)\n",
      "projected output: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.0195, -0.0183, -0.0093, -0.0033],\n",
      "         [ 0.0033, -0.0102, -0.0014, -0.0159],\n",
      "         [-0.0008, -0.0154, -0.0035, -0.0200]]], grad_fn=<UnsafeViewBackward0>)\n",
      "----------------- END MultiQueryAttention.forwardTensor() --------------------\n",
      "y: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.0195, -0.0183, -0.0093, -0.0033],\n",
      "         [ 0.0033, -0.0102, -0.0014, -0.0159],\n",
      "         [-0.0008, -0.0154, -0.0035, -0.0200]]], grad_fn=<UnsafeViewBackward0>)\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the second sub-model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[0.4157, 0.5135, 0.3598, 0.0763],\n",
      "         [0.6283, 0.9163, 0.9226, 0.4627],\n",
      "         [0.8563, 0.8610, 0.3432, 0.9650]]])\n",
      "---------- Attention Input: torch.Tensor ------------\n",
      "----------------- MultiQueryAttention.forwardTensor() --------------------\n",
      "x shape: torch.Size([1, 3, 4])\n",
      "d_skip: 4\n",
      "models_in_this_level: 2\n",
      "h_dim: 2\n",
      "h_skip: 2\n",
      "self.Wqkv: torch.Size([8, 16])\n",
      "Parameter containing:\n",
      "tensor([[-0.2718,  0.3313,  0.2489,  0.1108,  0.0673,  0.3328,  0.1733, -0.2552,\n",
      "         -0.1136,  0.3208,  0.1988, -0.1039, -0.2444,  0.2802, -0.1434, -0.0161],\n",
      "        [ 0.1894, -0.1737,  0.2785,  0.2978,  0.1004, -0.1075,  0.0923,  0.2685,\n",
      "          0.2514, -0.2606,  0.3526,  0.3417, -0.2354, -0.2304,  0.1385,  0.1825],\n",
      "        [-0.2838,  0.0256, -0.1265, -0.3338, -0.2754,  0.1979, -0.3349, -0.2171,\n",
      "         -0.0401, -0.0898, -0.1154, -0.1890, -0.2932,  0.0636, -0.2690, -0.1561],\n",
      "        [ 0.2827, -0.3011,  0.1630, -0.3331, -0.2782, -0.1406,  0.2586, -0.0060,\n",
      "          0.0584,  0.2535,  0.2538, -0.0855,  0.2837, -0.2754,  0.0800, -0.0646],\n",
      "        [-0.2938, -0.2070,  0.2071, -0.1965,  0.0167,  0.0180, -0.0735,  0.1947,\n",
      "         -0.0728,  0.3157,  0.3364,  0.2059,  0.1540, -0.0438, -0.2143,  0.3017],\n",
      "        [ 0.2482,  0.0451, -0.3405,  0.0049,  0.0731, -0.3036, -0.0321, -0.0437,\n",
      "         -0.2727, -0.0040, -0.2477,  0.1545,  0.3236, -0.1127, -0.1478, -0.0938],\n",
      "        [ 0.2997, -0.3016, -0.2539,  0.0763,  0.0618, -0.2870,  0.0584,  0.0720,\n",
      "         -0.0534,  0.1998,  0.2873,  0.3364,  0.0202,  0.0285,  0.1615, -0.2469],\n",
      "        [-0.0208, -0.1810,  0.2577,  0.3315,  0.2745, -0.1425,  0.2935,  0.0193,\n",
      "          0.1088, -0.1203,  0.1781, -0.0397,  0.2895, -0.2158, -0.1094,  0.1041]],\n",
      "       requires_grad=True)\n",
      "Wq: torch.Size([8, 8])\n",
      "tensor([[-0.2718,  0.3313,  0.2489,  0.1108,  0.0673,  0.3328,  0.1733, -0.2552],\n",
      "        [ 0.1894, -0.1737,  0.2785,  0.2978,  0.1004, -0.1075,  0.0923,  0.2685],\n",
      "        [-0.2838,  0.0256, -0.1265, -0.3338, -0.2754,  0.1979, -0.3349, -0.2171],\n",
      "        [ 0.2827, -0.3011,  0.1630, -0.3331, -0.2782, -0.1406,  0.2586, -0.0060],\n",
      "        [-0.2938, -0.2070,  0.2071, -0.1965,  0.0167,  0.0180, -0.0735,  0.1947],\n",
      "        [ 0.2482,  0.0451, -0.3405,  0.0049,  0.0731, -0.3036, -0.0321, -0.0437],\n",
      "        [ 0.2997, -0.3016, -0.2539,  0.0763,  0.0618, -0.2870,  0.0584,  0.0720],\n",
      "        [-0.0208, -0.1810,  0.2577,  0.3315,  0.2745, -0.1425,  0.2935,  0.0193]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wk: torch.Size([8, 4])\n",
      "tensor([[-0.1136,  0.3208,  0.1988, -0.1039],\n",
      "        [ 0.2514, -0.2606,  0.3526,  0.3417],\n",
      "        [-0.0401, -0.0898, -0.1154, -0.1890],\n",
      "        [ 0.0584,  0.2535,  0.2538, -0.0855],\n",
      "        [-0.0728,  0.3157,  0.3364,  0.2059],\n",
      "        [-0.2727, -0.0040, -0.2477,  0.1545],\n",
      "        [-0.0534,  0.1998,  0.2873,  0.3364],\n",
      "        [ 0.1088, -0.1203,  0.1781, -0.0397]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wv: torch.Size([8, 4])\n",
      "tensor([[-0.2444,  0.2802, -0.1434, -0.0161],\n",
      "        [-0.2354, -0.2304,  0.1385,  0.1825],\n",
      "        [-0.2932,  0.0636, -0.2690, -0.1561],\n",
      "        [ 0.2837, -0.2754,  0.0800, -0.0646],\n",
      "        [ 0.1540, -0.0438, -0.2143,  0.3017],\n",
      "        [ 0.3236, -0.1127, -0.1478, -0.0938],\n",
      "        [ 0.0202,  0.0285,  0.1615, -0.2469],\n",
      "        [ 0.2895, -0.2158, -0.1094,  0.1041]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wq spliced: torch.Size([4, 4])\n",
      "tensor([[ 0.2071, -0.1965, -0.0735,  0.1947],\n",
      "        [-0.3405,  0.0049, -0.0321, -0.0437],\n",
      "        [-0.2539,  0.0763,  0.0584,  0.0720],\n",
      "        [ 0.2577,  0.3315,  0.2935,  0.0193]], grad_fn=<CatBackward0>)\n",
      "Wk spliced: torch.Size([4, 2])\n",
      "tensor([[ 0.3364,  0.2059],\n",
      "        [-0.2477,  0.1545],\n",
      "        [ 0.2873,  0.3364],\n",
      "        [ 0.1781, -0.0397]], grad_fn=<CatBackward0>)\n",
      "Wv spliced: torch.Size([4, 2])\n",
      "tensor([[-0.2143,  0.3017],\n",
      "        [-0.1478, -0.0938],\n",
      "        [ 0.1615, -0.2469],\n",
      "        [-0.1094,  0.1041]], grad_fn=<CatBackward0>)\n",
      "Wqkv_spliced: torch.Size([4, 8])\n",
      "tensor([[ 0.2071, -0.1965, -0.0735,  0.1947,  0.3364,  0.2059, -0.2143,  0.3017],\n",
      "        [-0.3405,  0.0049, -0.0321, -0.0437, -0.2477,  0.1545, -0.1478, -0.0938],\n",
      "        [-0.2539,  0.0763,  0.0584,  0.0720,  0.2873,  0.3364,  0.1615, -0.2469],\n",
      "        [ 0.2577,  0.3315,  0.2935,  0.0193,  0.1781, -0.0397, -0.1094,  0.1041]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "xqkv: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.1604, -0.0264, -0.0036,  0.0859,  0.1296,  0.2829, -0.1152,\n",
      "          -0.0036],\n",
      "         [-0.2969,  0.1048,  0.1141,  0.1577,  0.3319,  0.5629, -0.1717,\n",
      "          -0.0759],\n",
      "         [ 0.0457,  0.1820,  0.2127,  0.1725,  0.3453,  0.3865, -0.3610,\n",
      "           0.1934]]], grad_fn=<UnsafeViewBackward0>)\n",
      "xq: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.1604, -0.0264, -0.0036,  0.0859],\n",
      "         [-0.2969,  0.1048,  0.1141,  0.1577],\n",
      "         [ 0.0457,  0.1820,  0.2127,  0.1725]]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "xk: torch.Size([1, 3, 2])\n",
      "tensor([[[0.1296, 0.2829],\n",
      "         [0.3319, 0.5629],\n",
      "         [0.3453, 0.3865]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xv: torch.Size([1, 3, 2])\n",
      "tensor([[[-0.1152, -0.0036],\n",
      "         [-0.1717, -0.0759],\n",
      "         [-0.3610,  0.1934]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xq reshaped: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[-0.1604, -0.0264],\n",
      "          [-0.0036,  0.0859]],\n",
      "\n",
      "         [[-0.2969,  0.1048],\n",
      "          [ 0.1141,  0.1577]],\n",
      "\n",
      "         [[ 0.0457,  0.1820],\n",
      "          [ 0.2127,  0.1725]]]], grad_fn=<ViewBackward0>)\n",
      "xk reshaped: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[0.1296, 0.2829]],\n",
      "\n",
      "         [[0.3319, 0.5629]],\n",
      "\n",
      "         [[0.3453, 0.3865]]]], grad_fn=<ViewBackward0>)\n",
      "xv reshaped: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[-0.1152, -0.0036]],\n",
      "\n",
      "         [[-0.1717, -0.0759]],\n",
      "\n",
      "         [[-0.3610,  0.1934]]]], grad_fn=<ViewBackward0>)\n",
      "rotated xq: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[-0.1604, -0.0264],\n",
      "          [-0.0036,  0.0859]],\n",
      "\n",
      "         [[-0.2486, -0.1932],\n",
      "          [-0.0711,  0.1812]],\n",
      "\n",
      "         [[-0.1845, -0.0341],\n",
      "          [-0.2453,  0.1216]]]], grad_fn=<TransposeBackward0>)\n",
      "rotated xk: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[ 0.1296,  0.2829]],\n",
      "\n",
      "         [[-0.2944,  0.5835]],\n",
      "\n",
      "         [[-0.4951,  0.1531]]]], grad_fn=<TransposeBackward0>)\n",
      "repeat_interleaved xk: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[ 0.1296,  0.2829],\n",
      "          [ 0.1296,  0.2829]],\n",
      "\n",
      "         [[-0.2944,  0.5835],\n",
      "          [-0.2944,  0.5835]],\n",
      "\n",
      "         [[-0.4951,  0.1531],\n",
      "          [-0.4951,  0.1531]]]], grad_fn=<ViewBackward0>)\n",
      "repeat_interleaved xv: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[-0.1152, -0.0036],\n",
      "          [-0.1152, -0.0036]],\n",
      "\n",
      "         [[-0.1717, -0.0759],\n",
      "          [-0.1717, -0.0759]],\n",
      "\n",
      "         [[-0.3610,  0.1934],\n",
      "          [-0.3610,  0.1934]]]], grad_fn=<ViewBackward0>)\n",
      "transposed xq: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[-0.1604, -0.0264],\n",
      "          [-0.2486, -0.1932],\n",
      "          [-0.1845, -0.0341]],\n",
      "\n",
      "         [[-0.0036,  0.0859],\n",
      "          [-0.0711,  0.1812],\n",
      "          [-0.2453,  0.1216]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xk: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[ 0.1296,  0.2829],\n",
      "          [-0.2944,  0.5835],\n",
      "          [-0.4951,  0.1531]],\n",
      "\n",
      "         [[ 0.1296,  0.2829],\n",
      "          [-0.2944,  0.5835],\n",
      "          [-0.4951,  0.1531]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xv: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[-0.1152, -0.0036],\n",
      "          [-0.1717, -0.0759],\n",
      "          [-0.3610,  0.1934]],\n",
      "\n",
      "         [[-0.1152, -0.0036],\n",
      "          [-0.1717, -0.0759],\n",
      "          [-0.3610,  0.1934]]]], grad_fn=<TransposeBackward0>)\n",
      "scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[-0.0200,  0.0225,  0.0533],\n",
      "          [-0.0614, -0.0280,  0.0661],\n",
      "          [-0.0237,  0.0243,  0.0609]],\n",
      "\n",
      "         [[ 0.0169,  0.0362,  0.0106],\n",
      "          [ 0.0297,  0.0895,  0.0445],\n",
      "          [ 0.0018,  0.1012,  0.0991]]]], grad_fn=<MulBackward0>)\n",
      "mask: torch.Size([1, 1, 3, 3])\n",
      "tensor([[[[ 0.0000e+00, -2.3820e+38, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]]]])\n",
      "masked scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[-1.9996e-02, -2.3820e+38, -2.3820e+38],\n",
      "          [-6.1436e-02, -2.7971e-02, -2.3820e+38],\n",
      "          [-2.3742e-02,  2.4311e-02,  6.0886e-02]],\n",
      "\n",
      "         [[ 1.6851e-02, -2.3820e+38, -2.3820e+38],\n",
      "          [ 2.9737e-02,  8.9544e-02, -2.3820e+38],\n",
      "          [ 1.8469e-03,  1.0125e-01,  9.9053e-02]]]], grad_fn=<AddBackward0>)\n",
      "softmaxed scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [0.4916, 0.5084, 0.0000],\n",
      "          [0.3187, 0.3344, 0.3469]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.4851, 0.5149, 0.0000],\n",
      "          [0.3119, 0.3444, 0.3437]]]], grad_fn=<SoftmaxBackward0>)\n",
      "attention: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[-0.1152, -0.0036],\n",
      "          [-0.1440, -0.0404],\n",
      "          [-0.2194,  0.0406]],\n",
      "\n",
      "         [[-0.1152, -0.0036],\n",
      "          [-0.1443, -0.0408],\n",
      "          [-0.2192,  0.0392]]]], grad_fn=<UnsafeViewBackward0>)\n",
      "reshaped attention: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.1152, -0.0036, -0.1152, -0.0036],\n",
      "         [-0.1440, -0.0404, -0.1443, -0.0408],\n",
      "         [-0.2194,  0.0406, -0.2192,  0.0392]]], grad_fn=<ViewBackward0>)\n",
      "self.Wo: torch.Size([8, 8])\n",
      "Parameter containing:\n",
      "tensor([[ 0.2741,  0.0026,  0.2354,  0.1950, -0.0440, -0.0966,  0.0701,  0.2493],\n",
      "        [-0.0249, -0.1072, -0.0694, -0.0458,  0.3488, -0.2691, -0.0926,  0.1393],\n",
      "        [-0.0715,  0.0159, -0.1406,  0.3201,  0.2066,  0.3211, -0.2955, -0.0100],\n",
      "        [-0.3446,  0.1936, -0.0149, -0.0247,  0.0304, -0.2946,  0.1084, -0.2914],\n",
      "        [-0.1308,  0.3483,  0.0465,  0.1175, -0.1425, -0.1963, -0.0680,  0.1441],\n",
      "        [-0.2967,  0.1158, -0.0496, -0.1108,  0.1678,  0.1922, -0.2702, -0.3362],\n",
      "        [-0.0712, -0.0516,  0.2523,  0.3399, -0.1561,  0.2491,  0.2337,  0.1523],\n",
      "        [ 0.0871,  0.3382,  0.2241,  0.1722, -0.1704, -0.2323, -0.0473,  0.1215]],\n",
      "       requires_grad=True)\n",
      "spliced Wo: torch.Size([4, 4])\n",
      "tensor([[ 0.2066,  0.3211, -0.2955, -0.0100],\n",
      "        [ 0.0304, -0.2946,  0.1084, -0.2914],\n",
      "        [-0.1561,  0.2491,  0.2337,  0.1523],\n",
      "        [-0.1704, -0.2323, -0.0473,  0.1215]], grad_fn=<CatBackward0>)\n",
      "projected output: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.0053, -0.0638,  0.0069, -0.0158],\n",
      "         [-0.0015, -0.0608,  0.0064, -0.0137],\n",
      "         [-0.0166, -0.1461,  0.0162, -0.0382]]], grad_fn=<UnsafeViewBackward0>)\n",
      "----------------- END MultiQueryAttention.forwardTensor() --------------------\n",
      "y: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.0053, -0.0638,  0.0069, -0.0158],\n",
      "         [-0.0015, -0.0608,  0.0064, -0.0137],\n",
      "         [-0.0166, -0.1461,  0.0162, -0.0382]]], grad_fn=<UnsafeViewBackward0>)\n",
      "---------- RESET CONFIG --------\n",
      "model_count:  [1, 2, 4]\n",
      "model_dim_list:  [128, 64, 32]\n",
      "head_dim_list:  [32, 16, 8]\n"
     ]
    }
   ],
   "source": [
    "# Testing our Attention's forwardTensor()\n",
    "verbose = True\n",
    "\n",
    "print(\"--------- Micro Hyperparameters -------\")\n",
    "hold1, hold2, hold3, hold4 = config.hidden_size, config.num_attention_heads, config.head_dim, config.max_position_embeddings\n",
    "config.hidden_size = 8\n",
    "config.num_attention_heads = 2\n",
    "config.head_dim = 4\n",
    "config.max_position_embeddings = 3\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "print(\"head_dim_list: \", config.head_dim_list)\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the big model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,3,8)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "att = MultiQueryAttention(config)\n",
    "y = att(x)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the first sub-model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,3,4)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "att = MultiQueryAttention(config)\n",
    "y = att(x)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the second sub-model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,3,4)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "att = MultiQueryAttention(config)\n",
    "y = att(x, model=1)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "verbose = False\n",
    "print(\"---------- RESET CONFIG --------\")\n",
    "config.hidden_size = hold1\n",
    "config.num_attention_heads = hold2\n",
    "config.head_dim = hold3\n",
    "config.max_position_embeddings = hold4\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "print(\"head_dim_list: \", config.head_dim_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ffcead5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Micro Hyperparameters -------\n",
      "model_count:  [1, 2]\n",
      "model_dim_list:  [8, 4]\n",
      "head_dim_list:  [4, 2]\n",
      "x: ((tensor([[[-0.3497, -2.9496, -0.4524,  1.9168, -0.0945, -0.4339, -0.2205,\n",
      "           0.3369],\n",
      "         [ 0.0089,  0.9981,  0.1863, -1.5402,  0.5119,  0.0959, -0.8294,\n",
      "           1.6965],\n",
      "         [ 1.3860, -1.5527,  0.2305, -2.2388,  0.9591, -0.5175,  0.9746,\n",
      "           1.2292]]]),), (tensor([[[-0.8564, -0.5784,  0.6057, -0.8484],\n",
      "         [ 1.2820, -0.1706,  1.0822,  0.0600],\n",
      "         [ 0.1781, -0.9001,  0.9855, -0.2710]]]), tensor([[[ 3.0712,  0.1522, -0.1656,  0.7491],\n",
      "         [ 0.8894,  0.9551, -0.5230, -0.2508],\n",
      "         [-1.0178, -0.4089, -0.9741, -0.9321]]])))\n",
      "---------- Attention Input: Tuple ------------\n",
      "------------- MultiQueryAttention.forwardTuple() ------------\n",
      "x: ((tensor([[[-0.3497, -2.9496, -0.4524,  1.9168, -0.0945, -0.4339, -0.2205,\n",
      "           0.3369],\n",
      "         [ 0.0089,  0.9981,  0.1863, -1.5402,  0.5119,  0.0959, -0.8294,\n",
      "           1.6965],\n",
      "         [ 1.3860, -1.5527,  0.2305, -2.2388,  0.9591, -0.5175,  0.9746,\n",
      "           1.2292]]]),), (tensor([[[-0.8564, -0.5784,  0.6057, -0.8484],\n",
      "         [ 1.2820, -0.1706,  1.0822,  0.0600],\n",
      "         [ 0.1781, -0.9001,  0.9855, -0.2710]]]), tensor([[[ 3.0712,  0.1522, -0.1656,  0.7491],\n",
      "         [ 0.8894,  0.9551, -0.5230, -0.2508],\n",
      "         [-1.0178, -0.4089, -0.9741, -0.9321]]])))\n",
      "input_len: 3\n",
      "num_levels: 2\n",
      "models_per_level: [1, 2]\n",
      "Level 0 from range(2)\n",
      "Model 0 from range(1)\n",
      "----------------- MultiQueryAttention.forwardTensor() --------------------\n",
      "x shape: torch.Size([1, 3, 8])\n",
      "d_skip: 0\n",
      "models_in_this_level: 1\n",
      "h_dim: 4\n",
      "h_skip: 0\n",
      "self.Wqkv: torch.Size([8, 16])\n",
      "Parameter containing:\n",
      "tensor([[-0.0568,  0.1715,  0.1291, -0.0538, -0.2727,  0.0305, -0.1010,  0.0500,\n",
      "          0.2281,  0.1489,  0.3497, -0.0064,  0.1537, -0.3108, -0.2085,  0.1568],\n",
      "        [ 0.1049,  0.2299,  0.2896,  0.2405,  0.0403, -0.1731,  0.0116,  0.2085,\n",
      "          0.0599, -0.0035,  0.1334, -0.3012,  0.0199,  0.1015, -0.3510,  0.3459],\n",
      "        [ 0.1127, -0.1474, -0.2341,  0.2732, -0.2841,  0.2047, -0.2790, -0.1626,\n",
      "          0.3147,  0.1628,  0.0012,  0.2319,  0.0268, -0.1385,  0.2386,  0.1793],\n",
      "        [ 0.1407, -0.3189,  0.2706, -0.0620, -0.1115, -0.0586,  0.3311,  0.2044,\n",
      "          0.2715, -0.0851,  0.1486,  0.0092, -0.3228,  0.3362, -0.0090, -0.0302],\n",
      "        [ 0.1030, -0.1064,  0.0953,  0.3040,  0.2656, -0.3465, -0.0165,  0.1346,\n",
      "         -0.0067,  0.0859,  0.0586,  0.3026, -0.1185, -0.0781, -0.1422, -0.1206],\n",
      "        [ 0.1096, -0.2188,  0.1101,  0.0192, -0.2362,  0.2024, -0.2135,  0.2610,\n",
      "         -0.0785, -0.2659,  0.2588,  0.2002,  0.2601, -0.0946, -0.1745,  0.1041],\n",
      "        [-0.2311, -0.2619, -0.2529,  0.3010, -0.2211,  0.0617,  0.2394, -0.2788,\n",
      "          0.2051,  0.0658,  0.1172,  0.2387,  0.0013, -0.0132, -0.1698, -0.2354],\n",
      "        [-0.2140, -0.1297,  0.2172, -0.2159, -0.0770,  0.3502,  0.0704, -0.1808,\n",
      "          0.0383, -0.1350, -0.0362,  0.2379, -0.3045, -0.3061,  0.2975,  0.2782]],\n",
      "       requires_grad=True)\n",
      "Wq: torch.Size([8, 8])\n",
      "tensor([[-0.0568,  0.1715,  0.1291, -0.0538, -0.2727,  0.0305, -0.1010,  0.0500],\n",
      "        [ 0.1049,  0.2299,  0.2896,  0.2405,  0.0403, -0.1731,  0.0116,  0.2085],\n",
      "        [ 0.1127, -0.1474, -0.2341,  0.2732, -0.2841,  0.2047, -0.2790, -0.1626],\n",
      "        [ 0.1407, -0.3189,  0.2706, -0.0620, -0.1115, -0.0586,  0.3311,  0.2044],\n",
      "        [ 0.1030, -0.1064,  0.0953,  0.3040,  0.2656, -0.3465, -0.0165,  0.1346],\n",
      "        [ 0.1096, -0.2188,  0.1101,  0.0192, -0.2362,  0.2024, -0.2135,  0.2610],\n",
      "        [-0.2311, -0.2619, -0.2529,  0.3010, -0.2211,  0.0617,  0.2394, -0.2788],\n",
      "        [-0.2140, -0.1297,  0.2172, -0.2159, -0.0770,  0.3502,  0.0704, -0.1808]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wk: torch.Size([8, 4])\n",
      "tensor([[ 0.2281,  0.1489,  0.3497, -0.0064],\n",
      "        [ 0.0599, -0.0035,  0.1334, -0.3012],\n",
      "        [ 0.3147,  0.1628,  0.0012,  0.2319],\n",
      "        [ 0.2715, -0.0851,  0.1486,  0.0092],\n",
      "        [-0.0067,  0.0859,  0.0586,  0.3026],\n",
      "        [-0.0785, -0.2659,  0.2588,  0.2002],\n",
      "        [ 0.2051,  0.0658,  0.1172,  0.2387],\n",
      "        [ 0.0383, -0.1350, -0.0362,  0.2379]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wv: torch.Size([8, 4])\n",
      "tensor([[ 0.1537, -0.3108, -0.2085,  0.1568],\n",
      "        [ 0.0199,  0.1015, -0.3510,  0.3459],\n",
      "        [ 0.0268, -0.1385,  0.2386,  0.1793],\n",
      "        [-0.3228,  0.3362, -0.0090, -0.0302],\n",
      "        [-0.1185, -0.0781, -0.1422, -0.1206],\n",
      "        [ 0.2601, -0.0946, -0.1745,  0.1041],\n",
      "        [ 0.0013, -0.0132, -0.1698, -0.2354],\n",
      "        [-0.3045, -0.3061,  0.2975,  0.2782]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wq spliced: torch.Size([8, 8])\n",
      "tensor([[-0.0568,  0.1715,  0.1291, -0.0538, -0.2727,  0.0305, -0.1010,  0.0500],\n",
      "        [ 0.1049,  0.2299,  0.2896,  0.2405,  0.0403, -0.1731,  0.0116,  0.2085],\n",
      "        [ 0.1127, -0.1474, -0.2341,  0.2732, -0.2841,  0.2047, -0.2790, -0.1626],\n",
      "        [ 0.1407, -0.3189,  0.2706, -0.0620, -0.1115, -0.0586,  0.3311,  0.2044],\n",
      "        [ 0.1030, -0.1064,  0.0953,  0.3040,  0.2656, -0.3465, -0.0165,  0.1346],\n",
      "        [ 0.1096, -0.2188,  0.1101,  0.0192, -0.2362,  0.2024, -0.2135,  0.2610],\n",
      "        [-0.2311, -0.2619, -0.2529,  0.3010, -0.2211,  0.0617,  0.2394, -0.2788],\n",
      "        [-0.2140, -0.1297,  0.2172, -0.2159, -0.0770,  0.3502,  0.0704, -0.1808]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Wk spliced: torch.Size([8, 4])\n",
      "tensor([[ 0.2281,  0.1489,  0.3497, -0.0064],\n",
      "        [ 0.0599, -0.0035,  0.1334, -0.3012],\n",
      "        [ 0.3147,  0.1628,  0.0012,  0.2319],\n",
      "        [ 0.2715, -0.0851,  0.1486,  0.0092],\n",
      "        [-0.0067,  0.0859,  0.0586,  0.3026],\n",
      "        [-0.0785, -0.2659,  0.2588,  0.2002],\n",
      "        [ 0.2051,  0.0658,  0.1172,  0.2387],\n",
      "        [ 0.0383, -0.1350, -0.0362,  0.2379]], grad_fn=<CatBackward0>)\n",
      "Wv spliced: torch.Size([8, 4])\n",
      "tensor([[ 0.1537, -0.3108, -0.2085,  0.1568],\n",
      "        [ 0.0199,  0.1015, -0.3510,  0.3459],\n",
      "        [ 0.0268, -0.1385,  0.2386,  0.1793],\n",
      "        [-0.3228,  0.3362, -0.0090, -0.0302],\n",
      "        [-0.1185, -0.0781, -0.1422, -0.1206],\n",
      "        [ 0.2601, -0.0946, -0.1745,  0.1041],\n",
      "        [ 0.0013, -0.0132, -0.1698, -0.2354],\n",
      "        [-0.3045, -0.3061,  0.2975,  0.2782]], grad_fn=<CatBackward0>)\n",
      "Wqkv_spliced: torch.Size([8, 16])\n",
      "tensor([[-0.0568,  0.1715,  0.1291, -0.0538, -0.2727,  0.0305, -0.1010,  0.0500,\n",
      "          0.2281,  0.1489,  0.3497, -0.0064,  0.1537, -0.3108, -0.2085,  0.1568],\n",
      "        [ 0.1049,  0.2299,  0.2896,  0.2405,  0.0403, -0.1731,  0.0116,  0.2085,\n",
      "          0.0599, -0.0035,  0.1334, -0.3012,  0.0199,  0.1015, -0.3510,  0.3459],\n",
      "        [ 0.1127, -0.1474, -0.2341,  0.2732, -0.2841,  0.2047, -0.2790, -0.1626,\n",
      "          0.3147,  0.1628,  0.0012,  0.2319,  0.0268, -0.1385,  0.2386,  0.1793],\n",
      "        [ 0.1407, -0.3189,  0.2706, -0.0620, -0.1115, -0.0586,  0.3311,  0.2044,\n",
      "          0.2715, -0.0851,  0.1486,  0.0092, -0.3228,  0.3362, -0.0090, -0.0302],\n",
      "        [ 0.1030, -0.1064,  0.0953,  0.3040,  0.2656, -0.3465, -0.0165,  0.1346,\n",
      "         -0.0067,  0.0859,  0.0586,  0.3026, -0.1185, -0.0781, -0.1422, -0.1206],\n",
      "        [ 0.1096, -0.2188,  0.1101,  0.0192, -0.2362,  0.2024, -0.2135,  0.2610,\n",
      "         -0.0785, -0.2659,  0.2588,  0.2002,  0.2601, -0.0946, -0.1745,  0.1041],\n",
      "        [-0.2311, -0.2619, -0.2529,  0.3010, -0.2211,  0.0617,  0.2394, -0.2788,\n",
      "          0.2051,  0.0658,  0.1172,  0.2387,  0.0013, -0.0132, -0.1698, -0.2354],\n",
      "        [-0.2140, -0.1297,  0.2172, -0.2159, -0.0770,  0.3502,  0.0704, -0.1808,\n",
      "          0.0383, -0.1350, -0.0362,  0.2379, -0.3045, -0.3061,  0.2975,  0.2782]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "xqkv: torch.Size([1, 3, 16])\n",
      "tensor([[[-0.1493, -1.1637, -0.2027, -1.1092, -0.0086,  0.3443,  0.8272,\n",
      "          -0.2927,  0.1241, -0.2312, -0.3873,  0.7154, -0.9477,  0.4647,\n",
      "           1.2098, -1.1022],\n",
      "         [-0.1998,  0.6164,  0.4673, -0.0724,  0.3227,  0.3410, -0.6594,\n",
      "          -0.1181, -0.4138, -0.1059, -0.1962,  0.1079, -0.0300, -1.0025,\n",
      "           0.2622,  1.0419],\n",
      "         [-0.9768,  0.1572, -0.8754,  0.0635, -0.1896,  0.5430, -0.5492,\n",
      "          -1.2495, -0.0311,  0.5580, -0.0628,  1.2032,  0.2898, -1.7881,\n",
      "           0.4854, -0.2679]]], grad_fn=<UnsafeViewBackward0>)\n",
      "xq: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.1493, -1.1637, -0.2027, -1.1092, -0.0086,  0.3443,  0.8272,\n",
      "          -0.2927],\n",
      "         [-0.1998,  0.6164,  0.4673, -0.0724,  0.3227,  0.3410, -0.6594,\n",
      "          -0.1181],\n",
      "         [-0.9768,  0.1572, -0.8754,  0.0635, -0.1896,  0.5430, -0.5492,\n",
      "          -1.2495]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xk: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.1241, -0.2312, -0.3873,  0.7154],\n",
      "         [-0.4138, -0.1059, -0.1962,  0.1079],\n",
      "         [-0.0311,  0.5580, -0.0628,  1.2032]]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "xv: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.9477,  0.4647,  1.2098, -1.1022],\n",
      "         [-0.0300, -1.0025,  0.2622,  1.0419],\n",
      "         [ 0.2898, -1.7881,  0.4854, -0.2679]]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "xq reshaped: torch.Size([1, 3, 2, 4])\n",
      "tensor([[[[-0.1493, -1.1637, -0.2027, -1.1092],\n",
      "          [-0.0086,  0.3443,  0.8272, -0.2927]],\n",
      "\n",
      "         [[-0.1998,  0.6164,  0.4673, -0.0724],\n",
      "          [ 0.3227,  0.3410, -0.6594, -0.1181]],\n",
      "\n",
      "         [[-0.9768,  0.1572, -0.8754,  0.0635],\n",
      "          [-0.1896,  0.5430, -0.5492, -1.2495]]]], grad_fn=<ViewBackward0>)\n",
      "xk reshaped: torch.Size([1, 3, 1, 4])\n",
      "tensor([[[[ 0.1241, -0.2312, -0.3873,  0.7154]],\n",
      "\n",
      "         [[-0.4138, -0.1059, -0.1962,  0.1079]],\n",
      "\n",
      "         [[-0.0311,  0.5580, -0.0628,  1.2032]]]], grad_fn=<ViewBackward0>)\n",
      "xv reshaped: torch.Size([1, 3, 1, 4])\n",
      "tensor([[[[-0.9477,  0.4647,  1.2098, -1.1022]],\n",
      "\n",
      "         [[-0.0300, -1.0025,  0.2622,  1.0419]],\n",
      "\n",
      "         [[ 0.2898, -1.7881,  0.4854, -0.2679]]]], grad_fn=<ViewBackward0>)\n",
      "rotated xq: torch.Size([1, 3, 2, 4])\n",
      "tensor([[[[-0.1493, -1.1637, -0.2027, -1.1092],\n",
      "          [-0.0086,  0.3443,  0.8272, -0.2927]],\n",
      "\n",
      "         [[-0.5012,  0.6206,  0.0844, -0.0105],\n",
      "          [ 0.7292,  0.3511, -0.0848, -0.0835]],\n",
      "\n",
      "         [[ 1.2025,  0.1415, -0.5239,  0.0935],\n",
      "          [ 0.5783,  0.7805,  0.0561, -1.1167]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "rotated xk: torch.Size([1, 3, 1, 4])\n",
      "tensor([[[[ 0.1241, -0.2312, -0.3873,  0.7154]],\n",
      "\n",
      "         [[-0.0585, -0.1161, -0.4542,  0.0968]],\n",
      "\n",
      "         [[ 0.0700,  0.3079, -0.0021,  1.2901]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "repeat_interleaved xk: torch.Size([1, 3, 2, 4])\n",
      "tensor([[[[ 0.1241, -0.2312, -0.3873,  0.7154],\n",
      "          [ 0.1241, -0.2312, -0.3873,  0.7154]],\n",
      "\n",
      "         [[-0.0585, -0.1161, -0.4542,  0.0968],\n",
      "          [-0.0585, -0.1161, -0.4542,  0.0968]],\n",
      "\n",
      "         [[ 0.0700,  0.3079, -0.0021,  1.2901],\n",
      "          [ 0.0700,  0.3079, -0.0021,  1.2901]]]], grad_fn=<ViewBackward0>)\n",
      "repeat_interleaved xv: torch.Size([1, 3, 2, 4])\n",
      "tensor([[[[-0.9477,  0.4647,  1.2098, -1.1022],\n",
      "          [-0.9477,  0.4647,  1.2098, -1.1022]],\n",
      "\n",
      "         [[-0.0300, -1.0025,  0.2622,  1.0419],\n",
      "          [-0.0300, -1.0025,  0.2622,  1.0419]],\n",
      "\n",
      "         [[ 0.2898, -1.7881,  0.4854, -0.2679],\n",
      "          [ 0.2898, -1.7881,  0.4854, -0.2679]]]], grad_fn=<ViewBackward0>)\n",
      "transposed xq: torch.Size([1, 2, 3, 4])\n",
      "tensor([[[[-0.1493, -1.1637, -0.2027, -1.1092],\n",
      "          [-0.5012,  0.6206,  0.0844, -0.0105],\n",
      "          [ 1.2025,  0.1415, -0.5239,  0.0935]],\n",
      "\n",
      "         [[-0.0086,  0.3443,  0.8272, -0.2927],\n",
      "          [ 0.7292,  0.3511, -0.0848, -0.0835],\n",
      "          [ 0.5783,  0.7805,  0.0561, -1.1167]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "transposed xk: torch.Size([1, 2, 3, 4])\n",
      "tensor([[[[ 0.1241, -0.2312, -0.3873,  0.7154],\n",
      "          [-0.0585, -0.1161, -0.4542,  0.0968],\n",
      "          [ 0.0700,  0.3079, -0.0021,  1.2901]],\n",
      "\n",
      "         [[ 0.1241, -0.2312, -0.3873,  0.7154],\n",
      "          [-0.0585, -0.1161, -0.4542,  0.0968],\n",
      "          [ 0.0700,  0.3079, -0.0021,  1.2901]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "transposed xv: torch.Size([1, 2, 3, 4])\n",
      "tensor([[[[-0.9477,  0.4647,  1.2098, -1.1022],\n",
      "          [-0.0300, -1.0025,  0.2622,  1.0419],\n",
      "          [ 0.2898, -1.7881,  0.4854, -0.2679]],\n",
      "\n",
      "         [[-0.9477,  0.4647,  1.2098, -1.1022],\n",
      "          [-0.0300, -1.0025,  0.2622,  1.0419],\n",
      "          [ 0.2898, -1.7881,  0.4854, -0.2679]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[-0.2323,  0.0643, -0.8996],\n",
      "          [-0.1229, -0.0411,  0.0711],\n",
      "          [ 0.1931,  0.0801,  0.1247]],\n",
      "\n",
      "         [[-0.3052, -0.2218, -0.1370],\n",
      "          [-0.0088, -0.0265,  0.0258],\n",
      "          [-0.4647, -0.1290, -0.5800]]]], grad_fn=<MulBackward0>)\n",
      "mask: torch.Size([1, 1, 3, 3])\n",
      "tensor([[[[ 0.0000e+00, -2.3820e+38, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]]]])\n",
      "masked scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[-2.3228e-01, -2.3820e+38, -2.3820e+38],\n",
      "          [-1.2292e-01, -4.1062e-02, -2.3820e+38],\n",
      "          [ 1.9315e-01,  8.0133e-02,  1.2474e-01]],\n",
      "\n",
      "         [[-3.0519e-01, -2.3820e+38, -2.3820e+38],\n",
      "          [-8.8016e-03, -2.6501e-02, -2.3820e+38],\n",
      "          [-4.6465e-01, -1.2901e-01, -5.7999e-01]]]], grad_fn=<AddBackward0>)\n",
      "softmaxed scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [0.4795, 0.5205, 0.0000],\n",
      "          [0.3537, 0.3159, 0.3303]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.5044, 0.4956, 0.0000],\n",
      "          [0.3040, 0.4252, 0.2708]]]], grad_fn=<SoftmaxBackward0>)\n",
      "attention: torch.Size([1, 2, 3, 4])\n",
      "tensor([[[[-0.9477,  0.4647,  1.2098, -1.1022],\n",
      "          [-0.4701, -0.2989,  0.7166,  0.0137],\n",
      "          [-0.2490, -0.7430,  0.6711, -0.1492]],\n",
      "\n",
      "         [[-0.9477,  0.4647,  1.2098, -1.1022],\n",
      "          [-0.4929, -0.2624,  0.7402, -0.0396],\n",
      "          [-0.2224, -0.7693,  0.6107,  0.0354]]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "reshaped attention: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.9477,  0.4647,  1.2098, -1.1022, -0.9477,  0.4647,  1.2098,\n",
      "          -1.1022],\n",
      "         [-0.4701, -0.2989,  0.7166,  0.0137, -0.4929, -0.2624,  0.7402,\n",
      "          -0.0396],\n",
      "         [-0.2490, -0.7430,  0.6711, -0.1492, -0.2224, -0.7693,  0.6107,\n",
      "           0.0354]]], grad_fn=<ViewBackward0>)\n",
      "self.Wo: torch.Size([8, 8])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0060,  0.3290,  0.2733,  0.2162,  0.3535,  0.1507,  0.2803,  0.0906],\n",
      "        [ 0.1291, -0.1482, -0.1375, -0.1027,  0.2824,  0.2626,  0.2897, -0.0449],\n",
      "        [ 0.2979,  0.2022,  0.3262, -0.2954,  0.0692,  0.1622,  0.0713, -0.0723],\n",
      "        [ 0.0566, -0.1006, -0.1048, -0.0061, -0.0865,  0.2769, -0.3314, -0.0614],\n",
      "        [ 0.0054, -0.3399,  0.3125, -0.1648,  0.1993,  0.3229,  0.1911, -0.2480],\n",
      "        [ 0.0134, -0.0465, -0.2447,  0.0723,  0.1158,  0.1092,  0.2752,  0.0673],\n",
      "        [-0.1324, -0.0816, -0.2656,  0.3021,  0.1316, -0.2970,  0.1574,  0.3460],\n",
      "        [-0.1654,  0.0770,  0.3162,  0.2686,  0.2592,  0.0153,  0.1718,  0.1629]],\n",
      "       requires_grad=True)\n",
      "spliced Wo: torch.Size([8, 8])\n",
      "tensor([[ 0.0060,  0.3290,  0.2733,  0.2162,  0.3535,  0.1507,  0.2803,  0.0906],\n",
      "        [ 0.1291, -0.1482, -0.1375, -0.1027,  0.2824,  0.2626,  0.2897, -0.0449],\n",
      "        [ 0.2979,  0.2022,  0.3262, -0.2954,  0.0692,  0.1622,  0.0713, -0.0723],\n",
      "        [ 0.0566, -0.1006, -0.1048, -0.0061, -0.0865,  0.2769, -0.3314, -0.0614],\n",
      "        [ 0.0054, -0.3399,  0.3125, -0.1648,  0.1993,  0.3229,  0.1911, -0.2480],\n",
      "        [ 0.0134, -0.0465, -0.2447,  0.0723,  0.1158,  0.1092,  0.2752,  0.0673],\n",
      "        [-0.1324, -0.0816, -0.2656,  0.3021,  0.1316, -0.2970,  0.1574,  0.3460],\n",
      "        [-0.1654,  0.0770,  0.3162,  0.2686,  0.2592,  0.0153,  0.1718,  0.1629]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "projected output: torch.Size([1, 3, 8])\n",
      "tensor([[[ 3.7566e-01,  9.1792e-02, -8.9258e-01, -3.4415e-01, -2.8628e-01,\n",
      "          -7.6127e-01,  2.6834e-01,  3.7884e-01],\n",
      "         [ 7.5212e-02,  1.4943e-01, -1.5409e-01, -7.5181e-03, -2.4370e-01,\n",
      "          -4.3760e-01, -2.2856e-01,  2.7245e-01],\n",
      "         [-4.1966e-03,  2.4312e-01,  2.3637e-01,  1.1925e-04, -2.8236e-01,\n",
      "          -5.0173e-01, -3.3980e-01,  1.9194e-01]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "----------------- END MultiQueryAttention.forwardTensor() --------------------\n",
      "forwardTensor() output: torch.Size([1, 3, 8])\n",
      "tensor([[[ 3.7566e-01,  9.1792e-02, -8.9258e-01, -3.4415e-01, -2.8628e-01,\n",
      "          -7.6127e-01,  2.6834e-01,  3.7884e-01],\n",
      "         [ 7.5212e-02,  1.4943e-01, -1.5409e-01, -7.5181e-03, -2.4370e-01,\n",
      "          -4.3760e-01, -2.2856e-01,  2.7245e-01],\n",
      "         [-4.1966e-03,  2.4312e-01,  2.3637e-01,  1.1925e-04, -2.8236e-01,\n",
      "          -5.0173e-01, -3.3980e-01,  1.9194e-01]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "Level 1 from range(2)\n",
      "Model 0 from range(2)\n",
      "----------------- MultiQueryAttention.forwardTensor() --------------------\n",
      "x shape: torch.Size([1, 3, 4])\n",
      "d_skip: 0\n",
      "models_in_this_level: 2\n",
      "h_dim: 2\n",
      "h_skip: 0\n",
      "self.Wqkv: torch.Size([8, 16])\n",
      "Parameter containing:\n",
      "tensor([[-0.0568,  0.1715,  0.1291, -0.0538, -0.2727,  0.0305, -0.1010,  0.0500,\n",
      "          0.2281,  0.1489,  0.3497, -0.0064,  0.1537, -0.3108, -0.2085,  0.1568],\n",
      "        [ 0.1049,  0.2299,  0.2896,  0.2405,  0.0403, -0.1731,  0.0116,  0.2085,\n",
      "          0.0599, -0.0035,  0.1334, -0.3012,  0.0199,  0.1015, -0.3510,  0.3459],\n",
      "        [ 0.1127, -0.1474, -0.2341,  0.2732, -0.2841,  0.2047, -0.2790, -0.1626,\n",
      "          0.3147,  0.1628,  0.0012,  0.2319,  0.0268, -0.1385,  0.2386,  0.1793],\n",
      "        [ 0.1407, -0.3189,  0.2706, -0.0620, -0.1115, -0.0586,  0.3311,  0.2044,\n",
      "          0.2715, -0.0851,  0.1486,  0.0092, -0.3228,  0.3362, -0.0090, -0.0302],\n",
      "        [ 0.1030, -0.1064,  0.0953,  0.3040,  0.2656, -0.3465, -0.0165,  0.1346,\n",
      "         -0.0067,  0.0859,  0.0586,  0.3026, -0.1185, -0.0781, -0.1422, -0.1206],\n",
      "        [ 0.1096, -0.2188,  0.1101,  0.0192, -0.2362,  0.2024, -0.2135,  0.2610,\n",
      "         -0.0785, -0.2659,  0.2588,  0.2002,  0.2601, -0.0946, -0.1745,  0.1041],\n",
      "        [-0.2311, -0.2619, -0.2529,  0.3010, -0.2211,  0.0617,  0.2394, -0.2788,\n",
      "          0.2051,  0.0658,  0.1172,  0.2387,  0.0013, -0.0132, -0.1698, -0.2354],\n",
      "        [-0.2140, -0.1297,  0.2172, -0.2159, -0.0770,  0.3502,  0.0704, -0.1808,\n",
      "          0.0383, -0.1350, -0.0362,  0.2379, -0.3045, -0.3061,  0.2975,  0.2782]],\n",
      "       requires_grad=True)\n",
      "Wq: torch.Size([8, 8])\n",
      "tensor([[-0.0568,  0.1715,  0.1291, -0.0538, -0.2727,  0.0305, -0.1010,  0.0500],\n",
      "        [ 0.1049,  0.2299,  0.2896,  0.2405,  0.0403, -0.1731,  0.0116,  0.2085],\n",
      "        [ 0.1127, -0.1474, -0.2341,  0.2732, -0.2841,  0.2047, -0.2790, -0.1626],\n",
      "        [ 0.1407, -0.3189,  0.2706, -0.0620, -0.1115, -0.0586,  0.3311,  0.2044],\n",
      "        [ 0.1030, -0.1064,  0.0953,  0.3040,  0.2656, -0.3465, -0.0165,  0.1346],\n",
      "        [ 0.1096, -0.2188,  0.1101,  0.0192, -0.2362,  0.2024, -0.2135,  0.2610],\n",
      "        [-0.2311, -0.2619, -0.2529,  0.3010, -0.2211,  0.0617,  0.2394, -0.2788],\n",
      "        [-0.2140, -0.1297,  0.2172, -0.2159, -0.0770,  0.3502,  0.0704, -0.1808]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wk: torch.Size([8, 4])\n",
      "tensor([[ 0.2281,  0.1489,  0.3497, -0.0064],\n",
      "        [ 0.0599, -0.0035,  0.1334, -0.3012],\n",
      "        [ 0.3147,  0.1628,  0.0012,  0.2319],\n",
      "        [ 0.2715, -0.0851,  0.1486,  0.0092],\n",
      "        [-0.0067,  0.0859,  0.0586,  0.3026],\n",
      "        [-0.0785, -0.2659,  0.2588,  0.2002],\n",
      "        [ 0.2051,  0.0658,  0.1172,  0.2387],\n",
      "        [ 0.0383, -0.1350, -0.0362,  0.2379]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wv: torch.Size([8, 4])\n",
      "tensor([[ 0.1537, -0.3108, -0.2085,  0.1568],\n",
      "        [ 0.0199,  0.1015, -0.3510,  0.3459],\n",
      "        [ 0.0268, -0.1385,  0.2386,  0.1793],\n",
      "        [-0.3228,  0.3362, -0.0090, -0.0302],\n",
      "        [-0.1185, -0.0781, -0.1422, -0.1206],\n",
      "        [ 0.2601, -0.0946, -0.1745,  0.1041],\n",
      "        [ 0.0013, -0.0132, -0.1698, -0.2354],\n",
      "        [-0.3045, -0.3061,  0.2975,  0.2782]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wq spliced: torch.Size([4, 4])\n",
      "tensor([[-0.0568,  0.1715, -0.2727,  0.0305],\n",
      "        [ 0.1049,  0.2299,  0.0403, -0.1731],\n",
      "        [ 0.1127, -0.1474, -0.2841,  0.2047],\n",
      "        [ 0.1407, -0.3189, -0.1115, -0.0586]], grad_fn=<CatBackward0>)\n",
      "Wk spliced: torch.Size([4, 2])\n",
      "tensor([[ 0.2281,  0.1489],\n",
      "        [ 0.0599, -0.0035],\n",
      "        [ 0.3147,  0.1628],\n",
      "        [ 0.2715, -0.0851]], grad_fn=<CatBackward0>)\n",
      "Wv spliced: torch.Size([4, 2])\n",
      "tensor([[ 0.1537, -0.3108],\n",
      "        [ 0.0199,  0.1015],\n",
      "        [ 0.0268, -0.1385],\n",
      "        [-0.3228,  0.3362]], grad_fn=<CatBackward0>)\n",
      "Wqkv_spliced: torch.Size([4, 8])\n",
      "tensor([[-0.0568,  0.1715, -0.2727,  0.0305,  0.2281,  0.1489,  0.1537, -0.3108],\n",
      "        [ 0.1049,  0.2299,  0.0403, -0.1731,  0.0599, -0.0035,  0.0199,  0.1015],\n",
      "        [ 0.1127, -0.1474, -0.2841,  0.2047,  0.3147,  0.1628,  0.0268, -0.1385],\n",
      "        [ 0.1407, -0.3189, -0.1115, -0.0586,  0.2715, -0.0851, -0.3228,  0.3362]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "xqkv: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.0632, -0.0985,  0.1327,  0.2477, -0.2697,  0.0453,  0.1469,\n",
      "          -0.1617],\n",
      "         [ 0.0398,  0.0019, -0.6707,  0.2867,  0.6390,  0.3626,  0.2033,\n",
      "          -0.5455],\n",
      "         [-0.0316, -0.2352, -0.3347,  0.3789,  0.2232,  0.2132,  0.1234,\n",
      "          -0.3743]]], grad_fn=<UnsafeViewBackward0>)\n",
      "xq: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.0632, -0.0985,  0.1327,  0.2477],\n",
      "         [ 0.0398,  0.0019, -0.6707,  0.2867],\n",
      "         [-0.0316, -0.2352, -0.3347,  0.3789]]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "xk: torch.Size([1, 3, 2])\n",
      "tensor([[[-0.2697,  0.0453],\n",
      "         [ 0.6390,  0.3626],\n",
      "         [ 0.2232,  0.2132]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xv: torch.Size([1, 3, 2])\n",
      "tensor([[[ 0.1469, -0.1617],\n",
      "         [ 0.2033, -0.5455],\n",
      "         [ 0.1234, -0.3743]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xq reshaped: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[-0.0632, -0.0985],\n",
      "          [ 0.1327,  0.2477]],\n",
      "\n",
      "         [[ 0.0398,  0.0019],\n",
      "          [-0.6707,  0.2867]],\n",
      "\n",
      "         [[-0.0316, -0.2352],\n",
      "          [-0.3347,  0.3789]]]], grad_fn=<ViewBackward0>)\n",
      "xk reshaped: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[-0.2697,  0.0453]],\n",
      "\n",
      "         [[ 0.6390,  0.3626]],\n",
      "\n",
      "         [[ 0.2232,  0.2132]]]], grad_fn=<ViewBackward0>)\n",
      "xv reshaped: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[ 0.1469, -0.1617]],\n",
      "\n",
      "         [[ 0.2033, -0.5455]],\n",
      "\n",
      "         [[ 0.1234, -0.3743]]]], grad_fn=<ViewBackward0>)\n",
      "rotated xq: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[-0.0632, -0.0985],\n",
      "          [ 0.1327,  0.2477]],\n",
      "\n",
      "         [[ 0.0199,  0.0345],\n",
      "          [-0.6036, -0.4095]],\n",
      "\n",
      "         [[ 0.2270,  0.0692],\n",
      "          [-0.2052, -0.4620]]]], grad_fn=<TransposeBackward0>)\n",
      "rotated xk: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[-0.2697,  0.0453]],\n",
      "\n",
      "         [[ 0.0401,  0.7336]],\n",
      "\n",
      "         [[-0.2868,  0.1142]]]], grad_fn=<TransposeBackward0>)\n",
      "repeat_interleaved xk: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[-0.2697,  0.0453],\n",
      "          [-0.2697,  0.0453]],\n",
      "\n",
      "         [[ 0.0401,  0.7336],\n",
      "          [ 0.0401,  0.7336]],\n",
      "\n",
      "         [[-0.2868,  0.1142],\n",
      "          [-0.2868,  0.1142]]]], grad_fn=<ViewBackward0>)\n",
      "repeat_interleaved xv: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[ 0.1469, -0.1617],\n",
      "          [ 0.1469, -0.1617]],\n",
      "\n",
      "         [[ 0.2033, -0.5455],\n",
      "          [ 0.2033, -0.5455]],\n",
      "\n",
      "         [[ 0.1234, -0.3743],\n",
      "          [ 0.1234, -0.3743]]]], grad_fn=<ViewBackward0>)\n",
      "transposed xq: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[-0.0632, -0.0985],\n",
      "          [ 0.0199,  0.0345],\n",
      "          [ 0.2270,  0.0692]],\n",
      "\n",
      "         [[ 0.1327,  0.2477],\n",
      "          [-0.6036, -0.4095],\n",
      "          [-0.2052, -0.4620]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xk: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[-0.2697,  0.0453],\n",
      "          [ 0.0401,  0.7336],\n",
      "          [-0.2868,  0.1142]],\n",
      "\n",
      "         [[-0.2697,  0.0453],\n",
      "          [ 0.0401,  0.7336],\n",
      "          [-0.2868,  0.1142]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xv: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[ 0.1469, -0.1617],\n",
      "          [ 0.2033, -0.5455],\n",
      "          [ 0.1234, -0.3743]],\n",
      "\n",
      "         [[ 0.1469, -0.1617],\n",
      "          [ 0.2033, -0.5455],\n",
      "          [ 0.1234, -0.3743]]]], grad_fn=<TransposeBackward0>)\n",
      "scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[ 0.0089, -0.0529,  0.0049],\n",
      "          [-0.0027,  0.0185, -0.0012],\n",
      "          [-0.0411,  0.0423, -0.0404]],\n",
      "\n",
      "         [[-0.0174,  0.1323, -0.0069],\n",
      "          [ 0.1020, -0.2295,  0.0893],\n",
      "          [ 0.0243, -0.2455,  0.0043]]]], grad_fn=<MulBackward0>)\n",
      "mask: torch.Size([1, 1, 3, 3])\n",
      "tensor([[[[ 0.0000e+00, -2.3820e+38, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]]]])\n",
      "masked scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[ 8.8926e-03, -2.3820e+38, -2.3820e+38],\n",
      "          [-2.6876e-03,  1.8459e-02, -2.3820e+38],\n",
      "          [-4.1084e-02,  4.2325e-02, -4.0449e-02]],\n",
      "\n",
      "         [[-1.7364e-02, -2.3820e+38, -2.3820e+38],\n",
      "          [ 1.0200e-01, -2.2953e-01, -2.3820e+38],\n",
      "          [ 2.4339e-02, -2.4548e-01,  4.2940e-03]]]], grad_fn=<AddBackward0>)\n",
      "softmaxed scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [0.4947, 0.5053, 0.0000],\n",
      "          [0.3239, 0.3520, 0.3241]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.5821, 0.4179, 0.0000],\n",
      "          [0.3645, 0.2783, 0.3572]]]], grad_fn=<SoftmaxBackward0>)\n",
      "attention: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[ 0.1469, -0.1617],\n",
      "          [ 0.1754, -0.3556],\n",
      "          [ 0.1591, -0.3657]],\n",
      "\n",
      "         [[ 0.1469, -0.1617],\n",
      "          [ 0.1705, -0.3221],\n",
      "          [ 0.1542, -0.3445]]]], grad_fn=<UnsafeViewBackward0>)\n",
      "reshaped attention: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.1469, -0.1617,  0.1469, -0.1617],\n",
      "         [ 0.1754, -0.3556,  0.1705, -0.3221],\n",
      "         [ 0.1591, -0.3657,  0.1542, -0.3445]]], grad_fn=<ViewBackward0>)\n",
      "self.Wo: torch.Size([8, 8])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0060,  0.3290,  0.2733,  0.2162,  0.3535,  0.1507,  0.2803,  0.0906],\n",
      "        [ 0.1291, -0.1482, -0.1375, -0.1027,  0.2824,  0.2626,  0.2897, -0.0449],\n",
      "        [ 0.2979,  0.2022,  0.3262, -0.2954,  0.0692,  0.1622,  0.0713, -0.0723],\n",
      "        [ 0.0566, -0.1006, -0.1048, -0.0061, -0.0865,  0.2769, -0.3314, -0.0614],\n",
      "        [ 0.0054, -0.3399,  0.3125, -0.1648,  0.1993,  0.3229,  0.1911, -0.2480],\n",
      "        [ 0.0134, -0.0465, -0.2447,  0.0723,  0.1158,  0.1092,  0.2752,  0.0673],\n",
      "        [-0.1324, -0.0816, -0.2656,  0.3021,  0.1316, -0.2970,  0.1574,  0.3460],\n",
      "        [-0.1654,  0.0770,  0.3162,  0.2686,  0.2592,  0.0153,  0.1718,  0.1629]],\n",
      "       requires_grad=True)\n",
      "spliced Wo: torch.Size([4, 4])\n",
      "tensor([[ 0.0060,  0.3290,  0.2733,  0.2162],\n",
      "        [ 0.1291, -0.1482, -0.1375, -0.1027],\n",
      "        [ 0.0054, -0.3399,  0.3125, -0.1648],\n",
      "        [ 0.0134, -0.0465, -0.2447,  0.0723]], grad_fn=<CatBackward0>)\n",
      "projected output: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.0214,  0.0299,  0.1479,  0.0125],\n",
      "         [-0.0483,  0.0674,  0.2289,  0.0231],\n",
      "         [-0.0501,  0.0702,  0.2263,  0.0216]]], grad_fn=<UnsafeViewBackward0>)\n",
      "----------------- END MultiQueryAttention.forwardTensor() --------------------\n",
      "forwardTensor() output: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.0214,  0.0299,  0.1479,  0.0125],\n",
      "         [-0.0483,  0.0674,  0.2289,  0.0231],\n",
      "         [-0.0501,  0.0702,  0.2263,  0.0216]]], grad_fn=<UnsafeViewBackward0>)\n",
      "Model 1 from range(2)\n",
      "----------------- MultiQueryAttention.forwardTensor() --------------------\n",
      "x shape: torch.Size([1, 3, 4])\n",
      "d_skip: 4\n",
      "models_in_this_level: 2\n",
      "h_dim: 2\n",
      "h_skip: 2\n",
      "self.Wqkv: torch.Size([8, 16])\n",
      "Parameter containing:\n",
      "tensor([[-0.0568,  0.1715,  0.1291, -0.0538, -0.2727,  0.0305, -0.1010,  0.0500,\n",
      "          0.2281,  0.1489,  0.3497, -0.0064,  0.1537, -0.3108, -0.2085,  0.1568],\n",
      "        [ 0.1049,  0.2299,  0.2896,  0.2405,  0.0403, -0.1731,  0.0116,  0.2085,\n",
      "          0.0599, -0.0035,  0.1334, -0.3012,  0.0199,  0.1015, -0.3510,  0.3459],\n",
      "        [ 0.1127, -0.1474, -0.2341,  0.2732, -0.2841,  0.2047, -0.2790, -0.1626,\n",
      "          0.3147,  0.1628,  0.0012,  0.2319,  0.0268, -0.1385,  0.2386,  0.1793],\n",
      "        [ 0.1407, -0.3189,  0.2706, -0.0620, -0.1115, -0.0586,  0.3311,  0.2044,\n",
      "          0.2715, -0.0851,  0.1486,  0.0092, -0.3228,  0.3362, -0.0090, -0.0302],\n",
      "        [ 0.1030, -0.1064,  0.0953,  0.3040,  0.2656, -0.3465, -0.0165,  0.1346,\n",
      "         -0.0067,  0.0859,  0.0586,  0.3026, -0.1185, -0.0781, -0.1422, -0.1206],\n",
      "        [ 0.1096, -0.2188,  0.1101,  0.0192, -0.2362,  0.2024, -0.2135,  0.2610,\n",
      "         -0.0785, -0.2659,  0.2588,  0.2002,  0.2601, -0.0946, -0.1745,  0.1041],\n",
      "        [-0.2311, -0.2619, -0.2529,  0.3010, -0.2211,  0.0617,  0.2394, -0.2788,\n",
      "          0.2051,  0.0658,  0.1172,  0.2387,  0.0013, -0.0132, -0.1698, -0.2354],\n",
      "        [-0.2140, -0.1297,  0.2172, -0.2159, -0.0770,  0.3502,  0.0704, -0.1808,\n",
      "          0.0383, -0.1350, -0.0362,  0.2379, -0.3045, -0.3061,  0.2975,  0.2782]],\n",
      "       requires_grad=True)\n",
      "Wq: torch.Size([8, 8])\n",
      "tensor([[-0.0568,  0.1715,  0.1291, -0.0538, -0.2727,  0.0305, -0.1010,  0.0500],\n",
      "        [ 0.1049,  0.2299,  0.2896,  0.2405,  0.0403, -0.1731,  0.0116,  0.2085],\n",
      "        [ 0.1127, -0.1474, -0.2341,  0.2732, -0.2841,  0.2047, -0.2790, -0.1626],\n",
      "        [ 0.1407, -0.3189,  0.2706, -0.0620, -0.1115, -0.0586,  0.3311,  0.2044],\n",
      "        [ 0.1030, -0.1064,  0.0953,  0.3040,  0.2656, -0.3465, -0.0165,  0.1346],\n",
      "        [ 0.1096, -0.2188,  0.1101,  0.0192, -0.2362,  0.2024, -0.2135,  0.2610],\n",
      "        [-0.2311, -0.2619, -0.2529,  0.3010, -0.2211,  0.0617,  0.2394, -0.2788],\n",
      "        [-0.2140, -0.1297,  0.2172, -0.2159, -0.0770,  0.3502,  0.0704, -0.1808]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wk: torch.Size([8, 4])\n",
      "tensor([[ 0.2281,  0.1489,  0.3497, -0.0064],\n",
      "        [ 0.0599, -0.0035,  0.1334, -0.3012],\n",
      "        [ 0.3147,  0.1628,  0.0012,  0.2319],\n",
      "        [ 0.2715, -0.0851,  0.1486,  0.0092],\n",
      "        [-0.0067,  0.0859,  0.0586,  0.3026],\n",
      "        [-0.0785, -0.2659,  0.2588,  0.2002],\n",
      "        [ 0.2051,  0.0658,  0.1172,  0.2387],\n",
      "        [ 0.0383, -0.1350, -0.0362,  0.2379]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wv: torch.Size([8, 4])\n",
      "tensor([[ 0.1537, -0.3108, -0.2085,  0.1568],\n",
      "        [ 0.0199,  0.1015, -0.3510,  0.3459],\n",
      "        [ 0.0268, -0.1385,  0.2386,  0.1793],\n",
      "        [-0.3228,  0.3362, -0.0090, -0.0302],\n",
      "        [-0.1185, -0.0781, -0.1422, -0.1206],\n",
      "        [ 0.2601, -0.0946, -0.1745,  0.1041],\n",
      "        [ 0.0013, -0.0132, -0.1698, -0.2354],\n",
      "        [-0.3045, -0.3061,  0.2975,  0.2782]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wq spliced: torch.Size([4, 4])\n",
      "tensor([[ 0.0953,  0.3040, -0.0165,  0.1346],\n",
      "        [ 0.1101,  0.0192, -0.2135,  0.2610],\n",
      "        [-0.2529,  0.3010,  0.2394, -0.2788],\n",
      "        [ 0.2172, -0.2159,  0.0704, -0.1808]], grad_fn=<CatBackward0>)\n",
      "Wk spliced: torch.Size([4, 2])\n",
      "tensor([[ 0.0586,  0.3026],\n",
      "        [ 0.2588,  0.2002],\n",
      "        [ 0.1172,  0.2387],\n",
      "        [-0.0362,  0.2379]], grad_fn=<CatBackward0>)\n",
      "Wv spliced: torch.Size([4, 2])\n",
      "tensor([[-0.1422, -0.1206],\n",
      "        [-0.1745,  0.1041],\n",
      "        [-0.1698, -0.2354],\n",
      "        [ 0.2975,  0.2782]], grad_fn=<CatBackward0>)\n",
      "Wqkv_spliced: torch.Size([4, 8])\n",
      "tensor([[ 0.0953,  0.3040, -0.0165,  0.1346,  0.0586,  0.3026, -0.1422, -0.1206],\n",
      "        [ 0.1101,  0.0192, -0.2135,  0.2610,  0.2588,  0.2002, -0.1745,  0.1041],\n",
      "        [-0.2529,  0.3010,  0.2394, -0.2788,  0.1172,  0.2387, -0.1698, -0.2354],\n",
      "        [ 0.2172, -0.2159,  0.0704, -0.1808, -0.0362,  0.2379,  0.2975,  0.2782]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "xqkv: torch.Size([1, 3, 8])\n",
      "tensor([[[ 0.5140,  0.7251, -0.0702,  0.3637,  0.1727,  1.0983, -0.2123,\n",
      "          -0.1071],\n",
      "         [ 0.2677,  0.1854, -0.3614,  0.5601,  0.2470,  0.2758, -0.2790,\n",
      "           0.0456],\n",
      "         [-0.0981, -0.4093, -0.1947,  0.1965, -0.2459, -0.8441,  0.1041,\n",
      "           0.0501]]], grad_fn=<UnsafeViewBackward0>)\n",
      "xq: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.5140,  0.7251, -0.0702,  0.3637],\n",
      "         [ 0.2677,  0.1854, -0.3614,  0.5601],\n",
      "         [-0.0981, -0.4093, -0.1947,  0.1965]]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "xk: torch.Size([1, 3, 2])\n",
      "tensor([[[ 0.1727,  1.0983],\n",
      "         [ 0.2470,  0.2758],\n",
      "         [-0.2459, -0.8441]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xv: torch.Size([1, 3, 2])\n",
      "tensor([[[-0.2123, -0.1071],\n",
      "         [-0.2790,  0.0456],\n",
      "         [ 0.1041,  0.0501]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xq reshaped: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[ 0.5140,  0.7251],\n",
      "          [-0.0702,  0.3637]],\n",
      "\n",
      "         [[ 0.2677,  0.1854],\n",
      "          [-0.3614,  0.5601]],\n",
      "\n",
      "         [[-0.0981, -0.4093],\n",
      "          [-0.1947,  0.1965]]]], grad_fn=<ViewBackward0>)\n",
      "xk reshaped: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[ 0.1727,  1.0983]],\n",
      "\n",
      "         [[ 0.2470,  0.2758]],\n",
      "\n",
      "         [[-0.2459, -0.8441]]]], grad_fn=<ViewBackward0>)\n",
      "xv reshaped: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[-0.2123, -0.1071]],\n",
      "\n",
      "         [[-0.2790,  0.0456]],\n",
      "\n",
      "         [[ 0.1041,  0.0501]]]], grad_fn=<ViewBackward0>)\n",
      "rotated xq: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[ 0.5140,  0.7251],\n",
      "          [-0.0702,  0.3637]],\n",
      "\n",
      "         [[-0.0114,  0.3255],\n",
      "          [-0.6666, -0.0015]],\n",
      "\n",
      "         [[ 0.4130,  0.0811],\n",
      "          [-0.0976, -0.2588]]]], grad_fn=<TransposeBackward0>)\n",
      "rotated xk: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[ 0.1727,  1.0983]],\n",
      "\n",
      "         [[-0.0986,  0.3569]],\n",
      "\n",
      "         [[ 0.8698,  0.1276]]]], grad_fn=<TransposeBackward0>)\n",
      "repeat_interleaved xk: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[ 0.1727,  1.0983],\n",
      "          [ 0.1727,  1.0983]],\n",
      "\n",
      "         [[-0.0986,  0.3569],\n",
      "          [-0.0986,  0.3569]],\n",
      "\n",
      "         [[ 0.8698,  0.1276],\n",
      "          [ 0.8698,  0.1276]]]], grad_fn=<ViewBackward0>)\n",
      "repeat_interleaved xv: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[-0.2123, -0.1071],\n",
      "          [-0.2123, -0.1071]],\n",
      "\n",
      "         [[-0.2790,  0.0456],\n",
      "          [-0.2790,  0.0456]],\n",
      "\n",
      "         [[ 0.1041,  0.0501],\n",
      "          [ 0.1041,  0.0501]]]], grad_fn=<ViewBackward0>)\n",
      "transposed xq: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[ 0.5140,  0.7251],\n",
      "          [-0.0114,  0.3255],\n",
      "          [ 0.4130,  0.0811]],\n",
      "\n",
      "         [[-0.0702,  0.3637],\n",
      "          [-0.6666, -0.0015],\n",
      "          [-0.0976, -0.2588]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xk: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[ 0.1727,  1.0983],\n",
      "          [-0.0986,  0.3569],\n",
      "          [ 0.8698,  0.1276]],\n",
      "\n",
      "         [[ 0.1727,  1.0983],\n",
      "          [-0.0986,  0.3569],\n",
      "          [ 0.8698,  0.1276]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xv: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[-0.2123, -0.1071],\n",
      "          [-0.2790,  0.0456],\n",
      "          [ 0.1041,  0.0501]],\n",
      "\n",
      "         [[-0.2123, -0.1071],\n",
      "          [-0.2790,  0.0456],\n",
      "          [ 0.1041,  0.0501]]]], grad_fn=<TransposeBackward0>)\n",
      "scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[ 0.6259,  0.1471,  0.3816],\n",
      "          [ 0.2514,  0.0829,  0.0224],\n",
      "          [ 0.1134, -0.0083,  0.2613]],\n",
      "\n",
      "         [[ 0.2739,  0.0967, -0.0103],\n",
      "          [-0.0826,  0.0461, -0.4102],\n",
      "          [-0.2129, -0.0585, -0.0834]]]], grad_fn=<MulBackward0>)\n",
      "mask: torch.Size([1, 1, 3, 3])\n",
      "tensor([[[[ 0.0000e+00, -2.3820e+38, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]]]])\n",
      "masked scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[ 6.2588e-01, -2.3820e+38, -2.3820e+38],\n",
      "          [ 2.5139e-01,  8.2931e-02, -2.3820e+38],\n",
      "          [ 1.1341e-01, -8.3259e-03,  2.6132e-01]],\n",
      "\n",
      "         [[ 2.7386e-01, -2.3820e+38, -2.3820e+38],\n",
      "          [-8.2593e-02,  4.6087e-02, -2.3820e+38],\n",
      "          [-2.1291e-01, -5.8498e-02, -8.3410e-02]]]], grad_fn=<AddBackward0>)\n",
      "softmaxed scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [0.5420, 0.4580, 0.0000],\n",
      "          [0.3284, 0.2908, 0.3808]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.4679, 0.5321, 0.0000],\n",
      "          [0.3026, 0.3531, 0.3444]]]], grad_fn=<SoftmaxBackward0>)\n",
      "attention: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[-0.2123, -0.1071],\n",
      "          [-0.2429, -0.0372],\n",
      "          [-0.1112, -0.0028]],\n",
      "\n",
      "         [[-0.2123, -0.1071],\n",
      "          [-0.2478, -0.0258],\n",
      "          [-0.1269,  0.0010]]]], grad_fn=<UnsafeViewBackward0>)\n",
      "reshaped attention: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.2123, -0.1071, -0.2123, -0.1071],\n",
      "         [-0.2429, -0.0372, -0.2478, -0.0258],\n",
      "         [-0.1112, -0.0028, -0.1269,  0.0010]]], grad_fn=<ViewBackward0>)\n",
      "self.Wo: torch.Size([8, 8])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0060,  0.3290,  0.2733,  0.2162,  0.3535,  0.1507,  0.2803,  0.0906],\n",
      "        [ 0.1291, -0.1482, -0.1375, -0.1027,  0.2824,  0.2626,  0.2897, -0.0449],\n",
      "        [ 0.2979,  0.2022,  0.3262, -0.2954,  0.0692,  0.1622,  0.0713, -0.0723],\n",
      "        [ 0.0566, -0.1006, -0.1048, -0.0061, -0.0865,  0.2769, -0.3314, -0.0614],\n",
      "        [ 0.0054, -0.3399,  0.3125, -0.1648,  0.1993,  0.3229,  0.1911, -0.2480],\n",
      "        [ 0.0134, -0.0465, -0.2447,  0.0723,  0.1158,  0.1092,  0.2752,  0.0673],\n",
      "        [-0.1324, -0.0816, -0.2656,  0.3021,  0.1316, -0.2970,  0.1574,  0.3460],\n",
      "        [-0.1654,  0.0770,  0.3162,  0.2686,  0.2592,  0.0153,  0.1718,  0.1629]],\n",
      "       requires_grad=True)\n",
      "spliced Wo: torch.Size([4, 4])\n",
      "tensor([[ 0.0692,  0.1622,  0.0713, -0.0723],\n",
      "        [-0.0865,  0.2769, -0.3314, -0.0614],\n",
      "        [ 0.1316, -0.2970,  0.1574,  0.3460],\n",
      "        [ 0.2592,  0.0153,  0.1718,  0.1629]], grad_fn=<CatBackward0>)\n",
      "projected output: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.0611, -0.0027, -0.0315, -0.0690],\n",
      "         [-0.0529,  0.0235, -0.0484, -0.0701],\n",
      "         [-0.0239,  0.0189, -0.0268, -0.0355]]], grad_fn=<UnsafeViewBackward0>)\n",
      "----------------- END MultiQueryAttention.forwardTensor() --------------------\n",
      "forwardTensor() output: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.0611, -0.0027, -0.0315, -0.0690],\n",
      "         [-0.0529,  0.0235, -0.0484, -0.0701],\n",
      "         [-0.0239,  0.0189, -0.0268, -0.0355]]], grad_fn=<UnsafeViewBackward0>)\n",
      "final output: ((tensor([[[ 4.1740e-01,  1.0199e-01, -9.9175e-01, -3.8238e-01, -3.1809e-01,\n",
      "          -8.4586e-01,  2.9816e-01,  4.2093e-01],\n",
      "         [ 0.0000e+00,  1.6603e-01, -1.7121e-01, -8.3534e-03, -2.7078e-01,\n",
      "          -4.8622e-01, -2.5396e-01,  0.0000e+00],\n",
      "         [-4.6629e-03,  2.7014e-01,  2.6263e-01,  1.3250e-04, -3.1374e-01,\n",
      "          -5.5747e-01, -3.7756e-01,  0.0000e+00]]], grad_fn=<MulBackward0>),), (tensor([[[-0.0238,  0.0332,  0.1643,  0.0139],\n",
      "         [-0.0536,  0.0000,  0.2544,  0.0256],\n",
      "         [-0.0556,  0.0779,  0.2514,  0.0241]]], grad_fn=<MulBackward0>), tensor([[[-0.0679, -0.0030, -0.0350, -0.0766],\n",
      "         [-0.0588,  0.0261, -0.0538, -0.0779],\n",
      "         [-0.0266,  0.0210, -0.0298, -0.0395]]], grad_fn=<MulBackward0>)))\n",
      "------------- END MultiQueryAttention.forwardTuple() ------------\n",
      "out: ((tensor([[[ 4.1740e-01,  1.0199e-01, -9.9175e-01, -3.8238e-01, -3.1809e-01,\n",
      "          -8.4586e-01,  2.9816e-01,  4.2093e-01],\n",
      "         [ 0.0000e+00,  1.6603e-01, -1.7121e-01, -8.3534e-03, -2.7078e-01,\n",
      "          -4.8622e-01, -2.5396e-01,  0.0000e+00],\n",
      "         [-4.6629e-03,  2.7014e-01,  2.6263e-01,  1.3250e-04, -3.1374e-01,\n",
      "          -5.5747e-01, -3.7756e-01,  0.0000e+00]]], grad_fn=<MulBackward0>),), (tensor([[[-0.0238,  0.0332,  0.1643,  0.0139],\n",
      "         [-0.0536,  0.0000,  0.2544,  0.0256],\n",
      "         [-0.0556,  0.0779,  0.2514,  0.0241]]], grad_fn=<MulBackward0>), tensor([[[-0.0679, -0.0030, -0.0350, -0.0766],\n",
      "         [-0.0588,  0.0261, -0.0538, -0.0779],\n",
      "         [-0.0266,  0.0210, -0.0298, -0.0395]]], grad_fn=<MulBackward0>)))\n",
      "---------- RESET CONFIG --------\n",
      "model_count:  [1, 2, 4]\n",
      "model_dim_list:  [128, 64, 32]\n",
      "head_dim_list:  [32, 16, 8]\n"
     ]
    }
   ],
   "source": [
    "# Testing our Attention's forwardTuple()\n",
    "verbose = True\n",
    "\n",
    "print(\"--------- Micro Hyperparameters -------\")\n",
    "hold1, hold2, hold3, hold4, hold5 = config.hidden_size, config.num_attention_heads, config.head_dim, config.levels, config.max_position_embeddings\n",
    "config.hidden_size = 8\n",
    "config.num_attention_heads = 2\n",
    "config.head_dim = 4\n",
    "config.levels = 2\n",
    "config.max_position_embeddings = 3\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "print(\"head_dim_list: \", config.head_dim_list)\n",
    "\n",
    "att = MultiQueryAttention(config)\n",
    "# we need to make sure to send in a tuple of the expected size. above we set hidden_size=4 and levels=2\n",
    "x = ((torch.randn((1,3,8)),),(torch.randn((1,3,4)),torch.randn((1,3,4))))\n",
    "print(f\"x: {x}\")\n",
    "out = att(x)\n",
    "print(f\"out: {out}\")\n",
    "\n",
    "verbose = False\n",
    "print(\"---------- RESET CONFIG --------\")\n",
    "config.hidden_size = hold1\n",
    "config.num_attention_heads = hold2\n",
    "config.head_dim = hold3\n",
    "config.levels = hold4\n",
    "config.max_position_embeddings = hold5\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "print(\"head_dim_list: \", config.head_dim_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391da4a9",
   "metadata": {},
   "source": [
    "# Layer\n",
    "\n",
    "nothing too interesting here besides the absurd amount of memory we're probably taking up with these tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb6e171e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(nn.Module):\n",
    "    \"\"\"\n",
    "    A decoder layer that integrates the MultiQueryAttention and MLP. It includes\n",
    "    normalization steps both before and after the attention mechanism to stabilize and accelerate training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initializes the GemmaAttention mechanism with parameters from the config, enabling self-attention within the decoder layer.\n",
    "        self.self_attn = MultiQueryAttention(config)\n",
    "        \n",
    "        # Initializes the GemmaMLP module, providing a non-linear transformation after the attention mechanism.\n",
    "        self.mlp = MLP(\n",
    "            # the hidden dimension of the model\n",
    "            hidden_size = config.hidden_size,\n",
    "            # the number of nodes in the center of the two feedforward layers\n",
    "            intermediate_size = config.intermediate_size,\n",
    "            # the % of neurons to set to 0 during training\n",
    "            dropout = config.dropout,\n",
    "        )\n",
    "        \n",
    "        # Applies RMSNorm normalization to the input of the decoder layer for stable training dynamics.\n",
    "        self.input_layernorm = RMSNorm(config.hidden_size,\n",
    "                                       eps = config.rms_norm_eps)\n",
    "        \n",
    "        # Applies RMSNorm after the attention mechanism and before the MLP to ensure the output is well-conditioned for further processing.\n",
    "        self.post_attention_layernorm = RMSNorm(config.hidden_size,\n",
    "                                                eps = config.rms_norm_eps)\n",
    "\n",
    "    def forwardTensor(self,\n",
    "                # The input tensor to the decoder layer. shape (batch_size, input_len, hidden_size)\n",
    "                x: torch.Tensor,\n",
    "                model: int = 0,\n",
    "                drop_bool: bool = False\n",
    "                ) -> torch.Tensor:\n",
    "        global verbose\n",
    "        if verbose: print(\"----------------- Layer.forwardTensor() --------------------\")\n",
    "        \n",
    "        # Self Attention Block\n",
    "        # Stores the original input for use as a residual connection, aiding in mitigating the vanishing gradient problem\n",
    "        residual_connection = x\n",
    "        # Normalizes the input before processing by the attention mechanism.\n",
    "        x = self.input_layernorm(x, model)\n",
    "        # Processes the normalized input through the GemmaAttention mechanism\n",
    "        x = self.self_attn(x, model, drop_bool)\n",
    "        # The aforementioned residual connection\n",
    "        x = residual_connection + x\n",
    "        if verbose: print(f\"x in layer after MQA & resid connection and before MLP:\\n{x}\")\n",
    "\n",
    "        # MLP Block\n",
    "        # Again, stores the output of the attention block for use as a residual connection before processing by the MLP.\n",
    "        residual_connection = x\n",
    "        # Normalizes the output of the attention block before passing it to the MLP, ensuring a stable input distribution.\n",
    "        x = self.post_attention_layernorm(x, model)\n",
    "        # Transforms the normalized attention output through the MLP, introducing additional non-linearity and capacity to the model.\n",
    "        x = self.mlp(x, model, drop_bool)\n",
    "        # Another residual connection\n",
    "        x = residual_connection + x\n",
    "        if verbose: \n",
    "            print(f\"layer's final residual state:\\n{x}\")\n",
    "            print(\"----------------- END Layer.forwardTensor() --------------------\")\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forwardTuple(self,\n",
    "                     x: Tuple[Tuple[torch.Tensor]],\n",
    "                    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Defines the forward pass of a decoder layer during training.\n",
    "\n",
    "        Parameters:\n",
    "            x (Tuple[Tuple[Tensor]]): \n",
    "                The input tuple of tuples of tensors \n",
    "                first tuple is of length config.levels and second layer of tuples have lengths of config.model_count\n",
    "                tensors are shape (batch size, sequence length, hidden dimension) where hidden dimension changes by which model was used\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Tuple[Tensor]]: \n",
    "                The output tuple of tuples of tensors after applying the decoder layer\n",
    "        \"\"\"\n",
    "        global verbose\n",
    "        if verbose: \n",
    "            print(\"------------- Layer.forwardTuple() ------------\")\n",
    "            print(f\"x:\\n{x}\")\n",
    "            \n",
    "        # forwardTuple() should only be used during training, so we assert input_len == max_position_embeddings\n",
    "        input_len = x[0][0].shape[1]\n",
    "        if verbose: print(f\"input_len: {input_len}\")\n",
    "        assert input_len == config.max_position_embeddings\n",
    "\n",
    "        # we could define these from the config but this way the method is more flexible to testing\n",
    "        num_levels = len(x)\n",
    "        models_per_level = [len(x[i]) for i in range(num_levels)]\n",
    "        if verbose: \n",
    "            print(f\"num_levels: {num_levels}\")\n",
    "            print(f\"models_per_level: {models_per_level}\")\n",
    "\n",
    "        # the loop that iterates over levels, aka the different potential sizes of models\n",
    "        out = ()\n",
    "        for i in range(num_levels):\n",
    "            if verbose: print(f\"Level {i} from range({num_levels})\")\n",
    "\n",
    "            # now for the loop that iterates over models in this level\n",
    "            out_lvl = ()\n",
    "            for j in range(models_per_level[i]):\n",
    "                if verbose: print(f\"Model {j} from range({models_per_level[i]})\")\n",
    "\n",
    "                output = self.forwardTensor(x[i][j], model = j, drop_bool = True)\n",
    "                if verbose: print(f\"forwardTensor() output: {output.shape}\\n{output}\")\n",
    "                \n",
    "                out_lvl += (output,)\n",
    "            \n",
    "            out += (out_lvl,)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"final output: {out}\")\n",
    "            print(\"------------- END Layer.forwardTuple() ------------\")\n",
    "\n",
    "        return out\n",
    "        \n",
    "    def forward(self, x, model=0):\n",
    "        train = True if type(x) == tuple else False\n",
    "        print(f\"---------- Layer Input: {'Tuple' if train else 'torch.Tensor'} ------------\")\n",
    "        return self.forwardTuple(x) if train else self.forwardTensor(x, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e674a900-16d8-451f-8425-5ec924de603a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Micro Hyperparameters -------\n",
      "model_count:  [1, 2, 4]\n",
      "model_dim_list:  [8, 4, 2]\n",
      "head_dim_list:  [4, 2, 1]\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the big model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 3, 8])\n",
      "tensor([[[0.9469, 0.5231, 0.5780, 0.0538, 0.4399, 0.5627, 0.7584, 0.9368],\n",
      "         [0.0660, 0.3250, 0.0122, 0.8770, 0.5177, 0.9000, 0.7999, 0.7379],\n",
      "         [0.6697, 0.2385, 0.2210, 0.2782, 0.0758, 0.9189, 0.5016, 0.0058]]])\n",
      "---------- Layer Input: torch.Tensor ------------\n",
      "----------------- Layer.forwardTensor() --------------------\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 3, 8])\n",
      "tensor([[[0.9469, 0.5231, 0.5780, 0.0538, 0.4399, 0.5627, 0.7584, 0.9368],\n",
      "         [0.0660, 0.3250, 0.0122, 0.8770, 0.5177, 0.9000, 0.7999, 0.7379],\n",
      "         [0.6697, 0.2385, 0.2210, 0.2782, 0.0758, 0.9189, 0.5016, 0.0058]]])\n",
      "normed x: torch.Size([1, 3, 8])\n",
      "tensor([[[1.4377, 0.7942, 0.8776, 0.0817, 0.6679, 0.8545, 1.1516, 1.4223],\n",
      "         [0.1054, 0.5187, 0.0194, 1.3995, 0.8262, 1.4362, 1.2765, 1.1775],\n",
      "         [1.4388, 0.5124, 0.4747, 0.5977, 0.1628, 1.9741, 1.0775, 0.0125]]])\n",
      "dim: 8\n",
      "skip: 0\n",
      "spliced scale: torch.Size([8])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 3, 8])\n",
      "tensor([[[1.4377, 0.7942, 0.8776, 0.0817, 0.6679, 0.8545, 1.1516, 1.4223],\n",
      "         [0.1054, 0.5187, 0.0194, 1.3995, 0.8262, 1.4362, 1.2765, 1.1775],\n",
      "         [1.4388, 0.5124, 0.4747, 0.5977, 0.1628, 1.9741, 1.0775, 0.0125]]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "---------- Attention Input: torch.Tensor ------------\n",
      "----------------- MultiQueryAttention.forwardTensor() --------------------\n",
      "x shape: torch.Size([1, 3, 8])\n",
      "d_skip: 0\n",
      "models_in_this_level: 1\n",
      "h_dim: 4\n",
      "h_skip: 0\n",
      "self.Wqkv: torch.Size([8, 16])\n",
      "Parameter containing:\n",
      "tensor([[ 0.2506, -0.3218,  0.0847,  0.1760, -0.1203, -0.1277, -0.1649, -0.1982,\n",
      "          0.2161,  0.1575,  0.2699,  0.1370, -0.2828, -0.0345,  0.0005,  0.0346],\n",
      "        [ 0.1161, -0.2346,  0.3384,  0.2911,  0.1269, -0.1843, -0.1594,  0.0134,\n",
      "         -0.3355, -0.2112, -0.2617,  0.3253, -0.3384, -0.1982,  0.1501, -0.3033],\n",
      "        [-0.1836, -0.3106, -0.0165, -0.3347,  0.0288,  0.2988, -0.0249,  0.1139,\n",
      "         -0.3368, -0.0683, -0.1894,  0.2543, -0.3040, -0.3160,  0.3404, -0.3073],\n",
      "        [-0.0768, -0.2449,  0.2694, -0.2957,  0.0792,  0.1079,  0.2093, -0.0195,\n",
      "         -0.3336, -0.1938,  0.0516, -0.1549,  0.0271,  0.1356, -0.1417,  0.0621],\n",
      "        [ 0.0127,  0.0450, -0.2105, -0.2715, -0.0647,  0.1514,  0.1027,  0.0403,\n",
      "          0.0690, -0.1224,  0.2849, -0.0489, -0.1749,  0.0950,  0.0925, -0.1879],\n",
      "        [-0.0546, -0.0804, -0.2648,  0.2992, -0.0196, -0.0087,  0.1883, -0.1467,\n",
      "         -0.1578,  0.2429,  0.2345, -0.0039,  0.2549, -0.0312, -0.1271, -0.2708],\n",
      "        [-0.2672, -0.3015, -0.0778, -0.0319,  0.2236, -0.1538,  0.0316, -0.1843,\n",
      "         -0.1414,  0.2363,  0.1218,  0.1559,  0.1376, -0.1315, -0.0518,  0.1587],\n",
      "        [ 0.3217,  0.2113, -0.2521,  0.2808,  0.0308, -0.2306,  0.0592,  0.1424,\n",
      "          0.3055,  0.2543, -0.1810, -0.3159,  0.3375, -0.3317,  0.3094,  0.0641]],\n",
      "       requires_grad=True)\n",
      "Wq: torch.Size([8, 8])\n",
      "tensor([[ 0.2506, -0.3218,  0.0847,  0.1760, -0.1203, -0.1277, -0.1649, -0.1982],\n",
      "        [ 0.1161, -0.2346,  0.3384,  0.2911,  0.1269, -0.1843, -0.1594,  0.0134],\n",
      "        [-0.1836, -0.3106, -0.0165, -0.3347,  0.0288,  0.2988, -0.0249,  0.1139],\n",
      "        [-0.0768, -0.2449,  0.2694, -0.2957,  0.0792,  0.1079,  0.2093, -0.0195],\n",
      "        [ 0.0127,  0.0450, -0.2105, -0.2715, -0.0647,  0.1514,  0.1027,  0.0403],\n",
      "        [-0.0546, -0.0804, -0.2648,  0.2992, -0.0196, -0.0087,  0.1883, -0.1467],\n",
      "        [-0.2672, -0.3015, -0.0778, -0.0319,  0.2236, -0.1538,  0.0316, -0.1843],\n",
      "        [ 0.3217,  0.2113, -0.2521,  0.2808,  0.0308, -0.2306,  0.0592,  0.1424]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wk: torch.Size([8, 4])\n",
      "tensor([[ 0.2161,  0.1575,  0.2699,  0.1370],\n",
      "        [-0.3355, -0.2112, -0.2617,  0.3253],\n",
      "        [-0.3368, -0.0683, -0.1894,  0.2543],\n",
      "        [-0.3336, -0.1938,  0.0516, -0.1549],\n",
      "        [ 0.0690, -0.1224,  0.2849, -0.0489],\n",
      "        [-0.1578,  0.2429,  0.2345, -0.0039],\n",
      "        [-0.1414,  0.2363,  0.1218,  0.1559],\n",
      "        [ 0.3055,  0.2543, -0.1810, -0.3159]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wv: torch.Size([8, 4])\n",
      "tensor([[-0.2828, -0.0345,  0.0005,  0.0346],\n",
      "        [-0.3384, -0.1982,  0.1501, -0.3033],\n",
      "        [-0.3040, -0.3160,  0.3404, -0.3073],\n",
      "        [ 0.0271,  0.1356, -0.1417,  0.0621],\n",
      "        [-0.1749,  0.0950,  0.0925, -0.1879],\n",
      "        [ 0.2549, -0.0312, -0.1271, -0.2708],\n",
      "        [ 0.1376, -0.1315, -0.0518,  0.1587],\n",
      "        [ 0.3375, -0.3317,  0.3094,  0.0641]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wq spliced: torch.Size([8, 8])\n",
      "tensor([[ 0.2506, -0.3218,  0.0847,  0.1760, -0.1203, -0.1277, -0.1649, -0.1982],\n",
      "        [ 0.1161, -0.2346,  0.3384,  0.2911,  0.1269, -0.1843, -0.1594,  0.0134],\n",
      "        [-0.1836, -0.3106, -0.0165, -0.3347,  0.0288,  0.2988, -0.0249,  0.1139],\n",
      "        [-0.0768, -0.2449,  0.2694, -0.2957,  0.0792,  0.1079,  0.2093, -0.0195],\n",
      "        [ 0.0127,  0.0450, -0.2105, -0.2715, -0.0647,  0.1514,  0.1027,  0.0403],\n",
      "        [-0.0546, -0.0804, -0.2648,  0.2992, -0.0196, -0.0087,  0.1883, -0.1467],\n",
      "        [-0.2672, -0.3015, -0.0778, -0.0319,  0.2236, -0.1538,  0.0316, -0.1843],\n",
      "        [ 0.3217,  0.2113, -0.2521,  0.2808,  0.0308, -0.2306,  0.0592,  0.1424]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Wk spliced: torch.Size([8, 4])\n",
      "tensor([[ 0.2161,  0.1575,  0.2699,  0.1370],\n",
      "        [-0.3355, -0.2112, -0.2617,  0.3253],\n",
      "        [-0.3368, -0.0683, -0.1894,  0.2543],\n",
      "        [-0.3336, -0.1938,  0.0516, -0.1549],\n",
      "        [ 0.0690, -0.1224,  0.2849, -0.0489],\n",
      "        [-0.1578,  0.2429,  0.2345, -0.0039],\n",
      "        [-0.1414,  0.2363,  0.1218,  0.1559],\n",
      "        [ 0.3055,  0.2543, -0.1810, -0.3159]], grad_fn=<CatBackward0>)\n",
      "Wv spliced: torch.Size([8, 4])\n",
      "tensor([[-0.2828, -0.0345,  0.0005,  0.0346],\n",
      "        [-0.3384, -0.1982,  0.1501, -0.3033],\n",
      "        [-0.3040, -0.3160,  0.3404, -0.3073],\n",
      "        [ 0.0271,  0.1356, -0.1417,  0.0621],\n",
      "        [-0.1749,  0.0950,  0.0925, -0.1879],\n",
      "        [ 0.2549, -0.0312, -0.1271, -0.2708],\n",
      "        [ 0.1376, -0.1315, -0.0518,  0.1587],\n",
      "        [ 0.3375, -0.3317,  0.3094,  0.0641]], grad_fn=<CatBackward0>)\n",
      "Wqkv_spliced: torch.Size([8, 16])\n",
      "tensor([[ 0.2506, -0.3218,  0.0847,  0.1760, -0.1203, -0.1277, -0.1649, -0.1982,\n",
      "          0.2161,  0.1575,  0.2699,  0.1370, -0.2828, -0.0345,  0.0005,  0.0346],\n",
      "        [ 0.1161, -0.2346,  0.3384,  0.2911,  0.1269, -0.1843, -0.1594,  0.0134,\n",
      "         -0.3355, -0.2112, -0.2617,  0.3253, -0.3384, -0.1982,  0.1501, -0.3033],\n",
      "        [-0.1836, -0.3106, -0.0165, -0.3347,  0.0288,  0.2988, -0.0249,  0.1139,\n",
      "         -0.3368, -0.0683, -0.1894,  0.2543, -0.3040, -0.3160,  0.3404, -0.3073],\n",
      "        [-0.0768, -0.2449,  0.2694, -0.2957,  0.0792,  0.1079,  0.2093, -0.0195,\n",
      "         -0.3336, -0.1938,  0.0516, -0.1549,  0.0271,  0.1356, -0.1417,  0.0621],\n",
      "        [ 0.0127,  0.0450, -0.2105, -0.2715, -0.0647,  0.1514,  0.1027,  0.0403,\n",
      "          0.0690, -0.1224,  0.2849, -0.0489, -0.1749,  0.0950,  0.0925, -0.1879],\n",
      "        [-0.0546, -0.0804, -0.2648,  0.2992, -0.0196, -0.0087,  0.1883, -0.1467,\n",
      "         -0.1578,  0.2429,  0.2345, -0.0039,  0.2549, -0.0312, -0.1271, -0.2708],\n",
      "        [-0.2672, -0.3015, -0.0778, -0.0319,  0.2236, -0.1538,  0.0316, -0.1843,\n",
      "         -0.1414,  0.2363,  0.1218,  0.1559,  0.1376, -0.1315, -0.0518,  0.1587],\n",
      "        [ 0.3217,  0.2113, -0.2521,  0.2808,  0.0308, -0.2306,  0.0592,  0.1424,\n",
      "          0.3055,  0.2543, -0.1810, -0.3159,  0.3375, -0.3317,  0.3094,  0.0641]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "xqkv: torch.Size([1, 3, 16])\n",
      "tensor([[[ 0.3969, -1.0270, -0.4170,  0.6034,  0.2009, -0.4704, -0.0184,\n",
      "          -0.2841, -0.0956,  0.7425,  0.2918,  0.3602, -0.2006, -1.0597,\n",
      "           0.7406, -0.5386],\n",
      "         [-0.0545, -0.7188, -0.3892,  0.2445,  0.4046, -0.3074,  0.6577,\n",
      "          -0.2840, -0.6151,  0.4833,  0.4757, -0.2476,  0.6211, -0.4475,\n",
      "           0.0782, -0.3388],\n",
      "         [-0.1025, -1.3507, -0.1955,  0.5824,  0.1450, -0.2330,  0.2176,\n",
      "          -0.7157, -0.6691,  0.6876,  0.8336,  0.5404, -0.0812, -0.4121,\n",
      "          -0.1334, -0.6077]]], grad_fn=<UnsafeViewBackward0>)\n",
      "xq: torch.Size([1, 3, 8])\n",
      "tensor([[[ 0.3969, -1.0270, -0.4170,  0.6034,  0.2009, -0.4704, -0.0184,\n",
      "          -0.2841],\n",
      "         [-0.0545, -0.7188, -0.3892,  0.2445,  0.4046, -0.3074,  0.6577,\n",
      "          -0.2840],\n",
      "         [-0.1025, -1.3507, -0.1955,  0.5824,  0.1450, -0.2330,  0.2176,\n",
      "          -0.7157]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xk: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.0956,  0.7425,  0.2918,  0.3602],\n",
      "         [-0.6151,  0.4833,  0.4757, -0.2476],\n",
      "         [-0.6691,  0.6876,  0.8336,  0.5404]]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "xv: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.2006, -1.0597,  0.7406, -0.5386],\n",
      "         [ 0.6211, -0.4475,  0.0782, -0.3388],\n",
      "         [-0.0812, -0.4121, -0.1334, -0.6077]]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "xq reshaped: torch.Size([1, 3, 2, 4])\n",
      "tensor([[[[ 0.3969, -1.0270, -0.4170,  0.6034],\n",
      "          [ 0.2009, -0.4704, -0.0184, -0.2841]],\n",
      "\n",
      "         [[-0.0545, -0.7188, -0.3892,  0.2445],\n",
      "          [ 0.4046, -0.3074,  0.6577, -0.2840]],\n",
      "\n",
      "         [[-0.1025, -1.3507, -0.1955,  0.5824],\n",
      "          [ 0.1450, -0.2330,  0.2176, -0.7157]]]], grad_fn=<ViewBackward0>)\n",
      "xk reshaped: torch.Size([1, 3, 1, 4])\n",
      "tensor([[[[-0.0956,  0.7425,  0.2918,  0.3602]],\n",
      "\n",
      "         [[-0.6151,  0.4833,  0.4757, -0.2476]],\n",
      "\n",
      "         [[-0.6691,  0.6876,  0.8336,  0.5404]]]], grad_fn=<ViewBackward0>)\n",
      "xv reshaped: torch.Size([1, 3, 1, 4])\n",
      "tensor([[[[-0.2006, -1.0597,  0.7406, -0.5386]],\n",
      "\n",
      "         [[ 0.6211, -0.4475,  0.0782, -0.3388]],\n",
      "\n",
      "         [[-0.0812, -0.4121, -0.1334, -0.6077]]]], grad_fn=<ViewBackward0>)\n",
      "rotated xq: torch.Size([1, 3, 2, 4])\n",
      "tensor([[[[ 0.3969, -1.0270, -0.4170,  0.6034],\n",
      "          [ 0.2009, -0.4704, -0.0184, -0.2841]],\n",
      "\n",
      "         [[ 0.2980, -0.7397, -0.2561,  0.1716],\n",
      "          [-0.3349, -0.2776,  0.6958, -0.3133]],\n",
      "\n",
      "         [[ 0.2204, -1.4394, -0.0119,  0.3024],\n",
      "          [-0.2582, -0.0861,  0.0413, -0.7478]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "rotated xk: torch.Size([1, 3, 1, 4])\n",
      "tensor([[[[-0.0956,  0.7425,  0.2918,  0.3602]],\n",
      "\n",
      "         [[-0.7327,  0.5056, -0.2605, -0.1981]],\n",
      "\n",
      "         [[-0.4795,  0.5665, -0.9553,  0.6662]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "repeat_interleaved xk: torch.Size([1, 3, 2, 4])\n",
      "tensor([[[[-0.0956,  0.7425,  0.2918,  0.3602],\n",
      "          [-0.0956,  0.7425,  0.2918,  0.3602]],\n",
      "\n",
      "         [[-0.7327,  0.5056, -0.2605, -0.1981],\n",
      "          [-0.7327,  0.5056, -0.2605, -0.1981]],\n",
      "\n",
      "         [[-0.4795,  0.5665, -0.9553,  0.6662],\n",
      "          [-0.4795,  0.5665, -0.9553,  0.6662]]]], grad_fn=<ViewBackward0>)\n",
      "repeat_interleaved xv: torch.Size([1, 3, 2, 4])\n",
      "tensor([[[[-0.2006, -1.0597,  0.7406, -0.5386],\n",
      "          [-0.2006, -1.0597,  0.7406, -0.5386]],\n",
      "\n",
      "         [[ 0.6211, -0.4475,  0.0782, -0.3388],\n",
      "          [ 0.6211, -0.4475,  0.0782, -0.3388]],\n",
      "\n",
      "         [[-0.0812, -0.4121, -0.1334, -0.6077],\n",
      "          [-0.0812, -0.4121, -0.1334, -0.6077]]]], grad_fn=<ViewBackward0>)\n",
      "transposed xq: torch.Size([1, 2, 3, 4])\n",
      "tensor([[[[ 0.3969, -1.0270, -0.4170,  0.6034],\n",
      "          [ 0.2980, -0.7397, -0.2561,  0.1716],\n",
      "          [ 0.2204, -1.4394, -0.0119,  0.3024]],\n",
      "\n",
      "         [[ 0.2009, -0.4704, -0.0184, -0.2841],\n",
      "          [-0.3349, -0.2776,  0.6958, -0.3133],\n",
      "          [-0.2582, -0.0861,  0.0413, -0.7478]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "transposed xk: torch.Size([1, 2, 3, 4])\n",
      "tensor([[[[-0.0956,  0.7425,  0.2918,  0.3602],\n",
      "          [-0.7327,  0.5056, -0.2605, -0.1981],\n",
      "          [-0.4795,  0.5665, -0.9553,  0.6662]],\n",
      "\n",
      "         [[-0.0956,  0.7425,  0.2918,  0.3602],\n",
      "          [-0.7327,  0.5056, -0.2605, -0.1981],\n",
      "          [-0.4795,  0.5665, -0.9553,  0.6662]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "transposed xv: torch.Size([1, 2, 3, 4])\n",
      "tensor([[[[-0.2006, -1.0597,  0.7406, -0.5386],\n",
      "          [ 0.6211, -0.4475,  0.0782, -0.3388],\n",
      "          [-0.0812, -0.4121, -0.1334, -0.6077]],\n",
      "\n",
      "         [[-0.2006, -1.0597,  0.7406, -0.5386],\n",
      "          [ 0.6211, -0.4475,  0.0782, -0.3388],\n",
      "          [-0.0812, -0.4121, -0.1334, -0.6077]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[-0.3524, -0.4104,  0.0141],\n",
      "          [-0.2953, -0.2798, -0.1015],\n",
      "          [-0.4922, -0.4730, -0.3542]],\n",
      "\n",
      "         [[-0.2381, -0.1620, -0.2673],\n",
      "          [-0.0419, -0.0071, -0.4350],\n",
      "          [-0.1483,  0.1415, -0.2313]]]], grad_fn=<MulBackward0>)\n",
      "mask: torch.Size([1, 1, 3, 3])\n",
      "tensor([[[[ 0.0000e+00, -2.3820e+38, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]]]])\n",
      "masked scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[-3.5242e-01, -2.3820e+38, -2.3820e+38],\n",
      "          [-2.9533e-01, -2.7978e-01, -2.3820e+38],\n",
      "          [-4.9223e-01, -4.7303e-01, -3.5415e-01]],\n",
      "\n",
      "         [[-2.3809e-01, -2.3820e+38, -2.3820e+38],\n",
      "          [-4.1945e-02, -7.1062e-03, -2.3820e+38],\n",
      "          [-1.4827e-01,  1.4150e-01, -2.3129e-01]]]], grad_fn=<AddBackward0>)\n",
      "softmaxed scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [0.4961, 0.5039, 0.0000],\n",
      "          [0.3157, 0.3218, 0.3625]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.4913, 0.5087, 0.0000],\n",
      "          [0.3071, 0.4103, 0.2826]]]], grad_fn=<SoftmaxBackward0>)\n",
      "attention: torch.Size([1, 2, 3, 4])\n",
      "tensor([[[[-0.2006, -1.0597,  0.7406, -0.5386],\n",
      "          [ 0.2135, -0.7512,  0.4069, -0.4379],\n",
      "          [ 0.1071, -0.6280,  0.2107, -0.4993]],\n",
      "\n",
      "         [[-0.2006, -1.0597,  0.7406, -0.5386],\n",
      "          [ 0.2174, -0.7483,  0.4037, -0.4369],\n",
      "          [ 0.1703, -0.6255,  0.2218, -0.4761]]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "reshaped attention: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.2006, -1.0597,  0.7406, -0.5386, -0.2006, -1.0597,  0.7406,\n",
      "          -0.5386],\n",
      "         [ 0.2135, -0.7512,  0.4069, -0.4379,  0.2174, -0.7483,  0.4037,\n",
      "          -0.4369],\n",
      "         [ 0.1071, -0.6280,  0.2107, -0.4993,  0.1703, -0.6255,  0.2218,\n",
      "          -0.4761]]], grad_fn=<ViewBackward0>)\n",
      "self.Wo: torch.Size([8, 8])\n",
      "Parameter containing:\n",
      "tensor([[-0.0181,  0.3315, -0.0259, -0.1195, -0.1019,  0.3319,  0.0060,  0.2975],\n",
      "        [-0.2934,  0.0896,  0.0890,  0.0484, -0.2544, -0.1270, -0.1124, -0.1091],\n",
      "        [ 0.2495, -0.1875, -0.1658,  0.0032, -0.0663,  0.3415,  0.0190,  0.2628],\n",
      "        [-0.3253,  0.2796, -0.0295, -0.2092,  0.3265, -0.2142,  0.3383,  0.1221],\n",
      "        [ 0.2673, -0.1370, -0.2550,  0.2432, -0.0765, -0.0796,  0.2102, -0.1013],\n",
      "        [-0.0639,  0.3508,  0.1304, -0.2011, -0.2521, -0.3133, -0.1848, -0.2130],\n",
      "        [ 0.1735, -0.2611, -0.0794, -0.1795, -0.1528, -0.3439,  0.0483, -0.3415],\n",
      "        [-0.3437, -0.3501,  0.0105, -0.0054, -0.1849, -0.0125, -0.0600, -0.2345]],\n",
      "       requires_grad=True)\n",
      "spliced Wo: torch.Size([8, 8])\n",
      "tensor([[-0.0181,  0.3315, -0.0259, -0.1195, -0.1019,  0.3319,  0.0060,  0.2975],\n",
      "        [-0.2934,  0.0896,  0.0890,  0.0484, -0.2544, -0.1270, -0.1124, -0.1091],\n",
      "        [ 0.2495, -0.1875, -0.1658,  0.0032, -0.0663,  0.3415,  0.0190,  0.2628],\n",
      "        [-0.3253,  0.2796, -0.0295, -0.2092,  0.3265, -0.2142,  0.3383,  0.1221],\n",
      "        [ 0.2673, -0.1370, -0.2550,  0.2432, -0.0765, -0.0796,  0.2102, -0.1013],\n",
      "        [-0.0639,  0.3508,  0.1304, -0.2011, -0.2521, -0.3133, -0.1848, -0.2130],\n",
      "        [ 0.1735, -0.2611, -0.0794, -0.1795, -0.1528, -0.3439,  0.0483, -0.3415],\n",
      "        [-0.3437, -0.3501,  0.0105, -0.0054, -0.1849, -0.0125, -0.0600, -0.2345]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "projected output: torch.Size([1, 3, 8])\n",
      "tensor([[[ 1.0022, -0.8000, -0.3475,  0.1221,  0.3340,  0.5362,  0.1715,\n",
      "           0.3042],\n",
      "         [ 0.7867, -0.4400, -0.3165,  0.1643,  0.1906,  0.4827,  0.1750,\n",
      "           0.3009],\n",
      "         [ 0.6850, -0.3338, -0.2264,  0.1919,  0.1707,  0.4062,  0.0969,\n",
      "           0.2466]]], grad_fn=<UnsafeViewBackward0>)\n",
      "----------------- END MultiQueryAttention.forwardTensor() --------------------\n",
      "x in layer after MQA & resid connection and before MLP:\n",
      "tensor([[[ 1.9491, -0.2770,  0.2305,  0.1759,  0.7739,  1.0990,  0.9300,\n",
      "           1.2410],\n",
      "         [ 0.8527, -0.1149, -0.3044,  1.0413,  0.7083,  1.3827,  0.9749,\n",
      "           1.0387],\n",
      "         [ 1.3547, -0.0953, -0.0055,  0.4701,  0.2465,  1.3252,  0.5985,\n",
      "           0.2525]]], grad_fn=<AddBackward0>)\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 3, 8])\n",
      "tensor([[[ 1.9491, -0.2770,  0.2305,  0.1759,  0.7739,  1.0990,  0.9300,\n",
      "           1.2410],\n",
      "         [ 0.8527, -0.1149, -0.3044,  1.0413,  0.7083,  1.3827,  0.9749,\n",
      "           1.0387],\n",
      "         [ 1.3547, -0.0953, -0.0055,  0.4701,  0.2465,  1.3252,  0.5985,\n",
      "           0.2525]]], grad_fn=<AddBackward0>)\n",
      "normed x: torch.Size([1, 3, 8])\n",
      "tensor([[[ 1.9286, -0.2740,  0.2281,  0.1740,  0.7658,  1.0874,  0.9201,\n",
      "           1.2279],\n",
      "         [ 0.9563, -0.1289, -0.3414,  1.1679,  0.7944,  1.5507,  1.0933,\n",
      "           1.1650],\n",
      "         [ 1.8469, -0.1300, -0.0075,  0.6410,  0.3360,  1.8067,  0.8160,\n",
      "           0.3442]]], grad_fn=<MulBackward0>)\n",
      "dim: 8\n",
      "skip: 0\n",
      "spliced scale: torch.Size([8])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 3, 8])\n",
      "tensor([[[ 1.9286, -0.2740,  0.2281,  0.1740,  0.7658,  1.0874,  0.9201,\n",
      "           1.2279],\n",
      "         [ 0.9563, -0.1289, -0.3414,  1.1679,  0.7944,  1.5507,  1.0933,\n",
      "           1.1650],\n",
      "         [ 1.8469, -0.1300, -0.0075,  0.6410,  0.3360,  1.8067,  0.8160,\n",
      "           0.3442]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "---------- MLP Input: torch.Tensor ------------\n",
      "------------- MLP.forwardTensor() ------------\n",
      "x: torch.Size([1, 3, 8])\n",
      "tensor([[[ 1.9286, -0.2740,  0.2281,  0.1740,  0.7658,  1.0874,  0.9201,\n",
      "           1.2279],\n",
      "         [ 0.9563, -0.1289, -0.3414,  1.1679,  0.7944,  1.5507,  1.0933,\n",
      "           1.1650],\n",
      "         [ 1.8469, -0.1300, -0.0075,  0.6410,  0.3360,  1.8067,  0.8160,\n",
      "           0.3442]]], grad_fn=<MulBackward0>)\n",
      "d_dim: 8\n",
      "d_skip: 0\n",
      "i_dim: 32\n",
      "i_skip: 0\n",
      "Wgate: torch.Size([8, 32])\n",
      "Parameter containing:\n",
      "tensor([[-0.1568, -0.0997,  0.2145,  0.0983,  0.0561, -0.0422,  0.1296, -0.1722,\n",
      "          0.3355, -0.0140, -0.2869,  0.2809,  0.0825, -0.2697,  0.0664, -0.0920,\n",
      "          0.1181,  0.2408, -0.2379, -0.2080, -0.0600, -0.2836,  0.2345,  0.2550,\n",
      "         -0.2740, -0.2796,  0.0943, -0.2293,  0.0868, -0.0740,  0.1208, -0.0045],\n",
      "        [-0.2910, -0.1849,  0.1058,  0.0526,  0.2703, -0.3522, -0.1214, -0.1087,\n",
      "         -0.0153, -0.1631, -0.3014, -0.1277, -0.0474, -0.1564, -0.0928,  0.2137,\n",
      "          0.1734,  0.2786, -0.1809, -0.2815,  0.0231, -0.1504,  0.1834,  0.3425,\n",
      "          0.1067, -0.2587,  0.3439, -0.2290,  0.1644,  0.2223,  0.1421,  0.0897],\n",
      "        [-0.0845,  0.1224,  0.2939, -0.2583, -0.0401, -0.3513,  0.0088, -0.1701,\n",
      "         -0.0771, -0.0806,  0.2124,  0.0258, -0.0489, -0.2683, -0.2108,  0.2557,\n",
      "         -0.1817, -0.0968, -0.1035, -0.1182, -0.3319,  0.0957, -0.3299,  0.0848,\n",
      "          0.0752, -0.2189,  0.2235,  0.3366, -0.0995, -0.1087, -0.3191, -0.3112],\n",
      "        [ 0.2135, -0.1641, -0.1915,  0.1834, -0.1006, -0.0966,  0.1365,  0.1516,\n",
      "         -0.0683,  0.0151,  0.0231, -0.0955,  0.1208,  0.1826,  0.0794,  0.0208,\n",
      "         -0.2589,  0.1435, -0.1228,  0.1986,  0.1540,  0.2119, -0.2749,  0.1783,\n",
      "          0.1707,  0.2715, -0.0063, -0.0942,  0.2120,  0.3513, -0.2320,  0.0412],\n",
      "        [-0.2422, -0.1883, -0.0382,  0.0008,  0.0380, -0.0737, -0.0454, -0.1889,\n",
      "         -0.1223,  0.0781, -0.0026,  0.1876, -0.0574,  0.1543, -0.2059,  0.1645,\n",
      "          0.0910,  0.3149,  0.0010, -0.0283,  0.2601,  0.2542,  0.2960,  0.2364,\n",
      "          0.0366,  0.0690,  0.1472, -0.2704,  0.2199, -0.2335,  0.0329,  0.0251],\n",
      "        [ 0.3382, -0.2146,  0.2616, -0.2641,  0.1855, -0.2187,  0.0004, -0.1210,\n",
      "          0.3281,  0.3498,  0.0643,  0.2195,  0.2569, -0.2979,  0.2269, -0.1765,\n",
      "         -0.2048, -0.1683, -0.3347,  0.2291,  0.2664, -0.1201, -0.1682,  0.2995,\n",
      "         -0.0999, -0.2861,  0.2582,  0.2150, -0.2029, -0.2447,  0.0415,  0.0792],\n",
      "        [-0.0591,  0.0159,  0.0312, -0.1465,  0.2822,  0.2216, -0.1564, -0.0591,\n",
      "         -0.1251, -0.1119, -0.3236, -0.0847, -0.2090,  0.2230,  0.2164,  0.1109,\n",
      "         -0.0072,  0.2253,  0.2254,  0.0606, -0.1901, -0.1115, -0.1594, -0.2750,\n",
      "         -0.1838, -0.0555,  0.0271, -0.0843,  0.2414,  0.0251, -0.1242,  0.2849],\n",
      "        [ 0.2844, -0.2060, -0.0588, -0.2270,  0.3348,  0.1182, -0.2463, -0.2932,\n",
      "          0.1420,  0.0582,  0.0325,  0.0340,  0.0740,  0.1341, -0.0483, -0.1128,\n",
      "          0.2430, -0.3189, -0.2697,  0.3369, -0.0458,  0.1207, -0.1419,  0.0079,\n",
      "         -0.2319, -0.3231, -0.2048,  0.3383, -0.2367, -0.2566,  0.1645, -0.2375]],\n",
      "       requires_grad=True)\n",
      "Wgate spliced: torch.Size([8, 32])\n",
      "tensor([[-0.1568, -0.0997,  0.2145,  0.0983,  0.0561, -0.0422,  0.1296, -0.1722,\n",
      "          0.3355, -0.0140, -0.2869,  0.2809,  0.0825, -0.2697,  0.0664, -0.0920,\n",
      "          0.1181,  0.2408, -0.2379, -0.2080, -0.0600, -0.2836,  0.2345,  0.2550,\n",
      "         -0.2740, -0.2796,  0.0943, -0.2293,  0.0868, -0.0740,  0.1208, -0.0045],\n",
      "        [-0.2910, -0.1849,  0.1058,  0.0526,  0.2703, -0.3522, -0.1214, -0.1087,\n",
      "         -0.0153, -0.1631, -0.3014, -0.1277, -0.0474, -0.1564, -0.0928,  0.2137,\n",
      "          0.1734,  0.2786, -0.1809, -0.2815,  0.0231, -0.1504,  0.1834,  0.3425,\n",
      "          0.1067, -0.2587,  0.3439, -0.2290,  0.1644,  0.2223,  0.1421,  0.0897],\n",
      "        [-0.0845,  0.1224,  0.2939, -0.2583, -0.0401, -0.3513,  0.0088, -0.1701,\n",
      "         -0.0771, -0.0806,  0.2124,  0.0258, -0.0489, -0.2683, -0.2108,  0.2557,\n",
      "         -0.1817, -0.0968, -0.1035, -0.1182, -0.3319,  0.0957, -0.3299,  0.0848,\n",
      "          0.0752, -0.2189,  0.2235,  0.3366, -0.0995, -0.1087, -0.3191, -0.3112],\n",
      "        [ 0.2135, -0.1641, -0.1915,  0.1834, -0.1006, -0.0966,  0.1365,  0.1516,\n",
      "         -0.0683,  0.0151,  0.0231, -0.0955,  0.1208,  0.1826,  0.0794,  0.0208,\n",
      "         -0.2589,  0.1435, -0.1228,  0.1986,  0.1540,  0.2119, -0.2749,  0.1783,\n",
      "          0.1707,  0.2715, -0.0063, -0.0942,  0.2120,  0.3513, -0.2320,  0.0412],\n",
      "        [-0.2422, -0.1883, -0.0382,  0.0008,  0.0380, -0.0737, -0.0454, -0.1889,\n",
      "         -0.1223,  0.0781, -0.0026,  0.1876, -0.0574,  0.1543, -0.2059,  0.1645,\n",
      "          0.0910,  0.3149,  0.0010, -0.0283,  0.2601,  0.2542,  0.2960,  0.2364,\n",
      "          0.0366,  0.0690,  0.1472, -0.2704,  0.2199, -0.2335,  0.0329,  0.0251],\n",
      "        [ 0.3382, -0.2146,  0.2616, -0.2641,  0.1855, -0.2187,  0.0004, -0.1210,\n",
      "          0.3281,  0.3498,  0.0643,  0.2195,  0.2569, -0.2979,  0.2269, -0.1765,\n",
      "         -0.2048, -0.1683, -0.3347,  0.2291,  0.2664, -0.1201, -0.1682,  0.2995,\n",
      "         -0.0999, -0.2861,  0.2582,  0.2150, -0.2029, -0.2447,  0.0415,  0.0792],\n",
      "        [-0.0591,  0.0159,  0.0312, -0.1465,  0.2822,  0.2216, -0.1564, -0.0591,\n",
      "         -0.1251, -0.1119, -0.3236, -0.0847, -0.2090,  0.2230,  0.2164,  0.1109,\n",
      "         -0.0072,  0.2253,  0.2254,  0.0606, -0.1901, -0.1115, -0.1594, -0.2750,\n",
      "         -0.1838, -0.0555,  0.0271, -0.0843,  0.2414,  0.0251, -0.1242,  0.2849],\n",
      "        [ 0.2844, -0.2060, -0.0588, -0.2270,  0.3348,  0.1182, -0.2463, -0.2932,\n",
      "          0.1420,  0.0582,  0.0325,  0.0340,  0.0740,  0.1341, -0.0483, -0.1128,\n",
      "          0.2430, -0.3189, -0.2697,  0.3369, -0.0458,  0.1207, -0.1419,  0.0079,\n",
      "         -0.2319, -0.3231, -0.2048,  0.3383, -0.2367, -0.2566,  0.1645, -0.2375]],\n",
      "       grad_fn=<AliasBackward0>)\n",
      "Bgate: torch.Size([32])\n",
      "Parameter containing:\n",
      "tensor([-0.3078,  0.2578,  0.0366, -0.0220,  0.0283,  0.1648,  0.2669,  0.2917,\n",
      "         0.3312,  0.1306, -0.1449,  0.1948, -0.1634, -0.2270,  0.1039, -0.2299,\n",
      "        -0.1114,  0.3190, -0.1895, -0.2295,  0.1298,  0.0413, -0.2719, -0.0466,\n",
      "        -0.0479,  0.2354, -0.1221,  0.0687, -0.0744,  0.2795,  0.1939,  0.2310],\n",
      "       requires_grad=True)\n",
      "Bgate spliced: torch.Size([32])\n",
      "tensor([-0.3078,  0.2578,  0.0366, -0.0220,  0.0283,  0.1648,  0.2669,  0.2917,\n",
      "         0.3312,  0.1306, -0.1449,  0.1948, -0.1634, -0.2270,  0.1039, -0.2299,\n",
      "        -0.1114,  0.3190, -0.1895, -0.2295,  0.1298,  0.0413, -0.2719, -0.0466,\n",
      "        -0.0479,  0.2354, -0.1221,  0.0687, -0.0744,  0.2795,  0.1939,  0.2310],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xgate: torch.Size([1, 3, 32])\n",
      "tensor([[[-0.0354, -0.5003,  0.6667, -0.5739,  0.9372,  0.1378,  0.0952,\n",
      "          -0.7137,  1.2754,  0.5412, -0.7529,  1.1068,  0.1525, -0.5695,\n",
      "           0.4519, -0.5064,  0.1211,  0.5840, -1.1306,  0.1510,  0.2165,\n",
      "          -0.2960, -0.2700,  0.6653, -1.0932, -0.9416,  0.1824,  0.1142,\n",
      "          -0.0586, -0.6247,  0.4328,  0.2097],\n",
      "         [ 0.4567, -0.7521,  0.2451, -0.4659,  0.9596,  0.1592,  0.0695,\n",
      "          -0.3678,  1.0410,  0.7333, -0.6440,  0.7959,  0.2899, -0.0991,\n",
      "           0.7127, -0.5617, -0.2312,  0.5781, -1.0877,  0.6717,  0.7211,\n",
      "           0.0387, -0.6449,  0.6934, -0.7469, -0.4327,  0.1486,  0.0746,\n",
      "           0.1171, -0.2090,  0.2755,  0.5470],\n",
      "         [ 0.1573, -0.5173,  0.7591, -0.4023,  0.7259, -0.1250,  0.3825,\n",
      "          -0.3450,  1.4081,  0.7231, -0.7598,  1.0708,  0.3727, -0.8439,\n",
      "           0.7917, -0.6281, -0.3421,  0.6962, -1.1964,  0.1209,  0.5150,\n",
      "          -0.5087, -0.4197,  0.8925, -0.8569, -0.7218,  0.4693, -0.0426,\n",
      "           0.0240, -0.2485,  0.2936,  0.5419]]], grad_fn=<AddBackward0>)\n",
      "GeLU'ed Xgate: torch.Size([1, 3, 32])\n",
      "tensor([[[-0.0172, -0.1543,  0.4983, -0.1624,  0.7738,  0.0765,  0.0512,\n",
      "          -0.1697,  1.1465,  0.3820, -0.1700,  0.9583,  0.0855, -0.1620,\n",
      "           0.3048, -0.1551,  0.0664,  0.4207, -0.1460,  0.0845,  0.1268,\n",
      "          -0.1135, -0.1063,  0.4970, -0.1499, -0.1631,  0.1044,  0.0623,\n",
      "          -0.0279, -0.1662,  0.2889,  0.1223],\n",
      "         [ 0.3088, -0.1700,  0.1463, -0.1494,  0.7978,  0.0897,  0.0367,\n",
      "          -0.1311,  0.8859,  0.5634, -0.1673,  0.6263,  0.1780, -0.0456,\n",
      "           0.5430, -0.1613, -0.0945,  0.4153, -0.1505,  0.5031,  0.5513,\n",
      "           0.0200, -0.1674,  0.5242, -0.1700, -0.1439,  0.0831,  0.0395,\n",
      "           0.0640, -0.0872,  0.1677,  0.3872],\n",
      "         [ 0.0885, -0.1565,  0.5891, -0.1383,  0.5561, -0.0563,  0.2483,\n",
      "          -0.1260,  1.2961,  0.5533, -0.1700,  0.9186,  0.2405, -0.1682,\n",
      "           0.6220, -0.1664, -0.1252,  0.5269, -0.1385,  0.0663,  0.3588,\n",
      "          -0.1554, -0.1416,  0.7265, -0.1677, -0.1698,  0.3194, -0.0206,\n",
      "           0.0122, -0.0999,  0.1807,  0.3826]]], grad_fn=<GeluBackward0>)\n",
      "Wup: torch.Size([8, 32])\n",
      "Parameter containing:\n",
      "tensor([[ 4.2319e-02,  2.9230e-01, -6.9937e-03,  1.2115e-02,  1.0085e-01,\n",
      "         -2.6741e-01, -2.4688e-01, -9.9945e-02,  2.2165e-01, -2.0147e-01,\n",
      "         -3.0114e-01, -2.8121e-02, -2.5006e-01,  2.5319e-01,  1.8907e-02,\n",
      "          2.0975e-01, -2.6814e-01,  2.0142e-01,  3.9011e-02,  3.4945e-02,\n",
      "         -2.9845e-01, -1.3691e-01, -1.7856e-01, -2.1682e-01, -9.7724e-02,\n",
      "         -3.3241e-01,  1.3962e-01,  9.8252e-02,  1.5122e-01, -1.8207e-01,\n",
      "         -2.5992e-02, -3.3915e-01],\n",
      "        [ 2.5335e-01,  6.1975e-02,  5.1779e-02,  2.8736e-01,  1.6210e-02,\n",
      "          3.5289e-01,  2.3101e-01,  1.1281e-01,  2.5539e-01,  1.0711e-01,\n",
      "          3.3327e-01, -2.9255e-01, -5.1757e-02, -1.8153e-01,  1.9843e-01,\n",
      "         -2.8309e-01, -3.4721e-01, -2.7227e-01, -3.5095e-01,  1.5830e-01,\n",
      "         -1.3123e-01, -2.6311e-01, -2.1583e-01, -2.7002e-01, -1.9617e-02,\n",
      "          8.3525e-02, -3.0602e-01,  1.6498e-01,  3.1728e-02,  1.3955e-01,\n",
      "          6.0900e-03,  1.4788e-01],\n",
      "        [ 1.2492e-01,  6.0298e-02, -1.6424e-01,  1.8990e-01, -1.7205e-01,\n",
      "          1.7364e-02,  1.8916e-01, -3.3856e-01, -2.8186e-01,  1.3278e-01,\n",
      "         -2.1960e-01,  1.7659e-01,  1.4906e-01,  2.8139e-01,  3.6063e-03,\n",
      "         -2.8741e-01,  2.2501e-01, -1.0666e-01, -1.6393e-01, -1.7943e-01,\n",
      "         -1.9344e-01, -1.7264e-02,  4.3912e-02,  3.4453e-01, -4.6047e-02,\n",
      "         -1.0094e-01,  1.9724e-01, -1.0559e-01,  2.2161e-01, -3.2253e-01,\n",
      "         -2.7132e-02,  1.7911e-01],\n",
      "        [ 3.4604e-02, -3.8121e-02, -6.8547e-02,  3.3003e-01,  6.1634e-03,\n",
      "         -3.4121e-01,  1.9274e-02,  6.6103e-02,  2.1338e-01, -4.8570e-02,\n",
      "         -2.6739e-01, -1.1795e-01, -1.7485e-01, -8.2128e-02, -2.1308e-01,\n",
      "         -1.3419e-01,  2.5131e-01, -5.2237e-02, -3.2635e-01,  1.6295e-01,\n",
      "         -8.1626e-02,  3.3657e-01, -1.3445e-01,  2.6947e-01,  2.3134e-01,\n",
      "         -1.1423e-01, -8.2666e-02, -1.9579e-01,  2.0387e-02, -8.0611e-02,\n",
      "         -2.6728e-01,  1.2959e-01],\n",
      "        [-3.1586e-01, -2.4693e-01, -3.2028e-01,  1.0217e-01,  3.0961e-01,\n",
      "         -2.3977e-01,  4.1927e-02, -2.6788e-01, -1.0883e-01, -1.6859e-01,\n",
      "         -1.7185e-01, -2.0784e-02, -1.9797e-01,  4.5419e-02,  2.9918e-01,\n",
      "          5.0682e-02, -2.2559e-01,  3.1199e-01,  3.5245e-02,  1.2065e-01,\n",
      "         -2.8632e-01,  8.5161e-02,  3.0558e-02, -3.0763e-01, -3.0861e-01,\n",
      "          1.1032e-01,  3.0758e-01, -2.6475e-01, -3.1512e-01,  1.4728e-01,\n",
      "          9.3713e-02,  3.4958e-01],\n",
      "        [-2.7868e-01, -1.7770e-01,  2.1425e-01,  7.2404e-03,  4.1609e-02,\n",
      "          3.0731e-01,  2.5740e-01, -1.3630e-01, -3.4789e-02, -9.9674e-02,\n",
      "          2.3331e-01,  9.7677e-02,  2.0914e-01,  6.6870e-02,  3.6619e-02,\n",
      "          3.0892e-01, -1.7940e-01, -1.2774e-01,  1.0332e-01,  3.4501e-02,\n",
      "         -3.3481e-01,  2.0650e-01,  1.2092e-01,  2.7572e-01, -3.1892e-01,\n",
      "         -3.0447e-01, -6.9866e-02,  2.0671e-01, -2.9082e-01, -5.5709e-02,\n",
      "          1.7752e-01, -2.4321e-01],\n",
      "        [ 1.9338e-01,  2.6631e-01, -1.2889e-01,  1.3261e-01,  2.3632e-01,\n",
      "         -6.0735e-02,  3.2193e-01, -2.8863e-01,  3.1817e-01,  1.3311e-01,\n",
      "          1.3612e-01, -3.4726e-01,  1.2296e-01, -3.2135e-01, -2.4591e-01,\n",
      "          2.3362e-02,  6.0751e-02,  4.6615e-02,  6.2549e-02,  3.4758e-02,\n",
      "         -1.6944e-01, -3.2282e-01, -2.3992e-01, -1.2928e-01, -2.5502e-01,\n",
      "         -3.3109e-01,  2.6448e-01, -6.3623e-02,  3.0960e-01,  1.4072e-01,\n",
      "          1.0107e-01,  3.2831e-01],\n",
      "        [-2.9491e-01,  3.4788e-01,  2.8321e-02, -9.5107e-02,  1.4070e-01,\n",
      "          1.5558e-02, -3.1681e-01, -5.4112e-02, -2.4555e-02, -2.2254e-01,\n",
      "          3.4804e-01, -3.2389e-01, -2.5191e-01, -3.4127e-01, -1.7684e-01,\n",
      "          3.5062e-01,  2.2025e-01, -1.2362e-01,  2.5670e-01,  2.6722e-01,\n",
      "          2.2229e-01, -3.3786e-01, -2.4778e-04,  3.2678e-01,  3.2578e-01,\n",
      "         -2.6153e-01,  1.5975e-01,  1.0630e-01, -2.7190e-02,  1.2946e-01,\n",
      "          1.7653e-01,  2.1419e-01]], requires_grad=True)\n",
      "Wup spliced: torch.Size([8, 32])\n",
      "tensor([[ 4.2319e-02,  2.9230e-01, -6.9937e-03,  1.2115e-02,  1.0085e-01,\n",
      "         -2.6741e-01, -2.4688e-01, -9.9945e-02,  2.2165e-01, -2.0147e-01,\n",
      "         -3.0114e-01, -2.8121e-02, -2.5006e-01,  2.5319e-01,  1.8907e-02,\n",
      "          2.0975e-01, -2.6814e-01,  2.0142e-01,  3.9011e-02,  3.4945e-02,\n",
      "         -2.9845e-01, -1.3691e-01, -1.7856e-01, -2.1682e-01, -9.7724e-02,\n",
      "         -3.3241e-01,  1.3962e-01,  9.8252e-02,  1.5122e-01, -1.8207e-01,\n",
      "         -2.5992e-02, -3.3915e-01],\n",
      "        [ 2.5335e-01,  6.1975e-02,  5.1779e-02,  2.8736e-01,  1.6210e-02,\n",
      "          3.5289e-01,  2.3101e-01,  1.1281e-01,  2.5539e-01,  1.0711e-01,\n",
      "          3.3327e-01, -2.9255e-01, -5.1757e-02, -1.8153e-01,  1.9843e-01,\n",
      "         -2.8309e-01, -3.4721e-01, -2.7227e-01, -3.5095e-01,  1.5830e-01,\n",
      "         -1.3123e-01, -2.6311e-01, -2.1583e-01, -2.7002e-01, -1.9617e-02,\n",
      "          8.3525e-02, -3.0602e-01,  1.6498e-01,  3.1728e-02,  1.3955e-01,\n",
      "          6.0900e-03,  1.4788e-01],\n",
      "        [ 1.2492e-01,  6.0298e-02, -1.6424e-01,  1.8990e-01, -1.7205e-01,\n",
      "          1.7364e-02,  1.8916e-01, -3.3856e-01, -2.8186e-01,  1.3278e-01,\n",
      "         -2.1960e-01,  1.7659e-01,  1.4906e-01,  2.8139e-01,  3.6063e-03,\n",
      "         -2.8741e-01,  2.2501e-01, -1.0666e-01, -1.6393e-01, -1.7943e-01,\n",
      "         -1.9344e-01, -1.7264e-02,  4.3912e-02,  3.4453e-01, -4.6047e-02,\n",
      "         -1.0094e-01,  1.9724e-01, -1.0559e-01,  2.2161e-01, -3.2253e-01,\n",
      "         -2.7132e-02,  1.7911e-01],\n",
      "        [ 3.4604e-02, -3.8121e-02, -6.8547e-02,  3.3003e-01,  6.1634e-03,\n",
      "         -3.4121e-01,  1.9274e-02,  6.6103e-02,  2.1338e-01, -4.8570e-02,\n",
      "         -2.6739e-01, -1.1795e-01, -1.7485e-01, -8.2128e-02, -2.1308e-01,\n",
      "         -1.3419e-01,  2.5131e-01, -5.2237e-02, -3.2635e-01,  1.6295e-01,\n",
      "         -8.1626e-02,  3.3657e-01, -1.3445e-01,  2.6947e-01,  2.3134e-01,\n",
      "         -1.1423e-01, -8.2666e-02, -1.9579e-01,  2.0387e-02, -8.0611e-02,\n",
      "         -2.6728e-01,  1.2959e-01],\n",
      "        [-3.1586e-01, -2.4693e-01, -3.2028e-01,  1.0217e-01,  3.0961e-01,\n",
      "         -2.3977e-01,  4.1927e-02, -2.6788e-01, -1.0883e-01, -1.6859e-01,\n",
      "         -1.7185e-01, -2.0784e-02, -1.9797e-01,  4.5419e-02,  2.9918e-01,\n",
      "          5.0682e-02, -2.2559e-01,  3.1199e-01,  3.5245e-02,  1.2065e-01,\n",
      "         -2.8632e-01,  8.5161e-02,  3.0558e-02, -3.0763e-01, -3.0861e-01,\n",
      "          1.1032e-01,  3.0758e-01, -2.6475e-01, -3.1512e-01,  1.4728e-01,\n",
      "          9.3713e-02,  3.4958e-01],\n",
      "        [-2.7868e-01, -1.7770e-01,  2.1425e-01,  7.2404e-03,  4.1609e-02,\n",
      "          3.0731e-01,  2.5740e-01, -1.3630e-01, -3.4789e-02, -9.9674e-02,\n",
      "          2.3331e-01,  9.7677e-02,  2.0914e-01,  6.6870e-02,  3.6619e-02,\n",
      "          3.0892e-01, -1.7940e-01, -1.2774e-01,  1.0332e-01,  3.4501e-02,\n",
      "         -3.3481e-01,  2.0650e-01,  1.2092e-01,  2.7572e-01, -3.1892e-01,\n",
      "         -3.0447e-01, -6.9866e-02,  2.0671e-01, -2.9082e-01, -5.5709e-02,\n",
      "          1.7752e-01, -2.4321e-01],\n",
      "        [ 1.9338e-01,  2.6631e-01, -1.2889e-01,  1.3261e-01,  2.3632e-01,\n",
      "         -6.0735e-02,  3.2193e-01, -2.8863e-01,  3.1817e-01,  1.3311e-01,\n",
      "          1.3612e-01, -3.4726e-01,  1.2296e-01, -3.2135e-01, -2.4591e-01,\n",
      "          2.3362e-02,  6.0751e-02,  4.6615e-02,  6.2549e-02,  3.4758e-02,\n",
      "         -1.6944e-01, -3.2282e-01, -2.3992e-01, -1.2928e-01, -2.5502e-01,\n",
      "         -3.3109e-01,  2.6448e-01, -6.3623e-02,  3.0960e-01,  1.4072e-01,\n",
      "          1.0107e-01,  3.2831e-01],\n",
      "        [-2.9491e-01,  3.4788e-01,  2.8321e-02, -9.5107e-02,  1.4070e-01,\n",
      "          1.5558e-02, -3.1681e-01, -5.4112e-02, -2.4555e-02, -2.2254e-01,\n",
      "          3.4804e-01, -3.2389e-01, -2.5191e-01, -3.4127e-01, -1.7684e-01,\n",
      "          3.5062e-01,  2.2025e-01, -1.2362e-01,  2.5670e-01,  2.6722e-01,\n",
      "          2.2229e-01, -3.3786e-01, -2.4778e-04,  3.2678e-01,  3.2578e-01,\n",
      "         -2.6153e-01,  1.5975e-01,  1.0630e-01, -2.7190e-02,  1.2946e-01,\n",
      "          1.7653e-01,  2.1419e-01]], grad_fn=<AliasBackward0>)\n",
      "Bup: torch.Size([32])\n",
      "Parameter containing:\n",
      "tensor([ 0.0389,  0.1338,  0.2406, -0.3302,  0.1093,  0.1526,  0.0072,  0.2522,\n",
      "         0.1132, -0.2677, -0.0885, -0.1176, -0.1703,  0.1797,  0.2519,  0.2260,\n",
      "        -0.2642,  0.3522,  0.0295, -0.0605,  0.0700,  0.1663, -0.2668, -0.3068,\n",
      "         0.2400, -0.0181, -0.2886,  0.0393, -0.2380,  0.2117, -0.1423, -0.3333],\n",
      "       requires_grad=True)\n",
      "Bup spliced: torch.Size([32])\n",
      "tensor([ 0.0389,  0.1338,  0.2406, -0.3302,  0.1093,  0.1526,  0.0072,  0.2522,\n",
      "         0.1132, -0.2677, -0.0885, -0.1176, -0.1703,  0.1797,  0.2519,  0.2260,\n",
      "        -0.2642,  0.3522,  0.0295, -0.0605,  0.0700,  0.1663, -0.2668, -0.3068,\n",
      "         0.2400, -0.0181, -0.2886,  0.0393, -0.2380,  0.2117, -0.1423, -0.3333],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xup: torch.Size([1, 3, 32])\n",
      "tensor([[[-0.6434,  0.9775,  0.0674, -0.1935,  0.9338, -0.4015, -0.2665,\n",
      "          -0.7226,  0.5850, -1.0520, -0.1826, -0.6988, -0.7552,  0.1604,\n",
      "           0.0233,  1.4459, -0.6326,  0.7729,  0.6189,  0.4409, -0.9942,\n",
      "          -0.3932, -0.6316, -0.1790, -0.3310, -1.5974,  0.6942,  0.2195,\n",
      "          -0.2070,  0.0754,  0.3278, -0.3962],\n",
      "         [-0.7707,  0.5649,  0.1731,  0.0915,  1.0024, -0.3153,  0.1146,\n",
      "          -0.4680,  0.8165, -0.9784,  0.1228, -0.9268, -0.6498, -0.3559,\n",
      "          -0.1861,  1.3578, -0.3935,  0.5120,  0.3426,  0.7027, -0.9007,\n",
      "           0.1096, -0.6326,  0.1403, -0.2040, -1.4969,  0.3317,  0.0839,\n",
      "          -0.5438,  0.3708,  0.1951, -0.0775],\n",
      "         [-0.4479,  0.5737,  0.3623, -0.0122,  0.7192, -0.1756,  0.1649,\n",
      "          -0.4926,  0.7800, -0.8905, -0.2631, -0.4338, -0.4136,  0.3726,\n",
      "           0.0296,  1.2812, -0.8294,  0.5964,  0.2771,  0.3124, -1.2778,\n",
      "           0.1855, -0.6222, -0.1002, -0.5651, -1.5886,  0.2025,  0.3438,\n",
      "          -0.3394, -0.0838,  0.1333, -0.8775]]], grad_fn=<AddBackward0>)\n",
      "Xfuse: torch.Size([1, 3, 32])\n",
      "tensor([[[ 0.0111, -0.1508,  0.0336,  0.0314,  0.7226, -0.0307, -0.0137,\n",
      "           0.1226,  0.6706, -0.4018,  0.0310, -0.6697, -0.0646, -0.0260,\n",
      "           0.0071, -0.2243, -0.0420,  0.3252, -0.0903,  0.0373, -0.1261,\n",
      "           0.0446,  0.0671, -0.0889,  0.0496,  0.2605,  0.0725,  0.0137,\n",
      "           0.0058, -0.0125,  0.0947, -0.0484],\n",
      "         [-0.2380, -0.0960,  0.0253, -0.0137,  0.7997, -0.0283,  0.0042,\n",
      "           0.0614,  0.7234, -0.5512, -0.0205, -0.5805, -0.1157,  0.0162,\n",
      "          -0.1010, -0.2190,  0.0372,  0.2126, -0.0516,  0.3536, -0.4966,\n",
      "           0.0022,  0.1059,  0.0735,  0.0347,  0.2154,  0.0276,  0.0033,\n",
      "          -0.0348, -0.0323,  0.0327, -0.0300],\n",
      "         [-0.0396, -0.0898,  0.2134,  0.0017,  0.3999,  0.0099,  0.0409,\n",
      "           0.0620,  1.0109, -0.4927,  0.0447, -0.3985, -0.0995, -0.0627,\n",
      "           0.0184, -0.2132,  0.1039,  0.3142, -0.0384,  0.0207, -0.4585,\n",
      "          -0.0288,  0.0881, -0.0728,  0.0948,  0.2697,  0.0647, -0.0071,\n",
      "          -0.0042,  0.0084,  0.0241, -0.3357]]], grad_fn=<MulBackward0>)\n",
      "Wdown: torch.Size([32, 8])\n",
      "Parameter containing:\n",
      "tensor([[ 0.1621, -0.1709,  0.1371, -0.1062, -0.0558, -0.0942, -0.0133, -0.0765],\n",
      "        [-0.1575,  0.0337,  0.1404,  0.0392, -0.0170, -0.1652,  0.0778,  0.0583],\n",
      "        [-0.0810, -0.0440,  0.0728, -0.0885, -0.0262,  0.0566, -0.1118,  0.0722],\n",
      "        [ 0.1668,  0.1119, -0.1435, -0.0820,  0.0380, -0.0422,  0.0765,  0.0252],\n",
      "        [-0.0754, -0.0043,  0.1759, -0.1120,  0.0069, -0.0447, -0.0119,  0.0708],\n",
      "        [-0.0308,  0.0732, -0.0577, -0.1167, -0.0976, -0.0060, -0.0298,  0.0108],\n",
      "        [ 0.0516,  0.1471,  0.1523, -0.1085, -0.0967, -0.0522,  0.0668, -0.1615],\n",
      "        [ 0.0368,  0.1121,  0.1146, -0.1120, -0.0697,  0.0897, -0.1595, -0.1440],\n",
      "        [-0.1570,  0.0189, -0.0948,  0.0623, -0.1106, -0.1029,  0.0497,  0.1539],\n",
      "        [-0.1442,  0.1243, -0.0322,  0.0735,  0.0504,  0.1063,  0.0636, -0.0137],\n",
      "        [-0.0087,  0.1233, -0.0558,  0.0666, -0.0796, -0.1574, -0.0554,  0.0710],\n",
      "        [ 0.1046,  0.0684, -0.0161, -0.1746, -0.0755,  0.1723,  0.1451,  0.1236],\n",
      "        [-0.0110,  0.1088,  0.0893,  0.1025, -0.1646, -0.1188,  0.0340,  0.0735],\n",
      "        [-0.0512, -0.0241,  0.1518,  0.1385, -0.1682, -0.1599, -0.1187, -0.1731],\n",
      "        [ 0.0709,  0.0868,  0.1035,  0.0550, -0.1068, -0.0556,  0.1687, -0.1611],\n",
      "        [-0.0980, -0.0250,  0.0572, -0.1099,  0.0536, -0.0473,  0.0997,  0.0577],\n",
      "        [-0.1743, -0.1054,  0.1564, -0.1710,  0.1039,  0.1339,  0.1344, -0.1014],\n",
      "        [ 0.1718, -0.1305, -0.1531,  0.0972,  0.0058, -0.1724,  0.0624, -0.0095],\n",
      "        [-0.1106, -0.1628, -0.1663,  0.0534,  0.0032,  0.0361, -0.0192,  0.0619],\n",
      "        [-0.0542,  0.0813, -0.1615,  0.1190, -0.0433,  0.1713, -0.0696,  0.0484],\n",
      "        [ 0.1247,  0.1426, -0.0458, -0.1391,  0.0457,  0.1718, -0.0399,  0.1350],\n",
      "        [ 0.0550, -0.0700, -0.1694,  0.0645,  0.0869,  0.1689, -0.1351,  0.0523],\n",
      "        [ 0.0333, -0.0752, -0.1092, -0.1677,  0.0030,  0.1118,  0.1039,  0.1216],\n",
      "        [-0.1313,  0.1234,  0.0524, -0.0362,  0.1747,  0.0942, -0.1746, -0.0385],\n",
      "        [ 0.0903,  0.0570, -0.0405,  0.1322, -0.1315,  0.0846, -0.0429, -0.0802],\n",
      "        [-0.0300, -0.0154,  0.1075,  0.0148, -0.1132, -0.0459, -0.0297, -0.0291],\n",
      "        [-0.0630,  0.1343,  0.0339, -0.1158,  0.0788, -0.1606, -0.1684, -0.1148],\n",
      "        [ 0.0359, -0.1316, -0.1061,  0.1318,  0.1629, -0.1658, -0.0404,  0.1502],\n",
      "        [-0.0491, -0.0084,  0.0271,  0.0216, -0.1479, -0.1258, -0.0140,  0.1385],\n",
      "        [-0.1604, -0.1264,  0.0066,  0.1609, -0.0498,  0.1677,  0.0518, -0.0353],\n",
      "        [-0.0563,  0.1268, -0.0862,  0.0391,  0.0315, -0.0063, -0.1012,  0.1461],\n",
      "        [ 0.1025,  0.1126,  0.1092,  0.1412, -0.1198,  0.1671, -0.0329,  0.0657]],\n",
      "       requires_grad=True)\n",
      "Wdown spliced: torch.Size([32, 8])\n",
      "tensor([[ 0.1621, -0.1709,  0.1371, -0.1062, -0.0558, -0.0942, -0.0133, -0.0765],\n",
      "        [-0.1575,  0.0337,  0.1404,  0.0392, -0.0170, -0.1652,  0.0778,  0.0583],\n",
      "        [-0.0810, -0.0440,  0.0728, -0.0885, -0.0262,  0.0566, -0.1118,  0.0722],\n",
      "        [ 0.1668,  0.1119, -0.1435, -0.0820,  0.0380, -0.0422,  0.0765,  0.0252],\n",
      "        [-0.0754, -0.0043,  0.1759, -0.1120,  0.0069, -0.0447, -0.0119,  0.0708],\n",
      "        [-0.0308,  0.0732, -0.0577, -0.1167, -0.0976, -0.0060, -0.0298,  0.0108],\n",
      "        [ 0.0516,  0.1471,  0.1523, -0.1085, -0.0967, -0.0522,  0.0668, -0.1615],\n",
      "        [ 0.0368,  0.1121,  0.1146, -0.1120, -0.0697,  0.0897, -0.1595, -0.1440],\n",
      "        [-0.1570,  0.0189, -0.0948,  0.0623, -0.1106, -0.1029,  0.0497,  0.1539],\n",
      "        [-0.1442,  0.1243, -0.0322,  0.0735,  0.0504,  0.1063,  0.0636, -0.0137],\n",
      "        [-0.0087,  0.1233, -0.0558,  0.0666, -0.0796, -0.1574, -0.0554,  0.0710],\n",
      "        [ 0.1046,  0.0684, -0.0161, -0.1746, -0.0755,  0.1723,  0.1451,  0.1236],\n",
      "        [-0.0110,  0.1088,  0.0893,  0.1025, -0.1646, -0.1188,  0.0340,  0.0735],\n",
      "        [-0.0512, -0.0241,  0.1518,  0.1385, -0.1682, -0.1599, -0.1187, -0.1731],\n",
      "        [ 0.0709,  0.0868,  0.1035,  0.0550, -0.1068, -0.0556,  0.1687, -0.1611],\n",
      "        [-0.0980, -0.0250,  0.0572, -0.1099,  0.0536, -0.0473,  0.0997,  0.0577],\n",
      "        [-0.1743, -0.1054,  0.1564, -0.1710,  0.1039,  0.1339,  0.1344, -0.1014],\n",
      "        [ 0.1718, -0.1305, -0.1531,  0.0972,  0.0058, -0.1724,  0.0624, -0.0095],\n",
      "        [-0.1106, -0.1628, -0.1663,  0.0534,  0.0032,  0.0361, -0.0192,  0.0619],\n",
      "        [-0.0542,  0.0813, -0.1615,  0.1190, -0.0433,  0.1713, -0.0696,  0.0484],\n",
      "        [ 0.1247,  0.1426, -0.0458, -0.1391,  0.0457,  0.1718, -0.0399,  0.1350],\n",
      "        [ 0.0550, -0.0700, -0.1694,  0.0645,  0.0869,  0.1689, -0.1351,  0.0523],\n",
      "        [ 0.0333, -0.0752, -0.1092, -0.1677,  0.0030,  0.1118,  0.1039,  0.1216],\n",
      "        [-0.1313,  0.1234,  0.0524, -0.0362,  0.1747,  0.0942, -0.1746, -0.0385],\n",
      "        [ 0.0903,  0.0570, -0.0405,  0.1322, -0.1315,  0.0846, -0.0429, -0.0802],\n",
      "        [-0.0300, -0.0154,  0.1075,  0.0148, -0.1132, -0.0459, -0.0297, -0.0291],\n",
      "        [-0.0630,  0.1343,  0.0339, -0.1158,  0.0788, -0.1606, -0.1684, -0.1148],\n",
      "        [ 0.0359, -0.1316, -0.1061,  0.1318,  0.1629, -0.1658, -0.0404,  0.1502],\n",
      "        [-0.0491, -0.0084,  0.0271,  0.0216, -0.1479, -0.1258, -0.0140,  0.1385],\n",
      "        [-0.1604, -0.1264,  0.0066,  0.1609, -0.0498,  0.1677,  0.0518, -0.0353],\n",
      "        [-0.0563,  0.1268, -0.0862,  0.0391,  0.0315, -0.0063, -0.1012,  0.1461],\n",
      "        [ 0.1025,  0.1126,  0.1092,  0.1412, -0.1198,  0.1671, -0.0329,  0.0657]],\n",
      "       grad_fn=<AliasBackward0>)\n",
      "Bdown: torch.Size([8])\n",
      "Parameter containing:\n",
      "tensor([-0.0545, -0.1077, -0.1066, -0.1024,  0.0135, -0.0430,  0.1375, -0.0506],\n",
      "       requires_grad=True)\n",
      "Bdown spliced: torch.Size([8])\n",
      "tensor([-0.0545, -0.1077, -0.1066, -0.1024,  0.0135, -0.0430,  0.1375, -0.0506],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "outputs: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.1135, -0.2283, -0.0984, -0.0090, -0.0687, -0.3556, -0.0113,\n",
      "          -0.0194],\n",
      "         [-0.2735, -0.2417, -0.1156,  0.0364, -0.0507, -0.2930, -0.0197,\n",
      "          -0.0130],\n",
      "         [-0.2412, -0.3154, -0.1580, -0.0473, -0.1135, -0.4147,  0.1029,\n",
      "          -0.0280]]], grad_fn=<AddBackward0>)\n",
      "------------- END MLP.forwardTensor() ------------\n",
      "layer's final residual state:\n",
      "tensor([[[ 1.8356, -0.5053,  0.1321,  0.1669,  0.7052,  0.7434,  0.9186,\n",
      "           1.2215],\n",
      "         [ 0.5792, -0.3566, -0.4200,  1.0777,  0.6576,  1.0897,  0.9552,\n",
      "           1.0257],\n",
      "         [ 1.1135, -0.4107, -0.1635,  0.4228,  0.1329,  0.9104,  0.7014,\n",
      "           0.2244]]], grad_fn=<AddBackward0>)\n",
      "----------------- END Layer.forwardTensor() --------------------\n",
      "y: torch.Size([1, 3, 8])\n",
      "tensor([[[ 1.8356, -0.5053,  0.1321,  0.1669,  0.7052,  0.7434,  0.9186,\n",
      "           1.2215],\n",
      "         [ 0.5792, -0.3566, -0.4200,  1.0777,  0.6576,  1.0897,  0.9552,\n",
      "           1.0257],\n",
      "         [ 1.1135, -0.4107, -0.1635,  0.4228,  0.1329,  0.9104,  0.7014,\n",
      "           0.2244]]], grad_fn=<AddBackward0>)\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the first sub-model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[0.0799, 0.8793, 0.5686, 0.0230],\n",
      "         [0.5028, 0.0760, 0.0824, 0.9742],\n",
      "         [0.1241, 0.1286, 0.1264, 0.5248]]])\n",
      "---------- Layer Input: torch.Tensor ------------\n",
      "----------------- Layer.forwardTensor() --------------------\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[0.0799, 0.8793, 0.5686, 0.0230],\n",
      "         [0.5028, 0.0760, 0.0824, 0.9742],\n",
      "         [0.1241, 0.1286, 0.1264, 0.5248]]])\n",
      "normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[0.1521, 1.6742, 1.0826, 0.0437],\n",
      "         [0.9125, 0.1379, 0.1496, 1.7680],\n",
      "         [0.4366, 0.4523, 0.4446, 1.8458]]])\n",
      "dim: 4\n",
      "skip: 0\n",
      "spliced scale: torch.Size([4])\n",
      "tensor([0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[0.1521, 1.6742, 1.0826, 0.0437],\n",
      "         [0.9125, 0.1379, 0.1496, 1.7680],\n",
      "         [0.4366, 0.4523, 0.4446, 1.8458]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "---------- Attention Input: torch.Tensor ------------\n",
      "----------------- MultiQueryAttention.forwardTensor() --------------------\n",
      "x shape: torch.Size([1, 3, 4])\n",
      "d_skip: 0\n",
      "models_in_this_level: 2\n",
      "h_dim: 2\n",
      "h_skip: 0\n",
      "self.Wqkv: torch.Size([8, 16])\n",
      "Parameter containing:\n",
      "tensor([[ 2.0702e-01,  6.4118e-02, -1.2613e-01,  6.6395e-02,  2.3395e-01,\n",
      "         -1.6330e-01, -2.3705e-01, -1.9287e-01,  2.8360e-01, -1.8192e-01,\n",
      "         -1.1451e-01,  2.7055e-01,  2.8591e-01, -2.9814e-02, -1.9184e-01,\n",
      "         -3.3076e-01],\n",
      "        [-2.9885e-01, -2.5014e-01,  2.8536e-01, -3.1258e-01,  2.2733e-01,\n",
      "         -2.8512e-01,  1.5649e-01,  8.0250e-02, -2.7886e-01, -1.8872e-01,\n",
      "         -1.6147e-02,  2.0864e-01,  1.2717e-01,  2.5776e-01,  2.9930e-01,\n",
      "         -2.4047e-01],\n",
      "        [-8.2106e-02,  2.0744e-01, -2.9336e-01, -1.2577e-01,  3.1041e-01,\n",
      "         -1.1347e-01,  7.9519e-02,  9.2856e-03, -3.5075e-01,  2.1356e-01,\n",
      "          1.1815e-01, -1.0258e-01,  2.9311e-01, -7.3002e-02,  3.0950e-01,\n",
      "         -2.1562e-01],\n",
      "        [ 2.0227e-01, -2.2253e-01,  1.2799e-01, -1.6161e-01, -5.8874e-02,\n",
      "         -3.0920e-01,  2.5895e-01,  2.2118e-01,  2.3010e-02, -1.8029e-01,\n",
      "         -1.7323e-01,  1.9589e-01,  2.3175e-02, -6.7463e-02, -2.5493e-01,\n",
      "          3.3584e-01],\n",
      "        [-2.6585e-01,  3.2614e-01, -4.6001e-02,  2.3347e-01, -2.7832e-01,\n",
      "          1.7337e-01,  2.8728e-01,  1.9048e-01, -1.3930e-01,  1.9527e-01,\n",
      "          1.0067e-01, -2.9379e-01, -2.7763e-01,  1.8283e-01, -2.2916e-01,\n",
      "          3.2144e-01],\n",
      "        [-1.9634e-01, -2.9469e-04,  2.5225e-01, -1.1077e-01,  6.7529e-02,\n",
      "          3.4334e-01,  1.4410e-01,  8.0650e-02,  6.8357e-02,  2.5984e-02,\n",
      "          5.4219e-02, -5.4840e-02,  7.3328e-02,  3.3648e-01,  3.4547e-01,\n",
      "         -1.3449e-01],\n",
      "        [ 1.7800e-01, -1.6859e-01, -3.0324e-01,  8.1556e-02,  1.8543e-01,\n",
      "          7.9592e-03,  2.9457e-01,  1.0125e-01,  1.8614e-02,  1.2639e-01,\n",
      "          3.0781e-01,  4.8880e-02,  3.0163e-02, -2.9571e-01, -3.1014e-01,\n",
      "          8.8922e-02],\n",
      "        [-1.4662e-01, -4.2236e-02,  3.0250e-01, -2.7546e-01, -3.5252e-01,\n",
      "         -1.3962e-01,  6.5359e-02, -6.3485e-02,  2.8290e-01,  1.3399e-01,\n",
      "         -2.5967e-01, -1.1979e-01, -1.1366e-02,  2.5576e-01,  7.4515e-03,\n",
      "          2.1582e-01]], requires_grad=True)\n",
      "Wq: torch.Size([8, 8])\n",
      "tensor([[ 2.0702e-01,  6.4118e-02, -1.2613e-01,  6.6395e-02,  2.3395e-01,\n",
      "         -1.6330e-01, -2.3705e-01, -1.9287e-01],\n",
      "        [-2.9885e-01, -2.5014e-01,  2.8536e-01, -3.1258e-01,  2.2733e-01,\n",
      "         -2.8512e-01,  1.5649e-01,  8.0250e-02],\n",
      "        [-8.2106e-02,  2.0744e-01, -2.9336e-01, -1.2577e-01,  3.1041e-01,\n",
      "         -1.1347e-01,  7.9519e-02,  9.2856e-03],\n",
      "        [ 2.0227e-01, -2.2253e-01,  1.2799e-01, -1.6161e-01, -5.8874e-02,\n",
      "         -3.0920e-01,  2.5895e-01,  2.2118e-01],\n",
      "        [-2.6585e-01,  3.2614e-01, -4.6001e-02,  2.3347e-01, -2.7832e-01,\n",
      "          1.7337e-01,  2.8728e-01,  1.9048e-01],\n",
      "        [-1.9634e-01, -2.9469e-04,  2.5225e-01, -1.1077e-01,  6.7529e-02,\n",
      "          3.4334e-01,  1.4410e-01,  8.0650e-02],\n",
      "        [ 1.7800e-01, -1.6859e-01, -3.0324e-01,  8.1556e-02,  1.8543e-01,\n",
      "          7.9592e-03,  2.9457e-01,  1.0125e-01],\n",
      "        [-1.4662e-01, -4.2236e-02,  3.0250e-01, -2.7546e-01, -3.5252e-01,\n",
      "         -1.3962e-01,  6.5359e-02, -6.3485e-02]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wk: torch.Size([8, 4])\n",
      "tensor([[ 0.2836, -0.1819, -0.1145,  0.2705],\n",
      "        [-0.2789, -0.1887, -0.0161,  0.2086],\n",
      "        [-0.3508,  0.2136,  0.1181, -0.1026],\n",
      "        [ 0.0230, -0.1803, -0.1732,  0.1959],\n",
      "        [-0.1393,  0.1953,  0.1007, -0.2938],\n",
      "        [ 0.0684,  0.0260,  0.0542, -0.0548],\n",
      "        [ 0.0186,  0.1264,  0.3078,  0.0489],\n",
      "        [ 0.2829,  0.1340, -0.2597, -0.1198]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wv: torch.Size([8, 4])\n",
      "tensor([[ 0.2859, -0.0298, -0.1918, -0.3308],\n",
      "        [ 0.1272,  0.2578,  0.2993, -0.2405],\n",
      "        [ 0.2931, -0.0730,  0.3095, -0.2156],\n",
      "        [ 0.0232, -0.0675, -0.2549,  0.3358],\n",
      "        [-0.2776,  0.1828, -0.2292,  0.3214],\n",
      "        [ 0.0733,  0.3365,  0.3455, -0.1345],\n",
      "        [ 0.0302, -0.2957, -0.3101,  0.0889],\n",
      "        [-0.0114,  0.2558,  0.0075,  0.2158]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wq spliced: torch.Size([4, 4])\n",
      "tensor([[ 0.2070,  0.0641,  0.2340, -0.1633],\n",
      "        [-0.2988, -0.2501,  0.2273, -0.2851],\n",
      "        [-0.0821,  0.2074,  0.3104, -0.1135],\n",
      "        [ 0.2023, -0.2225, -0.0589, -0.3092]], grad_fn=<CatBackward0>)\n",
      "Wk spliced: torch.Size([4, 2])\n",
      "tensor([[ 0.2836, -0.1819],\n",
      "        [-0.2789, -0.1887],\n",
      "        [-0.3508,  0.2136],\n",
      "        [ 0.0230, -0.1803]], grad_fn=<CatBackward0>)\n",
      "Wv spliced: torch.Size([4, 2])\n",
      "tensor([[ 0.2859, -0.0298],\n",
      "        [ 0.1272,  0.2578],\n",
      "        [ 0.2931, -0.0730],\n",
      "        [ 0.0232, -0.0675]], grad_fn=<CatBackward0>)\n",
      "Wqkv_spliced: torch.Size([4, 8])\n",
      "tensor([[ 0.2070,  0.0641,  0.2340, -0.1633,  0.2836, -0.1819,  0.2859, -0.0298],\n",
      "        [-0.2988, -0.2501,  0.2273, -0.2851, -0.2789, -0.1887,  0.1272,  0.2578],\n",
      "        [-0.0821,  0.2074,  0.3104, -0.1135, -0.3508,  0.2136,  0.2931, -0.0730],\n",
      "        [ 0.2023, -0.2225, -0.0589, -0.3092,  0.0230, -0.1803,  0.0232, -0.0675]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "xqkv: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.5489, -0.1942,  0.7496, -0.6385, -0.8025, -0.1203,  0.5747,\n",
      "           0.3450],\n",
      "         [ 0.4930, -0.3384,  0.1872, -0.7520,  0.2085, -0.4788,  0.3632,\n",
      "          -0.1218],\n",
      "         [ 0.2921, -0.4037,  0.2343, -0.8214, -0.1158, -0.4026,  0.3554,\n",
      "          -0.0534]]], grad_fn=<UnsafeViewBackward0>)\n",
      "xq: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.5489, -0.1942,  0.7496, -0.6385],\n",
      "         [ 0.4930, -0.3384,  0.1872, -0.7520],\n",
      "         [ 0.2921, -0.4037,  0.2343, -0.8214]]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "xk: torch.Size([1, 3, 2])\n",
      "tensor([[[-0.8025, -0.1203],\n",
      "         [ 0.2085, -0.4788],\n",
      "         [-0.1158, -0.4026]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xv: torch.Size([1, 3, 2])\n",
      "tensor([[[ 0.5747,  0.3450],\n",
      "         [ 0.3632, -0.1218],\n",
      "         [ 0.3554, -0.0534]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xq reshaped: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[-0.5489, -0.1942],\n",
      "          [ 0.7496, -0.6385]],\n",
      "\n",
      "         [[ 0.4930, -0.3384],\n",
      "          [ 0.1872, -0.7520]],\n",
      "\n",
      "         [[ 0.2921, -0.4037],\n",
      "          [ 0.2343, -0.8214]]]], grad_fn=<ViewBackward0>)\n",
      "xk reshaped: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[-0.8025, -0.1203]],\n",
      "\n",
      "         [[ 0.2085, -0.4788]],\n",
      "\n",
      "         [[-0.1158, -0.4026]]]], grad_fn=<ViewBackward0>)\n",
      "xv reshaped: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[ 0.5747,  0.3450]],\n",
      "\n",
      "         [[ 0.3632, -0.1218]],\n",
      "\n",
      "         [[ 0.3554, -0.0534]]]], grad_fn=<ViewBackward0>)\n",
      "rotated xq: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[-0.5489, -0.1942],\n",
      "          [ 0.7496, -0.6385]],\n",
      "\n",
      "         [[ 0.5512,  0.2320],\n",
      "          [ 0.7339, -0.2488]],\n",
      "\n",
      "         [[ 0.2455,  0.4336],\n",
      "          [ 0.6494,  0.5549]]]], grad_fn=<TransposeBackward0>)\n",
      "rotated xk: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[-0.8025, -0.1203]],\n",
      "\n",
      "         [[ 0.5156, -0.0833]],\n",
      "\n",
      "         [[ 0.4143,  0.0623]]]], grad_fn=<TransposeBackward0>)\n",
      "repeat_interleaved xk: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[-0.8025, -0.1203],\n",
      "          [-0.8025, -0.1203]],\n",
      "\n",
      "         [[ 0.5156, -0.0833],\n",
      "          [ 0.5156, -0.0833]],\n",
      "\n",
      "         [[ 0.4143,  0.0623],\n",
      "          [ 0.4143,  0.0623]]]], grad_fn=<ViewBackward0>)\n",
      "repeat_interleaved xv: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[ 0.5747,  0.3450],\n",
      "          [ 0.5747,  0.3450]],\n",
      "\n",
      "         [[ 0.3632, -0.1218],\n",
      "          [ 0.3632, -0.1218]],\n",
      "\n",
      "         [[ 0.3554, -0.0534],\n",
      "          [ 0.3554, -0.0534]]]], grad_fn=<ViewBackward0>)\n",
      "transposed xq: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[-0.5489, -0.1942],\n",
      "          [ 0.5512,  0.2320],\n",
      "          [ 0.2455,  0.4336]],\n",
      "\n",
      "         [[ 0.7496, -0.6385],\n",
      "          [ 0.7339, -0.2488],\n",
      "          [ 0.6494,  0.5549]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xk: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[-0.8025, -0.1203],\n",
      "          [ 0.5156, -0.0833],\n",
      "          [ 0.4143,  0.0623]],\n",
      "\n",
      "         [[-0.8025, -0.1203],\n",
      "          [ 0.5156, -0.0833],\n",
      "          [ 0.4143,  0.0623]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xv: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[ 0.5747,  0.3450],\n",
      "          [ 0.3632, -0.1218],\n",
      "          [ 0.3554, -0.0534]],\n",
      "\n",
      "         [[ 0.5747,  0.3450],\n",
      "          [ 0.3632, -0.1218],\n",
      "          [ 0.3554, -0.0534]]]], grad_fn=<TransposeBackward0>)\n",
      "scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[ 0.3280, -0.1887, -0.1693],\n",
      "          [-0.3325,  0.1873,  0.1717],\n",
      "          [-0.1762,  0.0640,  0.0910]],\n",
      "\n",
      "         [[-0.3711,  0.3109,  0.1915],\n",
      "          [-0.3953,  0.2822,  0.2040],\n",
      "          [-0.4157,  0.2041,  0.2147]]]], grad_fn=<MulBackward0>)\n",
      "mask: torch.Size([1, 1, 3, 3])\n",
      "tensor([[[[ 0.0000e+00, -2.3820e+38, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]]]])\n",
      "masked scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[ 3.2797e-01, -2.3820e+38, -2.3820e+38],\n",
      "          [-3.3247e-01,  1.8728e-01, -2.3820e+38],\n",
      "          [-1.7619e-01,  6.3989e-02,  9.1019e-02]],\n",
      "\n",
      "         [[-3.7105e-01, -2.3820e+38, -2.3820e+38],\n",
      "          [-3.9527e-01,  2.8222e-01, -2.3820e+38],\n",
      "          [-4.1570e-01,  2.0411e-01,  2.1468e-01]]]], grad_fn=<AddBackward0>)\n",
      "softmaxed scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [0.3729, 0.6271, 0.0000],\n",
      "          [0.2795, 0.3554, 0.3651]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.3368, 0.6632, 0.0000],\n",
      "          [0.2111, 0.3924, 0.3965]]]], grad_fn=<SoftmaxBackward0>)\n",
      "attention: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[0.5747, 0.3450],\n",
      "          [0.4421, 0.0522],\n",
      "          [0.4195, 0.0336]],\n",
      "\n",
      "         [[0.5747, 0.3450],\n",
      "          [0.4345, 0.0354],\n",
      "          [0.4048, 0.0038]]]], grad_fn=<UnsafeViewBackward0>)\n",
      "reshaped attention: torch.Size([1, 3, 4])\n",
      "tensor([[[0.5747, 0.3450, 0.5747, 0.3450],\n",
      "         [0.4421, 0.0522, 0.4345, 0.0354],\n",
      "         [0.4195, 0.0336, 0.4048, 0.0038]]], grad_fn=<ViewBackward0>)\n",
      "self.Wo: torch.Size([8, 8])\n",
      "Parameter containing:\n",
      "tensor([[-0.2266,  0.0936, -0.3057,  0.3420, -0.2875,  0.2037, -0.2096, -0.1751],\n",
      "        [-0.0166, -0.3479, -0.3198,  0.1795,  0.2080,  0.1628, -0.3114, -0.2264],\n",
      "        [-0.1321, -0.3448,  0.1478,  0.0007,  0.0570, -0.0599, -0.2147, -0.0096],\n",
      "        [-0.1373,  0.2099,  0.0720,  0.0211, -0.2262, -0.2143, -0.1482, -0.2927],\n",
      "        [ 0.3218,  0.0986, -0.2518,  0.3455, -0.1162,  0.2619, -0.0528, -0.2996],\n",
      "        [ 0.2275, -0.2815,  0.1916,  0.1392,  0.2340,  0.1219,  0.1267, -0.0548],\n",
      "        [-0.3121,  0.0377, -0.2585,  0.2183, -0.0895, -0.3505, -0.2968,  0.0591],\n",
      "        [-0.1614,  0.2509, -0.1401, -0.0922,  0.0563,  0.0629,  0.0433,  0.2691]],\n",
      "       requires_grad=True)\n",
      "spliced Wo: torch.Size([4, 4])\n",
      "tensor([[-0.2266,  0.0936, -0.3057,  0.3420],\n",
      "        [-0.0166, -0.3479, -0.3198,  0.1795],\n",
      "        [ 0.3218,  0.0986, -0.2518,  0.3455],\n",
      "        [ 0.2275, -0.2815,  0.1916,  0.1392]], grad_fn=<CatBackward0>)\n",
      "projected output: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.1275, -0.1067, -0.3647,  0.5051],\n",
      "         [ 0.0468,  0.0561, -0.2545,  0.3156],\n",
      "         [ 0.0355,  0.0664, -0.2402,  0.2899]]], grad_fn=<UnsafeViewBackward0>)\n",
      "----------------- END MultiQueryAttention.forwardTensor() --------------------\n",
      "x in layer after MQA & resid connection and before MLP:\n",
      "tensor([[[ 0.2074,  0.7726,  0.2039,  0.5281],\n",
      "         [ 0.5496,  0.1321, -0.1721,  1.2899],\n",
      "         [ 0.1597,  0.1950, -0.1138,  0.8147]]], grad_fn=<AddBackward0>)\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.2074,  0.7726,  0.2039,  0.5281],\n",
      "         [ 0.5496,  0.1321, -0.1721,  1.2899],\n",
      "         [ 0.1597,  0.1950, -0.1138,  0.8147]]], grad_fn=<AddBackward0>)\n",
      "normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.4232,  1.5768,  0.4162,  1.0777],\n",
      "         [ 0.7748,  0.1862, -0.2426,  1.8183],\n",
      "         [ 0.3712,  0.4533, -0.2646,  1.8939]]], grad_fn=<MulBackward0>)\n",
      "dim: 4\n",
      "skip: 0\n",
      "spliced scale: torch.Size([4])\n",
      "tensor([0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.4232,  1.5768,  0.4162,  1.0777],\n",
      "         [ 0.7748,  0.1862, -0.2426,  1.8183],\n",
      "         [ 0.3712,  0.4533, -0.2646,  1.8939]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "---------- MLP Input: torch.Tensor ------------\n",
      "------------- MLP.forwardTensor() ------------\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.4232,  1.5768,  0.4162,  1.0777],\n",
      "         [ 0.7748,  0.1862, -0.2426,  1.8183],\n",
      "         [ 0.3712,  0.4533, -0.2646,  1.8939]]], grad_fn=<MulBackward0>)\n",
      "d_dim: 4\n",
      "d_skip: 0\n",
      "i_dim: 16\n",
      "i_skip: 0\n",
      "Wgate: torch.Size([8, 32])\n",
      "Parameter containing:\n",
      "tensor([[-0.1289,  0.1969, -0.1112, -0.3040,  0.0256,  0.3259,  0.1530,  0.3237,\n",
      "         -0.3011,  0.2621,  0.2657, -0.0728,  0.0805, -0.2921,  0.1220, -0.1451,\n",
      "          0.1776,  0.3489,  0.1491, -0.3237, -0.1215,  0.2421, -0.2338, -0.1404,\n",
      "          0.0366,  0.1616,  0.1742,  0.2344, -0.0369, -0.1814,  0.3132,  0.3355],\n",
      "        [ 0.1960,  0.1693, -0.3288,  0.1713, -0.1709,  0.0480,  0.2562,  0.0406,\n",
      "          0.2857, -0.3349,  0.0128, -0.3295, -0.0600,  0.1235, -0.1776,  0.1847,\n",
      "         -0.0715,  0.2343, -0.1194, -0.2221, -0.0056, -0.0779, -0.1443, -0.0323,\n",
      "          0.1247,  0.1879, -0.3152, -0.0565, -0.2243, -0.0679,  0.1079,  0.1319],\n",
      "        [ 0.0386, -0.1673, -0.2099,  0.1778, -0.1155,  0.2045, -0.0830,  0.2707,\n",
      "         -0.0786,  0.1436, -0.1537,  0.2014,  0.0415, -0.2376,  0.3330,  0.0965,\n",
      "          0.0618, -0.2139, -0.3499,  0.2657, -0.1222,  0.0930,  0.0630, -0.2961,\n",
      "          0.2812,  0.0618,  0.1124,  0.2624, -0.1160,  0.3132, -0.3494,  0.3368],\n",
      "        [-0.2217,  0.2893, -0.0764, -0.3331, -0.0236,  0.0885,  0.0815,  0.2425,\n",
      "          0.3176,  0.2169, -0.1597, -0.3311,  0.1796,  0.2958, -0.1264, -0.3170,\n",
      "          0.0050, -0.0730, -0.0601, -0.2727,  0.2065,  0.2168, -0.3396, -0.0271,\n",
      "          0.0896, -0.1709,  0.0746, -0.1176,  0.0818,  0.3265,  0.3038,  0.0719],\n",
      "        [ 0.3313, -0.3411, -0.2511, -0.0274, -0.1441,  0.3448,  0.2723,  0.0378,\n",
      "          0.2604,  0.1008, -0.1790, -0.3413, -0.1118,  0.3125, -0.0733, -0.2676,\n",
      "         -0.2720, -0.1250,  0.1935,  0.1670,  0.0893,  0.3123, -0.2819,  0.2008,\n",
      "         -0.2814, -0.2081,  0.0700,  0.2840,  0.2976,  0.0449,  0.3113,  0.1454],\n",
      "        [ 0.1082, -0.2500,  0.0714, -0.1651,  0.2102,  0.0950, -0.2628, -0.1242,\n",
      "         -0.2218,  0.3194,  0.0614,  0.0601,  0.0204,  0.3484,  0.2823, -0.1181,\n",
      "         -0.1028,  0.2263, -0.2639, -0.1876,  0.0055, -0.0674, -0.2292, -0.0231,\n",
      "         -0.1707, -0.2461,  0.0346,  0.1250,  0.0721, -0.2174, -0.0489,  0.3499],\n",
      "        [ 0.3170, -0.1791, -0.2021,  0.1310,  0.1164,  0.0814, -0.2853,  0.3075,\n",
      "          0.0187, -0.1009,  0.0180, -0.1810,  0.1702, -0.0067, -0.2766,  0.1965,\n",
      "         -0.2948, -0.0709, -0.0755, -0.0053,  0.2850, -0.2996,  0.2942, -0.1974,\n",
      "          0.3183,  0.0669, -0.0488, -0.0103,  0.2617, -0.3291, -0.2677, -0.0253],\n",
      "        [-0.1498, -0.2646, -0.2761, -0.0489,  0.3231,  0.1408, -0.0206,  0.0627,\n",
      "          0.0762, -0.1439,  0.0414,  0.1989, -0.0766, -0.3382, -0.2947,  0.2870,\n",
      "          0.1834, -0.0378, -0.2937, -0.1682, -0.2938,  0.0717, -0.1744,  0.2026,\n",
      "          0.0462,  0.0487,  0.1463, -0.1057, -0.2714, -0.1601,  0.1401, -0.2529]],\n",
      "       requires_grad=True)\n",
      "Wgate spliced: torch.Size([4, 16])\n",
      "tensor([[-0.1289,  0.1969, -0.1112, -0.3040,  0.0256,  0.3259,  0.1530,  0.3237,\n",
      "         -0.3011,  0.2621,  0.2657, -0.0728,  0.0805, -0.2921,  0.1220, -0.1451],\n",
      "        [ 0.1960,  0.1693, -0.3288,  0.1713, -0.1709,  0.0480,  0.2562,  0.0406,\n",
      "          0.2857, -0.3349,  0.0128, -0.3295, -0.0600,  0.1235, -0.1776,  0.1847],\n",
      "        [ 0.0386, -0.1673, -0.2099,  0.1778, -0.1155,  0.2045, -0.0830,  0.2707,\n",
      "         -0.0786,  0.1436, -0.1537,  0.2014,  0.0415, -0.2376,  0.3330,  0.0965],\n",
      "        [-0.2217,  0.2893, -0.0764, -0.3331, -0.0236,  0.0885,  0.0815,  0.2425,\n",
      "          0.3176,  0.2169, -0.1597, -0.3311,  0.1796,  0.2958, -0.1264, -0.3170]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Bgate: torch.Size([32])\n",
      "Parameter containing:\n",
      "tensor([-0.1976, -0.1026, -0.1424,  0.1746,  0.2842, -0.1167,  0.3057, -0.3163,\n",
      "        -0.2467,  0.0285,  0.1179,  0.0071,  0.1863, -0.2272, -0.1948,  0.3475,\n",
      "        -0.0786,  0.0625, -0.0095,  0.2740, -0.0863,  0.0282, -0.2240, -0.1504,\n",
      "        -0.1843,  0.3197,  0.1520,  0.1567,  0.1206,  0.1951,  0.2162, -0.2705],\n",
      "       requires_grad=True)\n",
      "Bgate spliced: torch.Size([16])\n",
      "tensor([-0.1976, -0.1026, -0.1424,  0.1746,  0.2842, -0.1167,  0.3057, -0.3163,\n",
      "        -0.2467,  0.0285,  0.1179,  0.0071,  0.1863, -0.2272, -0.1948,  0.3475],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xgate: torch.Size([1, 3, 16])\n",
      "tensor([[[-0.1660,  0.4899, -0.8776,  0.0310, -0.0479,  0.2775,  0.8277,\n",
      "           0.2587,  0.3859, -0.0950,  0.0144, -0.8162,  0.3367,  0.0638,\n",
      "          -0.4208,  0.2758],\n",
      "         [-0.6736,  0.6482, -0.3778, -0.6780,  0.2574,  0.2561,  0.6403,\n",
      "           0.3173,  0.1697,  0.5288,  0.0731, -0.7615,  0.5541,  0.1649,\n",
      "          -0.4440, -0.3303],\n",
      "         [-0.5868,  0.6395, -0.4219, -0.5386,  0.2022,  0.1395,  0.6550,\n",
      "           0.2099,  0.3933,  0.3467, -0.0394, -0.8496,  0.5183,  0.3434,\n",
      "          -0.5575, -0.2485]]], grad_fn=<AddBackward0>)\n",
      "GeLU'ed Xgate: torch.Size([1, 3, 16])\n",
      "tensor([[[-0.0721,  0.3370, -0.1668,  0.0159, -0.0230,  0.1691,  0.6589,\n",
      "           0.1558,  0.2509, -0.0439,  0.0073, -0.1691,  0.2128,  0.0335,\n",
      "          -0.1418,  0.1679],\n",
      "         [-0.1686,  0.4807, -0.1333, -0.1687,  0.1549,  0.1539,  0.4732,\n",
      "           0.1982,  0.0963,  0.3709,  0.0387, -0.1700,  0.3936,  0.0933,\n",
      "          -0.1459, -0.1224],\n",
      "         [-0.1635,  0.4724, -0.1420, -0.1589,  0.1173,  0.0775,  0.4871,\n",
      "           0.1224,  0.2568,  0.2204, -0.0191, -0.1680,  0.3617,  0.2178,\n",
      "          -0.1609, -0.0999]]], grad_fn=<GeluBackward0>)\n",
      "Wup: torch.Size([8, 32])\n",
      "Parameter containing:\n",
      "tensor([[-0.2741,  0.2815,  0.1217,  0.0877, -0.2717, -0.2743, -0.3520,  0.3191,\n",
      "         -0.0347, -0.1266,  0.3024,  0.2227,  0.0548,  0.0727, -0.0612,  0.2773,\n",
      "         -0.2069,  0.0486,  0.1578,  0.1623, -0.2593,  0.2194,  0.1725,  0.0813,\n",
      "         -0.3026,  0.0726, -0.0622, -0.0504, -0.0462, -0.2114,  0.0297, -0.1258],\n",
      "        [-0.1066,  0.0185,  0.1244,  0.1343, -0.0040,  0.3245, -0.1219,  0.1535,\n",
      "          0.1939,  0.1921,  0.0743,  0.1904, -0.2701, -0.2174,  0.1159,  0.1370,\n",
      "          0.1045,  0.0080,  0.1275, -0.3121,  0.0620,  0.1925,  0.1780,  0.2020,\n",
      "          0.0266, -0.2843,  0.0632,  0.0018, -0.1047, -0.1423, -0.0693, -0.2050],\n",
      "        [-0.2546,  0.2375, -0.1895,  0.2369, -0.1226, -0.1657,  0.2568, -0.2030,\n",
      "         -0.0508,  0.2762,  0.3489,  0.2658,  0.0290,  0.2453, -0.2461, -0.1030,\n",
      "          0.1981,  0.0445,  0.0624, -0.0776,  0.1527,  0.0652, -0.2653, -0.0230,\n",
      "          0.1094,  0.1625,  0.1837,  0.1934, -0.0246,  0.0182, -0.2569, -0.1474],\n",
      "        [-0.1050,  0.0765, -0.3351, -0.0987,  0.1895, -0.1666,  0.0256, -0.1588,\n",
      "          0.2778, -0.2298,  0.2139, -0.1324, -0.2248,  0.2687, -0.3314,  0.0593,\n",
      "         -0.1131,  0.2542, -0.2838,  0.3296, -0.0096,  0.1192,  0.0530,  0.3368,\n",
      "         -0.0299,  0.2732,  0.3269,  0.1520, -0.1221,  0.0772, -0.2016, -0.1061],\n",
      "        [ 0.0463, -0.2287,  0.1453,  0.0562, -0.1503,  0.0602, -0.1690,  0.2613,\n",
      "         -0.0123, -0.1400, -0.1642,  0.0446,  0.2561, -0.3458,  0.2523, -0.0157,\n",
      "         -0.1478, -0.2101, -0.2109, -0.0463,  0.0324,  0.0340, -0.1074,  0.0710,\n",
      "         -0.0825,  0.2404, -0.3497, -0.1357,  0.1856, -0.2651, -0.0783,  0.0676],\n",
      "        [-0.0837,  0.1149, -0.1521, -0.2451, -0.0538, -0.2349, -0.3411,  0.2669,\n",
      "          0.1058, -0.0359,  0.0356, -0.1496,  0.0616,  0.0616, -0.3322, -0.0758,\n",
      "          0.2694, -0.1163,  0.1482, -0.0304,  0.3279, -0.3317,  0.1556,  0.1556,\n",
      "          0.2094, -0.3141, -0.1766, -0.1795, -0.3068,  0.3289, -0.0152, -0.1649],\n",
      "        [ 0.0093,  0.2491, -0.2967, -0.2065, -0.0788, -0.2269,  0.0561,  0.2417,\n",
      "          0.2134, -0.3228, -0.0038,  0.0314,  0.1237,  0.3065,  0.0703,  0.1294,\n",
      "         -0.3007, -0.3394,  0.0833,  0.1926, -0.1931,  0.1349,  0.3258,  0.2225,\n",
      "         -0.1870, -0.1225, -0.2523, -0.3357,  0.0954,  0.2288, -0.0357, -0.0672],\n",
      "        [-0.3379, -0.3328, -0.2540,  0.1646, -0.1807, -0.2958,  0.1082,  0.2875,\n",
      "          0.1004, -0.3027, -0.2463, -0.0351, -0.0951, -0.3428,  0.2382, -0.0077,\n",
      "          0.0011,  0.2578, -0.3462,  0.1690, -0.1591,  0.2148, -0.1809,  0.2410,\n",
      "          0.0095,  0.2360, -0.3312,  0.1083,  0.2263, -0.0933,  0.2342, -0.0572]],\n",
      "       requires_grad=True)\n",
      "Wup spliced: torch.Size([4, 16])\n",
      "tensor([[-0.2741,  0.2815,  0.1217,  0.0877, -0.2717, -0.2743, -0.3520,  0.3191,\n",
      "         -0.0347, -0.1266,  0.3024,  0.2227,  0.0548,  0.0727, -0.0612,  0.2773],\n",
      "        [-0.1066,  0.0185,  0.1244,  0.1343, -0.0040,  0.3245, -0.1219,  0.1535,\n",
      "          0.1939,  0.1921,  0.0743,  0.1904, -0.2701, -0.2174,  0.1159,  0.1370],\n",
      "        [-0.2546,  0.2375, -0.1895,  0.2369, -0.1226, -0.1657,  0.2568, -0.2030,\n",
      "         -0.0508,  0.2762,  0.3489,  0.2658,  0.0290,  0.2453, -0.2461, -0.1030],\n",
      "        [-0.1050,  0.0765, -0.3351, -0.0987,  0.1895, -0.1666,  0.0256, -0.1588,\n",
      "          0.2778, -0.2298,  0.2139, -0.1324, -0.2248,  0.2687, -0.3314,  0.0593]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Bup: torch.Size([32])\n",
      "Parameter containing:\n",
      "tensor([-0.1016, -0.2534, -0.1299,  0.1768,  0.1770,  0.3366,  0.0021, -0.2485,\n",
      "        -0.0871,  0.1780, -0.1682,  0.0282, -0.2293,  0.1580,  0.0905,  0.0484,\n",
      "        -0.0778, -0.1093, -0.2433,  0.2298,  0.0434,  0.3038,  0.1871, -0.2052,\n",
      "         0.1974, -0.1770,  0.2445,  0.2494,  0.0239,  0.1323, -0.1185, -0.2071],\n",
      "       requires_grad=True)\n",
      "Bup spliced: torch.Size([16])\n",
      "tensor([-0.1016, -0.2534, -0.1299,  0.1768,  0.1770,  0.3366,  0.0021, -0.2485,\n",
      "        -0.0871,  0.1780, -0.1682,  0.0282, -0.2293,  0.1580,  0.0905,  0.0484],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xup: torch.Size([1, 3, 16])\n",
      "tensor([[[-0.6047,  0.0762, -0.3222,  0.4178,  0.2089,  0.4837, -0.2047,\n",
      "          -0.1271,  0.4822,  0.2946,  0.4526,  0.3906, -0.8622,  0.2376,\n",
      "          -0.2122,  0.4028],\n",
      "         [-0.4630,  0.0496, -0.5757,  0.0328,  0.3400, -0.0782, -0.3091,\n",
      "          -0.2122,  0.4396, -0.3692,  0.3842, -0.0691, -0.6529,  0.6028,\n",
      "          -0.4782,  0.4217],\n",
      "         [-0.3831, -0.0585, -0.6127,  0.0206,  0.4656,  0.1103, -0.2033,\n",
      "          -0.3076,  0.5275, -0.2902,  0.2904, -0.1240, -0.7649,  0.5303,\n",
      "          -0.4422,  0.3531]]], grad_fn=<AddBackward0>)\n",
      "Xfuse: torch.Size([1, 3, 16])\n",
      "tensor([[[ 0.0436,  0.0257,  0.0537,  0.0066, -0.0048,  0.0818, -0.1348,\n",
      "          -0.0198,  0.1210, -0.0129,  0.0033, -0.0660, -0.1834,  0.0080,\n",
      "           0.0301,  0.0676],\n",
      "         [ 0.0781,  0.0238,  0.0767, -0.0055,  0.0527, -0.0120, -0.1463,\n",
      "          -0.0421,  0.0423, -0.1369,  0.0149,  0.0117, -0.2570,  0.0562,\n",
      "           0.0698, -0.0516],\n",
      "         [ 0.0626, -0.0277,  0.0870, -0.0033,  0.0546,  0.0085, -0.0990,\n",
      "          -0.0376,  0.1355, -0.0640, -0.0055,  0.0208, -0.2766,  0.1155,\n",
      "           0.0711, -0.0353]]], grad_fn=<MulBackward0>)\n",
      "Wdown: torch.Size([32, 8])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0473,  0.1235, -0.1102, -0.0375, -0.1133, -0.0770, -0.1064, -0.0330],\n",
      "        [-0.0287,  0.0457,  0.0069,  0.1272,  0.0748, -0.0811, -0.1361, -0.1531],\n",
      "        [-0.1096,  0.0910,  0.0326, -0.1673,  0.1398,  0.1678,  0.1109,  0.0877],\n",
      "        [ 0.1565, -0.1375, -0.0285, -0.1266, -0.0362, -0.0806, -0.0367, -0.1338],\n",
      "        [ 0.0710, -0.0441,  0.0559,  0.0260,  0.1312,  0.1407, -0.1023, -0.0081],\n",
      "        [-0.1081,  0.0265,  0.1537, -0.1478,  0.0735,  0.0461,  0.1466,  0.0423],\n",
      "        [-0.1003, -0.0601,  0.0528, -0.0520, -0.1710, -0.1743,  0.0357, -0.0873],\n",
      "        [-0.1124, -0.0615,  0.0649,  0.1213, -0.1697, -0.0526, -0.0416,  0.0824],\n",
      "        [-0.1744,  0.1031,  0.0367,  0.1270, -0.0054,  0.1730, -0.0651,  0.1065],\n",
      "        [ 0.0236, -0.0673,  0.0687, -0.0888, -0.1361,  0.0724,  0.0034, -0.1494],\n",
      "        [-0.0698, -0.0784,  0.1745,  0.1535, -0.1044, -0.1615,  0.0744,  0.0435],\n",
      "        [-0.0713,  0.1205, -0.1408, -0.0227,  0.1498, -0.0410, -0.0155,  0.0219],\n",
      "        [ 0.0714, -0.1331, -0.0717, -0.1638,  0.1637,  0.1095, -0.0324,  0.1044],\n",
      "        [ 0.1438, -0.0525, -0.0794,  0.1380,  0.1675, -0.0392, -0.0612, -0.0817],\n",
      "        [ 0.0158, -0.1180, -0.0936,  0.0700, -0.1183, -0.1285, -0.0810, -0.1596],\n",
      "        [ 0.0509, -0.0220, -0.1671,  0.0873, -0.1675,  0.0842,  0.0319,  0.1239],\n",
      "        [ 0.1095,  0.1729,  0.0979, -0.1621, -0.1201,  0.1297,  0.0808,  0.1575],\n",
      "        [-0.0234,  0.0603,  0.1510, -0.0438, -0.0501,  0.1724, -0.0400, -0.0404],\n",
      "        [-0.0865,  0.1425, -0.0999,  0.0700, -0.0231, -0.0484,  0.1482, -0.1384],\n",
      "        [-0.1041, -0.1551,  0.1089, -0.1193, -0.0668,  0.1191,  0.0719, -0.1538],\n",
      "        [ 0.1484,  0.0256, -0.0782,  0.0375,  0.1525, -0.1406, -0.0895, -0.0899],\n",
      "        [-0.0840, -0.0121,  0.0255,  0.1297,  0.0404,  0.0009, -0.1766,  0.1277],\n",
      "        [-0.0165, -0.0104,  0.1346,  0.1708,  0.0951, -0.0653, -0.1205, -0.1424],\n",
      "        [ 0.1084, -0.1356, -0.1426,  0.0181,  0.1173, -0.0395, -0.0956, -0.0881],\n",
      "        [-0.1367,  0.1153,  0.0622,  0.1434,  0.1043, -0.1081,  0.0650,  0.1592],\n",
      "        [ 0.0798,  0.1334, -0.0248, -0.1480, -0.1396, -0.0766, -0.0004,  0.0954],\n",
      "        [-0.1069,  0.1288, -0.0145,  0.0695,  0.1269,  0.0430, -0.0470, -0.0610],\n",
      "        [-0.0561, -0.0017, -0.1366,  0.1436,  0.0357, -0.1237,  0.0277,  0.1232],\n",
      "        [-0.1172, -0.1593,  0.1175,  0.0813, -0.1101, -0.1290,  0.0258,  0.0318],\n",
      "        [-0.0106,  0.1591,  0.0744,  0.1157, -0.0299,  0.0745,  0.0492, -0.0202],\n",
      "        [ 0.0724, -0.1449, -0.0420, -0.0794, -0.0891,  0.0636, -0.0945, -0.1602],\n",
      "        [ 0.0900,  0.1031, -0.1242, -0.1678, -0.0711,  0.0918,  0.1515, -0.0773]],\n",
      "       requires_grad=True)\n",
      "Wdown spliced: torch.Size([16, 4])\n",
      "tensor([[ 0.0473,  0.1235, -0.1102, -0.0375],\n",
      "        [-0.0287,  0.0457,  0.0069,  0.1272],\n",
      "        [-0.1096,  0.0910,  0.0326, -0.1673],\n",
      "        [ 0.1565, -0.1375, -0.0285, -0.1266],\n",
      "        [ 0.0710, -0.0441,  0.0559,  0.0260],\n",
      "        [-0.1081,  0.0265,  0.1537, -0.1478],\n",
      "        [-0.1003, -0.0601,  0.0528, -0.0520],\n",
      "        [-0.1124, -0.0615,  0.0649,  0.1213],\n",
      "        [-0.1744,  0.1031,  0.0367,  0.1270],\n",
      "        [ 0.0236, -0.0673,  0.0687, -0.0888],\n",
      "        [-0.0698, -0.0784,  0.1745,  0.1535],\n",
      "        [-0.0713,  0.1205, -0.1408, -0.0227],\n",
      "        [ 0.0714, -0.1331, -0.0717, -0.1638],\n",
      "        [ 0.1438, -0.0525, -0.0794,  0.1380],\n",
      "        [ 0.0158, -0.1180, -0.0936,  0.0700],\n",
      "        [ 0.0509, -0.0220, -0.1671,  0.0873]], grad_fn=<SliceBackward0>)\n",
      "Bdown: torch.Size([8])\n",
      "Parameter containing:\n",
      "tensor([ 0.0777, -0.1183, -0.0661, -0.1587,  0.0918, -0.1118,  0.0936, -0.0436],\n",
      "       requires_grad=True)\n",
      "Bdown spliced: torch.Size([4])\n",
      "tensor([ 0.0777, -0.1183, -0.0661, -0.1587], grad_fn=<SliceBackward0>)\n",
      "outputs: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.0558, -0.0720, -0.0534, -0.1169],\n",
      "         [ 0.0716, -0.0531, -0.0721, -0.0954],\n",
      "         [ 0.0585, -0.0530, -0.0670, -0.0933]]], grad_fn=<AddBackward0>)\n",
      "------------- END MLP.forwardTensor() ------------\n",
      "layer's final residual state:\n",
      "tensor([[[ 0.2632,  0.7007,  0.1505,  0.4112],\n",
      "         [ 0.6213,  0.0790, -0.2441,  1.1945],\n",
      "         [ 0.2182,  0.1420, -0.1808,  0.7214]]], grad_fn=<AddBackward0>)\n",
      "----------------- END Layer.forwardTensor() --------------------\n",
      "y: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.2632,  0.7007,  0.1505,  0.4112],\n",
      "         [ 0.6213,  0.0790, -0.2441,  1.1945],\n",
      "         [ 0.2182,  0.1420, -0.1808,  0.7214]]], grad_fn=<AddBackward0>)\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the second sub-model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[0.1329, 0.8152, 0.6673, 0.1014],\n",
      "         [0.2244, 0.0543, 0.4245, 0.8154],\n",
      "         [0.7445, 0.9465, 0.4489, 0.6016]]])\n",
      "---------- Layer Input: torch.Tensor ------------\n",
      "----------------- Layer.forwardTensor() --------------------\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[0.1329, 0.8152, 0.6673, 0.1014],\n",
      "         [0.2244, 0.0543, 0.4245, 0.8154],\n",
      "         [0.7445, 0.9465, 0.4489, 0.6016]]])\n",
      "normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[0.2491, 1.5285, 1.2512, 0.1901],\n",
      "         [0.4735, 0.1147, 0.8957, 1.7206],\n",
      "         [1.0493, 1.3341, 0.6327, 0.8479]]])\n",
      "dim: 4\n",
      "skip: 4\n",
      "spliced scale: torch.Size([4])\n",
      "tensor([0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[0.2491, 1.5285, 1.2512, 0.1901],\n",
      "         [0.4735, 0.1147, 0.8957, 1.7206],\n",
      "         [1.0493, 1.3341, 0.6327, 0.8479]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "---------- Attention Input: torch.Tensor ------------\n",
      "----------------- MultiQueryAttention.forwardTensor() --------------------\n",
      "x shape: torch.Size([1, 3, 4])\n",
      "d_skip: 4\n",
      "models_in_this_level: 2\n",
      "h_dim: 2\n",
      "h_skip: 2\n",
      "self.Wqkv: torch.Size([8, 16])\n",
      "Parameter containing:\n",
      "tensor([[-0.3180, -0.2298,  0.2394, -0.0406,  0.3116, -0.2900,  0.0732,  0.2944,\n",
      "          0.2225, -0.2598,  0.2105, -0.0564,  0.0199,  0.2348,  0.1857, -0.3292],\n",
      "        [ 0.1221, -0.2723, -0.0068, -0.0340, -0.1364,  0.0060, -0.0703,  0.3486,\n",
      "          0.1857, -0.2839, -0.2247, -0.2803,  0.0521,  0.2406, -0.2891,  0.2842],\n",
      "        [-0.0073, -0.3272,  0.2769, -0.1216, -0.3079, -0.3335, -0.2090, -0.2900,\n",
      "          0.0197, -0.2950,  0.2162, -0.1412,  0.3115, -0.3253,  0.0937,  0.0917],\n",
      "        [ 0.0047, -0.0826,  0.0321, -0.2232,  0.1569, -0.0200, -0.2494,  0.1485,\n",
      "         -0.0656,  0.0781,  0.2033,  0.3080,  0.1552,  0.0566,  0.0580,  0.0666],\n",
      "        [-0.2025,  0.1437,  0.0865,  0.2081, -0.1146, -0.1350,  0.2895,  0.2513,\n",
      "         -0.0631, -0.0008,  0.2061,  0.0420, -0.1135, -0.3426,  0.3450,  0.2688],\n",
      "        [-0.2876,  0.0427, -0.2301, -0.3294, -0.2758,  0.1664,  0.2798,  0.1099,\n",
      "          0.0246, -0.0450,  0.1345, -0.0045, -0.1210,  0.0405, -0.3373, -0.1184],\n",
      "        [ 0.3248, -0.2282, -0.0875,  0.2812, -0.1387, -0.0023, -0.3528, -0.0408,\n",
      "          0.1690, -0.1210, -0.3479,  0.1314, -0.2750,  0.0950,  0.1679,  0.3077],\n",
      "        [-0.0178,  0.1660, -0.0772, -0.2709, -0.0638, -0.2676,  0.0476, -0.0952,\n",
      "         -0.0715,  0.2205,  0.3363,  0.1248, -0.2727,  0.0172, -0.2753, -0.0190]],\n",
      "       requires_grad=True)\n",
      "Wq: torch.Size([8, 8])\n",
      "tensor([[-0.3180, -0.2298,  0.2394, -0.0406,  0.3116, -0.2900,  0.0732,  0.2944],\n",
      "        [ 0.1221, -0.2723, -0.0068, -0.0340, -0.1364,  0.0060, -0.0703,  0.3486],\n",
      "        [-0.0073, -0.3272,  0.2769, -0.1216, -0.3079, -0.3335, -0.2090, -0.2900],\n",
      "        [ 0.0047, -0.0826,  0.0321, -0.2232,  0.1569, -0.0200, -0.2494,  0.1485],\n",
      "        [-0.2025,  0.1437,  0.0865,  0.2081, -0.1146, -0.1350,  0.2895,  0.2513],\n",
      "        [-0.2876,  0.0427, -0.2301, -0.3294, -0.2758,  0.1664,  0.2798,  0.1099],\n",
      "        [ 0.3248, -0.2282, -0.0875,  0.2812, -0.1387, -0.0023, -0.3528, -0.0408],\n",
      "        [-0.0178,  0.1660, -0.0772, -0.2709, -0.0638, -0.2676,  0.0476, -0.0952]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wk: torch.Size([8, 4])\n",
      "tensor([[ 0.2225, -0.2598,  0.2105, -0.0564],\n",
      "        [ 0.1857, -0.2839, -0.2247, -0.2803],\n",
      "        [ 0.0197, -0.2950,  0.2162, -0.1412],\n",
      "        [-0.0656,  0.0781,  0.2033,  0.3080],\n",
      "        [-0.0631, -0.0008,  0.2061,  0.0420],\n",
      "        [ 0.0246, -0.0450,  0.1345, -0.0045],\n",
      "        [ 0.1690, -0.1210, -0.3479,  0.1314],\n",
      "        [-0.0715,  0.2205,  0.3363,  0.1248]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wv: torch.Size([8, 4])\n",
      "tensor([[ 0.0199,  0.2348,  0.1857, -0.3292],\n",
      "        [ 0.0521,  0.2406, -0.2891,  0.2842],\n",
      "        [ 0.3115, -0.3253,  0.0937,  0.0917],\n",
      "        [ 0.1552,  0.0566,  0.0580,  0.0666],\n",
      "        [-0.1135, -0.3426,  0.3450,  0.2688],\n",
      "        [-0.1210,  0.0405, -0.3373, -0.1184],\n",
      "        [-0.2750,  0.0950,  0.1679,  0.3077],\n",
      "        [-0.2727,  0.0172, -0.2753, -0.0190]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wq spliced: torch.Size([4, 4])\n",
      "tensor([[ 0.0865,  0.2081,  0.2895,  0.2513],\n",
      "        [-0.2301, -0.3294,  0.2798,  0.1099],\n",
      "        [-0.0875,  0.2812, -0.3528, -0.0408],\n",
      "        [-0.0772, -0.2709,  0.0476, -0.0952]], grad_fn=<CatBackward0>)\n",
      "Wk spliced: torch.Size([4, 2])\n",
      "tensor([[ 0.2061,  0.0420],\n",
      "        [ 0.1345, -0.0045],\n",
      "        [-0.3479,  0.1314],\n",
      "        [ 0.3363,  0.1248]], grad_fn=<CatBackward0>)\n",
      "Wv spliced: torch.Size([4, 2])\n",
      "tensor([[ 0.3450,  0.2688],\n",
      "        [-0.3373, -0.1184],\n",
      "        [ 0.1679,  0.3077],\n",
      "        [-0.2753, -0.0190]], grad_fn=<CatBackward0>)\n",
      "Wqkv_spliced: torch.Size([4, 8])\n",
      "tensor([[ 0.0865,  0.2081,  0.2895,  0.2513,  0.2061,  0.0420,  0.3450,  0.2688],\n",
      "        [-0.2301, -0.3294,  0.2798,  0.1099,  0.1345, -0.0045, -0.3373, -0.1184],\n",
      "        [-0.0875,  0.2812, -0.3528, -0.0408, -0.3479,  0.1314,  0.1679,  0.3077],\n",
      "        [-0.0772, -0.2709,  0.0476, -0.0952,  0.3363,  0.1248, -0.2753, -0.0190]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "xqkv: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.4542, -0.1513,  0.0674,  0.1614, -0.1145,  0.1918, -0.2719,\n",
      "           0.2673],\n",
      "         [-0.1967, -0.1534, -0.0649, -0.0687,  0.3801,  0.3518, -0.1987,\n",
      "           0.3566],\n",
      "         [-0.3370, -0.2728,  0.4942,  0.3038,  0.4608,  0.2270, -0.2152,\n",
      "           0.3026]]], grad_fn=<UnsafeViewBackward0>)\n",
      "xq: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.4542, -0.1513,  0.0674,  0.1614],\n",
      "         [-0.1967, -0.1534, -0.0649, -0.0687],\n",
      "         [-0.3370, -0.2728,  0.4942,  0.3038]]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "xk: torch.Size([1, 3, 2])\n",
      "tensor([[[-0.1145,  0.1918],\n",
      "         [ 0.3801,  0.3518],\n",
      "         [ 0.4608,  0.2270]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xv: torch.Size([1, 3, 2])\n",
      "tensor([[[-0.2719,  0.2673],\n",
      "         [-0.1987,  0.3566],\n",
      "         [-0.2152,  0.3026]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xq reshaped: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[-0.4542, -0.1513],\n",
      "          [ 0.0674,  0.1614]],\n",
      "\n",
      "         [[-0.1967, -0.1534],\n",
      "          [-0.0649, -0.0687]],\n",
      "\n",
      "         [[-0.3370, -0.2728],\n",
      "          [ 0.4942,  0.3038]]]], grad_fn=<ViewBackward0>)\n",
      "xk reshaped: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[-0.1145,  0.1918]],\n",
      "\n",
      "         [[ 0.3801,  0.3518]],\n",
      "\n",
      "         [[ 0.4608,  0.2270]]]], grad_fn=<ViewBackward0>)\n",
      "xv reshaped: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[-0.2719,  0.2673]],\n",
      "\n",
      "         [[-0.1987,  0.3566]],\n",
      "\n",
      "         [[-0.2152,  0.3026]]]], grad_fn=<ViewBackward0>)\n",
      "rotated xq: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[-0.4542, -0.1513],\n",
      "          [ 0.0674,  0.1614]],\n",
      "\n",
      "         [[ 0.0228, -0.2484],\n",
      "          [ 0.0228, -0.0917]],\n",
      "\n",
      "         [[ 0.3883, -0.1929],\n",
      "          [-0.4819,  0.3229]]]], grad_fn=<TransposeBackward0>)\n",
      "rotated xk: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[-0.1145,  0.1918]],\n",
      "\n",
      "         [[-0.0907,  0.5100]],\n",
      "\n",
      "         [[-0.3982,  0.3245]]]], grad_fn=<TransposeBackward0>)\n",
      "repeat_interleaved xk: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[-0.1145,  0.1918],\n",
      "          [-0.1145,  0.1918]],\n",
      "\n",
      "         [[-0.0907,  0.5100],\n",
      "          [-0.0907,  0.5100]],\n",
      "\n",
      "         [[-0.3982,  0.3245],\n",
      "          [-0.3982,  0.3245]]]], grad_fn=<ViewBackward0>)\n",
      "repeat_interleaved xv: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[-0.2719,  0.2673],\n",
      "          [-0.2719,  0.2673]],\n",
      "\n",
      "         [[-0.1987,  0.3566],\n",
      "          [-0.1987,  0.3566]],\n",
      "\n",
      "         [[-0.2152,  0.3026],\n",
      "          [-0.2152,  0.3026]]]], grad_fn=<ViewBackward0>)\n",
      "transposed xq: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[-0.4542, -0.1513],\n",
      "          [ 0.0228, -0.2484],\n",
      "          [ 0.3883, -0.1929]],\n",
      "\n",
      "         [[ 0.0674,  0.1614],\n",
      "          [ 0.0228, -0.0917],\n",
      "          [-0.4819,  0.3229]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xk: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[-0.1145,  0.1918],\n",
      "          [-0.0907,  0.5100],\n",
      "          [-0.3982,  0.3245]],\n",
      "\n",
      "         [[-0.1145,  0.1918],\n",
      "          [-0.0907,  0.5100],\n",
      "          [-0.3982,  0.3245]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xv: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[-0.2719,  0.2673],\n",
      "          [-0.1987,  0.3566],\n",
      "          [-0.2152,  0.3026]],\n",
      "\n",
      "         [[-0.2719,  0.2673],\n",
      "          [-0.1987,  0.3566],\n",
      "          [-0.2152,  0.3026]]]], grad_fn=<TransposeBackward0>)\n",
      "scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[ 0.0162, -0.0254,  0.0932],\n",
      "          [-0.0355, -0.0910, -0.0634],\n",
      "          [-0.0576, -0.0944, -0.1536]],\n",
      "\n",
      "         [[ 0.0164,  0.0539,  0.0181],\n",
      "          [-0.0143, -0.0345, -0.0275],\n",
      "          [ 0.0828,  0.1473,  0.2098]]]], grad_fn=<MulBackward0>)\n",
      "mask: torch.Size([1, 1, 3, 3])\n",
      "tensor([[[[ 0.0000e+00, -2.3820e+38, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]]]])\n",
      "masked scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[ 1.6248e-02, -2.3820e+38, -2.3820e+38],\n",
      "          [-3.5524e-02, -9.1016e-02, -2.3820e+38],\n",
      "          [-5.7584e-02, -9.4445e-02, -1.5358e-01]],\n",
      "\n",
      "         [[ 1.6440e-02, -2.3820e+38, -2.3820e+38],\n",
      "          [-1.4284e-02, -3.4542e-02, -2.3820e+38],\n",
      "          [ 8.2794e-02,  1.4734e-01,  2.0977e-01]]]], grad_fn=<AddBackward0>)\n",
      "softmaxed scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [0.5139, 0.4861, 0.0000],\n",
      "          [0.3482, 0.3356, 0.3163]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.5051, 0.4949, 0.0000],\n",
      "          [0.3123, 0.3331, 0.3546]]]], grad_fn=<SoftmaxBackward0>)\n",
      "attention: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[-0.2719,  0.2673],\n",
      "          [-0.2363,  0.3107],\n",
      "          [-0.2294,  0.3084]],\n",
      "\n",
      "         [[-0.2719,  0.2673],\n",
      "          [-0.2356,  0.3115],\n",
      "          [-0.2274,  0.3096]]]], grad_fn=<UnsafeViewBackward0>)\n",
      "reshaped attention: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.2719,  0.2673, -0.2719,  0.2673],\n",
      "         [-0.2363,  0.3107, -0.2356,  0.3115],\n",
      "         [-0.2294,  0.3084, -0.2274,  0.3096]]], grad_fn=<ViewBackward0>)\n",
      "self.Wo: torch.Size([8, 8])\n",
      "Parameter containing:\n",
      "tensor([[ 0.3285, -0.3064, -0.1306,  0.0588, -0.2345,  0.3499,  0.1323,  0.2803],\n",
      "        [-0.1039, -0.2912,  0.0454,  0.0270, -0.0493, -0.0976,  0.0227,  0.0316],\n",
      "        [ 0.3188, -0.1908,  0.1960, -0.0699,  0.1288, -0.2272, -0.2825, -0.2161],\n",
      "        [-0.3428,  0.1783,  0.2891,  0.1227, -0.1147, -0.0046, -0.2799, -0.1064],\n",
      "        [-0.1004,  0.0729,  0.0627, -0.0127, -0.0499,  0.2860,  0.1285, -0.1399],\n",
      "        [-0.0603, -0.0101,  0.0673,  0.0517,  0.1197,  0.0895, -0.2443, -0.0871],\n",
      "        [-0.0014,  0.1847,  0.2436,  0.0846, -0.1110, -0.2070,  0.0763, -0.1582],\n",
      "        [-0.2047, -0.1637, -0.1253,  0.2090,  0.2312, -0.0241,  0.3393, -0.1178]],\n",
      "       requires_grad=True)\n",
      "spliced Wo: torch.Size([4, 4])\n",
      "tensor([[ 0.1288, -0.2272, -0.2825, -0.2161],\n",
      "        [-0.1147, -0.0046, -0.2799, -0.1064],\n",
      "        [-0.1110, -0.2070,  0.0763, -0.1582],\n",
      "        [ 0.2312, -0.0241,  0.3393, -0.1178]], grad_fn=<CatBackward0>)\n",
      "projected output: torch.Size([1, 3, 4])\n",
      "tensor([[[0.0263, 0.1104, 0.0720, 0.0418],\n",
      "         [0.0321, 0.0935, 0.0675, 0.0186],\n",
      "         [0.0319, 0.0903, 0.0662, 0.0163]]], grad_fn=<UnsafeViewBackward0>)\n",
      "----------------- END MultiQueryAttention.forwardTensor() --------------------\n",
      "x in layer after MQA & resid connection and before MLP:\n",
      "tensor([[[0.1592, 0.9256, 0.7393, 0.1432],\n",
      "         [0.2565, 0.1478, 0.4920, 0.8340],\n",
      "         [0.7764, 1.0368, 0.5150, 0.6178]]], grad_fn=<AddBackward0>)\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[0.1592, 0.9256, 0.7393, 0.1432],\n",
      "         [0.2565, 0.1478, 0.4920, 0.8340],\n",
      "         [0.7764, 1.0368, 0.5150, 0.6178]]], grad_fn=<AddBackward0>)\n",
      "normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[0.2645, 1.5378, 1.2283, 0.2379],\n",
      "         [0.5067, 0.2920, 0.9717, 1.6473],\n",
      "         [1.0184, 1.3600, 0.6756, 0.8104]]], grad_fn=<MulBackward0>)\n",
      "dim: 4\n",
      "skip: 4\n",
      "spliced scale: torch.Size([4])\n",
      "tensor([0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[0.2645, 1.5378, 1.2283, 0.2379],\n",
      "         [0.5067, 0.2920, 0.9717, 1.6473],\n",
      "         [1.0184, 1.3600, 0.6756, 0.8104]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "---------- MLP Input: torch.Tensor ------------\n",
      "------------- MLP.forwardTensor() ------------\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[0.2645, 1.5378, 1.2283, 0.2379],\n",
      "         [0.5067, 0.2920, 0.9717, 1.6473],\n",
      "         [1.0184, 1.3600, 0.6756, 0.8104]]], grad_fn=<MulBackward0>)\n",
      "d_dim: 4\n",
      "d_skip: 4\n",
      "i_dim: 16\n",
      "i_skip: 16\n",
      "Wgate: torch.Size([8, 32])\n",
      "Parameter containing:\n",
      "tensor([[-0.0123,  0.0399,  0.2737, -0.0922,  0.2326, -0.2386,  0.0074, -0.2040,\n",
      "         -0.2200,  0.2602, -0.1753, -0.1695, -0.2042, -0.3373, -0.0937,  0.0463,\n",
      "          0.0045, -0.2728, -0.2387,  0.1078,  0.1801, -0.1131, -0.2452,  0.2643,\n",
      "         -0.0839, -0.1348,  0.2126, -0.3243, -0.0532, -0.0840,  0.0606,  0.2499],\n",
      "        [ 0.0708, -0.2579, -0.2040,  0.0150,  0.3406, -0.1710, -0.3415,  0.0488,\n",
      "          0.2793,  0.1092,  0.0696, -0.2679,  0.1083, -0.2105, -0.1895,  0.0646,\n",
      "         -0.1087, -0.2687, -0.2836, -0.2841, -0.2841,  0.0205, -0.1661,  0.0359,\n",
      "         -0.1298,  0.2178,  0.2648,  0.2243, -0.3007,  0.1049,  0.1012, -0.2634],\n",
      "        [-0.0451,  0.1804,  0.0738, -0.1978, -0.1160,  0.0704, -0.0078, -0.0997,\n",
      "         -0.2761,  0.2511, -0.1442,  0.3469, -0.2799, -0.1629, -0.2915,  0.1543,\n",
      "         -0.2025, -0.0540,  0.0400, -0.2612, -0.3474,  0.0753,  0.0305,  0.3130,\n",
      "         -0.2031,  0.1023,  0.0829, -0.1229,  0.1618, -0.0863, -0.0414,  0.2964],\n",
      "        [-0.0990,  0.2025,  0.2554,  0.1775,  0.3023, -0.3180,  0.3322,  0.2516,\n",
      "          0.0669,  0.3505, -0.2480,  0.0280,  0.1819, -0.0694,  0.1868, -0.3038,\n",
      "          0.1524,  0.1725, -0.2871, -0.2874,  0.0045,  0.2361, -0.1579,  0.3441,\n",
      "          0.2247, -0.0755,  0.0755,  0.1444,  0.1080,  0.0920, -0.0720,  0.2481],\n",
      "        [ 0.0953,  0.0398, -0.0336,  0.2852,  0.1679, -0.3385,  0.3004, -0.0264,\n",
      "          0.0726, -0.0664,  0.1595, -0.2588,  0.3464, -0.0341, -0.2582, -0.1611,\n",
      "         -0.2152,  0.1497, -0.1518,  0.1597,  0.0397, -0.2955, -0.1551,  0.0553,\n",
      "          0.2388,  0.1206, -0.2682, -0.2157,  0.0973,  0.2551,  0.2388,  0.0305],\n",
      "        [-0.3138, -0.0718, -0.2607, -0.3495, -0.1915,  0.3298,  0.1786,  0.0090,\n",
      "         -0.0537,  0.3528, -0.3090,  0.2082, -0.1929,  0.1923,  0.1655,  0.3327,\n",
      "         -0.2145, -0.2677,  0.3000, -0.0856,  0.3104,  0.3085,  0.1545, -0.3481,\n",
      "         -0.2612, -0.1064,  0.2002,  0.1089,  0.1859, -0.1043, -0.3479,  0.2676],\n",
      "        [ 0.3435,  0.0523,  0.2740,  0.0377, -0.2029, -0.1517,  0.1594,  0.1523,\n",
      "          0.2819,  0.2414, -0.0496,  0.1429,  0.1345, -0.1205, -0.2066,  0.0643,\n",
      "         -0.0667,  0.2367, -0.0062,  0.0377, -0.3493, -0.2505,  0.2509,  0.0128,\n",
      "         -0.0853,  0.0381, -0.1851, -0.0082,  0.0408,  0.1538,  0.1655, -0.0942],\n",
      "        [ 0.1087,  0.0446,  0.2749,  0.1700,  0.2710,  0.1495, -0.1165,  0.3255,\n",
      "         -0.1696, -0.0233, -0.1835, -0.3453,  0.2808,  0.1157, -0.1471,  0.0245,\n",
      "         -0.1898,  0.1417, -0.1789, -0.2999, -0.2019, -0.0572,  0.0152,  0.2197,\n",
      "          0.0415,  0.1307,  0.0372,  0.0279, -0.3509,  0.2701,  0.0842,  0.2271]],\n",
      "       requires_grad=True)\n",
      "Wgate spliced: torch.Size([4, 16])\n",
      "tensor([[-0.2152,  0.1497, -0.1518,  0.1597,  0.0397, -0.2955, -0.1551,  0.0553,\n",
      "          0.2388,  0.1206, -0.2682, -0.2157,  0.0973,  0.2551,  0.2388,  0.0305],\n",
      "        [-0.2145, -0.2677,  0.3000, -0.0856,  0.3104,  0.3085,  0.1545, -0.3481,\n",
      "         -0.2612, -0.1064,  0.2002,  0.1089,  0.1859, -0.1043, -0.3479,  0.2676],\n",
      "        [-0.0667,  0.2367, -0.0062,  0.0377, -0.3493, -0.2505,  0.2509,  0.0128,\n",
      "         -0.0853,  0.0381, -0.1851, -0.0082,  0.0408,  0.1538,  0.1655, -0.0942],\n",
      "        [-0.1898,  0.1417, -0.1789, -0.2999, -0.2019, -0.0572,  0.0152,  0.2197,\n",
      "          0.0415,  0.1307,  0.0372,  0.0279, -0.3509,  0.2701,  0.0842,  0.2271]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Bgate: torch.Size([32])\n",
      "Parameter containing:\n",
      "tensor([-0.2523, -0.0054,  0.3364, -0.0016, -0.0189,  0.1886,  0.3378, -0.1385,\n",
      "         0.2802, -0.0850,  0.0674, -0.0764,  0.2464,  0.0704,  0.2597, -0.1181,\n",
      "         0.1304,  0.0681,  0.1249,  0.3535, -0.2234, -0.1917, -0.0909, -0.2048,\n",
      "         0.1909, -0.0279, -0.3135,  0.2082, -0.1201,  0.1844,  0.1302, -0.0874],\n",
      "       requires_grad=True)\n",
      "Bgate spliced: torch.Size([16])\n",
      "tensor([ 0.1304,  0.0681,  0.1249,  0.3535, -0.2234, -0.1917, -0.0909, -0.2048,\n",
      "         0.1909, -0.0279, -0.3135,  0.2082, -0.1201,  0.1844,  0.1302, -0.0874],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xgate: torch.Size([1, 3, 16])\n",
      "tensor([[[-0.3834,  0.0205,  0.4958,  0.2391, -0.2126, -0.1167,  0.4174,\n",
      "          -0.6575, -0.2425, -0.0817, -0.2951,  0.3152,  0.1581,  0.3447,\n",
      "          -0.1182,  0.2705],\n",
      "         [-0.4188,  0.5291, -0.1651, -0.0479, -0.7847, -0.5890,  0.1445,\n",
      "           0.0958,  0.2211,  0.2544, -0.5095,  0.1686, -0.5550,  0.8776,\n",
      "           0.4491,  0.2888],\n",
      "         [-0.5794,  0.1312,  0.2291,  0.1822, -0.1605, -0.2886,  0.1431,\n",
      "          -0.4353,  0.0549,  0.0819, -0.4093,  0.1537, -0.0251,  0.6251,\n",
      "           0.0803,  0.4280]]], grad_fn=<AddBackward0>)\n",
      "GeLU'ed Xgate: torch.Size([1, 3, 16])\n",
      "tensor([[[-0.1345,  0.0104,  0.3421,  0.1421, -0.0884, -0.0529,  0.2763,\n",
      "          -0.1679, -0.0980, -0.0382, -0.1133,  0.1966,  0.0890,  0.2188,\n",
      "          -0.0536,  0.1641],\n",
      "         [-0.1414,  0.3713, -0.0717, -0.0230, -0.1697, -0.1637,  0.0805,\n",
      "           0.0516,  0.1299,  0.1527, -0.1555,  0.0956, -0.1606,  0.7107,\n",
      "           0.3024,  0.1772],\n",
      "         [-0.1629,  0.0725,  0.1353,  0.1043, -0.0700, -0.1115,  0.0797,\n",
      "          -0.1444,  0.0287,  0.0436, -0.1396,  0.0862, -0.0123,  0.4589,\n",
      "           0.0427,  0.2849]]], grad_fn=<GeluBackward0>)\n",
      "Wup: torch.Size([8, 32])\n",
      "Parameter containing:\n",
      "tensor([[ 0.2358, -0.1403,  0.2393,  0.2526, -0.2650, -0.3198, -0.2216, -0.2922,\n",
      "         -0.1289, -0.1763, -0.0831,  0.2275, -0.2880,  0.0025,  0.2747,  0.1729,\n",
      "         -0.3300,  0.0440, -0.2729,  0.1327, -0.2958, -0.3413, -0.0881,  0.3330,\n",
      "          0.3174,  0.1021, -0.3336,  0.1742,  0.1745, -0.1895, -0.1841,  0.0152],\n",
      "        [-0.2127,  0.1712,  0.1179, -0.0233,  0.2180, -0.1301,  0.2202, -0.0622,\n",
      "          0.0051, -0.1561, -0.3121, -0.1475,  0.2911, -0.0579, -0.1021,  0.3257,\n",
      "         -0.2662, -0.2710, -0.0334, -0.1764,  0.1517, -0.1846,  0.1131,  0.2174,\n",
      "          0.0933, -0.1199, -0.2415, -0.2758, -0.2917, -0.0841, -0.3052, -0.1812],\n",
      "        [ 0.0841,  0.1479, -0.0375, -0.2485, -0.1254, -0.2930, -0.2682,  0.0313,\n",
      "         -0.0252,  0.1428, -0.1984, -0.1256,  0.2242,  0.1453,  0.2510, -0.0734,\n",
      "         -0.2991,  0.0992,  0.3219, -0.2007,  0.2790,  0.2589,  0.0682,  0.0280,\n",
      "         -0.1925,  0.2399, -0.2976, -0.1620, -0.2682, -0.3102, -0.0270,  0.1858],\n",
      "        [-0.1396, -0.1651, -0.0708,  0.2799, -0.0584,  0.2085,  0.1478,  0.2344,\n",
      "         -0.0703, -0.1112, -0.0914,  0.2341,  0.3505, -0.1794, -0.1651, -0.0899,\n",
      "          0.1089, -0.0220, -0.2553,  0.2486,  0.1466, -0.0639, -0.0491, -0.3055,\n",
      "         -0.1759,  0.0763, -0.0022, -0.2897, -0.0467, -0.2293,  0.3417,  0.1666],\n",
      "        [ 0.3351, -0.1429,  0.3092,  0.0896,  0.2907, -0.0857,  0.0949, -0.1357,\n",
      "          0.1014, -0.3364,  0.1519, -0.1453,  0.0592, -0.0357,  0.3194, -0.0652,\n",
      "          0.0365, -0.0215, -0.0694, -0.2410,  0.2562,  0.2085,  0.1198,  0.3189,\n",
      "          0.1754,  0.0315,  0.2732,  0.2403, -0.0739,  0.1452,  0.0595,  0.3404],\n",
      "        [-0.3388,  0.2709,  0.2617,  0.3526,  0.1926, -0.1343, -0.0537,  0.0975,\n",
      "         -0.2693,  0.0304, -0.0552,  0.0584,  0.2879,  0.3318, -0.2337,  0.1410,\n",
      "          0.2661,  0.2017, -0.1308, -0.0523,  0.0783, -0.1951,  0.2207,  0.3311,\n",
      "          0.0290, -0.2545, -0.0913, -0.3415,  0.2820,  0.2142,  0.0662, -0.0879],\n",
      "        [ 0.1438, -0.0594, -0.1599,  0.2500, -0.3304, -0.2723, -0.3039,  0.1956,\n",
      "          0.3299,  0.2947,  0.3478, -0.0011, -0.1364,  0.2165, -0.1648, -0.3256,\n",
      "         -0.0138, -0.2982,  0.0678, -0.1502, -0.1609, -0.3300,  0.3203, -0.2436,\n",
      "         -0.3510, -0.2633,  0.1143,  0.1402, -0.2009,  0.1053,  0.0760, -0.0113],\n",
      "        [ 0.1037,  0.0288, -0.1326,  0.1594,  0.0525, -0.2789,  0.3052, -0.3113,\n",
      "          0.2751, -0.1184,  0.0321,  0.3326,  0.3353, -0.2007,  0.1972,  0.2288,\n",
      "         -0.2263,  0.2838, -0.1081, -0.0029,  0.1574,  0.0919, -0.0472,  0.0777,\n",
      "          0.1948, -0.1446, -0.2374,  0.0629,  0.1823, -0.2622,  0.2032,  0.1814]],\n",
      "       requires_grad=True)\n",
      "Wup spliced: torch.Size([4, 16])\n",
      "tensor([[ 0.0365, -0.0215, -0.0694, -0.2410,  0.2562,  0.2085,  0.1198,  0.3189,\n",
      "          0.1754,  0.0315,  0.2732,  0.2403, -0.0739,  0.1452,  0.0595,  0.3404],\n",
      "        [ 0.2661,  0.2017, -0.1308, -0.0523,  0.0783, -0.1951,  0.2207,  0.3311,\n",
      "          0.0290, -0.2545, -0.0913, -0.3415,  0.2820,  0.2142,  0.0662, -0.0879],\n",
      "        [-0.0138, -0.2982,  0.0678, -0.1502, -0.1609, -0.3300,  0.3203, -0.2436,\n",
      "         -0.3510, -0.2633,  0.1143,  0.1402, -0.2009,  0.1053,  0.0760, -0.0113],\n",
      "        [-0.2263,  0.2838, -0.1081, -0.0029,  0.1574,  0.0919, -0.0472,  0.0777,\n",
      "          0.1948, -0.1446, -0.2374,  0.0629,  0.1823, -0.2622,  0.2032,  0.1814]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Bup: torch.Size([32])\n",
      "Parameter containing:\n",
      "tensor([-0.3425, -0.3028,  0.0189,  0.3111, -0.2152,  0.0376, -0.1148, -0.3409,\n",
      "         0.1095, -0.3416,  0.3530, -0.3030, -0.3462,  0.3167, -0.1721, -0.0766,\n",
      "        -0.1396, -0.1110, -0.2723,  0.2469, -0.1721,  0.1306, -0.1114,  0.1328,\n",
      "         0.3360,  0.0578,  0.3154, -0.2715, -0.2146, -0.2518,  0.1358, -0.1442],\n",
      "       requires_grad=True)\n",
      "Bup spliced: torch.Size([16])\n",
      "tensor([-0.1396, -0.1110, -0.2723,  0.2469, -0.1721,  0.1306, -0.1114,  0.1328,\n",
      "         0.3360,  0.0578,  0.3154, -0.2715, -0.2146, -0.2518,  0.1358, -0.1442],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xup: torch.Size([1, 3, 16])\n",
      "tensor([[[ 0.2084, -0.1053, -0.4342, -0.0825, -0.1440, -0.4976,  0.6418,\n",
      "           0.4456,  0.0422, -0.6830,  0.3313, -0.5459, -0.0039,  0.1829,\n",
      "           0.3951, -0.1601],\n",
      "         [-0.4296,  0.1147, -0.4577, -0.0413,  0.0836,  0.0100,  0.2472,\n",
      "           0.2823,  0.4132, -0.4945,  0.1473, -0.0096, -0.0645, -0.4452,\n",
      "           0.5939,  0.2904],\n",
      "         [ 0.0668,  0.1699, -0.5626, -0.1735,  0.2142, -0.0708,  0.4888,\n",
      "           0.8063,  0.4748, -0.5512,  0.3544, -0.3455,  0.1057,  0.0461,\n",
      "           0.5026,  0.2222]]], grad_fn=<AddBackward0>)\n",
      "Xfuse: torch.Size([1, 3, 16])\n",
      "tensor([[[-0.0280, -0.0011, -0.1485, -0.0117,  0.0127,  0.0263,  0.1773,\n",
      "          -0.0748, -0.0041,  0.0261, -0.0375, -0.1073, -0.0003,  0.0400,\n",
      "          -0.0212, -0.0263],\n",
      "         [ 0.0608,  0.0426,  0.0328,  0.0010, -0.0142, -0.0016,  0.0199,\n",
      "           0.0146,  0.0537, -0.0755, -0.0229, -0.0009,  0.0104, -0.3164,\n",
      "           0.1796,  0.0515],\n",
      "         [-0.0109,  0.0123, -0.0761, -0.0181, -0.0150,  0.0079,  0.0389,\n",
      "          -0.1164,  0.0136, -0.0240, -0.0495, -0.0298, -0.0013,  0.0211,\n",
      "           0.0215,  0.0633]]], grad_fn=<MulBackward0>)\n",
      "Wdown: torch.Size([32, 8])\n",
      "Parameter containing:\n",
      "tensor([[-0.0377, -0.0074,  0.1619, -0.0312,  0.0496, -0.1705,  0.0924, -0.0219],\n",
      "        [-0.1569,  0.1763, -0.0100, -0.0792, -0.1206, -0.0886, -0.1080,  0.1747],\n",
      "        [ 0.0423, -0.1659, -0.0079, -0.0437,  0.0167,  0.1703, -0.0700, -0.1605],\n",
      "        [-0.1200,  0.1230, -0.0802, -0.1172, -0.1441,  0.0071,  0.0404, -0.0421],\n",
      "        [ 0.0549,  0.0827, -0.0237, -0.0367, -0.0094, -0.0758, -0.0045, -0.0135],\n",
      "        [-0.0988,  0.1456, -0.0578,  0.0501,  0.1615, -0.0282, -0.1321, -0.0877],\n",
      "        [-0.1694, -0.1303, -0.1694,  0.0470,  0.0008, -0.0524, -0.1159,  0.1063],\n",
      "        [-0.1278, -0.1308,  0.0007, -0.1518, -0.0878, -0.1274, -0.0501, -0.1564],\n",
      "        [ 0.1347,  0.0426, -0.0102, -0.1329,  0.1306, -0.0215,  0.0565, -0.1614],\n",
      "        [ 0.0418, -0.1211,  0.1035, -0.0629, -0.0809, -0.0046,  0.0486, -0.1698],\n",
      "        [ 0.1617,  0.1003, -0.1687, -0.1253, -0.1091,  0.0411,  0.1526, -0.0137],\n",
      "        [ 0.1033, -0.1307,  0.0289, -0.0463,  0.0922, -0.0502, -0.1761, -0.0761],\n",
      "        [-0.0804, -0.0716, -0.1241, -0.0741, -0.1390,  0.1312, -0.0323,  0.0194],\n",
      "        [ 0.1576,  0.0371, -0.0057,  0.1046,  0.0898,  0.1089, -0.1762, -0.1251],\n",
      "        [-0.0544, -0.1670, -0.0030, -0.1466, -0.0439,  0.1457, -0.0590, -0.1562],\n",
      "        [ 0.1751, -0.1589, -0.0807, -0.1362,  0.1213, -0.0229,  0.0264, -0.0125],\n",
      "        [-0.0890,  0.1516, -0.0944, -0.0037, -0.0512, -0.0703,  0.1727,  0.1200],\n",
      "        [-0.0717, -0.0612,  0.1481,  0.0636, -0.0803, -0.1510,  0.1412, -0.1018],\n",
      "        [-0.1016,  0.1585,  0.1198, -0.1082, -0.0416,  0.0507,  0.0712,  0.0423],\n",
      "        [-0.0799, -0.1314, -0.1642,  0.0146, -0.0660,  0.0842, -0.0644, -0.1282],\n",
      "        [-0.1020, -0.0396, -0.0759, -0.0026, -0.0462,  0.1196, -0.1137, -0.0075],\n",
      "        [-0.1242, -0.0970, -0.0631, -0.1494, -0.1004,  0.0594, -0.1585, -0.1267],\n",
      "        [-0.0911, -0.0409, -0.0624, -0.0044, -0.1346,  0.0041, -0.1226, -0.0050],\n",
      "        [-0.1326, -0.0759,  0.0879, -0.0958,  0.1459,  0.1532, -0.0257,  0.0875],\n",
      "        [-0.0620,  0.0294, -0.0634,  0.0419,  0.1610, -0.0135,  0.0314,  0.0595],\n",
      "        [ 0.0102, -0.0023, -0.1429,  0.1475, -0.0860,  0.0815, -0.1183, -0.1195],\n",
      "        [ 0.1127,  0.0066, -0.1374, -0.0998,  0.1477, -0.0138,  0.0802,  0.0839],\n",
      "        [ 0.0642, -0.1385, -0.1764,  0.0348,  0.0835, -0.1206,  0.0407, -0.0135],\n",
      "        [ 0.0370,  0.0710,  0.0779, -0.0009,  0.0967, -0.1569,  0.1381, -0.0976],\n",
      "        [ 0.1755, -0.0824,  0.1235,  0.0644,  0.0116,  0.0446, -0.0219, -0.1631],\n",
      "        [ 0.0171,  0.1389,  0.1098, -0.1589, -0.1513,  0.0670, -0.0247,  0.1478],\n",
      "        [ 0.0853, -0.1345, -0.1567, -0.0057,  0.0854,  0.0112, -0.1311, -0.0808]],\n",
      "       requires_grad=True)\n",
      "Wdown spliced: torch.Size([16, 4])\n",
      "tensor([[-0.0512, -0.0703,  0.1727,  0.1200],\n",
      "        [-0.0803, -0.1510,  0.1412, -0.1018],\n",
      "        [-0.0416,  0.0507,  0.0712,  0.0423],\n",
      "        [-0.0660,  0.0842, -0.0644, -0.1282],\n",
      "        [-0.0462,  0.1196, -0.1137, -0.0075],\n",
      "        [-0.1004,  0.0594, -0.1585, -0.1267],\n",
      "        [-0.1346,  0.0041, -0.1226, -0.0050],\n",
      "        [ 0.1459,  0.1532, -0.0257,  0.0875],\n",
      "        [ 0.1610, -0.0135,  0.0314,  0.0595],\n",
      "        [-0.0860,  0.0815, -0.1183, -0.1195],\n",
      "        [ 0.1477, -0.0138,  0.0802,  0.0839],\n",
      "        [ 0.0835, -0.1206,  0.0407, -0.0135],\n",
      "        [ 0.0967, -0.1569,  0.1381, -0.0976],\n",
      "        [ 0.0116,  0.0446, -0.0219, -0.1631],\n",
      "        [-0.1513,  0.0670, -0.0247,  0.1478],\n",
      "        [ 0.0854,  0.0112, -0.1311, -0.0808]], grad_fn=<SliceBackward0>)\n",
      "Bdown: torch.Size([8])\n",
      "Parameter containing:\n",
      "tensor([-0.1305, -0.0741, -0.0943, -0.1341,  0.0042, -0.0619,  0.0076, -0.1566],\n",
      "       requires_grad=True)\n",
      "Bdown spliced: torch.Size([4])\n",
      "tensor([ 0.0042, -0.0619,  0.0076, -0.1566], grad_fn=<SliceBackward0>)\n",
      "outputs: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.0414, -0.0602, -0.0402, -0.1880],\n",
      "         [-0.0173, -0.0799,  0.0314, -0.0675],\n",
      "         [-0.0175, -0.0819, -0.0095, -0.1766]]], grad_fn=<AddBackward0>)\n",
      "------------- END MLP.forwardTensor() ------------\n",
      "layer's final residual state:\n",
      "tensor([[[ 0.1178,  0.8654,  0.6991, -0.0448],\n",
      "         [ 0.2392,  0.0679,  0.5234,  0.7665],\n",
      "         [ 0.7589,  0.9548,  0.5055,  0.4412]]], grad_fn=<AddBackward0>)\n",
      "----------------- END Layer.forwardTensor() --------------------\n",
      "y: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.1178,  0.8654,  0.6991, -0.0448],\n",
      "         [ 0.2392,  0.0679,  0.5234,  0.7665],\n",
      "         [ 0.7589,  0.9548,  0.5055,  0.4412]]], grad_fn=<AddBackward0>)\n",
      "---------- RESET CONFIG --------\n",
      "model_count:  [1, 2, 4]\n",
      "model_dim_list:  [128, 64, 32]\n",
      "head_dim_list:  [32, 16, 8]\n"
     ]
    }
   ],
   "source": [
    "# Testing our Layer's forwardTensor()\n",
    "verbose = True\n",
    "\n",
    "print(\"--------- Micro Hyperparameters -------\")\n",
    "hold1, hold2, hold3, hold4 = config.hidden_size, config.num_attention_heads, config.head_dim, config.max_position_embeddings\n",
    "config.hidden_size = 8\n",
    "config.num_attention_heads = 2\n",
    "config.head_dim = 4\n",
    "config.max_position_embeddings = 3\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "print(\"head_dim_list: \", config.head_dim_list)\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the big model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,3,config.hidden_size)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "layer = Layer(config)\n",
    "y = layer(x)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the first sub-model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,3,config.hidden_size//config.split)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "layer = Layer(config)\n",
    "y = layer(x)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the second sub-model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,3,config.hidden_size//config.split)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "layer = Layer(config)\n",
    "y = layer(x, model=1)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "verbose = False\n",
    "print(\"---------- RESET CONFIG --------\")\n",
    "config.hidden_size = hold1\n",
    "config.num_attention_heads = hold2\n",
    "config.head_dim = hold3\n",
    "config.max_position_embeddings = hold4\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "print(\"head_dim_list: \", config.head_dim_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a2e1e7b-d356-4215-a5ec-1b18c21cd38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Micro Hyperparameters -------\n",
      "model_count:  [1, 2]\n",
      "model_dim_list:  [8, 4]\n",
      "head_dim_list:  [4, 2]\n",
      "x: ((tensor([[[ 2.2088,  0.2584,  0.7021, -0.9308,  0.1443,  1.9641,  1.0878,\n",
      "           0.3879],\n",
      "         [-0.2463, -1.1001,  0.0890,  0.6983,  0.2876,  0.4139, -2.8226,\n",
      "           0.2855],\n",
      "         [ 0.2658, -1.2915, -0.3992, -0.9281, -0.8413,  0.3240,  1.2216,\n",
      "          -0.2044]]]),), (tensor([[[ 0.2293, -0.9171, -0.9802, -0.5067],\n",
      "         [ 0.1680, -1.1694,  0.7582,  0.1348],\n",
      "         [-1.9322, -0.7696, -1.3604, -0.6910]]]), tensor([[[ 2.0527,  1.9986,  1.9487, -0.8838],\n",
      "         [-1.3443,  0.4112, -1.1328,  0.0875],\n",
      "         [-2.1688, -1.0016, -0.4252, -0.2276]]])))\n",
      "---------- Layer Input: Tuple ------------\n",
      "------------- Layer.forwardTuple() ------------\n",
      "x:\n",
      "((tensor([[[ 2.2088,  0.2584,  0.7021, -0.9308,  0.1443,  1.9641,  1.0878,\n",
      "           0.3879],\n",
      "         [-0.2463, -1.1001,  0.0890,  0.6983,  0.2876,  0.4139, -2.8226,\n",
      "           0.2855],\n",
      "         [ 0.2658, -1.2915, -0.3992, -0.9281, -0.8413,  0.3240,  1.2216,\n",
      "          -0.2044]]]),), (tensor([[[ 0.2293, -0.9171, -0.9802, -0.5067],\n",
      "         [ 0.1680, -1.1694,  0.7582,  0.1348],\n",
      "         [-1.9322, -0.7696, -1.3604, -0.6910]]]), tensor([[[ 2.0527,  1.9986,  1.9487, -0.8838],\n",
      "         [-1.3443,  0.4112, -1.1328,  0.0875],\n",
      "         [-2.1688, -1.0016, -0.4252, -0.2276]]])))\n",
      "input_len: 3\n",
      "num_levels: 2\n",
      "models_per_level: [1, 2]\n",
      "Level 0 from range(2)\n",
      "Model 0 from range(1)\n",
      "----------------- Layer.forwardTensor() --------------------\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 3, 8])\n",
      "tensor([[[ 2.2088,  0.2584,  0.7021, -0.9308,  0.1443,  1.9641,  1.0878,\n",
      "           0.3879],\n",
      "         [-0.2463, -1.1001,  0.0890,  0.6983,  0.2876,  0.4139, -2.8226,\n",
      "           0.2855],\n",
      "         [ 0.2658, -1.2915, -0.3992, -0.9281, -0.8413,  0.3240,  1.2216,\n",
      "          -0.2044]]])\n",
      "normed x: torch.Size([1, 3, 8])\n",
      "tensor([[[ 1.8409,  0.2154,  0.5851, -0.7757,  0.1202,  1.6370,  0.9066,\n",
      "           0.3233],\n",
      "         [-0.2196, -0.9806,  0.0794,  0.6225,  0.2563,  0.3690, -2.5159,\n",
      "           0.2544],\n",
      "         [ 0.3326, -1.6165, -0.4997, -1.1617, -1.0531,  0.4055,  1.5290,\n",
      "          -0.2559]]])\n",
      "dim: 8\n",
      "skip: 0\n",
      "spliced scale: torch.Size([8])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 3, 8])\n",
      "tensor([[[ 1.8409,  0.2154,  0.5851, -0.7757,  0.1202,  1.6370,  0.9066,\n",
      "           0.3233],\n",
      "         [-0.2196, -0.9806,  0.0794,  0.6225,  0.2563,  0.3690, -2.5159,\n",
      "           0.2544],\n",
      "         [ 0.3326, -1.6165, -0.4997, -1.1617, -1.0531,  0.4055,  1.5290,\n",
      "          -0.2559]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "---------- Attention Input: torch.Tensor ------------\n",
      "----------------- MultiQueryAttention.forwardTensor() --------------------\n",
      "x shape: torch.Size([1, 3, 8])\n",
      "d_skip: 0\n",
      "models_in_this_level: 1\n",
      "h_dim: 4\n",
      "h_skip: 0\n",
      "self.Wqkv: torch.Size([8, 16])\n",
      "Parameter containing:\n",
      "tensor([[-0.3398,  0.3307,  0.2225,  0.2923,  0.3199, -0.1244,  0.0621,  0.2447,\n",
      "         -0.3499, -0.1872,  0.3051,  0.0890,  0.2710,  0.1609,  0.3357,  0.0098],\n",
      "        [ 0.2298, -0.1531, -0.2039, -0.1286, -0.2591,  0.1291,  0.3100, -0.0835,\n",
      "         -0.2479, -0.1510, -0.1537,  0.0181,  0.2160,  0.3484, -0.3323, -0.1380],\n",
      "        [ 0.1080, -0.0664, -0.3089,  0.1810,  0.2769,  0.0328,  0.3347, -0.1358,\n",
      "         -0.0295,  0.2815, -0.3144,  0.1225,  0.2875,  0.0617, -0.0904, -0.1679],\n",
      "        [-0.2684, -0.2637, -0.0269,  0.3338,  0.0707, -0.3283,  0.1122, -0.1725,\n",
      "         -0.0218, -0.0872,  0.2521,  0.1246,  0.0284, -0.0780, -0.2766, -0.1182],\n",
      "        [-0.1520, -0.2973, -0.2569, -0.0197, -0.0728,  0.1796, -0.2989, -0.2442,\n",
      "          0.3104, -0.2646, -0.0299,  0.2852,  0.3015, -0.1969, -0.0207,  0.0691],\n",
      "        [-0.1881, -0.2773,  0.0862, -0.3252, -0.1013, -0.1286, -0.0218, -0.3159,\n",
      "         -0.3131, -0.2189,  0.2176, -0.3087, -0.3061,  0.2190, -0.2383,  0.0816],\n",
      "        [ 0.1831,  0.0092,  0.1412, -0.1254,  0.0028,  0.2533, -0.1741,  0.1159,\n",
      "          0.2736,  0.1486,  0.2726,  0.3237,  0.1083, -0.1763, -0.0169,  0.0802],\n",
      "        [ 0.0394, -0.1980, -0.1334, -0.2768, -0.0239,  0.0613,  0.0357, -0.0684,\n",
      "         -0.0411,  0.0550,  0.2665, -0.2499,  0.3059,  0.1568, -0.0046, -0.0349]],\n",
      "       requires_grad=True)\n",
      "Wq: torch.Size([8, 8])\n",
      "tensor([[-0.3398,  0.3307,  0.2225,  0.2923,  0.3199, -0.1244,  0.0621,  0.2447],\n",
      "        [ 0.2298, -0.1531, -0.2039, -0.1286, -0.2591,  0.1291,  0.3100, -0.0835],\n",
      "        [ 0.1080, -0.0664, -0.3089,  0.1810,  0.2769,  0.0328,  0.3347, -0.1358],\n",
      "        [-0.2684, -0.2637, -0.0269,  0.3338,  0.0707, -0.3283,  0.1122, -0.1725],\n",
      "        [-0.1520, -0.2973, -0.2569, -0.0197, -0.0728,  0.1796, -0.2989, -0.2442],\n",
      "        [-0.1881, -0.2773,  0.0862, -0.3252, -0.1013, -0.1286, -0.0218, -0.3159],\n",
      "        [ 0.1831,  0.0092,  0.1412, -0.1254,  0.0028,  0.2533, -0.1741,  0.1159],\n",
      "        [ 0.0394, -0.1980, -0.1334, -0.2768, -0.0239,  0.0613,  0.0357, -0.0684]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wk: torch.Size([8, 4])\n",
      "tensor([[-0.3499, -0.1872,  0.3051,  0.0890],\n",
      "        [-0.2479, -0.1510, -0.1537,  0.0181],\n",
      "        [-0.0295,  0.2815, -0.3144,  0.1225],\n",
      "        [-0.0218, -0.0872,  0.2521,  0.1246],\n",
      "        [ 0.3104, -0.2646, -0.0299,  0.2852],\n",
      "        [-0.3131, -0.2189,  0.2176, -0.3087],\n",
      "        [ 0.2736,  0.1486,  0.2726,  0.3237],\n",
      "        [-0.0411,  0.0550,  0.2665, -0.2499]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wv: torch.Size([8, 4])\n",
      "tensor([[ 0.2710,  0.1609,  0.3357,  0.0098],\n",
      "        [ 0.2160,  0.3484, -0.3323, -0.1380],\n",
      "        [ 0.2875,  0.0617, -0.0904, -0.1679],\n",
      "        [ 0.0284, -0.0780, -0.2766, -0.1182],\n",
      "        [ 0.3015, -0.1969, -0.0207,  0.0691],\n",
      "        [-0.3061,  0.2190, -0.2383,  0.0816],\n",
      "        [ 0.1083, -0.1763, -0.0169,  0.0802],\n",
      "        [ 0.3059,  0.1568, -0.0046, -0.0349]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wq spliced: torch.Size([8, 8])\n",
      "tensor([[-0.3398,  0.3307,  0.2225,  0.2923,  0.3199, -0.1244,  0.0621,  0.2447],\n",
      "        [ 0.2298, -0.1531, -0.2039, -0.1286, -0.2591,  0.1291,  0.3100, -0.0835],\n",
      "        [ 0.1080, -0.0664, -0.3089,  0.1810,  0.2769,  0.0328,  0.3347, -0.1358],\n",
      "        [-0.2684, -0.2637, -0.0269,  0.3338,  0.0707, -0.3283,  0.1122, -0.1725],\n",
      "        [-0.1520, -0.2973, -0.2569, -0.0197, -0.0728,  0.1796, -0.2989, -0.2442],\n",
      "        [-0.1881, -0.2773,  0.0862, -0.3252, -0.1013, -0.1286, -0.0218, -0.3159],\n",
      "        [ 0.1831,  0.0092,  0.1412, -0.1254,  0.0028,  0.2533, -0.1741,  0.1159],\n",
      "        [ 0.0394, -0.1980, -0.1334, -0.2768, -0.0239,  0.0613,  0.0357, -0.0684]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Wk spliced: torch.Size([8, 4])\n",
      "tensor([[-0.3499, -0.1872,  0.3051,  0.0890],\n",
      "        [-0.2479, -0.1510, -0.1537,  0.0181],\n",
      "        [-0.0295,  0.2815, -0.3144,  0.1225],\n",
      "        [-0.0218, -0.0872,  0.2521,  0.1246],\n",
      "        [ 0.3104, -0.2646, -0.0299,  0.2852],\n",
      "        [-0.3131, -0.2189,  0.2176, -0.3087],\n",
      "        [ 0.2736,  0.1486,  0.2726,  0.3237],\n",
      "        [-0.0411,  0.0550,  0.2665, -0.2499]], grad_fn=<CatBackward0>)\n",
      "Wv spliced: torch.Size([8, 4])\n",
      "tensor([[ 0.2710,  0.1609,  0.3357,  0.0098],\n",
      "        [ 0.2160,  0.3484, -0.3323, -0.1380],\n",
      "        [ 0.2875,  0.0617, -0.0904, -0.1679],\n",
      "        [ 0.0284, -0.0780, -0.2766, -0.1182],\n",
      "        [ 0.3015, -0.1969, -0.0207,  0.0691],\n",
      "        [-0.3061,  0.2190, -0.2383,  0.0816],\n",
      "        [ 0.1083, -0.1763, -0.0169,  0.0802],\n",
      "        [ 0.3059,  0.1568, -0.0046, -0.0349]], grad_fn=<CatBackward0>)\n",
      "Wqkv_spliced: torch.Size([8, 16])\n",
      "tensor([[-0.3398,  0.3307,  0.2225,  0.2923,  0.3199, -0.1244,  0.0621,  0.2447,\n",
      "         -0.3499, -0.1872,  0.3051,  0.0890,  0.2710,  0.1609,  0.3357,  0.0098],\n",
      "        [ 0.2298, -0.1531, -0.2039, -0.1286, -0.2591,  0.1291,  0.3100, -0.0835,\n",
      "         -0.2479, -0.1510, -0.1537,  0.0181,  0.2160,  0.3484, -0.3323, -0.1380],\n",
      "        [ 0.1080, -0.0664, -0.3089,  0.1810,  0.2769,  0.0328,  0.3347, -0.1358,\n",
      "         -0.0295,  0.2815, -0.3144,  0.1225,  0.2875,  0.0617, -0.0904, -0.1679],\n",
      "        [-0.2684, -0.2637, -0.0269,  0.3338,  0.0707, -0.3283,  0.1122, -0.1725,\n",
      "         -0.0218, -0.0872,  0.2521,  0.1246,  0.0284, -0.0780, -0.2766, -0.1182],\n",
      "        [-0.1520, -0.2973, -0.2569, -0.0197, -0.0728,  0.1796, -0.2989, -0.2442,\n",
      "          0.3104, -0.2646, -0.0299,  0.2852,  0.3015, -0.1969, -0.0207,  0.0691],\n",
      "        [-0.1881, -0.2773,  0.0862, -0.3252, -0.1013, -0.1286, -0.0218, -0.3159,\n",
      "         -0.3131, -0.2189,  0.2176, -0.3087, -0.3061,  0.2190, -0.2383,  0.0816],\n",
      "        [ 0.1831,  0.0092,  0.1412, -0.1254,  0.0028,  0.2533, -0.1741,  0.1159,\n",
      "          0.2736,  0.1486,  0.2726,  0.3237,  0.1083, -0.1763, -0.0169,  0.0802],\n",
      "        [ 0.0394, -0.1980, -0.1334, -0.2768, -0.0239,  0.0613,  0.0357, -0.0684,\n",
      "         -0.0411,  0.0550,  0.2665, -0.2499,  0.3059,  0.1568, -0.0046, -0.0349]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "xqkv: torch.Size([1, 3, 16])\n",
      "tensor([[[-0.4521,  0.1962,  0.4010, -0.3805,  0.4604,  0.1331,  0.0721,\n",
      "           0.0234, -0.9384, -0.3824,  0.8349, -0.1156,  0.4238,  0.6936,\n",
      "           0.2987,  0.1849],\n",
      "         [-0.8683, -0.3439, -0.3133,  0.4042,  0.1806, -0.9241,  0.1412,\n",
      "          -0.5782, -0.4308, -0.3511, -0.3296, -0.8688, -0.4610,  0.0931,\n",
      "           0.0209, -0.1166],\n",
      "         [ 0.1270,  0.9625,  1.1447, -0.4052,  0.3506,  0.2453, -0.7476,\n",
      "           0.8083,  0.2993,  0.5453,  0.6825, -0.0723, -0.7900, -0.4635,\n",
      "           0.9158,  0.5395]]], grad_fn=<UnsafeViewBackward0>)\n",
      "xq: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.4521,  0.1962,  0.4010, -0.3805,  0.4604,  0.1331,  0.0721,\n",
      "           0.0234],\n",
      "         [-0.8683, -0.3439, -0.3133,  0.4042,  0.1806, -0.9241,  0.1412,\n",
      "          -0.5782],\n",
      "         [ 0.1270,  0.9625,  1.1447, -0.4052,  0.3506,  0.2453, -0.7476,\n",
      "           0.8083]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xk: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.9384, -0.3824,  0.8349, -0.1156],\n",
      "         [-0.4308, -0.3511, -0.3296, -0.8688],\n",
      "         [ 0.2993,  0.5453,  0.6825, -0.0723]]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "xv: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.4238,  0.6936,  0.2987,  0.1849],\n",
      "         [-0.4610,  0.0931,  0.0209, -0.1166],\n",
      "         [-0.7900, -0.4635,  0.9158,  0.5395]]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "xq reshaped: torch.Size([1, 3, 2, 4])\n",
      "tensor([[[[-0.4521,  0.1962,  0.4010, -0.3805],\n",
      "          [ 0.4604,  0.1331,  0.0721,  0.0234]],\n",
      "\n",
      "         [[-0.8683, -0.3439, -0.3133,  0.4042],\n",
      "          [ 0.1806, -0.9241,  0.1412, -0.5782]],\n",
      "\n",
      "         [[ 0.1270,  0.9625,  1.1447, -0.4052],\n",
      "          [ 0.3506,  0.2453, -0.7476,  0.8083]]]], grad_fn=<ViewBackward0>)\n",
      "xk reshaped: torch.Size([1, 3, 1, 4])\n",
      "tensor([[[[-0.9384, -0.3824,  0.8349, -0.1156]],\n",
      "\n",
      "         [[-0.4308, -0.3511, -0.3296, -0.8688]],\n",
      "\n",
      "         [[ 0.2993,  0.5453,  0.6825, -0.0723]]]], grad_fn=<ViewBackward0>)\n",
      "xv reshaped: torch.Size([1, 3, 1, 4])\n",
      "tensor([[[[ 0.4238,  0.6936,  0.2987,  0.1849]],\n",
      "\n",
      "         [[-0.4610,  0.0931,  0.0209, -0.1166]],\n",
      "\n",
      "         [[-0.7900, -0.4635,  0.9158,  0.5395]]]], grad_fn=<ViewBackward0>)\n",
      "rotated xq: torch.Size([1, 3, 2, 4])\n",
      "tensor([[[[-0.4521,  0.1962,  0.4010, -0.3805],\n",
      "          [ 0.4604,  0.1331,  0.0721,  0.0234]],\n",
      "\n",
      "         [[-0.2055, -0.3825, -0.9000,  0.3678],\n",
      "          [-0.0212, -0.8618,  0.2283, -0.6676]],\n",
      "\n",
      "         [[-1.0938,  1.0238, -0.3609, -0.2059],\n",
      "          [ 0.5339,  0.0798,  0.6299,  0.8409]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "rotated xk: torch.Size([1, 3, 1, 4])\n",
      "tensor([[[[-0.9384, -0.3824,  0.8349, -0.1156]],\n",
      "\n",
      "         [[ 0.0446, -0.2626, -0.5406, -0.8995]],\n",
      "\n",
      "         [[-0.7451,  0.5488, -0.0118,  0.0375]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "repeat_interleaved xk: torch.Size([1, 3, 2, 4])\n",
      "tensor([[[[-0.9384, -0.3824,  0.8349, -0.1156],\n",
      "          [-0.9384, -0.3824,  0.8349, -0.1156]],\n",
      "\n",
      "         [[ 0.0446, -0.2626, -0.5406, -0.8995],\n",
      "          [ 0.0446, -0.2626, -0.5406, -0.8995]],\n",
      "\n",
      "         [[-0.7451,  0.5488, -0.0118,  0.0375],\n",
      "          [-0.7451,  0.5488, -0.0118,  0.0375]]]], grad_fn=<ViewBackward0>)\n",
      "repeat_interleaved xv: torch.Size([1, 3, 2, 4])\n",
      "tensor([[[[ 0.4238,  0.6936,  0.2987,  0.1849],\n",
      "          [ 0.4238,  0.6936,  0.2987,  0.1849]],\n",
      "\n",
      "         [[-0.4610,  0.0931,  0.0209, -0.1166],\n",
      "          [-0.4610,  0.0931,  0.0209, -0.1166]],\n",
      "\n",
      "         [[-0.7900, -0.4635,  0.9158,  0.5395],\n",
      "          [-0.7900, -0.4635,  0.9158,  0.5395]]]], grad_fn=<ViewBackward0>)\n",
      "transposed xq: torch.Size([1, 2, 3, 4])\n",
      "tensor([[[[-0.4521,  0.1962,  0.4010, -0.3805],\n",
      "          [-0.2055, -0.3825, -0.9000,  0.3678],\n",
      "          [-1.0938,  1.0238, -0.3609, -0.2059]],\n",
      "\n",
      "         [[ 0.4604,  0.1331,  0.0721,  0.0234],\n",
      "          [-0.0212, -0.8618,  0.2283, -0.6676],\n",
      "          [ 0.5339,  0.0798,  0.6299,  0.8409]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "transposed xk: torch.Size([1, 2, 3, 4])\n",
      "tensor([[[[-0.9384, -0.3824,  0.8349, -0.1156],\n",
      "          [ 0.0446, -0.2626, -0.5406, -0.8995],\n",
      "          [-0.7451,  0.5488, -0.0118,  0.0375]],\n",
      "\n",
      "         [[-0.9384, -0.3824,  0.8349, -0.1156],\n",
      "          [ 0.0446, -0.2626, -0.5406, -0.8995],\n",
      "          [-0.7451,  0.5488, -0.0118,  0.0375]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "transposed xv: torch.Size([1, 2, 3, 4])\n",
      "tensor([[[[ 0.4238,  0.6936,  0.2987,  0.1849],\n",
      "          [-0.4610,  0.0931,  0.0209, -0.1166],\n",
      "          [-0.7900, -0.4635,  0.9158,  0.5395]],\n",
      "\n",
      "         [[ 0.4238,  0.6936,  0.2987,  0.1849],\n",
      "          [-0.4610,  0.0931,  0.0209, -0.1166],\n",
      "          [-0.7900, -0.4635,  0.9158,  0.5395]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[ 0.3640,  0.0269,  0.2128],\n",
      "          [-0.2274,  0.1235, -0.0162],\n",
      "          [ 0.1787,  0.0313,  0.6867]],\n",
      "\n",
      "         [[-0.2127, -0.0372, -0.1350],\n",
      "          [ 0.3086,  0.3512, -0.2424],\n",
      "          [-0.0514, -0.5471, -0.1650]]]], grad_fn=<MulBackward0>)\n",
      "mask: torch.Size([1, 1, 3, 3])\n",
      "tensor([[[[ 0.0000e+00, -2.3820e+38, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]]]])\n",
      "masked scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[ 3.6404e-01, -2.3820e+38, -2.3820e+38],\n",
      "          [-2.2740e-01,  1.2346e-01, -2.3820e+38],\n",
      "          [ 1.7869e-01,  3.1332e-02,  6.8670e-01]],\n",
      "\n",
      "         [[-2.1274e-01, -2.3820e+38, -2.3820e+38],\n",
      "          [ 3.0863e-01,  3.5122e-01, -2.3820e+38],\n",
      "          [-5.1394e-02, -5.4705e-01, -1.6496e-01]]]], grad_fn=<AddBackward0>)\n",
      "softmaxed scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [0.4132, 0.5868, 0.0000],\n",
      "          [0.2837, 0.2448, 0.4715]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.4894, 0.5106, 0.0000],\n",
      "          [0.3997, 0.2435, 0.3568]]]], grad_fn=<SoftmaxBackward0>)\n",
      "attention: torch.Size([1, 2, 3, 4])\n",
      "tensor([[[[ 0.4238,  0.6936,  0.2987,  0.1849],\n",
      "          [-0.0954,  0.3412,  0.1357,  0.0080],\n",
      "          [-0.3651,  0.0010,  0.5217,  0.2783]],\n",
      "\n",
      "         [[ 0.4238,  0.6936,  0.2987,  0.1849],\n",
      "          [-0.0280,  0.3869,  0.1568,  0.0310],\n",
      "          [-0.2247,  0.1345,  0.4512,  0.2380]]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "reshaped attention: torch.Size([1, 3, 8])\n",
      "tensor([[[ 0.4238,  0.6936,  0.2987,  0.1849,  0.4238,  0.6936,  0.2987,\n",
      "           0.1849],\n",
      "         [-0.0954,  0.3412,  0.1357,  0.0080, -0.0280,  0.3869,  0.1568,\n",
      "           0.0310],\n",
      "         [-0.3651,  0.0010,  0.5217,  0.2783, -0.2247,  0.1345,  0.4512,\n",
      "           0.2380]]], grad_fn=<ViewBackward0>)\n",
      "self.Wo: torch.Size([8, 8])\n",
      "Parameter containing:\n",
      "tensor([[-0.3331, -0.0057, -0.1193, -0.0371,  0.0632, -0.1175, -0.1165, -0.1170],\n",
      "        [ 0.3058,  0.1560, -0.2966, -0.3141, -0.3463, -0.2982,  0.2295,  0.2648],\n",
      "        [ 0.1682, -0.3533,  0.3376,  0.2572, -0.0676, -0.0124,  0.1574,  0.1800],\n",
      "        [-0.1671,  0.0802, -0.2675,  0.0040, -0.2241, -0.2670,  0.1884, -0.2433],\n",
      "        [ 0.1423,  0.2316,  0.0518,  0.0542, -0.0278,  0.3124,  0.0649,  0.3067],\n",
      "        [-0.1438,  0.1334, -0.2953, -0.2768,  0.0403,  0.0571, -0.0478, -0.1368],\n",
      "        [-0.2495, -0.1464, -0.3111,  0.2914, -0.1158, -0.0949,  0.1083,  0.0125],\n",
      "        [-0.2954,  0.3428,  0.0848, -0.2212, -0.2844,  0.2948, -0.1303,  0.3506]],\n",
      "       requires_grad=True)\n",
      "spliced Wo: torch.Size([8, 8])\n",
      "tensor([[-0.3331, -0.0057, -0.1193, -0.0371,  0.0632, -0.1175, -0.1165, -0.1170],\n",
      "        [ 0.3058,  0.1560, -0.2966, -0.3141, -0.3463, -0.2982,  0.2295,  0.2648],\n",
      "        [ 0.1682, -0.3533,  0.3376,  0.2572, -0.0676, -0.0124,  0.1574,  0.1800],\n",
      "        [-0.1671,  0.0802, -0.2675,  0.0040, -0.2241, -0.2670,  0.1884, -0.2433],\n",
      "        [ 0.1423,  0.2316,  0.0518,  0.0542, -0.0278,  0.3124,  0.0649,  0.3067],\n",
      "        [-0.1438,  0.1334, -0.2953, -0.2768,  0.0403,  0.0571, -0.0478, -0.1368],\n",
      "        [-0.2495, -0.1464, -0.3111,  0.2914, -0.1158, -0.0949,  0.1083,  0.0125],\n",
      "        [-0.2954,  0.3428,  0.0848, -0.2212, -0.2844,  0.2948, -0.1303,  0.3506]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "projected output: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.0783,  0.2255, -0.4650, -0.2789, -0.3461, -0.1116,  0.1943,\n",
      "           0.2466],\n",
      "         [ 0.0497,  0.0393, -0.2080, -0.1385, -0.1458, -0.0868,  0.1049,\n",
      "           0.0753],\n",
      "         [-0.0711, -0.1783, -0.0266,  0.1779, -0.2293, -0.0734,  0.1741,\n",
      "           0.0709]]], grad_fn=<UnsafeViewBackward0>)\n",
      "----------------- END MultiQueryAttention.forwardTensor() --------------------\n",
      "x in layer after MQA & resid connection and before MLP:\n",
      "tensor([[[ 2.1305,  0.4839,  0.2371, -1.2097, -0.2018,  1.8526,  1.2821,\n",
      "           0.6345],\n",
      "         [-0.1967, -1.0608, -0.1190,  0.5598,  0.1418,  0.3272, -2.7177,\n",
      "           0.3608],\n",
      "         [ 0.1947, -1.4698, -0.4258, -0.7502, -1.0707,  0.2506,  1.3957,\n",
      "          -0.1335]]], grad_fn=<AddBackward0>)\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 3, 8])\n",
      "tensor([[[ 2.1305,  0.4839,  0.2371, -1.2097, -0.2018,  1.8526,  1.2821,\n",
      "           0.6345],\n",
      "         [-0.1967, -1.0608, -0.1190,  0.5598,  0.1418,  0.3272, -2.7177,\n",
      "           0.3608],\n",
      "         [ 0.1947, -1.4698, -0.4258, -0.7502, -1.0707,  0.2506,  1.3957,\n",
      "          -0.1335]]], grad_fn=<AddBackward0>)\n",
      "normed x: torch.Size([1, 3, 8])\n",
      "tensor([[[ 1.7533,  0.3982,  0.1951, -0.9956, -0.1661,  1.5246,  1.0551,\n",
      "           0.5222],\n",
      "         [-0.1840, -0.9927, -0.1114,  0.5239,  0.1327,  0.3062, -2.5433,\n",
      "           0.3376],\n",
      "         [ 0.2226, -1.6808, -0.4870, -0.8579, -1.2244,  0.2866,  1.5961,\n",
      "          -0.1526]]], grad_fn=<MulBackward0>)\n",
      "dim: 8\n",
      "skip: 0\n",
      "spliced scale: torch.Size([8])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 3, 8])\n",
      "tensor([[[ 1.7533,  0.3982,  0.1951, -0.9956, -0.1661,  1.5246,  1.0551,\n",
      "           0.5222],\n",
      "         [-0.1840, -0.9927, -0.1114,  0.5239,  0.1327,  0.3062, -2.5433,\n",
      "           0.3376],\n",
      "         [ 0.2226, -1.6808, -0.4870, -0.8579, -1.2244,  0.2866,  1.5961,\n",
      "          -0.1526]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "---------- MLP Input: torch.Tensor ------------\n",
      "------------- MLP.forwardTensor() ------------\n",
      "x: torch.Size([1, 3, 8])\n",
      "tensor([[[ 1.7533,  0.3982,  0.1951, -0.9956, -0.1661,  1.5246,  1.0551,\n",
      "           0.5222],\n",
      "         [-0.1840, -0.9927, -0.1114,  0.5239,  0.1327,  0.3062, -2.5433,\n",
      "           0.3376],\n",
      "         [ 0.2226, -1.6808, -0.4870, -0.8579, -1.2244,  0.2866,  1.5961,\n",
      "          -0.1526]]], grad_fn=<MulBackward0>)\n",
      "d_dim: 8\n",
      "d_skip: 0\n",
      "i_dim: 32\n",
      "i_skip: 0\n",
      "Wgate: torch.Size([8, 32])\n",
      "Parameter containing:\n",
      "tensor([[-2.0378e-01,  1.9155e-02, -2.5673e-01, -6.6076e-02, -1.5894e-01,\n",
      "         -5.8077e-02,  1.1944e-01,  3.1422e-01,  2.6704e-01,  6.0142e-02,\n",
      "          2.8076e-01,  6.6552e-02,  1.2014e-02, -3.0538e-01,  2.2672e-01,\n",
      "         -2.8452e-01, -3.3381e-01, -1.0627e-01, -5.5746e-02,  2.2767e-03,\n",
      "         -8.7861e-02,  3.5301e-01,  9.5396e-02, -1.4413e-01,  2.2739e-01,\n",
      "         -3.0789e-01,  2.5667e-01,  1.9587e-01,  1.9188e-01,  1.9864e-01,\n",
      "          7.8174e-02, -1.4472e-01],\n",
      "        [ 2.5881e-01,  1.8849e-01, -1.9905e-01,  9.8076e-03,  3.1139e-02,\n",
      "          2.0969e-01, -1.7834e-01, -2.0160e-01, -2.3034e-01,  1.6279e-01,\n",
      "          3.0912e-01, -2.8784e-01, -2.4899e-02,  9.5506e-02,  3.1060e-01,\n",
      "          2.0226e-01, -1.2328e-02,  1.5144e-01, -3.1324e-01, -3.0475e-01,\n",
      "         -1.3892e-01, -2.6066e-01,  8.7560e-02, -7.3888e-02, -1.3559e-01,\n",
      "         -6.5933e-02, -9.5685e-02, -1.1806e-01,  3.3973e-01, -5.2369e-02,\n",
      "         -7.2154e-04,  3.1122e-01],\n",
      "        [ 3.2107e-01, -2.6924e-01,  3.1704e-01,  2.3734e-01, -2.0238e-01,\n",
      "          2.7264e-01, -3.1823e-01,  2.2726e-01,  1.3663e-01,  1.3384e-01,\n",
      "         -3.3406e-01, -8.7860e-02, -1.7728e-02,  1.2769e-01, -3.1617e-01,\n",
      "         -2.1740e-01,  3.7097e-03, -1.0999e-02, -2.5561e-01,  1.0879e-01,\n",
      "          1.0981e-01,  2.1874e-01,  9.8952e-02,  2.1657e-01,  2.1956e-01,\n",
      "         -3.4451e-01, -2.0934e-01,  1.7877e-01, -1.7365e-01,  1.1120e-02,\n",
      "         -1.0411e-01, -8.7443e-02],\n",
      "        [ 3.0102e-01,  1.8049e-01, -9.3183e-03, -3.0233e-01,  4.4365e-02,\n",
      "          1.5189e-01, -2.9368e-01, -3.8777e-02, -1.9838e-01,  3.5013e-01,\n",
      "          1.1811e-01,  1.2853e-01, -2.8444e-01, -2.3235e-01,  1.2906e-01,\n",
      "          2.5792e-01, -3.0151e-01,  8.0087e-03, -8.7959e-03,  7.7719e-02,\n",
      "         -1.5681e-01,  1.5885e-01, -1.8853e-01, -1.3954e-01,  2.9057e-01,\n",
      "          6.7097e-02, -2.2614e-01, -3.3118e-01, -5.7758e-02, -1.8288e-01,\n",
      "          1.8856e-01, -1.0947e-01],\n",
      "        [ 3.3077e-01, -1.5129e-01,  2.2010e-01, -1.5577e-01,  3.0039e-01,\n",
      "          4.0851e-02, -1.7102e-01,  1.7203e-01, -1.8540e-01,  8.4346e-02,\n",
      "          1.7938e-02, -1.2574e-01, -8.7709e-02, -1.0353e-01, -3.1317e-01,\n",
      "          2.7271e-01,  1.7714e-01,  3.0434e-01, -1.7270e-01,  2.7432e-01,\n",
      "         -4.6780e-02,  1.1775e-01, -2.9078e-01, -4.1688e-02,  1.8667e-01,\n",
      "          2.2608e-01,  2.0868e-01, -1.2030e-01, -4.9017e-02, -8.0353e-03,\n",
      "          6.4723e-02,  7.7618e-02],\n",
      "        [ 3.1620e-01, -1.3605e-01, -2.4454e-02, -3.0133e-01, -2.2047e-01,\n",
      "         -6.0336e-03,  1.4947e-01,  1.1901e-02,  2.4008e-01,  2.5826e-01,\n",
      "         -6.3199e-03,  2.3596e-01, -2.0454e-01,  2.1074e-01, -2.1293e-02,\n",
      "          3.2555e-01, -7.3171e-02,  9.9653e-02,  3.2077e-01,  2.5021e-01,\n",
      "         -3.1209e-02,  3.1273e-01, -3.0262e-01, -2.2676e-01, -5.4055e-02,\n",
      "          2.3609e-01,  8.8153e-02, -8.7053e-02, -3.4416e-01,  1.9098e-01,\n",
      "          9.2401e-02, -3.0739e-01],\n",
      "        [-8.3603e-03, -2.4367e-01, -2.9212e-01,  3.2735e-01, -1.8916e-01,\n",
      "          3.3348e-01,  3.4109e-01,  1.8712e-01, -1.0089e-01,  3.1761e-01,\n",
      "         -1.4733e-01, -2.7488e-01, -4.3122e-02,  1.3442e-01, -3.5292e-02,\n",
      "          1.4565e-01, -2.6186e-03,  1.1012e-01, -9.4417e-03,  2.9175e-01,\n",
      "         -3.1957e-01,  2.4675e-01, -1.9900e-01, -1.5049e-01, -3.1706e-01,\n",
      "         -8.3404e-03,  2.9004e-01, -3.2020e-02,  2.4540e-01,  3.1980e-01,\n",
      "          9.4737e-02,  2.1059e-01],\n",
      "        [ 3.1330e-01,  6.9906e-02,  1.7583e-01, -2.2837e-01, -7.3682e-02,\n",
      "         -7.1424e-02,  1.4529e-01,  2.1353e-02, -9.1109e-03, -3.2874e-01,\n",
      "          2.0343e-01, -2.0165e-01, -2.6832e-01, -3.1146e-04, -2.9679e-01,\n",
      "          2.1311e-01,  3.8456e-02,  1.5998e-01,  4.3876e-02,  5.8273e-03,\n",
      "          3.1290e-01, -6.7655e-02, -1.9788e-01,  1.6022e-01, -3.3185e-01,\n",
      "         -1.9130e-01,  2.2986e-01, -2.1936e-01,  1.2736e-01, -1.7436e-01,\n",
      "          2.3090e-01, -8.5473e-02]], requires_grad=True)\n",
      "Wgate spliced: torch.Size([8, 32])\n",
      "tensor([[-2.0378e-01,  1.9155e-02, -2.5673e-01, -6.6076e-02, -1.5894e-01,\n",
      "         -5.8077e-02,  1.1944e-01,  3.1422e-01,  2.6704e-01,  6.0142e-02,\n",
      "          2.8076e-01,  6.6552e-02,  1.2014e-02, -3.0538e-01,  2.2672e-01,\n",
      "         -2.8452e-01, -3.3381e-01, -1.0627e-01, -5.5746e-02,  2.2767e-03,\n",
      "         -8.7861e-02,  3.5301e-01,  9.5396e-02, -1.4413e-01,  2.2739e-01,\n",
      "         -3.0789e-01,  2.5667e-01,  1.9587e-01,  1.9188e-01,  1.9864e-01,\n",
      "          7.8174e-02, -1.4472e-01],\n",
      "        [ 2.5881e-01,  1.8849e-01, -1.9905e-01,  9.8076e-03,  3.1139e-02,\n",
      "          2.0969e-01, -1.7834e-01, -2.0160e-01, -2.3034e-01,  1.6279e-01,\n",
      "          3.0912e-01, -2.8784e-01, -2.4899e-02,  9.5506e-02,  3.1060e-01,\n",
      "          2.0226e-01, -1.2328e-02,  1.5144e-01, -3.1324e-01, -3.0475e-01,\n",
      "         -1.3892e-01, -2.6066e-01,  8.7560e-02, -7.3888e-02, -1.3559e-01,\n",
      "         -6.5933e-02, -9.5685e-02, -1.1806e-01,  3.3973e-01, -5.2369e-02,\n",
      "         -7.2154e-04,  3.1122e-01],\n",
      "        [ 3.2107e-01, -2.6924e-01,  3.1704e-01,  2.3734e-01, -2.0238e-01,\n",
      "          2.7264e-01, -3.1823e-01,  2.2726e-01,  1.3663e-01,  1.3384e-01,\n",
      "         -3.3406e-01, -8.7860e-02, -1.7728e-02,  1.2769e-01, -3.1617e-01,\n",
      "         -2.1740e-01,  3.7097e-03, -1.0999e-02, -2.5561e-01,  1.0879e-01,\n",
      "          1.0981e-01,  2.1874e-01,  9.8952e-02,  2.1657e-01,  2.1956e-01,\n",
      "         -3.4451e-01, -2.0934e-01,  1.7877e-01, -1.7365e-01,  1.1120e-02,\n",
      "         -1.0411e-01, -8.7443e-02],\n",
      "        [ 3.0102e-01,  1.8049e-01, -9.3183e-03, -3.0233e-01,  4.4365e-02,\n",
      "          1.5189e-01, -2.9368e-01, -3.8777e-02, -1.9838e-01,  3.5013e-01,\n",
      "          1.1811e-01,  1.2853e-01, -2.8444e-01, -2.3235e-01,  1.2906e-01,\n",
      "          2.5792e-01, -3.0151e-01,  8.0087e-03, -8.7959e-03,  7.7719e-02,\n",
      "         -1.5681e-01,  1.5885e-01, -1.8853e-01, -1.3954e-01,  2.9057e-01,\n",
      "          6.7097e-02, -2.2614e-01, -3.3118e-01, -5.7758e-02, -1.8288e-01,\n",
      "          1.8856e-01, -1.0947e-01],\n",
      "        [ 3.3077e-01, -1.5129e-01,  2.2010e-01, -1.5577e-01,  3.0039e-01,\n",
      "          4.0851e-02, -1.7102e-01,  1.7203e-01, -1.8540e-01,  8.4346e-02,\n",
      "          1.7938e-02, -1.2574e-01, -8.7709e-02, -1.0353e-01, -3.1317e-01,\n",
      "          2.7271e-01,  1.7714e-01,  3.0434e-01, -1.7270e-01,  2.7432e-01,\n",
      "         -4.6780e-02,  1.1775e-01, -2.9078e-01, -4.1688e-02,  1.8667e-01,\n",
      "          2.2608e-01,  2.0868e-01, -1.2030e-01, -4.9017e-02, -8.0353e-03,\n",
      "          6.4723e-02,  7.7618e-02],\n",
      "        [ 3.1620e-01, -1.3605e-01, -2.4454e-02, -3.0133e-01, -2.2047e-01,\n",
      "         -6.0336e-03,  1.4947e-01,  1.1901e-02,  2.4008e-01,  2.5826e-01,\n",
      "         -6.3199e-03,  2.3596e-01, -2.0454e-01,  2.1074e-01, -2.1293e-02,\n",
      "          3.2555e-01, -7.3171e-02,  9.9653e-02,  3.2077e-01,  2.5021e-01,\n",
      "         -3.1209e-02,  3.1273e-01, -3.0262e-01, -2.2676e-01, -5.4055e-02,\n",
      "          2.3609e-01,  8.8153e-02, -8.7053e-02, -3.4416e-01,  1.9098e-01,\n",
      "          9.2401e-02, -3.0739e-01],\n",
      "        [-8.3603e-03, -2.4367e-01, -2.9212e-01,  3.2735e-01, -1.8916e-01,\n",
      "          3.3348e-01,  3.4109e-01,  1.8712e-01, -1.0089e-01,  3.1761e-01,\n",
      "         -1.4733e-01, -2.7488e-01, -4.3122e-02,  1.3442e-01, -3.5292e-02,\n",
      "          1.4565e-01, -2.6186e-03,  1.1012e-01, -9.4417e-03,  2.9175e-01,\n",
      "         -3.1957e-01,  2.4675e-01, -1.9900e-01, -1.5049e-01, -3.1706e-01,\n",
      "         -8.3404e-03,  2.9004e-01, -3.2020e-02,  2.4540e-01,  3.1980e-01,\n",
      "          9.4737e-02,  2.1059e-01],\n",
      "        [ 3.1330e-01,  6.9906e-02,  1.7583e-01, -2.2837e-01, -7.3682e-02,\n",
      "         -7.1424e-02,  1.4529e-01,  2.1353e-02, -9.1109e-03, -3.2874e-01,\n",
      "          2.0343e-01, -2.0165e-01, -2.6832e-01, -3.1146e-04, -2.9679e-01,\n",
      "          2.1311e-01,  3.8456e-02,  1.5998e-01,  4.3876e-02,  5.8273e-03,\n",
      "          3.1290e-01, -6.7655e-02, -1.9788e-01,  1.6022e-01, -3.3185e-01,\n",
      "         -1.9130e-01,  2.2986e-01, -2.1936e-01,  1.2736e-01, -1.7436e-01,\n",
      "          2.3090e-01, -8.5473e-02]], grad_fn=<AliasBackward0>)\n",
      "Bgate: torch.Size([32])\n",
      "Parameter containing:\n",
      "tensor([-0.2264,  0.2777, -0.0718, -0.1366, -0.2667,  0.1974, -0.2165,  0.1699,\n",
      "         0.3184,  0.0362,  0.2520,  0.3413,  0.1179,  0.1998,  0.3149, -0.0912,\n",
      "        -0.2235, -0.0496, -0.0858,  0.3456,  0.2254, -0.3406,  0.3058, -0.3508,\n",
      "         0.1816,  0.3093, -0.2261, -0.0809, -0.0694, -0.1643, -0.1804,  0.2403],\n",
      "       requires_grad=True)\n",
      "Bgate spliced: torch.Size([32])\n",
      "tensor([-0.2264,  0.2777, -0.0718, -0.1366, -0.2667,  0.1974, -0.2165,  0.1699,\n",
      "         0.3184,  0.0362,  0.2520,  0.3413,  0.1179,  0.1998,  0.3149, -0.0912,\n",
      "        -0.2235, -0.0496, -0.0858,  0.3456,  0.2254, -0.3406,  0.3058, -0.3508,\n",
      "         0.1816,  0.3093, -0.2261, -0.0809, -0.0694, -0.1643, -0.1804,  0.2403],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xgate: torch.Size([1, 3, 32])\n",
      "tensor([[[-0.1357, -0.2488, -0.8203, -0.1086, -1.2407,  0.3796,  0.8442,\n",
      "           0.9216,  1.2046,  0.4272,  0.6227,  0.1836, -0.0741,  0.4388,\n",
      "           0.4733, -0.0927, -0.6364,  0.1153,  0.1813,  0.8188, -0.0200,\n",
      "           0.7413, -0.0115, -0.8657, -0.3414, -0.1771,  0.8959,  0.3191,\n",
      "           0.2348,  0.8862,  0.0990, -0.1016],\n",
      "         [-0.0561,  0.7932,  0.9569, -1.3415,  0.2060, -0.8196, -0.9753,\n",
      "          -0.1757,  0.6811, -0.7964,  0.4364,  1.3785, -0.0619, -0.2660,\n",
      "           0.0091, -0.2429, -0.2874, -0.3302,  0.3734,  0.0493,  1.1878,\n",
      "          -0.6270,  0.3996,  0.0138,  1.1047,  0.5639, -0.8789, -0.2284,\n",
      "          -1.1460, -1.0608, -0.2098, -0.7379],\n",
      "         [-1.4969, -0.3120, -0.7103,  0.6378, -1.0156,  0.0603,  1.2912,\n",
      "           0.5894,  1.0048, -0.0617, -0.4337,  0.5861,  0.4359,  0.5102,\n",
      "           0.2529, -0.6506, -0.2681, -0.5221,  0.8419,  0.9393,  0.0109,\n",
      "           0.2829,  0.2753, -0.5230, -0.5955,  0.2685,  0.4854,  0.4630,\n",
      "          -0.1300,  0.7210, -0.2097, -0.0125]]], grad_fn=<AddBackward0>)\n",
      "GeLU'ed Xgate: torch.Size([1, 3, 32])\n",
      "tensor([[[-0.0605, -0.0999, -0.1690, -0.0496, -0.1332,  0.2459,  0.6760,\n",
      "           0.7572,  1.0671,  0.2843,  0.4566,  0.1052, -0.0349,  0.2938,\n",
      "           0.3228, -0.0429, -0.1669,  0.0630,  0.1037,  0.6498, -0.0098,\n",
      "           0.5714, -0.0057, -0.1674, -0.1251, -0.0761,  0.7300,  0.1995,\n",
      "           0.1392,  0.7198,  0.0534, -0.0467],\n",
      "         [-0.0268,  0.6236,  0.7949, -0.1206,  0.1198, -0.1690, -0.1606,\n",
      "          -0.0756,  0.5123, -0.1696,  0.2918,  1.2627, -0.0294, -0.1051,\n",
      "           0.0046, -0.0981, -0.1112, -0.1224,  0.2411,  0.0256,  1.0482,\n",
      "          -0.1664,  0.2618,  0.0070,  0.9560,  0.4023, -0.1668, -0.0936,\n",
      "          -0.1443, -0.1532, -0.0875, -0.1699],\n",
      "         [-0.1006, -0.1178, -0.1696,  0.4708, -0.1573,  0.0316,  1.1643,\n",
      "           0.4257,  0.8465, -0.0293, -0.1441,  0.4226,  0.2915,  0.3546,\n",
      "           0.1517, -0.1676, -0.1057, -0.1570,  0.6736,  0.7760,  0.0055,\n",
      "           0.1730,  0.1675, -0.1572, -0.1642,  0.1627,  0.3331,  0.3140,\n",
      "          -0.0583,  0.5512, -0.0874, -0.0062]]], grad_fn=<GeluBackward0>)\n",
      "Wup: torch.Size([8, 32])\n",
      "Parameter containing:\n",
      "tensor([[ 0.1891, -0.3348,  0.1431,  0.2513, -0.1019,  0.2473,  0.3179, -0.2051,\n",
      "          0.3282, -0.1770, -0.3482,  0.2050,  0.1590,  0.0381, -0.2299, -0.1161,\n",
      "          0.2286,  0.0283,  0.2427, -0.3277, -0.3090,  0.0499, -0.0448, -0.1890,\n",
      "         -0.1538,  0.2338, -0.2407,  0.0591, -0.3110, -0.3443,  0.3113,  0.3400],\n",
      "        [-0.2720, -0.1980, -0.2918,  0.2277,  0.1436, -0.0712,  0.2805, -0.2086,\n",
      "         -0.3358,  0.0009,  0.1926, -0.2556,  0.1962,  0.2934, -0.0669, -0.1529,\n",
      "          0.1861, -0.3025,  0.0016,  0.2442, -0.0454,  0.3393,  0.2080, -0.3150,\n",
      "          0.0084, -0.1488, -0.2285,  0.3414, -0.1945,  0.3108, -0.0665, -0.2185],\n",
      "        [ 0.0034, -0.1525, -0.3498, -0.3306, -0.0650,  0.0450,  0.2885, -0.0640,\n",
      "         -0.2515, -0.0544,  0.0966, -0.3490, -0.0808,  0.1167,  0.0109, -0.0537,\n",
      "          0.0025, -0.2318, -0.1315, -0.1360, -0.0682, -0.3040,  0.3258, -0.2382,\n",
      "         -0.2034, -0.3028, -0.0071,  0.0127,  0.2829, -0.2993,  0.3113, -0.0200],\n",
      "        [ 0.3333,  0.2307,  0.3451, -0.3023, -0.1704, -0.0212,  0.3249, -0.2695,\n",
      "         -0.0329,  0.2475, -0.2836, -0.0755,  0.2599, -0.2236, -0.3102,  0.0324,\n",
      "         -0.2449, -0.2291, -0.2291, -0.0349, -0.2352, -0.2767,  0.3527, -0.3438,\n",
      "         -0.0973,  0.0151, -0.0009,  0.0269, -0.0377,  0.2885, -0.0231,  0.2471],\n",
      "        [-0.0255,  0.1854,  0.2569,  0.2652, -0.1578,  0.1092,  0.2218, -0.2025,\n",
      "         -0.1547,  0.3488, -0.1787,  0.3313,  0.2785, -0.1284, -0.2222, -0.0724,\n",
      "          0.1638, -0.0323, -0.2788, -0.0774, -0.2185,  0.0230, -0.3394,  0.0077,\n",
      "         -0.3189,  0.2470,  0.0507, -0.0696,  0.3521,  0.2695,  0.1502,  0.3292],\n",
      "        [-0.3306, -0.1778, -0.0753,  0.2954,  0.2052, -0.1997,  0.0728, -0.2261,\n",
      "         -0.0628,  0.0681, -0.0183, -0.2011,  0.2070,  0.2789, -0.0077,  0.1752,\n",
      "         -0.2285, -0.2642, -0.2965, -0.2129,  0.2878,  0.2500,  0.1709,  0.1211,\n",
      "         -0.0039,  0.3450, -0.0835,  0.0534,  0.0063, -0.2916, -0.2434, -0.0075],\n",
      "        [-0.2189, -0.0074,  0.0152,  0.1700,  0.0678,  0.0495, -0.0782, -0.0932,\n",
      "         -0.2998,  0.3001,  0.0643, -0.1114,  0.1595,  0.3484, -0.1022,  0.0340,\n",
      "          0.2111,  0.1723,  0.2733, -0.0897,  0.2623, -0.3368, -0.1903, -0.3080,\n",
      "          0.3486,  0.1027, -0.1949,  0.3073, -0.0341,  0.1220,  0.0937,  0.3201],\n",
      "        [ 0.2910,  0.2288, -0.0279,  0.3497, -0.3246,  0.2042, -0.2583,  0.0809,\n",
      "          0.2776, -0.2417, -0.3348,  0.2586, -0.2370, -0.2478, -0.1746, -0.2704,\n",
      "          0.0050,  0.2293, -0.2476, -0.3414,  0.0631,  0.3129, -0.1799,  0.1883,\n",
      "          0.2111,  0.0105, -0.1609, -0.0887, -0.2803, -0.3330, -0.3441, -0.0239]],\n",
      "       requires_grad=True)\n",
      "Wup spliced: torch.Size([8, 32])\n",
      "tensor([[ 0.1891, -0.3348,  0.1431,  0.2513, -0.1019,  0.2473,  0.3179, -0.2051,\n",
      "          0.3282, -0.1770, -0.3482,  0.2050,  0.1590,  0.0381, -0.2299, -0.1161,\n",
      "          0.2286,  0.0283,  0.2427, -0.3277, -0.3090,  0.0499, -0.0448, -0.1890,\n",
      "         -0.1538,  0.2338, -0.2407,  0.0591, -0.3110, -0.3443,  0.3113,  0.3400],\n",
      "        [-0.2720, -0.1980, -0.2918,  0.2277,  0.1436, -0.0712,  0.2805, -0.2086,\n",
      "         -0.3358,  0.0009,  0.1926, -0.2556,  0.1962,  0.2934, -0.0669, -0.1529,\n",
      "          0.1861, -0.3025,  0.0016,  0.2442, -0.0454,  0.3393,  0.2080, -0.3150,\n",
      "          0.0084, -0.1488, -0.2285,  0.3414, -0.1945,  0.3108, -0.0665, -0.2185],\n",
      "        [ 0.0034, -0.1525, -0.3498, -0.3306, -0.0650,  0.0450,  0.2885, -0.0640,\n",
      "         -0.2515, -0.0544,  0.0966, -0.3490, -0.0808,  0.1167,  0.0109, -0.0537,\n",
      "          0.0025, -0.2318, -0.1315, -0.1360, -0.0682, -0.3040,  0.3258, -0.2382,\n",
      "         -0.2034, -0.3028, -0.0071,  0.0127,  0.2829, -0.2993,  0.3113, -0.0200],\n",
      "        [ 0.3333,  0.2307,  0.3451, -0.3023, -0.1704, -0.0212,  0.3249, -0.2695,\n",
      "         -0.0329,  0.2475, -0.2836, -0.0755,  0.2599, -0.2236, -0.3102,  0.0324,\n",
      "         -0.2449, -0.2291, -0.2291, -0.0349, -0.2352, -0.2767,  0.3527, -0.3438,\n",
      "         -0.0973,  0.0151, -0.0009,  0.0269, -0.0377,  0.2885, -0.0231,  0.2471],\n",
      "        [-0.0255,  0.1854,  0.2569,  0.2652, -0.1578,  0.1092,  0.2218, -0.2025,\n",
      "         -0.1547,  0.3488, -0.1787,  0.3313,  0.2785, -0.1284, -0.2222, -0.0724,\n",
      "          0.1638, -0.0323, -0.2788, -0.0774, -0.2185,  0.0230, -0.3394,  0.0077,\n",
      "         -0.3189,  0.2470,  0.0507, -0.0696,  0.3521,  0.2695,  0.1502,  0.3292],\n",
      "        [-0.3306, -0.1778, -0.0753,  0.2954,  0.2052, -0.1997,  0.0728, -0.2261,\n",
      "         -0.0628,  0.0681, -0.0183, -0.2011,  0.2070,  0.2789, -0.0077,  0.1752,\n",
      "         -0.2285, -0.2642, -0.2965, -0.2129,  0.2878,  0.2500,  0.1709,  0.1211,\n",
      "         -0.0039,  0.3450, -0.0835,  0.0534,  0.0063, -0.2916, -0.2434, -0.0075],\n",
      "        [-0.2189, -0.0074,  0.0152,  0.1700,  0.0678,  0.0495, -0.0782, -0.0932,\n",
      "         -0.2998,  0.3001,  0.0643, -0.1114,  0.1595,  0.3484, -0.1022,  0.0340,\n",
      "          0.2111,  0.1723,  0.2733, -0.0897,  0.2623, -0.3368, -0.1903, -0.3080,\n",
      "          0.3486,  0.1027, -0.1949,  0.3073, -0.0341,  0.1220,  0.0937,  0.3201],\n",
      "        [ 0.2910,  0.2288, -0.0279,  0.3497, -0.3246,  0.2042, -0.2583,  0.0809,\n",
      "          0.2776, -0.2417, -0.3348,  0.2586, -0.2370, -0.2478, -0.1746, -0.2704,\n",
      "          0.0050,  0.2293, -0.2476, -0.3414,  0.0631,  0.3129, -0.1799,  0.1883,\n",
      "          0.2111,  0.0105, -0.1609, -0.0887, -0.2803, -0.3330, -0.3441, -0.0239]],\n",
      "       grad_fn=<AliasBackward0>)\n",
      "Bup: torch.Size([32])\n",
      "Parameter containing:\n",
      "tensor([ 0.2824,  0.0903,  0.1206, -0.2387,  0.3339,  0.1524, -0.3451, -0.0093,\n",
      "        -0.0834,  0.1387, -0.1012, -0.2124, -0.0021, -0.3290, -0.0827, -0.1971,\n",
      "        -0.2421, -0.3223, -0.3180,  0.2969, -0.2551, -0.2841, -0.3353,  0.2673,\n",
      "        -0.2556,  0.0490,  0.0844,  0.0745, -0.1900, -0.2533, -0.2110, -0.2201],\n",
      "       requires_grad=True)\n",
      "Bup spliced: torch.Size([32])\n",
      "tensor([ 0.2824,  0.0903,  0.1206, -0.2387,  0.3339,  0.1524, -0.3451, -0.0093,\n",
      "        -0.0834,  0.1387, -0.1012, -0.2124, -0.0021, -0.3290, -0.0827, -0.1971,\n",
      "        -0.2421, -0.3223, -0.3180,  0.2969, -0.2551, -0.2841, -0.3353,  0.2673,\n",
      "        -0.2556,  0.0490,  0.0844,  0.0745, -0.1900, -0.2533, -0.2110, -0.2201],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xup: torch.Size([1, 3, 32])\n",
      "tensor([[[-0.4044, -1.0253, -0.3126,  1.2974,  0.6104,  0.4238, -0.0865,\n",
      "          -0.5632,  0.1008, -0.1920, -0.4390, -0.2919,  0.3940,  0.7847,\n",
      "          -0.3753, -0.3304,  0.3267, -0.3062,  0.0639, -0.7568,  0.1908,\n",
      "           0.3399, -0.5964,  0.0630,  0.0605,  0.9244, -0.8547,  0.6606,\n",
      "          -0.9511, -1.6133, -0.0847,  0.2984],\n",
      "         [ 1.2423,  0.5526,  0.5666, -0.8213, -0.1120,  0.0579, -0.3807,\n",
      "           0.2696,  1.0167, -0.4716, -0.6935,  0.3560, -0.4663, -1.6590,\n",
      "           0.0314, -0.1348, -1.1810, -0.5675, -1.3760,  0.1490, -0.8556,\n",
      "           0.3006,  0.0456,  1.3461, -1.1228,  0.0761,  0.7784, -1.0670,\n",
      "           0.0498, -0.7902, -0.6580, -0.7149],\n",
      "         [ 0.0368, -0.0998,  0.2096, -0.1673,  0.6575,  0.1803, -1.5011,\n",
      "           0.5801,  0.3554,  0.0202,  0.0610, -0.1831, -0.4710,  0.1524,\n",
      "           0.3729,  0.2668, -0.2251,  0.7057,  0.7243, -0.1477,  0.7464,\n",
      "          -0.9999, -0.9678,  0.6704,  0.7921,  0.4444,  0.0466,  0.0890,\n",
      "          -0.4788, -1.1220, -0.2133,  0.1300]]], grad_fn=<AddBackward0>)\n",
      "Xfuse: torch.Size([1, 3, 32])\n",
      "tensor([[[ 2.4486e-02,  1.0248e-01,  5.2822e-02, -6.4358e-02, -8.1303e-02,\n",
      "           1.0422e-01, -5.8450e-02, -4.2648e-01,  1.0752e-01, -5.4578e-02,\n",
      "          -2.0045e-01, -3.0700e-02, -1.3743e-02,  2.3056e-01, -1.2112e-01,\n",
      "           1.4180e-02, -5.4531e-02, -1.9280e-02,  6.6226e-03, -4.9177e-01,\n",
      "          -1.8781e-03,  1.9419e-01,  3.3871e-03, -1.0537e-02, -7.5725e-03,\n",
      "          -7.0349e-02, -6.2391e-01,  1.3178e-01, -1.3236e-01, -1.1613e+00,\n",
      "          -4.5241e-03, -1.3931e-02],\n",
      "         [-3.3292e-02,  3.4459e-01,  4.5045e-01,  9.9024e-02, -1.3425e-02,\n",
      "          -9.7911e-03,  6.1151e-02, -2.0380e-02,  5.2087e-01,  7.9968e-02,\n",
      "          -2.0236e-01,  4.4950e-01,  1.3710e-02,  1.7435e-01,  1.4428e-04,\n",
      "           1.3228e-02,  1.3133e-01,  6.9449e-02, -3.3170e-01,  3.8144e-03,\n",
      "          -8.9687e-01, -5.0006e-02,  1.1943e-02,  9.3629e-03, -1.0734e+00,\n",
      "           3.0636e-02, -1.2980e-01,  9.9826e-02, -7.1924e-03,  1.2104e-01,\n",
      "           5.7563e-02,  1.2148e-01],\n",
      "         [-3.7063e-03,  1.1756e-02, -3.5547e-02, -7.8744e-02, -1.0344e-01,\n",
      "           5.6993e-03, -1.7477e+00,  2.4696e-01,  3.0085e-01, -5.9349e-04,\n",
      "          -8.7919e-03, -7.7378e-02, -1.3729e-01,  5.4025e-02,  5.6557e-02,\n",
      "          -4.4730e-02,  2.3801e-02, -1.1082e-01,  4.8791e-01, -1.1459e-01,\n",
      "           4.1132e-03, -1.7296e-01, -1.6209e-01, -1.0537e-01, -1.3007e-01,\n",
      "           7.2297e-02,  1.5516e-02,  2.7957e-02,  2.7905e-02, -6.1851e-01,\n",
      "           1.8650e-02, -8.0462e-04]]], grad_fn=<MulBackward0>)\n",
      "Wdown: torch.Size([32, 8])\n",
      "Parameter containing:\n",
      "tensor([[-3.1355e-02,  3.6585e-02,  3.6894e-02, -1.3828e-02, -5.0864e-02,\n",
      "         -7.7739e-02,  7.8714e-02, -1.0312e-01],\n",
      "        [-1.3010e-01, -1.4110e-01,  1.2179e-01, -1.0945e-01, -1.2249e-01,\n",
      "          1.6907e-01, -8.7172e-02,  3.5724e-02],\n",
      "        [-4.5318e-02, -1.6887e-01, -1.3354e-02,  9.5296e-02, -1.7005e-01,\n",
      "         -1.3798e-01, -8.9512e-02,  1.3186e-01],\n",
      "        [ 8.8651e-02,  1.6334e-02,  1.5033e-01,  3.4891e-02,  2.7176e-02,\n",
      "         -9.8128e-02, -9.6446e-02, -7.1352e-02],\n",
      "        [-1.1127e-01, -1.7228e-01, -1.3319e-01, -4.7356e-02, -1.0979e-01,\n",
      "          5.3924e-02, -1.6958e-01, -8.5660e-02],\n",
      "        [-1.7094e-01, -5.4964e-03,  1.3648e-01, -1.1574e-01, -1.6192e-01,\n",
      "         -1.6018e-03,  5.2023e-02,  8.8789e-02],\n",
      "        [-1.2047e-01,  1.1335e-01,  5.3661e-03, -6.5454e-03, -1.7370e-01,\n",
      "          1.0451e-01,  4.7912e-02,  1.1133e-01],\n",
      "        [ 7.2820e-02, -1.5413e-01, -1.3741e-01, -1.0161e-01, -1.2747e-01,\n",
      "          1.5089e-01,  6.0612e-02, -7.3363e-02],\n",
      "        [-5.2269e-02,  1.4841e-01, -1.7492e-01, -1.3143e-01, -1.1084e-01,\n",
      "          5.1809e-02,  1.5182e-01, -1.5181e-01],\n",
      "        [ 1.6782e-01,  2.9896e-02,  4.0980e-03,  1.6873e-01, -1.6533e-01,\n",
      "          1.1474e-01, -1.6794e-02,  1.5594e-01],\n",
      "        [-1.5010e-01, -1.3560e-02,  7.3077e-02,  1.1726e-01, -6.8219e-03,\n",
      "          4.5938e-02, -9.9155e-02, -2.9751e-03],\n",
      "        [ 1.7348e-01, -1.1838e-01,  5.0580e-02, -1.6478e-01, -6.5153e-03,\n",
      "          1.6758e-01, -1.2350e-01,  1.2695e-01],\n",
      "        [ 3.7887e-02,  5.4085e-02, -7.3148e-03,  1.2091e-03,  3.4779e-02,\n",
      "          1.4121e-01,  3.5768e-02,  7.8466e-02],\n",
      "        [ 9.0600e-02, -1.5027e-01, -1.6800e-01, -7.4179e-02, -1.2019e-02,\n",
      "         -1.2385e-01, -6.3113e-02,  3.7009e-02],\n",
      "        [-4.0653e-02, -1.5005e-01, -1.4039e-01, -4.3561e-02,  8.3764e-02,\n",
      "          1.5913e-01, -9.9894e-02, -1.3029e-01],\n",
      "        [ 5.4227e-02, -4.3747e-02, -1.4250e-01,  1.6453e-01,  5.3544e-02,\n",
      "         -1.7382e-01,  7.5603e-02, -1.1672e-01],\n",
      "        [ 1.3068e-01,  3.2768e-02, -1.4872e-01,  1.8265e-02, -1.9594e-02,\n",
      "          1.5943e-01, -3.5449e-02, -1.5641e-01],\n",
      "        [ 3.1814e-02,  2.8846e-02,  1.0038e-01,  2.6652e-02,  1.7215e-01,\n",
      "          4.1252e-02,  4.1964e-02, -1.3067e-01],\n",
      "        [-1.4907e-01, -1.0314e-01, -8.9474e-02, -4.8484e-02, -1.4959e-01,\n",
      "         -1.1083e-01, -4.8872e-03,  1.8293e-03],\n",
      "        [ 1.6387e-01, -1.1631e-01,  8.6573e-02, -2.1528e-02, -6.8403e-02,\n",
      "         -1.6459e-01, -6.9652e-02,  7.4229e-02],\n",
      "        [-1.6679e-01, -9.3716e-02, -6.1629e-02,  6.1625e-02,  1.7168e-01,\n",
      "         -1.0182e-01,  2.6129e-02,  6.0644e-02],\n",
      "        [-4.2471e-02, -1.2998e-01, -1.3981e-01,  7.7821e-02,  4.2238e-02,\n",
      "          1.4273e-01, -8.6938e-02, -1.0105e-02],\n",
      "        [-1.5584e-01, -1.5044e-01,  5.7489e-02,  1.4627e-01, -5.7655e-03,\n",
      "         -1.2296e-02,  7.3597e-02, -1.9613e-02],\n",
      "        [-3.4707e-02, -1.5434e-01, -3.9184e-02,  3.1919e-02,  1.7107e-01,\n",
      "         -5.2179e-02, -2.2853e-02,  2.9895e-04],\n",
      "        [-3.8735e-02, -1.2177e-01,  3.6174e-02,  5.9803e-02,  4.9718e-02,\n",
      "          2.1142e-02, -6.5383e-03, -1.1767e-01],\n",
      "        [-1.2257e-01,  8.8373e-02,  2.2258e-02, -1.2801e-01, -5.6065e-02,\n",
      "         -9.9842e-02, -1.0121e-01, -8.6156e-02],\n",
      "        [ 1.1428e-01,  1.2171e-01,  2.3687e-02, -2.9700e-02,  1.6325e-01,\n",
      "         -5.5560e-02,  6.2100e-02,  7.3736e-02],\n",
      "        [-1.2759e-01,  6.2463e-02,  1.6956e-01,  2.0842e-02, -1.0427e-01,\n",
      "         -1.4254e-01, -3.8105e-02, -3.3327e-02],\n",
      "        [-8.4661e-02,  8.0418e-02,  7.8576e-02,  5.8173e-02,  1.5175e-01,\n",
      "          8.8992e-02, -1.3159e-02,  1.7578e-01],\n",
      "        [-4.8491e-02, -7.4514e-03,  6.8876e-04,  1.6711e-01,  9.7252e-02,\n",
      "         -4.0158e-02, -4.3876e-02, -1.2246e-01],\n",
      "        [ 7.1733e-03,  3.6569e-02,  4.6691e-02,  5.0983e-03, -6.5416e-05,\n",
      "          1.2602e-01,  1.3041e-01, -2.7197e-02],\n",
      "        [-6.5462e-02, -4.2117e-02, -1.3896e-01,  5.3109e-02, -1.5426e-01,\n",
      "         -1.4133e-01, -1.2919e-01, -1.5776e-01]], requires_grad=True)\n",
      "Wdown spliced: torch.Size([32, 8])\n",
      "tensor([[-3.1355e-02,  3.6585e-02,  3.6894e-02, -1.3828e-02, -5.0864e-02,\n",
      "         -7.7739e-02,  7.8714e-02, -1.0312e-01],\n",
      "        [-1.3010e-01, -1.4110e-01,  1.2179e-01, -1.0945e-01, -1.2249e-01,\n",
      "          1.6907e-01, -8.7172e-02,  3.5724e-02],\n",
      "        [-4.5318e-02, -1.6887e-01, -1.3354e-02,  9.5296e-02, -1.7005e-01,\n",
      "         -1.3798e-01, -8.9512e-02,  1.3186e-01],\n",
      "        [ 8.8651e-02,  1.6334e-02,  1.5033e-01,  3.4891e-02,  2.7176e-02,\n",
      "         -9.8128e-02, -9.6446e-02, -7.1352e-02],\n",
      "        [-1.1127e-01, -1.7228e-01, -1.3319e-01, -4.7356e-02, -1.0979e-01,\n",
      "          5.3924e-02, -1.6958e-01, -8.5660e-02],\n",
      "        [-1.7094e-01, -5.4964e-03,  1.3648e-01, -1.1574e-01, -1.6192e-01,\n",
      "         -1.6018e-03,  5.2023e-02,  8.8789e-02],\n",
      "        [-1.2047e-01,  1.1335e-01,  5.3661e-03, -6.5454e-03, -1.7370e-01,\n",
      "          1.0451e-01,  4.7912e-02,  1.1133e-01],\n",
      "        [ 7.2820e-02, -1.5413e-01, -1.3741e-01, -1.0161e-01, -1.2747e-01,\n",
      "          1.5089e-01,  6.0612e-02, -7.3363e-02],\n",
      "        [-5.2269e-02,  1.4841e-01, -1.7492e-01, -1.3143e-01, -1.1084e-01,\n",
      "          5.1809e-02,  1.5182e-01, -1.5181e-01],\n",
      "        [ 1.6782e-01,  2.9896e-02,  4.0980e-03,  1.6873e-01, -1.6533e-01,\n",
      "          1.1474e-01, -1.6794e-02,  1.5594e-01],\n",
      "        [-1.5010e-01, -1.3560e-02,  7.3077e-02,  1.1726e-01, -6.8219e-03,\n",
      "          4.5938e-02, -9.9155e-02, -2.9751e-03],\n",
      "        [ 1.7348e-01, -1.1838e-01,  5.0580e-02, -1.6478e-01, -6.5153e-03,\n",
      "          1.6758e-01, -1.2350e-01,  1.2695e-01],\n",
      "        [ 3.7887e-02,  5.4085e-02, -7.3148e-03,  1.2091e-03,  3.4779e-02,\n",
      "          1.4121e-01,  3.5768e-02,  7.8466e-02],\n",
      "        [ 9.0600e-02, -1.5027e-01, -1.6800e-01, -7.4179e-02, -1.2019e-02,\n",
      "         -1.2385e-01, -6.3113e-02,  3.7009e-02],\n",
      "        [-4.0653e-02, -1.5005e-01, -1.4039e-01, -4.3561e-02,  8.3764e-02,\n",
      "          1.5913e-01, -9.9894e-02, -1.3029e-01],\n",
      "        [ 5.4227e-02, -4.3747e-02, -1.4250e-01,  1.6453e-01,  5.3544e-02,\n",
      "         -1.7382e-01,  7.5603e-02, -1.1672e-01],\n",
      "        [ 1.3068e-01,  3.2768e-02, -1.4872e-01,  1.8265e-02, -1.9594e-02,\n",
      "          1.5943e-01, -3.5449e-02, -1.5641e-01],\n",
      "        [ 3.1814e-02,  2.8846e-02,  1.0038e-01,  2.6652e-02,  1.7215e-01,\n",
      "          4.1252e-02,  4.1964e-02, -1.3067e-01],\n",
      "        [-1.4907e-01, -1.0314e-01, -8.9474e-02, -4.8484e-02, -1.4959e-01,\n",
      "         -1.1083e-01, -4.8872e-03,  1.8293e-03],\n",
      "        [ 1.6387e-01, -1.1631e-01,  8.6573e-02, -2.1528e-02, -6.8403e-02,\n",
      "         -1.6459e-01, -6.9652e-02,  7.4229e-02],\n",
      "        [-1.6679e-01, -9.3716e-02, -6.1629e-02,  6.1625e-02,  1.7168e-01,\n",
      "         -1.0182e-01,  2.6129e-02,  6.0644e-02],\n",
      "        [-4.2471e-02, -1.2998e-01, -1.3981e-01,  7.7821e-02,  4.2238e-02,\n",
      "          1.4273e-01, -8.6938e-02, -1.0105e-02],\n",
      "        [-1.5584e-01, -1.5044e-01,  5.7489e-02,  1.4627e-01, -5.7655e-03,\n",
      "         -1.2296e-02,  7.3597e-02, -1.9613e-02],\n",
      "        [-3.4707e-02, -1.5434e-01, -3.9184e-02,  3.1919e-02,  1.7107e-01,\n",
      "         -5.2179e-02, -2.2853e-02,  2.9895e-04],\n",
      "        [-3.8735e-02, -1.2177e-01,  3.6174e-02,  5.9803e-02,  4.9718e-02,\n",
      "          2.1142e-02, -6.5383e-03, -1.1767e-01],\n",
      "        [-1.2257e-01,  8.8373e-02,  2.2258e-02, -1.2801e-01, -5.6065e-02,\n",
      "         -9.9842e-02, -1.0121e-01, -8.6156e-02],\n",
      "        [ 1.1428e-01,  1.2171e-01,  2.3687e-02, -2.9700e-02,  1.6325e-01,\n",
      "         -5.5560e-02,  6.2100e-02,  7.3736e-02],\n",
      "        [-1.2759e-01,  6.2463e-02,  1.6956e-01,  2.0842e-02, -1.0427e-01,\n",
      "         -1.4254e-01, -3.8105e-02, -3.3327e-02],\n",
      "        [-8.4661e-02,  8.0418e-02,  7.8576e-02,  5.8173e-02,  1.5175e-01,\n",
      "          8.8992e-02, -1.3159e-02,  1.7578e-01],\n",
      "        [-4.8491e-02, -7.4514e-03,  6.8876e-04,  1.6711e-01,  9.7252e-02,\n",
      "         -4.0158e-02, -4.3876e-02, -1.2246e-01],\n",
      "        [ 7.1733e-03,  3.6569e-02,  4.6691e-02,  5.0983e-03, -6.5416e-05,\n",
      "          1.2602e-01,  1.3041e-01, -2.7197e-02],\n",
      "        [-6.5462e-02, -4.2117e-02, -1.3896e-01,  5.3109e-02, -1.5426e-01,\n",
      "         -1.4133e-01, -1.2919e-01, -1.5776e-01]], grad_fn=<AliasBackward0>)\n",
      "Bdown: torch.Size([8])\n",
      "Parameter containing:\n",
      "tensor([-0.1412,  0.1029, -0.1046,  0.1465, -0.1325,  0.0145, -0.1405, -0.0524],\n",
      "       requires_grad=True)\n",
      "Bdown spliced: torch.Size([8])\n",
      "tensor([-0.1412,  0.1029, -0.1046,  0.1465, -0.1325,  0.0145, -0.1405, -0.0524],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "outputs: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.2684,  0.1104, -0.1443, -0.0269, -0.3209,  0.0448, -0.0802,\n",
      "           0.0450],\n",
      "         [ 0.1257,  0.2409, -0.1245, -0.0770, -0.5158,  0.1816, -0.2269,\n",
      "           0.0112],\n",
      "         [ 0.0239, -0.0240, -0.2550, -0.0948, -0.0581, -0.1674, -0.1147,\n",
      "          -0.2226]]], grad_fn=<AddBackward0>)\n",
      "------------- END MLP.forwardTensor() ------------\n",
      "layer's final residual state:\n",
      "tensor([[[ 1.8621,  0.5943,  0.0928, -1.2366, -0.5227,  1.8973,  1.2019,\n",
      "           0.6794],\n",
      "         [-0.0710, -0.8199, -0.2435,  0.4829, -0.3740,  0.5088, -2.9446,\n",
      "           0.3719],\n",
      "         [ 0.2186, -1.4938, -0.6808, -0.8450, -1.1288,  0.0832,  1.2810,\n",
      "          -0.3561]]], grad_fn=<AddBackward0>)\n",
      "----------------- END Layer.forwardTensor() --------------------\n",
      "forwardTensor() output: torch.Size([1, 3, 8])\n",
      "tensor([[[ 1.8621,  0.5943,  0.0928, -1.2366, -0.5227,  1.8973,  1.2019,\n",
      "           0.6794],\n",
      "         [-0.0710, -0.8199, -0.2435,  0.4829, -0.3740,  0.5088, -2.9446,\n",
      "           0.3719],\n",
      "         [ 0.2186, -1.4938, -0.6808, -0.8450, -1.1288,  0.0832,  1.2810,\n",
      "          -0.3561]]], grad_fn=<AddBackward0>)\n",
      "Level 1 from range(2)\n",
      "Model 0 from range(2)\n",
      "----------------- Layer.forwardTensor() --------------------\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.2293, -0.9171, -0.9802, -0.5067],\n",
      "         [ 0.1680, -1.1694,  0.7582,  0.1348],\n",
      "         [-1.9322, -0.7696, -1.3604, -0.6910]]])\n",
      "normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.3156, -1.2624, -1.3492, -0.6975],\n",
      "         [ 0.2383, -1.6584,  1.0753,  0.1912],\n",
      "         [-1.4981, -0.5967, -1.0548, -0.5358]]])\n",
      "dim: 4\n",
      "skip: 0\n",
      "spliced scale: torch.Size([4])\n",
      "tensor([0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.3156, -1.2624, -1.3492, -0.6975],\n",
      "         [ 0.2383, -1.6584,  1.0753,  0.1912],\n",
      "         [-1.4981, -0.5967, -1.0548, -0.5358]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "---------- Attention Input: torch.Tensor ------------\n",
      "----------------- MultiQueryAttention.forwardTensor() --------------------\n",
      "x shape: torch.Size([1, 3, 4])\n",
      "d_skip: 0\n",
      "models_in_this_level: 2\n",
      "h_dim: 2\n",
      "h_skip: 0\n",
      "self.Wqkv: torch.Size([8, 16])\n",
      "Parameter containing:\n",
      "tensor([[-0.3398,  0.3307,  0.2225,  0.2923,  0.3199, -0.1244,  0.0621,  0.2447,\n",
      "         -0.3499, -0.1872,  0.3051,  0.0890,  0.2710,  0.1609,  0.3357,  0.0098],\n",
      "        [ 0.2298, -0.1531, -0.2039, -0.1286, -0.2591,  0.1291,  0.3100, -0.0835,\n",
      "         -0.2479, -0.1510, -0.1537,  0.0181,  0.2160,  0.3484, -0.3323, -0.1380],\n",
      "        [ 0.1080, -0.0664, -0.3089,  0.1810,  0.2769,  0.0328,  0.3347, -0.1358,\n",
      "         -0.0295,  0.2815, -0.3144,  0.1225,  0.2875,  0.0617, -0.0904, -0.1679],\n",
      "        [-0.2684, -0.2637, -0.0269,  0.3338,  0.0707, -0.3283,  0.1122, -0.1725,\n",
      "         -0.0218, -0.0872,  0.2521,  0.1246,  0.0284, -0.0780, -0.2766, -0.1182],\n",
      "        [-0.1520, -0.2973, -0.2569, -0.0197, -0.0728,  0.1796, -0.2989, -0.2442,\n",
      "          0.3104, -0.2646, -0.0299,  0.2852,  0.3015, -0.1969, -0.0207,  0.0691],\n",
      "        [-0.1881, -0.2773,  0.0862, -0.3252, -0.1013, -0.1286, -0.0218, -0.3159,\n",
      "         -0.3131, -0.2189,  0.2176, -0.3087, -0.3061,  0.2190, -0.2383,  0.0816],\n",
      "        [ 0.1831,  0.0092,  0.1412, -0.1254,  0.0028,  0.2533, -0.1741,  0.1159,\n",
      "          0.2736,  0.1486,  0.2726,  0.3237,  0.1083, -0.1763, -0.0169,  0.0802],\n",
      "        [ 0.0394, -0.1980, -0.1334, -0.2768, -0.0239,  0.0613,  0.0357, -0.0684,\n",
      "         -0.0411,  0.0550,  0.2665, -0.2499,  0.3059,  0.1568, -0.0046, -0.0349]],\n",
      "       requires_grad=True)\n",
      "Wq: torch.Size([8, 8])\n",
      "tensor([[-0.3398,  0.3307,  0.2225,  0.2923,  0.3199, -0.1244,  0.0621,  0.2447],\n",
      "        [ 0.2298, -0.1531, -0.2039, -0.1286, -0.2591,  0.1291,  0.3100, -0.0835],\n",
      "        [ 0.1080, -0.0664, -0.3089,  0.1810,  0.2769,  0.0328,  0.3347, -0.1358],\n",
      "        [-0.2684, -0.2637, -0.0269,  0.3338,  0.0707, -0.3283,  0.1122, -0.1725],\n",
      "        [-0.1520, -0.2973, -0.2569, -0.0197, -0.0728,  0.1796, -0.2989, -0.2442],\n",
      "        [-0.1881, -0.2773,  0.0862, -0.3252, -0.1013, -0.1286, -0.0218, -0.3159],\n",
      "        [ 0.1831,  0.0092,  0.1412, -0.1254,  0.0028,  0.2533, -0.1741,  0.1159],\n",
      "        [ 0.0394, -0.1980, -0.1334, -0.2768, -0.0239,  0.0613,  0.0357, -0.0684]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wk: torch.Size([8, 4])\n",
      "tensor([[-0.3499, -0.1872,  0.3051,  0.0890],\n",
      "        [-0.2479, -0.1510, -0.1537,  0.0181],\n",
      "        [-0.0295,  0.2815, -0.3144,  0.1225],\n",
      "        [-0.0218, -0.0872,  0.2521,  0.1246],\n",
      "        [ 0.3104, -0.2646, -0.0299,  0.2852],\n",
      "        [-0.3131, -0.2189,  0.2176, -0.3087],\n",
      "        [ 0.2736,  0.1486,  0.2726,  0.3237],\n",
      "        [-0.0411,  0.0550,  0.2665, -0.2499]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wv: torch.Size([8, 4])\n",
      "tensor([[ 0.2710,  0.1609,  0.3357,  0.0098],\n",
      "        [ 0.2160,  0.3484, -0.3323, -0.1380],\n",
      "        [ 0.2875,  0.0617, -0.0904, -0.1679],\n",
      "        [ 0.0284, -0.0780, -0.2766, -0.1182],\n",
      "        [ 0.3015, -0.1969, -0.0207,  0.0691],\n",
      "        [-0.3061,  0.2190, -0.2383,  0.0816],\n",
      "        [ 0.1083, -0.1763, -0.0169,  0.0802],\n",
      "        [ 0.3059,  0.1568, -0.0046, -0.0349]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wq spliced: torch.Size([4, 4])\n",
      "tensor([[-0.3398,  0.3307,  0.3199, -0.1244],\n",
      "        [ 0.2298, -0.1531, -0.2591,  0.1291],\n",
      "        [ 0.1080, -0.0664,  0.2769,  0.0328],\n",
      "        [-0.2684, -0.2637,  0.0707, -0.3283]], grad_fn=<CatBackward0>)\n",
      "Wk spliced: torch.Size([4, 2])\n",
      "tensor([[-0.3499, -0.1872],\n",
      "        [-0.2479, -0.1510],\n",
      "        [-0.0295,  0.2815],\n",
      "        [-0.0218, -0.0872]], grad_fn=<CatBackward0>)\n",
      "Wv spliced: torch.Size([4, 2])\n",
      "tensor([[ 0.2710,  0.1609],\n",
      "        [ 0.2160,  0.3484],\n",
      "        [ 0.2875,  0.0617],\n",
      "        [ 0.0284, -0.0780]], grad_fn=<CatBackward0>)\n",
      "Wqkv_spliced: torch.Size([4, 8])\n",
      "tensor([[-0.3398,  0.3307,  0.3199, -0.1244, -0.3499, -0.1872,  0.2710,  0.1609],\n",
      "        [ 0.2298, -0.1531, -0.2591,  0.1291, -0.2479, -0.1510,  0.2160,  0.3484],\n",
      "        [ 0.1080, -0.0664,  0.2769,  0.0328, -0.0295,  0.2815,  0.2875,  0.0617],\n",
      "        [-0.2684, -0.2637,  0.0707, -0.3283, -0.0218, -0.0872,  0.0284, -0.0780]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "xqkv: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.3559,  0.5713,  0.0051, -0.0175,  0.2575, -0.1875, -0.5948,\n",
      "          -0.4179],\n",
      "         [-0.3973,  0.2109,  0.8171, -0.2713,  0.2918,  0.4918,  0.0210,\n",
      "          -0.4881],\n",
      "         [ 0.4018, -0.1927, -0.6546,  0.2507,  0.7149,  0.1203, -0.8534,\n",
      "          -0.4723]]], grad_fn=<UnsafeViewBackward0>)\n",
      "xq: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.3559,  0.5713,  0.0051, -0.0175],\n",
      "         [-0.3973,  0.2109,  0.8171, -0.2713],\n",
      "         [ 0.4018, -0.1927, -0.6546,  0.2507]]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "xk: torch.Size([1, 3, 2])\n",
      "tensor([[[ 0.2575, -0.1875],\n",
      "         [ 0.2918,  0.4918],\n",
      "         [ 0.7149,  0.1203]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xv: torch.Size([1, 3, 2])\n",
      "tensor([[[-0.5948, -0.4179],\n",
      "         [ 0.0210, -0.4881],\n",
      "         [-0.8534, -0.4723]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xq reshaped: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[-0.3559,  0.5713],\n",
      "          [ 0.0051, -0.0175]],\n",
      "\n",
      "         [[-0.3973,  0.2109],\n",
      "          [ 0.8171, -0.2713]],\n",
      "\n",
      "         [[ 0.4018, -0.1927],\n",
      "          [-0.6546,  0.2507]]]], grad_fn=<ViewBackward0>)\n",
      "xk reshaped: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[ 0.2575, -0.1875]],\n",
      "\n",
      "         [[ 0.2918,  0.4918]],\n",
      "\n",
      "         [[ 0.7149,  0.1203]]]], grad_fn=<ViewBackward0>)\n",
      "xv reshaped: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[-0.5948, -0.4179]],\n",
      "\n",
      "         [[ 0.0210, -0.4881]],\n",
      "\n",
      "         [[-0.8534, -0.4723]]]], grad_fn=<ViewBackward0>)\n",
      "rotated xq: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[-0.3559,  0.5713],\n",
      "          [ 0.0051, -0.0175]],\n",
      "\n",
      "         [[-0.3922, -0.2204],\n",
      "          [ 0.6698,  0.5410]],\n",
      "\n",
      "         [[ 0.0080,  0.4455],\n",
      "          [ 0.0445, -0.6995]]]], grad_fn=<TransposeBackward0>)\n",
      "rotated xk: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[ 0.2575, -0.1875]],\n",
      "\n",
      "         [[-0.2562,  0.5112]],\n",
      "\n",
      "         [[-0.4069,  0.6000]]]], grad_fn=<TransposeBackward0>)\n",
      "repeat_interleaved xk: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[ 0.2575, -0.1875],\n",
      "          [ 0.2575, -0.1875]],\n",
      "\n",
      "         [[-0.2562,  0.5112],\n",
      "          [-0.2562,  0.5112]],\n",
      "\n",
      "         [[-0.4069,  0.6000],\n",
      "          [-0.4069,  0.6000]]]], grad_fn=<ViewBackward0>)\n",
      "repeat_interleaved xv: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[-0.5948, -0.4179],\n",
      "          [-0.5948, -0.4179]],\n",
      "\n",
      "         [[ 0.0210, -0.4881],\n",
      "          [ 0.0210, -0.4881]],\n",
      "\n",
      "         [[-0.8534, -0.4723],\n",
      "          [-0.8534, -0.4723]]]], grad_fn=<ViewBackward0>)\n",
      "transposed xq: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[-0.3559,  0.5713],\n",
      "          [-0.3922, -0.2204],\n",
      "          [ 0.0080,  0.4455]],\n",
      "\n",
      "         [[ 0.0051, -0.0175],\n",
      "          [ 0.6698,  0.5410],\n",
      "          [ 0.0445, -0.6995]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xk: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[ 0.2575, -0.1875],\n",
      "          [-0.2562,  0.5112],\n",
      "          [-0.4069,  0.6000]],\n",
      "\n",
      "         [[ 0.2575, -0.1875],\n",
      "          [-0.2562,  0.5112],\n",
      "          [-0.4069,  0.6000]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xv: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[-0.5948, -0.4179],\n",
      "          [ 0.0210, -0.4881],\n",
      "          [-0.8534, -0.4723]],\n",
      "\n",
      "         [[-0.5948, -0.4179],\n",
      "          [ 0.0210, -0.4881],\n",
      "          [-0.8534, -0.4723]]]], grad_fn=<TransposeBackward0>)\n",
      "scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[-0.1406,  0.2710,  0.3448],\n",
      "          [-0.0422, -0.0086,  0.0193],\n",
      "          [-0.0576,  0.1596,  0.1867]],\n",
      "\n",
      "         [[ 0.0032, -0.0073, -0.0089],\n",
      "          [ 0.0502,  0.0742,  0.0369],\n",
      "          [ 0.1008, -0.2609, -0.3096]]]], grad_fn=<MulBackward0>)\n",
      "mask: torch.Size([1, 1, 3, 3])\n",
      "tensor([[[[ 0.0000e+00, -2.3820e+38, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]]]])\n",
      "masked scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[-1.4055e-01, -2.3820e+38, -2.3820e+38],\n",
      "          [-4.2192e-02, -8.6192e-03, -2.3820e+38],\n",
      "          [-5.7610e-02,  1.5961e-01,  1.8673e-01]],\n",
      "\n",
      "         [[ 3.2485e-03, -2.3820e+38, -2.3820e+38],\n",
      "          [ 5.0227e-02,  7.4243e-02, -2.3820e+38],\n",
      "          [ 1.0084e-01, -2.6093e-01, -3.0958e-01]]]], grad_fn=<AddBackward0>)\n",
      "softmaxed scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [0.4916, 0.5084, 0.0000],\n",
      "          [0.2841, 0.3531, 0.3628]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.4940, 0.5060, 0.0000],\n",
      "          [0.4238, 0.2951, 0.2811]]]], grad_fn=<SoftmaxBackward0>)\n",
      "attention: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[-0.5948, -0.4179],\n",
      "          [-0.2817, -0.4536],\n",
      "          [-0.4712, -0.4624]],\n",
      "\n",
      "         [[-0.5948, -0.4179],\n",
      "          [-0.2832, -0.4534],\n",
      "          [-0.4858, -0.4539]]]], grad_fn=<UnsafeViewBackward0>)\n",
      "reshaped attention: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.5948, -0.4179, -0.5948, -0.4179],\n",
      "         [-0.2817, -0.4536, -0.2832, -0.4534],\n",
      "         [-0.4712, -0.4624, -0.4858, -0.4539]]], grad_fn=<ViewBackward0>)\n",
      "self.Wo: torch.Size([8, 8])\n",
      "Parameter containing:\n",
      "tensor([[-0.3331, -0.0057, -0.1193, -0.0371,  0.0632, -0.1175, -0.1165, -0.1170],\n",
      "        [ 0.3058,  0.1560, -0.2966, -0.3141, -0.3463, -0.2982,  0.2295,  0.2648],\n",
      "        [ 0.1682, -0.3533,  0.3376,  0.2572, -0.0676, -0.0124,  0.1574,  0.1800],\n",
      "        [-0.1671,  0.0802, -0.2675,  0.0040, -0.2241, -0.2670,  0.1884, -0.2433],\n",
      "        [ 0.1423,  0.2316,  0.0518,  0.0542, -0.0278,  0.3124,  0.0649,  0.3067],\n",
      "        [-0.1438,  0.1334, -0.2953, -0.2768,  0.0403,  0.0571, -0.0478, -0.1368],\n",
      "        [-0.2495, -0.1464, -0.3111,  0.2914, -0.1158, -0.0949,  0.1083,  0.0125],\n",
      "        [-0.2954,  0.3428,  0.0848, -0.2212, -0.2844,  0.2948, -0.1303,  0.3506]],\n",
      "       requires_grad=True)\n",
      "spliced Wo: torch.Size([4, 4])\n",
      "tensor([[-0.3331, -0.0057, -0.1193, -0.0371],\n",
      "        [ 0.3058,  0.1560, -0.2966, -0.3141],\n",
      "        [ 0.1423,  0.2316,  0.0518,  0.0542],\n",
      "        [-0.1438,  0.1334, -0.2953, -0.2768]], grad_fn=<CatBackward0>)\n",
      "projected output: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.0458, -0.2553,  0.2875,  0.2368],\n",
      "         [-0.0199, -0.1952,  0.2874,  0.2631],\n",
      "         [ 0.0117, -0.2425,  0.3022,  0.2621]]], grad_fn=<UnsafeViewBackward0>)\n",
      "----------------- END MultiQueryAttention.forwardTensor() --------------------\n",
      "x in layer after MQA & resid connection and before MLP:\n",
      "tensor([[[ 0.2751, -1.1724, -0.6927, -0.2699],\n",
      "         [ 0.1481, -1.3646,  1.0456,  0.3979],\n",
      "         [-1.9205, -1.0121, -1.0581, -0.4290]]], grad_fn=<AddBackward0>)\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.2751, -1.1724, -0.6927, -0.2699],\n",
      "         [ 0.1481, -1.3646,  1.0456,  0.3979],\n",
      "         [-1.9205, -1.0121, -1.0581, -0.4290]]], grad_fn=<AddBackward0>)\n",
      "normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.3887, -1.6569, -0.9788, -0.3814],\n",
      "         [ 0.1673, -1.5413,  1.1809,  0.4494],\n",
      "         [-1.5659, -0.8253, -0.8628, -0.3498]]], grad_fn=<MulBackward0>)\n",
      "dim: 4\n",
      "skip: 0\n",
      "spliced scale: torch.Size([4])\n",
      "tensor([0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.3887, -1.6569, -0.9788, -0.3814],\n",
      "         [ 0.1673, -1.5413,  1.1809,  0.4494],\n",
      "         [-1.5659, -0.8253, -0.8628, -0.3498]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "---------- MLP Input: torch.Tensor ------------\n",
      "------------- MLP.forwardTensor() ------------\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.3887, -1.6569, -0.9788, -0.3814],\n",
      "         [ 0.1673, -1.5413,  1.1809,  0.4494],\n",
      "         [-1.5659, -0.8253, -0.8628, -0.3498]]], grad_fn=<MulBackward0>)\n",
      "d_dim: 4\n",
      "d_skip: 0\n",
      "i_dim: 16\n",
      "i_skip: 0\n",
      "Wgate: torch.Size([8, 32])\n",
      "Parameter containing:\n",
      "tensor([[-2.0378e-01,  1.9155e-02, -2.5673e-01, -6.6076e-02, -1.5894e-01,\n",
      "         -5.8077e-02,  1.1944e-01,  3.1422e-01,  2.6704e-01,  6.0142e-02,\n",
      "          2.8076e-01,  6.6552e-02,  1.2014e-02, -3.0538e-01,  2.2672e-01,\n",
      "         -2.8452e-01, -3.3381e-01, -1.0627e-01, -5.5746e-02,  2.2767e-03,\n",
      "         -8.7861e-02,  3.5301e-01,  9.5396e-02, -1.4413e-01,  2.2739e-01,\n",
      "         -3.0789e-01,  2.5667e-01,  1.9587e-01,  1.9188e-01,  1.9864e-01,\n",
      "          7.8174e-02, -1.4472e-01],\n",
      "        [ 2.5881e-01,  1.8849e-01, -1.9905e-01,  9.8076e-03,  3.1139e-02,\n",
      "          2.0969e-01, -1.7834e-01, -2.0160e-01, -2.3034e-01,  1.6279e-01,\n",
      "          3.0912e-01, -2.8784e-01, -2.4899e-02,  9.5506e-02,  3.1060e-01,\n",
      "          2.0226e-01, -1.2328e-02,  1.5144e-01, -3.1324e-01, -3.0475e-01,\n",
      "         -1.3892e-01, -2.6066e-01,  8.7560e-02, -7.3888e-02, -1.3559e-01,\n",
      "         -6.5933e-02, -9.5685e-02, -1.1806e-01,  3.3973e-01, -5.2369e-02,\n",
      "         -7.2154e-04,  3.1122e-01],\n",
      "        [ 3.2107e-01, -2.6924e-01,  3.1704e-01,  2.3734e-01, -2.0238e-01,\n",
      "          2.7264e-01, -3.1823e-01,  2.2726e-01,  1.3663e-01,  1.3384e-01,\n",
      "         -3.3406e-01, -8.7860e-02, -1.7728e-02,  1.2769e-01, -3.1617e-01,\n",
      "         -2.1740e-01,  3.7097e-03, -1.0999e-02, -2.5561e-01,  1.0879e-01,\n",
      "          1.0981e-01,  2.1874e-01,  9.8952e-02,  2.1657e-01,  2.1956e-01,\n",
      "         -3.4451e-01, -2.0934e-01,  1.7877e-01, -1.7365e-01,  1.1120e-02,\n",
      "         -1.0411e-01, -8.7443e-02],\n",
      "        [ 3.0102e-01,  1.8049e-01, -9.3183e-03, -3.0233e-01,  4.4365e-02,\n",
      "          1.5189e-01, -2.9368e-01, -3.8777e-02, -1.9838e-01,  3.5013e-01,\n",
      "          1.1811e-01,  1.2853e-01, -2.8444e-01, -2.3235e-01,  1.2906e-01,\n",
      "          2.5792e-01, -3.0151e-01,  8.0087e-03, -8.7959e-03,  7.7719e-02,\n",
      "         -1.5681e-01,  1.5885e-01, -1.8853e-01, -1.3954e-01,  2.9057e-01,\n",
      "          6.7097e-02, -2.2614e-01, -3.3118e-01, -5.7758e-02, -1.8288e-01,\n",
      "          1.8856e-01, -1.0947e-01],\n",
      "        [ 3.3077e-01, -1.5129e-01,  2.2010e-01, -1.5577e-01,  3.0039e-01,\n",
      "          4.0851e-02, -1.7102e-01,  1.7203e-01, -1.8540e-01,  8.4346e-02,\n",
      "          1.7938e-02, -1.2574e-01, -8.7709e-02, -1.0353e-01, -3.1317e-01,\n",
      "          2.7271e-01,  1.7714e-01,  3.0434e-01, -1.7270e-01,  2.7432e-01,\n",
      "         -4.6780e-02,  1.1775e-01, -2.9078e-01, -4.1688e-02,  1.8667e-01,\n",
      "          2.2608e-01,  2.0868e-01, -1.2030e-01, -4.9017e-02, -8.0353e-03,\n",
      "          6.4723e-02,  7.7618e-02],\n",
      "        [ 3.1620e-01, -1.3605e-01, -2.4454e-02, -3.0133e-01, -2.2047e-01,\n",
      "         -6.0336e-03,  1.4947e-01,  1.1901e-02,  2.4008e-01,  2.5826e-01,\n",
      "         -6.3199e-03,  2.3596e-01, -2.0454e-01,  2.1074e-01, -2.1293e-02,\n",
      "          3.2555e-01, -7.3171e-02,  9.9653e-02,  3.2077e-01,  2.5021e-01,\n",
      "         -3.1209e-02,  3.1273e-01, -3.0262e-01, -2.2676e-01, -5.4055e-02,\n",
      "          2.3609e-01,  8.8153e-02, -8.7053e-02, -3.4416e-01,  1.9098e-01,\n",
      "          9.2401e-02, -3.0739e-01],\n",
      "        [-8.3603e-03, -2.4367e-01, -2.9212e-01,  3.2735e-01, -1.8916e-01,\n",
      "          3.3348e-01,  3.4109e-01,  1.8712e-01, -1.0089e-01,  3.1761e-01,\n",
      "         -1.4733e-01, -2.7488e-01, -4.3122e-02,  1.3442e-01, -3.5292e-02,\n",
      "          1.4565e-01, -2.6186e-03,  1.1012e-01, -9.4417e-03,  2.9175e-01,\n",
      "         -3.1957e-01,  2.4675e-01, -1.9900e-01, -1.5049e-01, -3.1706e-01,\n",
      "         -8.3404e-03,  2.9004e-01, -3.2020e-02,  2.4540e-01,  3.1980e-01,\n",
      "          9.4737e-02,  2.1059e-01],\n",
      "        [ 3.1330e-01,  6.9906e-02,  1.7583e-01, -2.2837e-01, -7.3682e-02,\n",
      "         -7.1424e-02,  1.4529e-01,  2.1353e-02, -9.1109e-03, -3.2874e-01,\n",
      "          2.0343e-01, -2.0165e-01, -2.6832e-01, -3.1146e-04, -2.9679e-01,\n",
      "          2.1311e-01,  3.8456e-02,  1.5998e-01,  4.3876e-02,  5.8273e-03,\n",
      "          3.1290e-01, -6.7655e-02, -1.9788e-01,  1.6022e-01, -3.3185e-01,\n",
      "         -1.9130e-01,  2.2986e-01, -2.1936e-01,  1.2736e-01, -1.7436e-01,\n",
      "          2.3090e-01, -8.5473e-02]], requires_grad=True)\n",
      "Wgate spliced: torch.Size([4, 16])\n",
      "tensor([[-0.2038,  0.0192, -0.2567, -0.0661, -0.1589, -0.0581,  0.1194,  0.3142,\n",
      "          0.2670,  0.0601,  0.2808,  0.0666,  0.0120, -0.3054,  0.2267, -0.2845],\n",
      "        [ 0.2588,  0.1885, -0.1991,  0.0098,  0.0311,  0.2097, -0.1783, -0.2016,\n",
      "         -0.2303,  0.1628,  0.3091, -0.2878, -0.0249,  0.0955,  0.3106,  0.2023],\n",
      "        [ 0.3211, -0.2692,  0.3170,  0.2373, -0.2024,  0.2726, -0.3182,  0.2273,\n",
      "          0.1366,  0.1338, -0.3341, -0.0879, -0.0177,  0.1277, -0.3162, -0.2174],\n",
      "        [ 0.3010,  0.1805, -0.0093, -0.3023,  0.0444,  0.1519, -0.2937, -0.0388,\n",
      "         -0.1984,  0.3501,  0.1181,  0.1285, -0.2844, -0.2323,  0.1291,  0.2579]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Bgate: torch.Size([32])\n",
      "Parameter containing:\n",
      "tensor([-0.2264,  0.2777, -0.0718, -0.1366, -0.2667,  0.1974, -0.2165,  0.1699,\n",
      "         0.3184,  0.0362,  0.2520,  0.3413,  0.1179,  0.1998,  0.3149, -0.0912,\n",
      "        -0.2235, -0.0496, -0.0858,  0.3456,  0.2254, -0.3406,  0.3058, -0.3508,\n",
      "         0.1816,  0.3093, -0.2261, -0.0809, -0.0694, -0.1643, -0.1804,  0.2403],\n",
      "       requires_grad=True)\n",
      "Bgate spliced: torch.Size([16])\n",
      "tensor([-0.2264,  0.2777, -0.0718, -0.1366, -0.2667,  0.1974, -0.2165,  0.1699,\n",
      "         0.3184,  0.0362,  0.2520,  0.3413,  0.1179,  0.1998,  0.3149, -0.0912],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xgate: torch.Size([1, 3, 16])\n",
      "tensor([[[-1.1635,  0.1675, -0.1485, -0.2955, -0.1989, -0.4975,  0.5489,\n",
      "           0.4184,  0.7458, -0.4747,  0.1309,  0.8811,  0.2896, -0.1135,\n",
      "           0.1487, -0.4225],\n",
      "         [-0.1449, -0.2465,  0.5623, -0.0183, -0.5603,  0.2547, -0.4295,\n",
      "           0.7841,  0.7902,  0.1108, -0.5189,  0.7501,  0.0095,  0.0479,\n",
      "          -0.4413, -0.5913],\n",
      "         [-0.5032,  0.2613,  0.2243, -0.1402,  0.1156, -0.1731,  0.1209,\n",
      "          -0.3383,  0.0418, -0.4302, -0.1959,  0.5055,  0.2344,  0.5703,\n",
      "          -0.0688,  0.2848]]], grad_fn=<AddBackward0>)\n",
      "GeLU'ed Xgate: torch.Size([1, 3, 16])\n",
      "tensor([[[-0.1423,  0.0949, -0.0655, -0.1134, -0.0838, -0.1539,  0.3889,\n",
      "           0.2770,  0.5758, -0.1507,  0.0723,  0.7145,  0.1778, -0.0516,\n",
      "           0.0831, -0.1421],\n",
      "         [-0.0641, -0.0992,  0.4009, -0.0090, -0.1612,  0.1529, -0.1433,\n",
      "           0.6143,  0.6206,  0.0603, -0.1567,  0.5802,  0.0048,  0.0248,\n",
      "          -0.1454, -0.1639],\n",
      "         [-0.1547,  0.1576,  0.1320, -0.0623,  0.0631, -0.0747,  0.0663,\n",
      "          -0.1244,  0.0216, -0.1435, -0.0827,  0.3505,  0.1389,  0.4082,\n",
      "          -0.0325,  0.1743]]], grad_fn=<GeluBackward0>)\n",
      "Wup: torch.Size([8, 32])\n",
      "Parameter containing:\n",
      "tensor([[ 0.1891, -0.3348,  0.1431,  0.2513, -0.1019,  0.2473,  0.3179, -0.2051,\n",
      "          0.3282, -0.1770, -0.3482,  0.2050,  0.1590,  0.0381, -0.2299, -0.1161,\n",
      "          0.2286,  0.0283,  0.2427, -0.3277, -0.3090,  0.0499, -0.0448, -0.1890,\n",
      "         -0.1538,  0.2338, -0.2407,  0.0591, -0.3110, -0.3443,  0.3113,  0.3400],\n",
      "        [-0.2720, -0.1980, -0.2918,  0.2277,  0.1436, -0.0712,  0.2805, -0.2086,\n",
      "         -0.3358,  0.0009,  0.1926, -0.2556,  0.1962,  0.2934, -0.0669, -0.1529,\n",
      "          0.1861, -0.3025,  0.0016,  0.2442, -0.0454,  0.3393,  0.2080, -0.3150,\n",
      "          0.0084, -0.1488, -0.2285,  0.3414, -0.1945,  0.3108, -0.0665, -0.2185],\n",
      "        [ 0.0034, -0.1525, -0.3498, -0.3306, -0.0650,  0.0450,  0.2885, -0.0640,\n",
      "         -0.2515, -0.0544,  0.0966, -0.3490, -0.0808,  0.1167,  0.0109, -0.0537,\n",
      "          0.0025, -0.2318, -0.1315, -0.1360, -0.0682, -0.3040,  0.3258, -0.2382,\n",
      "         -0.2034, -0.3028, -0.0071,  0.0127,  0.2829, -0.2993,  0.3113, -0.0200],\n",
      "        [ 0.3333,  0.2307,  0.3451, -0.3023, -0.1704, -0.0212,  0.3249, -0.2695,\n",
      "         -0.0329,  0.2475, -0.2836, -0.0755,  0.2599, -0.2236, -0.3102,  0.0324,\n",
      "         -0.2449, -0.2291, -0.2291, -0.0349, -0.2352, -0.2767,  0.3527, -0.3438,\n",
      "         -0.0973,  0.0151, -0.0009,  0.0269, -0.0377,  0.2885, -0.0231,  0.2471],\n",
      "        [-0.0255,  0.1854,  0.2569,  0.2652, -0.1578,  0.1092,  0.2218, -0.2025,\n",
      "         -0.1547,  0.3488, -0.1787,  0.3313,  0.2785, -0.1284, -0.2222, -0.0724,\n",
      "          0.1638, -0.0323, -0.2788, -0.0774, -0.2185,  0.0230, -0.3394,  0.0077,\n",
      "         -0.3189,  0.2470,  0.0507, -0.0696,  0.3521,  0.2695,  0.1502,  0.3292],\n",
      "        [-0.3306, -0.1778, -0.0753,  0.2954,  0.2052, -0.1997,  0.0728, -0.2261,\n",
      "         -0.0628,  0.0681, -0.0183, -0.2011,  0.2070,  0.2789, -0.0077,  0.1752,\n",
      "         -0.2285, -0.2642, -0.2965, -0.2129,  0.2878,  0.2500,  0.1709,  0.1211,\n",
      "         -0.0039,  0.3450, -0.0835,  0.0534,  0.0063, -0.2916, -0.2434, -0.0075],\n",
      "        [-0.2189, -0.0074,  0.0152,  0.1700,  0.0678,  0.0495, -0.0782, -0.0932,\n",
      "         -0.2998,  0.3001,  0.0643, -0.1114,  0.1595,  0.3484, -0.1022,  0.0340,\n",
      "          0.2111,  0.1723,  0.2733, -0.0897,  0.2623, -0.3368, -0.1903, -0.3080,\n",
      "          0.3486,  0.1027, -0.1949,  0.3073, -0.0341,  0.1220,  0.0937,  0.3201],\n",
      "        [ 0.2910,  0.2288, -0.0279,  0.3497, -0.3246,  0.2042, -0.2583,  0.0809,\n",
      "          0.2776, -0.2417, -0.3348,  0.2586, -0.2370, -0.2478, -0.1746, -0.2704,\n",
      "          0.0050,  0.2293, -0.2476, -0.3414,  0.0631,  0.3129, -0.1799,  0.1883,\n",
      "          0.2111,  0.0105, -0.1609, -0.0887, -0.2803, -0.3330, -0.3441, -0.0239]],\n",
      "       requires_grad=True)\n",
      "Wup spliced: torch.Size([4, 16])\n",
      "tensor([[ 0.1891, -0.3348,  0.1431,  0.2513, -0.1019,  0.2473,  0.3179, -0.2051,\n",
      "          0.3282, -0.1770, -0.3482,  0.2050,  0.1590,  0.0381, -0.2299, -0.1161],\n",
      "        [-0.2720, -0.1980, -0.2918,  0.2277,  0.1436, -0.0712,  0.2805, -0.2086,\n",
      "         -0.3358,  0.0009,  0.1926, -0.2556,  0.1962,  0.2934, -0.0669, -0.1529],\n",
      "        [ 0.0034, -0.1525, -0.3498, -0.3306, -0.0650,  0.0450,  0.2885, -0.0640,\n",
      "         -0.2515, -0.0544,  0.0966, -0.3490, -0.0808,  0.1167,  0.0109, -0.0537],\n",
      "        [ 0.3333,  0.2307,  0.3451, -0.3023, -0.1704, -0.0212,  0.3249, -0.2695,\n",
      "         -0.0329,  0.2475, -0.2836, -0.0755,  0.2599, -0.2236, -0.3102,  0.0324]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Bup: torch.Size([32])\n",
      "Parameter containing:\n",
      "tensor([ 0.2824,  0.0903,  0.1206, -0.2387,  0.3339,  0.1524, -0.3451, -0.0093,\n",
      "        -0.0834,  0.1387, -0.1012, -0.2124, -0.0021, -0.3290, -0.0827, -0.1971,\n",
      "        -0.2421, -0.3223, -0.3180,  0.2969, -0.2551, -0.2841, -0.3353,  0.2673,\n",
      "        -0.2556,  0.0490,  0.0844,  0.0745, -0.1900, -0.2533, -0.2110, -0.2201],\n",
      "       requires_grad=True)\n",
      "Bup spliced: torch.Size([16])\n",
      "tensor([ 0.2824,  0.0903,  0.1206, -0.2387,  0.3339,  0.1524, -0.3451, -0.0093,\n",
      "        -0.0834,  0.1387, -0.1012, -0.2124, -0.0021, -0.3290, -0.0827, -0.1971],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xup: torch.Size([1, 3, 16])\n",
      "tensor([[[ 0.6762,  0.3495,  0.8704, -0.0794,  0.1850,  0.3305, -1.0926,\n",
      "           0.4221,  0.8593,  0.0271, -0.5421,  0.6612, -0.2855, -0.8293,\n",
      "           0.0465,  0.0513],\n",
      "         [ 0.8872,  0.2631,  0.3363, -1.0739, -0.0578,  0.3472, -0.2376,\n",
      "           0.0812,  0.1774,  0.1546, -0.4696, -0.2303, -0.2565, -0.7376,\n",
      "          -0.1446, -0.0298],\n",
      "         [ 0.0913,  0.8289,  0.3184, -0.4292,  0.4907, -0.2076, -1.4370,\n",
      "           0.6336, -0.0918,  0.3755,  0.3009,  0.0050, -0.4342, -0.6532,\n",
      "           0.4317,  0.1459]]], grad_fn=<AddBackward0>)\n",
      "Xfuse: torch.Size([1, 3, 16])\n",
      "tensor([[[-0.0962,  0.0332, -0.0570,  0.0090, -0.0155, -0.0509, -0.4249,\n",
      "           0.1169,  0.4948, -0.0041, -0.0392,  0.4724, -0.0508,  0.0428,\n",
      "           0.0039, -0.0073],\n",
      "         [-0.0569, -0.0261,  0.1348,  0.0097,  0.0093,  0.0531,  0.0341,\n",
      "           0.0499,  0.1101,  0.0093,  0.0736, -0.1336, -0.0012, -0.0183,\n",
      "           0.0210,  0.0049],\n",
      "         [-0.0141,  0.1306,  0.0420,  0.0267,  0.0310,  0.0155, -0.0952,\n",
      "          -0.0788, -0.0020, -0.0539, -0.0249,  0.0018, -0.0603, -0.2666,\n",
      "          -0.0140,  0.0254]]], grad_fn=<MulBackward0>)\n",
      "Wdown: torch.Size([32, 8])\n",
      "Parameter containing:\n",
      "tensor([[-3.1355e-02,  3.6585e-02,  3.6894e-02, -1.3828e-02, -5.0864e-02,\n",
      "         -7.7739e-02,  7.8714e-02, -1.0312e-01],\n",
      "        [-1.3010e-01, -1.4110e-01,  1.2179e-01, -1.0945e-01, -1.2249e-01,\n",
      "          1.6907e-01, -8.7172e-02,  3.5724e-02],\n",
      "        [-4.5318e-02, -1.6887e-01, -1.3354e-02,  9.5296e-02, -1.7005e-01,\n",
      "         -1.3798e-01, -8.9512e-02,  1.3186e-01],\n",
      "        [ 8.8651e-02,  1.6334e-02,  1.5033e-01,  3.4891e-02,  2.7176e-02,\n",
      "         -9.8128e-02, -9.6446e-02, -7.1352e-02],\n",
      "        [-1.1127e-01, -1.7228e-01, -1.3319e-01, -4.7356e-02, -1.0979e-01,\n",
      "          5.3924e-02, -1.6958e-01, -8.5660e-02],\n",
      "        [-1.7094e-01, -5.4964e-03,  1.3648e-01, -1.1574e-01, -1.6192e-01,\n",
      "         -1.6018e-03,  5.2023e-02,  8.8789e-02],\n",
      "        [-1.2047e-01,  1.1335e-01,  5.3661e-03, -6.5454e-03, -1.7370e-01,\n",
      "          1.0451e-01,  4.7912e-02,  1.1133e-01],\n",
      "        [ 7.2820e-02, -1.5413e-01, -1.3741e-01, -1.0161e-01, -1.2747e-01,\n",
      "          1.5089e-01,  6.0612e-02, -7.3363e-02],\n",
      "        [-5.2269e-02,  1.4841e-01, -1.7492e-01, -1.3143e-01, -1.1084e-01,\n",
      "          5.1809e-02,  1.5182e-01, -1.5181e-01],\n",
      "        [ 1.6782e-01,  2.9896e-02,  4.0980e-03,  1.6873e-01, -1.6533e-01,\n",
      "          1.1474e-01, -1.6794e-02,  1.5594e-01],\n",
      "        [-1.5010e-01, -1.3560e-02,  7.3077e-02,  1.1726e-01, -6.8219e-03,\n",
      "          4.5938e-02, -9.9155e-02, -2.9751e-03],\n",
      "        [ 1.7348e-01, -1.1838e-01,  5.0580e-02, -1.6478e-01, -6.5153e-03,\n",
      "          1.6758e-01, -1.2350e-01,  1.2695e-01],\n",
      "        [ 3.7887e-02,  5.4085e-02, -7.3148e-03,  1.2091e-03,  3.4779e-02,\n",
      "          1.4121e-01,  3.5768e-02,  7.8466e-02],\n",
      "        [ 9.0600e-02, -1.5027e-01, -1.6800e-01, -7.4179e-02, -1.2019e-02,\n",
      "         -1.2385e-01, -6.3113e-02,  3.7009e-02],\n",
      "        [-4.0653e-02, -1.5005e-01, -1.4039e-01, -4.3561e-02,  8.3764e-02,\n",
      "          1.5913e-01, -9.9894e-02, -1.3029e-01],\n",
      "        [ 5.4227e-02, -4.3747e-02, -1.4250e-01,  1.6453e-01,  5.3544e-02,\n",
      "         -1.7382e-01,  7.5603e-02, -1.1672e-01],\n",
      "        [ 1.3068e-01,  3.2768e-02, -1.4872e-01,  1.8265e-02, -1.9594e-02,\n",
      "          1.5943e-01, -3.5449e-02, -1.5641e-01],\n",
      "        [ 3.1814e-02,  2.8846e-02,  1.0038e-01,  2.6652e-02,  1.7215e-01,\n",
      "          4.1252e-02,  4.1964e-02, -1.3067e-01],\n",
      "        [-1.4907e-01, -1.0314e-01, -8.9474e-02, -4.8484e-02, -1.4959e-01,\n",
      "         -1.1083e-01, -4.8872e-03,  1.8293e-03],\n",
      "        [ 1.6387e-01, -1.1631e-01,  8.6573e-02, -2.1528e-02, -6.8403e-02,\n",
      "         -1.6459e-01, -6.9652e-02,  7.4229e-02],\n",
      "        [-1.6679e-01, -9.3716e-02, -6.1629e-02,  6.1625e-02,  1.7168e-01,\n",
      "         -1.0182e-01,  2.6129e-02,  6.0644e-02],\n",
      "        [-4.2471e-02, -1.2998e-01, -1.3981e-01,  7.7821e-02,  4.2238e-02,\n",
      "          1.4273e-01, -8.6938e-02, -1.0105e-02],\n",
      "        [-1.5584e-01, -1.5044e-01,  5.7489e-02,  1.4627e-01, -5.7655e-03,\n",
      "         -1.2296e-02,  7.3597e-02, -1.9613e-02],\n",
      "        [-3.4707e-02, -1.5434e-01, -3.9184e-02,  3.1919e-02,  1.7107e-01,\n",
      "         -5.2179e-02, -2.2853e-02,  2.9895e-04],\n",
      "        [-3.8735e-02, -1.2177e-01,  3.6174e-02,  5.9803e-02,  4.9718e-02,\n",
      "          2.1142e-02, -6.5383e-03, -1.1767e-01],\n",
      "        [-1.2257e-01,  8.8373e-02,  2.2258e-02, -1.2801e-01, -5.6065e-02,\n",
      "         -9.9842e-02, -1.0121e-01, -8.6156e-02],\n",
      "        [ 1.1428e-01,  1.2171e-01,  2.3687e-02, -2.9700e-02,  1.6325e-01,\n",
      "         -5.5560e-02,  6.2100e-02,  7.3736e-02],\n",
      "        [-1.2759e-01,  6.2463e-02,  1.6956e-01,  2.0842e-02, -1.0427e-01,\n",
      "         -1.4254e-01, -3.8105e-02, -3.3327e-02],\n",
      "        [-8.4661e-02,  8.0418e-02,  7.8576e-02,  5.8173e-02,  1.5175e-01,\n",
      "          8.8992e-02, -1.3159e-02,  1.7578e-01],\n",
      "        [-4.8491e-02, -7.4514e-03,  6.8876e-04,  1.6711e-01,  9.7252e-02,\n",
      "         -4.0158e-02, -4.3876e-02, -1.2246e-01],\n",
      "        [ 7.1733e-03,  3.6569e-02,  4.6691e-02,  5.0983e-03, -6.5416e-05,\n",
      "          1.2602e-01,  1.3041e-01, -2.7197e-02],\n",
      "        [-6.5462e-02, -4.2117e-02, -1.3896e-01,  5.3109e-02, -1.5426e-01,\n",
      "         -1.4133e-01, -1.2919e-01, -1.5776e-01]], requires_grad=True)\n",
      "Wdown spliced: torch.Size([16, 4])\n",
      "tensor([[-0.0314,  0.0366,  0.0369, -0.0138],\n",
      "        [-0.1301, -0.1411,  0.1218, -0.1094],\n",
      "        [-0.0453, -0.1689, -0.0134,  0.0953],\n",
      "        [ 0.0887,  0.0163,  0.1503,  0.0349],\n",
      "        [-0.1113, -0.1723, -0.1332, -0.0474],\n",
      "        [-0.1709, -0.0055,  0.1365, -0.1157],\n",
      "        [-0.1205,  0.1134,  0.0054, -0.0065],\n",
      "        [ 0.0728, -0.1541, -0.1374, -0.1016],\n",
      "        [-0.0523,  0.1484, -0.1749, -0.1314],\n",
      "        [ 0.1678,  0.0299,  0.0041,  0.1687],\n",
      "        [-0.1501, -0.0136,  0.0731,  0.1173],\n",
      "        [ 0.1735, -0.1184,  0.0506, -0.1648],\n",
      "        [ 0.0379,  0.0541, -0.0073,  0.0012],\n",
      "        [ 0.0906, -0.1503, -0.1680, -0.0742],\n",
      "        [-0.0407, -0.1500, -0.1404, -0.0436],\n",
      "        [ 0.0542, -0.0437, -0.1425,  0.1645]], grad_fn=<SliceBackward0>)\n",
      "Bdown: torch.Size([8])\n",
      "Parameter containing:\n",
      "tensor([-0.1412,  0.1029, -0.1046,  0.1465, -0.1325,  0.0145, -0.1405, -0.0524],\n",
      "       requires_grad=True)\n",
      "Bdown spliced: torch.Size([4])\n",
      "tensor([-0.1412,  0.1029, -0.1046,  0.1465], grad_fn=<SliceBackward0>)\n",
      "outputs: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.0063,  0.0497, -0.1971, -0.0162],\n",
      "         [-0.1926,  0.1069, -0.1320,  0.1704],\n",
      "         [-0.1871,  0.1092, -0.0355,  0.1551]]], grad_fn=<AddBackward0>)\n",
      "------------- END MLP.forwardTensor() ------------\n",
      "layer's final residual state:\n",
      "tensor([[[ 0.2688, -1.1227, -0.8897, -0.2861],\n",
      "         [-0.0445, -1.2577,  0.9135,  0.5683],\n",
      "         [-2.1076, -0.9029, -1.0936, -0.2738]]], grad_fn=<AddBackward0>)\n",
      "----------------- END Layer.forwardTensor() --------------------\n",
      "forwardTensor() output: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.2688, -1.1227, -0.8897, -0.2861],\n",
      "         [-0.0445, -1.2577,  0.9135,  0.5683],\n",
      "         [-2.1076, -0.9029, -1.0936, -0.2738]]], grad_fn=<AddBackward0>)\n",
      "Model 1 from range(2)\n",
      "----------------- Layer.forwardTensor() --------------------\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[ 2.0527,  1.9986,  1.9487, -0.8838],\n",
      "         [-1.3443,  0.4112, -1.1328,  0.0875],\n",
      "         [-2.1688, -1.0016, -0.4252, -0.2276]]])\n",
      "normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[ 1.1481,  1.1178,  1.0900, -0.4943],\n",
      "         [-1.4874,  0.4550, -1.2535,  0.0968],\n",
      "         [-1.7798, -0.8220, -0.3489, -0.1868]]])\n",
      "dim: 4\n",
      "skip: 4\n",
      "spliced scale: torch.Size([4])\n",
      "tensor([0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[ 1.1481,  1.1178,  1.0900, -0.4943],\n",
      "         [-1.4874,  0.4550, -1.2535,  0.0968],\n",
      "         [-1.7798, -0.8220, -0.3489, -0.1868]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "---------- Attention Input: torch.Tensor ------------\n",
      "----------------- MultiQueryAttention.forwardTensor() --------------------\n",
      "x shape: torch.Size([1, 3, 4])\n",
      "d_skip: 4\n",
      "models_in_this_level: 2\n",
      "h_dim: 2\n",
      "h_skip: 2\n",
      "self.Wqkv: torch.Size([8, 16])\n",
      "Parameter containing:\n",
      "tensor([[-0.3398,  0.3307,  0.2225,  0.2923,  0.3199, -0.1244,  0.0621,  0.2447,\n",
      "         -0.3499, -0.1872,  0.3051,  0.0890,  0.2710,  0.1609,  0.3357,  0.0098],\n",
      "        [ 0.2298, -0.1531, -0.2039, -0.1286, -0.2591,  0.1291,  0.3100, -0.0835,\n",
      "         -0.2479, -0.1510, -0.1537,  0.0181,  0.2160,  0.3484, -0.3323, -0.1380],\n",
      "        [ 0.1080, -0.0664, -0.3089,  0.1810,  0.2769,  0.0328,  0.3347, -0.1358,\n",
      "         -0.0295,  0.2815, -0.3144,  0.1225,  0.2875,  0.0617, -0.0904, -0.1679],\n",
      "        [-0.2684, -0.2637, -0.0269,  0.3338,  0.0707, -0.3283,  0.1122, -0.1725,\n",
      "         -0.0218, -0.0872,  0.2521,  0.1246,  0.0284, -0.0780, -0.2766, -0.1182],\n",
      "        [-0.1520, -0.2973, -0.2569, -0.0197, -0.0728,  0.1796, -0.2989, -0.2442,\n",
      "          0.3104, -0.2646, -0.0299,  0.2852,  0.3015, -0.1969, -0.0207,  0.0691],\n",
      "        [-0.1881, -0.2773,  0.0862, -0.3252, -0.1013, -0.1286, -0.0218, -0.3159,\n",
      "         -0.3131, -0.2189,  0.2176, -0.3087, -0.3061,  0.2190, -0.2383,  0.0816],\n",
      "        [ 0.1831,  0.0092,  0.1412, -0.1254,  0.0028,  0.2533, -0.1741,  0.1159,\n",
      "          0.2736,  0.1486,  0.2726,  0.3237,  0.1083, -0.1763, -0.0169,  0.0802],\n",
      "        [ 0.0394, -0.1980, -0.1334, -0.2768, -0.0239,  0.0613,  0.0357, -0.0684,\n",
      "         -0.0411,  0.0550,  0.2665, -0.2499,  0.3059,  0.1568, -0.0046, -0.0349]],\n",
      "       requires_grad=True)\n",
      "Wq: torch.Size([8, 8])\n",
      "tensor([[-0.3398,  0.3307,  0.2225,  0.2923,  0.3199, -0.1244,  0.0621,  0.2447],\n",
      "        [ 0.2298, -0.1531, -0.2039, -0.1286, -0.2591,  0.1291,  0.3100, -0.0835],\n",
      "        [ 0.1080, -0.0664, -0.3089,  0.1810,  0.2769,  0.0328,  0.3347, -0.1358],\n",
      "        [-0.2684, -0.2637, -0.0269,  0.3338,  0.0707, -0.3283,  0.1122, -0.1725],\n",
      "        [-0.1520, -0.2973, -0.2569, -0.0197, -0.0728,  0.1796, -0.2989, -0.2442],\n",
      "        [-0.1881, -0.2773,  0.0862, -0.3252, -0.1013, -0.1286, -0.0218, -0.3159],\n",
      "        [ 0.1831,  0.0092,  0.1412, -0.1254,  0.0028,  0.2533, -0.1741,  0.1159],\n",
      "        [ 0.0394, -0.1980, -0.1334, -0.2768, -0.0239,  0.0613,  0.0357, -0.0684]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wk: torch.Size([8, 4])\n",
      "tensor([[-0.3499, -0.1872,  0.3051,  0.0890],\n",
      "        [-0.2479, -0.1510, -0.1537,  0.0181],\n",
      "        [-0.0295,  0.2815, -0.3144,  0.1225],\n",
      "        [-0.0218, -0.0872,  0.2521,  0.1246],\n",
      "        [ 0.3104, -0.2646, -0.0299,  0.2852],\n",
      "        [-0.3131, -0.2189,  0.2176, -0.3087],\n",
      "        [ 0.2736,  0.1486,  0.2726,  0.3237],\n",
      "        [-0.0411,  0.0550,  0.2665, -0.2499]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wv: torch.Size([8, 4])\n",
      "tensor([[ 0.2710,  0.1609,  0.3357,  0.0098],\n",
      "        [ 0.2160,  0.3484, -0.3323, -0.1380],\n",
      "        [ 0.2875,  0.0617, -0.0904, -0.1679],\n",
      "        [ 0.0284, -0.0780, -0.2766, -0.1182],\n",
      "        [ 0.3015, -0.1969, -0.0207,  0.0691],\n",
      "        [-0.3061,  0.2190, -0.2383,  0.0816],\n",
      "        [ 0.1083, -0.1763, -0.0169,  0.0802],\n",
      "        [ 0.3059,  0.1568, -0.0046, -0.0349]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wq spliced: torch.Size([4, 4])\n",
      "tensor([[-0.2569, -0.0197, -0.2989, -0.2442],\n",
      "        [ 0.0862, -0.3252, -0.0218, -0.3159],\n",
      "        [ 0.1412, -0.1254, -0.1741,  0.1159],\n",
      "        [-0.1334, -0.2768,  0.0357, -0.0684]], grad_fn=<CatBackward0>)\n",
      "Wk spliced: torch.Size([4, 2])\n",
      "tensor([[-0.0299,  0.2852],\n",
      "        [ 0.2176, -0.3087],\n",
      "        [ 0.2726,  0.3237],\n",
      "        [ 0.2665, -0.2499]], grad_fn=<CatBackward0>)\n",
      "Wv spliced: torch.Size([4, 2])\n",
      "tensor([[-0.0207,  0.0691],\n",
      "        [-0.2383,  0.0816],\n",
      "        [-0.0169,  0.0802],\n",
      "        [-0.0046, -0.0349]], grad_fn=<CatBackward0>)\n",
      "Wqkv_spliced: torch.Size([4, 8])\n",
      "tensor([[-0.2569, -0.0197, -0.2989, -0.2442, -0.0299,  0.2852, -0.0207,  0.0691],\n",
      "        [ 0.0862, -0.3252, -0.0218, -0.3159,  0.2176, -0.3087, -0.2383,  0.0816],\n",
      "        [ 0.1412, -0.1254, -0.1741,  0.1159,  0.2726,  0.3237, -0.0169,  0.0802],\n",
      "        [-0.1334, -0.2768,  0.0357, -0.0684,  0.2665, -0.2499, -0.0046, -0.0349]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "xqkv: torch.Size([1, 3, 8])\n",
      "tensor([[[ 0.0212, -0.3860, -0.5749, -0.4734,  0.3743,  0.4587, -0.3064,\n",
      "           0.2751],\n",
      "         [ 0.2315,  0.0118,  0.6563,  0.0675, -0.1725, -0.9946, -0.0569,\n",
      "          -0.1695],\n",
      "         [ 0.3620,  0.3978,  0.6039,  0.6666, -0.2707, -0.3201,  0.2396,\n",
      "          -0.2114]]], grad_fn=<UnsafeViewBackward0>)\n",
      "xq: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.0212, -0.3860, -0.5749, -0.4734],\n",
      "         [ 0.2315,  0.0118,  0.6563,  0.0675],\n",
      "         [ 0.3620,  0.3978,  0.6039,  0.6666]]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "xk: torch.Size([1, 3, 2])\n",
      "tensor([[[ 0.3743,  0.4587],\n",
      "         [-0.1725, -0.9946],\n",
      "         [-0.2707, -0.3201]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xv: torch.Size([1, 3, 2])\n",
      "tensor([[[-0.3064,  0.2751],\n",
      "         [-0.0569, -0.1695],\n",
      "         [ 0.2396, -0.2114]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xq reshaped: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[ 0.0212, -0.3860],\n",
      "          [-0.5749, -0.4734]],\n",
      "\n",
      "         [[ 0.2315,  0.0118],\n",
      "          [ 0.6563,  0.0675]],\n",
      "\n",
      "         [[ 0.3620,  0.3978],\n",
      "          [ 0.6039,  0.6666]]]], grad_fn=<ViewBackward0>)\n",
      "xk reshaped: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[ 0.3743,  0.4587]],\n",
      "\n",
      "         [[-0.1725, -0.9946]],\n",
      "\n",
      "         [[-0.2707, -0.3201]]]], grad_fn=<ViewBackward0>)\n",
      "xv reshaped: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[-0.3064,  0.2751]],\n",
      "\n",
      "         [[-0.0569, -0.1695]],\n",
      "\n",
      "         [[ 0.2396, -0.2114]]]], grad_fn=<ViewBackward0>)\n",
      "rotated xq: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[ 0.0212, -0.3860],\n",
      "          [-0.5749, -0.4734]],\n",
      "\n",
      "         [[ 0.1152,  0.2012],\n",
      "          [ 0.2978,  0.5887]],\n",
      "\n",
      "         [[-0.5124,  0.1636],\n",
      "          [-0.8574,  0.2717]]]], grad_fn=<TransposeBackward0>)\n",
      "rotated xk: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[ 0.3743,  0.4587]],\n",
      "\n",
      "         [[ 0.7438, -0.6825]],\n",
      "\n",
      "         [[ 0.4037, -0.1129]]]], grad_fn=<TransposeBackward0>)\n",
      "repeat_interleaved xk: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[ 0.3743,  0.4587],\n",
      "          [ 0.3743,  0.4587]],\n",
      "\n",
      "         [[ 0.7438, -0.6825],\n",
      "          [ 0.7438, -0.6825]],\n",
      "\n",
      "         [[ 0.4037, -0.1129],\n",
      "          [ 0.4037, -0.1129]]]], grad_fn=<ViewBackward0>)\n",
      "repeat_interleaved xv: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[-0.3064,  0.2751],\n",
      "          [-0.3064,  0.2751]],\n",
      "\n",
      "         [[-0.0569, -0.1695],\n",
      "          [-0.0569, -0.1695]],\n",
      "\n",
      "         [[ 0.2396, -0.2114],\n",
      "          [ 0.2396, -0.2114]]]], grad_fn=<ViewBackward0>)\n",
      "transposed xq: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[ 0.0212, -0.3860],\n",
      "          [ 0.1152,  0.2012],\n",
      "          [-0.5124,  0.1636]],\n",
      "\n",
      "         [[-0.5749, -0.4734],\n",
      "          [ 0.2978,  0.5887],\n",
      "          [-0.8574,  0.2717]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xk: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[ 0.3743,  0.4587],\n",
      "          [ 0.7438, -0.6825],\n",
      "          [ 0.4037, -0.1129]],\n",
      "\n",
      "         [[ 0.3743,  0.4587],\n",
      "          [ 0.7438, -0.6825],\n",
      "          [ 0.4037, -0.1129]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xv: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[-0.3064,  0.2751],\n",
      "          [-0.0569, -0.1695],\n",
      "          [ 0.2396, -0.2114]],\n",
      "\n",
      "         [[-0.3064,  0.2751],\n",
      "          [-0.0569, -0.1695],\n",
      "          [ 0.2396, -0.2114]]]], grad_fn=<TransposeBackward0>)\n",
      "scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[-0.1196,  0.1975,  0.0369],\n",
      "          [ 0.0957, -0.0365,  0.0168],\n",
      "          [-0.0826, -0.3484, -0.1593]],\n",
      "\n",
      "         [[-0.3057, -0.0739, -0.1263],\n",
      "          [ 0.2698, -0.1275,  0.0380],\n",
      "          [-0.1388, -0.5821, -0.2665]]]], grad_fn=<MulBackward0>)\n",
      "mask: torch.Size([1, 1, 3, 3])\n",
      "tensor([[[[ 0.0000e+00, -2.3820e+38, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]]]])\n",
      "masked scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[-1.1959e-01, -2.3820e+38, -2.3820e+38],\n",
      "          [ 9.5746e-02, -3.6521e-02, -2.3820e+38],\n",
      "          [-8.2569e-02, -3.4845e-01, -1.5934e-01]],\n",
      "\n",
      "         [[-3.0572e-01, -2.3820e+38, -2.3820e+38],\n",
      "          [ 2.6980e-01, -1.2751e-01, -2.3820e+38],\n",
      "          [-1.3883e-01, -5.8208e-01, -2.6647e-01]]]], grad_fn=<AddBackward0>)\n",
      "softmaxed scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [0.5330, 0.4670, 0.0000],\n",
      "          [0.3714, 0.2847, 0.3439]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.5980, 0.4020, 0.0000],\n",
      "          [0.3965, 0.2545, 0.3490]]]], grad_fn=<SoftmaxBackward0>)\n",
      "attention: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[-0.3064,  0.2751],\n",
      "          [-0.1899,  0.0675],\n",
      "          [-0.0476, -0.0188]],\n",
      "\n",
      "         [[-0.3064,  0.2751],\n",
      "          [-0.2061,  0.0964],\n",
      "          [-0.0524, -0.0079]]]], grad_fn=<UnsafeViewBackward0>)\n",
      "reshaped attention: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.3064,  0.2751, -0.3064,  0.2751],\n",
      "         [-0.1899,  0.0675, -0.2061,  0.0964],\n",
      "         [-0.0476, -0.0188, -0.0524, -0.0079]]], grad_fn=<ViewBackward0>)\n",
      "self.Wo: torch.Size([8, 8])\n",
      "Parameter containing:\n",
      "tensor([[-0.3331, -0.0057, -0.1193, -0.0371,  0.0632, -0.1175, -0.1165, -0.1170],\n",
      "        [ 0.3058,  0.1560, -0.2966, -0.3141, -0.3463, -0.2982,  0.2295,  0.2648],\n",
      "        [ 0.1682, -0.3533,  0.3376,  0.2572, -0.0676, -0.0124,  0.1574,  0.1800],\n",
      "        [-0.1671,  0.0802, -0.2675,  0.0040, -0.2241, -0.2670,  0.1884, -0.2433],\n",
      "        [ 0.1423,  0.2316,  0.0518,  0.0542, -0.0278,  0.3124,  0.0649,  0.3067],\n",
      "        [-0.1438,  0.1334, -0.2953, -0.2768,  0.0403,  0.0571, -0.0478, -0.1368],\n",
      "        [-0.2495, -0.1464, -0.3111,  0.2914, -0.1158, -0.0949,  0.1083,  0.0125],\n",
      "        [-0.2954,  0.3428,  0.0848, -0.2212, -0.2844,  0.2948, -0.1303,  0.3506]],\n",
      "       requires_grad=True)\n",
      "spliced Wo: torch.Size([4, 4])\n",
      "tensor([[-0.0676, -0.0124,  0.1574,  0.1800],\n",
      "        [-0.2241, -0.2670,  0.1884, -0.2433],\n",
      "        [-0.1158, -0.0949,  0.1083,  0.0125],\n",
      "        [-0.2844,  0.2948, -0.1303,  0.3506]], grad_fn=<CatBackward0>)\n",
      "projected output: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.0837,  0.0405, -0.0654, -0.0295],\n",
      "         [-0.0058,  0.0323, -0.0520, -0.0194],\n",
      "         [ 0.0157,  0.0083, -0.0157, -0.0074]]], grad_fn=<UnsafeViewBackward0>)\n",
      "----------------- END MultiQueryAttention.forwardTensor() --------------------\n",
      "x in layer after MQA & resid connection and before MLP:\n",
      "tensor([[[ 1.9689,  2.0391,  1.8833, -0.9133],\n",
      "         [-1.3501,  0.4436, -1.1849,  0.0681],\n",
      "         [-2.1530, -0.9934, -0.4409, -0.2350]]], grad_fn=<AddBackward0>)\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[ 1.9689,  2.0391,  1.8833, -0.9133],\n",
      "         [-1.3501,  0.4436, -1.1849,  0.0681],\n",
      "         [-2.1530, -0.9934, -0.4409, -0.2350]]], grad_fn=<AddBackward0>)\n",
      "normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[ 1.1176,  1.1574,  1.0690, -0.5184],\n",
      "         [-1.4584,  0.4791, -1.2799,  0.0736],\n",
      "         [-1.7770, -0.8199, -0.3639, -0.1940]]], grad_fn=<MulBackward0>)\n",
      "dim: 4\n",
      "skip: 4\n",
      "spliced scale: torch.Size([4])\n",
      "tensor([0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[ 1.1176,  1.1574,  1.0690, -0.5184],\n",
      "         [-1.4584,  0.4791, -1.2799,  0.0736],\n",
      "         [-1.7770, -0.8199, -0.3639, -0.1940]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "---------- MLP Input: torch.Tensor ------------\n",
      "------------- MLP.forwardTensor() ------------\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[ 1.1176,  1.1574,  1.0690, -0.5184],\n",
      "         [-1.4584,  0.4791, -1.2799,  0.0736],\n",
      "         [-1.7770, -0.8199, -0.3639, -0.1940]]], grad_fn=<MulBackward0>)\n",
      "d_dim: 4\n",
      "d_skip: 4\n",
      "i_dim: 16\n",
      "i_skip: 16\n",
      "Wgate: torch.Size([8, 32])\n",
      "Parameter containing:\n",
      "tensor([[-2.0378e-01,  1.9155e-02, -2.5673e-01, -6.6076e-02, -1.5894e-01,\n",
      "         -5.8077e-02,  1.1944e-01,  3.1422e-01,  2.6704e-01,  6.0142e-02,\n",
      "          2.8076e-01,  6.6552e-02,  1.2014e-02, -3.0538e-01,  2.2672e-01,\n",
      "         -2.8452e-01, -3.3381e-01, -1.0627e-01, -5.5746e-02,  2.2767e-03,\n",
      "         -8.7861e-02,  3.5301e-01,  9.5396e-02, -1.4413e-01,  2.2739e-01,\n",
      "         -3.0789e-01,  2.5667e-01,  1.9587e-01,  1.9188e-01,  1.9864e-01,\n",
      "          7.8174e-02, -1.4472e-01],\n",
      "        [ 2.5881e-01,  1.8849e-01, -1.9905e-01,  9.8076e-03,  3.1139e-02,\n",
      "          2.0969e-01, -1.7834e-01, -2.0160e-01, -2.3034e-01,  1.6279e-01,\n",
      "          3.0912e-01, -2.8784e-01, -2.4899e-02,  9.5506e-02,  3.1060e-01,\n",
      "          2.0226e-01, -1.2328e-02,  1.5144e-01, -3.1324e-01, -3.0475e-01,\n",
      "         -1.3892e-01, -2.6066e-01,  8.7560e-02, -7.3888e-02, -1.3559e-01,\n",
      "         -6.5933e-02, -9.5685e-02, -1.1806e-01,  3.3973e-01, -5.2369e-02,\n",
      "         -7.2154e-04,  3.1122e-01],\n",
      "        [ 3.2107e-01, -2.6924e-01,  3.1704e-01,  2.3734e-01, -2.0238e-01,\n",
      "          2.7264e-01, -3.1823e-01,  2.2726e-01,  1.3663e-01,  1.3384e-01,\n",
      "         -3.3406e-01, -8.7860e-02, -1.7728e-02,  1.2769e-01, -3.1617e-01,\n",
      "         -2.1740e-01,  3.7097e-03, -1.0999e-02, -2.5561e-01,  1.0879e-01,\n",
      "          1.0981e-01,  2.1874e-01,  9.8952e-02,  2.1657e-01,  2.1956e-01,\n",
      "         -3.4451e-01, -2.0934e-01,  1.7877e-01, -1.7365e-01,  1.1120e-02,\n",
      "         -1.0411e-01, -8.7443e-02],\n",
      "        [ 3.0102e-01,  1.8049e-01, -9.3183e-03, -3.0233e-01,  4.4365e-02,\n",
      "          1.5189e-01, -2.9368e-01, -3.8777e-02, -1.9838e-01,  3.5013e-01,\n",
      "          1.1811e-01,  1.2853e-01, -2.8444e-01, -2.3235e-01,  1.2906e-01,\n",
      "          2.5792e-01, -3.0151e-01,  8.0087e-03, -8.7959e-03,  7.7719e-02,\n",
      "         -1.5681e-01,  1.5885e-01, -1.8853e-01, -1.3954e-01,  2.9057e-01,\n",
      "          6.7097e-02, -2.2614e-01, -3.3118e-01, -5.7758e-02, -1.8288e-01,\n",
      "          1.8856e-01, -1.0947e-01],\n",
      "        [ 3.3077e-01, -1.5129e-01,  2.2010e-01, -1.5577e-01,  3.0039e-01,\n",
      "          4.0851e-02, -1.7102e-01,  1.7203e-01, -1.8540e-01,  8.4346e-02,\n",
      "          1.7938e-02, -1.2574e-01, -8.7709e-02, -1.0353e-01, -3.1317e-01,\n",
      "          2.7271e-01,  1.7714e-01,  3.0434e-01, -1.7270e-01,  2.7432e-01,\n",
      "         -4.6780e-02,  1.1775e-01, -2.9078e-01, -4.1688e-02,  1.8667e-01,\n",
      "          2.2608e-01,  2.0868e-01, -1.2030e-01, -4.9017e-02, -8.0353e-03,\n",
      "          6.4723e-02,  7.7618e-02],\n",
      "        [ 3.1620e-01, -1.3605e-01, -2.4454e-02, -3.0133e-01, -2.2047e-01,\n",
      "         -6.0336e-03,  1.4947e-01,  1.1901e-02,  2.4008e-01,  2.5826e-01,\n",
      "         -6.3199e-03,  2.3596e-01, -2.0454e-01,  2.1074e-01, -2.1293e-02,\n",
      "          3.2555e-01, -7.3171e-02,  9.9653e-02,  3.2077e-01,  2.5021e-01,\n",
      "         -3.1209e-02,  3.1273e-01, -3.0262e-01, -2.2676e-01, -5.4055e-02,\n",
      "          2.3609e-01,  8.8153e-02, -8.7053e-02, -3.4416e-01,  1.9098e-01,\n",
      "          9.2401e-02, -3.0739e-01],\n",
      "        [-8.3603e-03, -2.4367e-01, -2.9212e-01,  3.2735e-01, -1.8916e-01,\n",
      "          3.3348e-01,  3.4109e-01,  1.8712e-01, -1.0089e-01,  3.1761e-01,\n",
      "         -1.4733e-01, -2.7488e-01, -4.3122e-02,  1.3442e-01, -3.5292e-02,\n",
      "          1.4565e-01, -2.6186e-03,  1.1012e-01, -9.4417e-03,  2.9175e-01,\n",
      "         -3.1957e-01,  2.4675e-01, -1.9900e-01, -1.5049e-01, -3.1706e-01,\n",
      "         -8.3404e-03,  2.9004e-01, -3.2020e-02,  2.4540e-01,  3.1980e-01,\n",
      "          9.4737e-02,  2.1059e-01],\n",
      "        [ 3.1330e-01,  6.9906e-02,  1.7583e-01, -2.2837e-01, -7.3682e-02,\n",
      "         -7.1424e-02,  1.4529e-01,  2.1353e-02, -9.1109e-03, -3.2874e-01,\n",
      "          2.0343e-01, -2.0165e-01, -2.6832e-01, -3.1146e-04, -2.9679e-01,\n",
      "          2.1311e-01,  3.8456e-02,  1.5998e-01,  4.3876e-02,  5.8273e-03,\n",
      "          3.1290e-01, -6.7655e-02, -1.9788e-01,  1.6022e-01, -3.3185e-01,\n",
      "         -1.9130e-01,  2.2986e-01, -2.1936e-01,  1.2736e-01, -1.7436e-01,\n",
      "          2.3090e-01, -8.5473e-02]], requires_grad=True)\n",
      "Wgate spliced: torch.Size([4, 16])\n",
      "tensor([[ 0.1771,  0.3043, -0.1727,  0.2743, -0.0468,  0.1177, -0.2908, -0.0417,\n",
      "          0.1867,  0.2261,  0.2087, -0.1203, -0.0490, -0.0080,  0.0647,  0.0776],\n",
      "        [-0.0732,  0.0997,  0.3208,  0.2502, -0.0312,  0.3127, -0.3026, -0.2268,\n",
      "         -0.0541,  0.2361,  0.0882, -0.0871, -0.3442,  0.1910,  0.0924, -0.3074],\n",
      "        [-0.0026,  0.1101, -0.0094,  0.2917, -0.3196,  0.2467, -0.1990, -0.1505,\n",
      "         -0.3171, -0.0083,  0.2900, -0.0320,  0.2454,  0.3198,  0.0947,  0.2106],\n",
      "        [ 0.0385,  0.1600,  0.0439,  0.0058,  0.3129, -0.0677, -0.1979,  0.1602,\n",
      "         -0.3318, -0.1913,  0.2299, -0.2194,  0.1274, -0.1744,  0.2309, -0.0855]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Bgate: torch.Size([32])\n",
      "Parameter containing:\n",
      "tensor([-0.2264,  0.2777, -0.0718, -0.1366, -0.2667,  0.1974, -0.2165,  0.1699,\n",
      "         0.3184,  0.0362,  0.2520,  0.3413,  0.1179,  0.1998,  0.3149, -0.0912,\n",
      "        -0.2235, -0.0496, -0.0858,  0.3456,  0.2254, -0.3406,  0.3058, -0.3508,\n",
      "         0.1816,  0.3093, -0.2261, -0.0809, -0.0694, -0.1643, -0.1804,  0.2403],\n",
      "       requires_grad=True)\n",
      "Bgate spliced: torch.Size([16])\n",
      "tensor([-0.2235, -0.0496, -0.0858,  0.3456,  0.2254, -0.3406,  0.3058, -0.3508,\n",
      "         0.1816,  0.3093, -0.2261, -0.0809, -0.0694, -0.1643, -0.1804,  0.2403],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xgate: torch.Size([1, 3, 16])\n",
      "tensor([[[-0.1329,  0.4406,  0.0596,  1.2506, -0.3668,  0.4518, -0.4796,\n",
      "          -0.9038,  0.1608,  0.9255,  0.3000, -0.2366, -0.3262,  0.4800,\n",
      "          -0.0196,  0.2407],\n",
      "         [-0.5107, -0.5749,  0.3351, -0.3076,  0.7107, -0.6833,  0.8250,\n",
      "          -0.1942,  0.2649,  0.0893, -0.8425,  0.0777, -0.4676, -0.4832,\n",
      "          -0.3348, -0.2960],\n",
      "         [-0.4848, -0.7433, -0.0470, -0.4543,  0.3898, -0.8829,  1.1814,\n",
      "          -0.0671,  0.0740, -0.2459, -0.8193,  0.2585,  0.1858, -0.3892,\n",
      "          -0.4505,  0.2943]]], grad_fn=<AddBackward0>)\n",
      "GeLU'ed Xgate: torch.Size([1, 3, 16])\n",
      "tensor([[[-0.0594,  0.2953,  0.0312,  1.1186, -0.1309,  0.3046, -0.1514,\n",
      "          -0.1654,  0.0906,  0.7613,  0.1854, -0.0962, -0.1214,  0.3285,\n",
      "          -0.0096,  0.1432],\n",
      "         [-0.1557, -0.1625,  0.2115, -0.1166,  0.5411, -0.1689,  0.6561,\n",
      "          -0.0822,  0.1601,  0.0478, -0.1683,  0.0413, -0.1496, -0.1520,\n",
      "          -0.1235, -0.1136],\n",
      "         [-0.1522, -0.1700, -0.0226, -0.1476,  0.2540, -0.1666,  1.0411,\n",
      "          -0.0318,  0.0392, -0.0991, -0.1690,  0.1556,  0.1066, -0.1357,\n",
      "          -0.1469,  0.1812]]], grad_fn=<GeluBackward0>)\n",
      "Wup: torch.Size([8, 32])\n",
      "Parameter containing:\n",
      "tensor([[ 0.1891, -0.3348,  0.1431,  0.2513, -0.1019,  0.2473,  0.3179, -0.2051,\n",
      "          0.3282, -0.1770, -0.3482,  0.2050,  0.1590,  0.0381, -0.2299, -0.1161,\n",
      "          0.2286,  0.0283,  0.2427, -0.3277, -0.3090,  0.0499, -0.0448, -0.1890,\n",
      "         -0.1538,  0.2338, -0.2407,  0.0591, -0.3110, -0.3443,  0.3113,  0.3400],\n",
      "        [-0.2720, -0.1980, -0.2918,  0.2277,  0.1436, -0.0712,  0.2805, -0.2086,\n",
      "         -0.3358,  0.0009,  0.1926, -0.2556,  0.1962,  0.2934, -0.0669, -0.1529,\n",
      "          0.1861, -0.3025,  0.0016,  0.2442, -0.0454,  0.3393,  0.2080, -0.3150,\n",
      "          0.0084, -0.1488, -0.2285,  0.3414, -0.1945,  0.3108, -0.0665, -0.2185],\n",
      "        [ 0.0034, -0.1525, -0.3498, -0.3306, -0.0650,  0.0450,  0.2885, -0.0640,\n",
      "         -0.2515, -0.0544,  0.0966, -0.3490, -0.0808,  0.1167,  0.0109, -0.0537,\n",
      "          0.0025, -0.2318, -0.1315, -0.1360, -0.0682, -0.3040,  0.3258, -0.2382,\n",
      "         -0.2034, -0.3028, -0.0071,  0.0127,  0.2829, -0.2993,  0.3113, -0.0200],\n",
      "        [ 0.3333,  0.2307,  0.3451, -0.3023, -0.1704, -0.0212,  0.3249, -0.2695,\n",
      "         -0.0329,  0.2475, -0.2836, -0.0755,  0.2599, -0.2236, -0.3102,  0.0324,\n",
      "         -0.2449, -0.2291, -0.2291, -0.0349, -0.2352, -0.2767,  0.3527, -0.3438,\n",
      "         -0.0973,  0.0151, -0.0009,  0.0269, -0.0377,  0.2885, -0.0231,  0.2471],\n",
      "        [-0.0255,  0.1854,  0.2569,  0.2652, -0.1578,  0.1092,  0.2218, -0.2025,\n",
      "         -0.1547,  0.3488, -0.1787,  0.3313,  0.2785, -0.1284, -0.2222, -0.0724,\n",
      "          0.1638, -0.0323, -0.2788, -0.0774, -0.2185,  0.0230, -0.3394,  0.0077,\n",
      "         -0.3189,  0.2470,  0.0507, -0.0696,  0.3521,  0.2695,  0.1502,  0.3292],\n",
      "        [-0.3306, -0.1778, -0.0753,  0.2954,  0.2052, -0.1997,  0.0728, -0.2261,\n",
      "         -0.0628,  0.0681, -0.0183, -0.2011,  0.2070,  0.2789, -0.0077,  0.1752,\n",
      "         -0.2285, -0.2642, -0.2965, -0.2129,  0.2878,  0.2500,  0.1709,  0.1211,\n",
      "         -0.0039,  0.3450, -0.0835,  0.0534,  0.0063, -0.2916, -0.2434, -0.0075],\n",
      "        [-0.2189, -0.0074,  0.0152,  0.1700,  0.0678,  0.0495, -0.0782, -0.0932,\n",
      "         -0.2998,  0.3001,  0.0643, -0.1114,  0.1595,  0.3484, -0.1022,  0.0340,\n",
      "          0.2111,  0.1723,  0.2733, -0.0897,  0.2623, -0.3368, -0.1903, -0.3080,\n",
      "          0.3486,  0.1027, -0.1949,  0.3073, -0.0341,  0.1220,  0.0937,  0.3201],\n",
      "        [ 0.2910,  0.2288, -0.0279,  0.3497, -0.3246,  0.2042, -0.2583,  0.0809,\n",
      "          0.2776, -0.2417, -0.3348,  0.2586, -0.2370, -0.2478, -0.1746, -0.2704,\n",
      "          0.0050,  0.2293, -0.2476, -0.3414,  0.0631,  0.3129, -0.1799,  0.1883,\n",
      "          0.2111,  0.0105, -0.1609, -0.0887, -0.2803, -0.3330, -0.3441, -0.0239]],\n",
      "       requires_grad=True)\n",
      "Wup spliced: torch.Size([4, 16])\n",
      "tensor([[ 0.1638, -0.0323, -0.2788, -0.0774, -0.2185,  0.0230, -0.3394,  0.0077,\n",
      "         -0.3189,  0.2470,  0.0507, -0.0696,  0.3521,  0.2695,  0.1502,  0.3292],\n",
      "        [-0.2285, -0.2642, -0.2965, -0.2129,  0.2878,  0.2500,  0.1709,  0.1211,\n",
      "         -0.0039,  0.3450, -0.0835,  0.0534,  0.0063, -0.2916, -0.2434, -0.0075],\n",
      "        [ 0.2111,  0.1723,  0.2733, -0.0897,  0.2623, -0.3368, -0.1903, -0.3080,\n",
      "          0.3486,  0.1027, -0.1949,  0.3073, -0.0341,  0.1220,  0.0937,  0.3201],\n",
      "        [ 0.0050,  0.2293, -0.2476, -0.3414,  0.0631,  0.3129, -0.1799,  0.1883,\n",
      "          0.2111,  0.0105, -0.1609, -0.0887, -0.2803, -0.3330, -0.3441, -0.0239]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Bup: torch.Size([32])\n",
      "Parameter containing:\n",
      "tensor([ 0.2824,  0.0903,  0.1206, -0.2387,  0.3339,  0.1524, -0.3451, -0.0093,\n",
      "        -0.0834,  0.1387, -0.1012, -0.2124, -0.0021, -0.3290, -0.0827, -0.1971,\n",
      "        -0.2421, -0.3223, -0.3180,  0.2969, -0.2551, -0.2841, -0.3353,  0.2673,\n",
      "        -0.2556,  0.0490,  0.0844,  0.0745, -0.1900, -0.2533, -0.2110, -0.2201],\n",
      "       requires_grad=True)\n",
      "Bup spliced: torch.Size([16])\n",
      "tensor([-0.2421, -0.3223, -0.3180,  0.2969, -0.2551, -0.2841, -0.3353,  0.2673,\n",
      "        -0.2556,  0.0490,  0.0844,  0.0745, -0.1900, -0.2533, -0.2110, -0.2201],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xup: torch.Size([1, 3, 16])\n",
      "tensor([[[-0.1005, -0.5989, -0.5523,  0.0451,  0.0816, -0.4914, -0.6270,\n",
      "          -0.0108, -0.3532,  0.8286, -0.0806,  0.4329,  0.3197,  0.0135,\n",
      "          -0.0464,  0.4938],\n",
      "         [-0.8604, -0.6055, -0.4214,  0.3973, -0.1297,  0.2562,  0.4720,\n",
      "           0.7221, -0.2230, -0.2766,  0.2081, -0.1982, -0.6775, -0.9668,\n",
      "          -0.6918, -1.1153],\n",
      "         [-0.4237, -0.1555,  0.3692,  0.7078, -0.2105, -0.4681,  0.2319,\n",
      "           0.2299,  0.1464, -0.7122,  0.1650,  0.0598, -0.7541, -0.4729,\n",
      "          -0.2456, -0.9109]]], grad_fn=<AddBackward0>)\n",
      "Xfuse: torch.Size([1, 3, 16])\n",
      "tensor([[[ 5.9709e-03, -1.7686e-01, -1.7255e-02,  5.0486e-02, -1.0675e-02,\n",
      "          -1.4968e-01,  9.4944e-02,  1.7935e-03, -3.2014e-02,  6.3084e-01,\n",
      "          -1.4951e-02, -4.1632e-02, -3.8810e-02,  4.4297e-03,  4.4694e-04,\n",
      "           7.0723e-02],\n",
      "         [ 1.3392e-01,  9.8407e-02, -8.9128e-02, -4.6346e-02, -7.0188e-02,\n",
      "          -4.3283e-02,  3.0966e-01, -5.9327e-02, -3.5702e-02, -1.3228e-02,\n",
      "          -3.5021e-02, -8.1756e-03,  1.0138e-01,  1.4691e-01,  8.5438e-02,\n",
      "           1.2665e-01],\n",
      "         [ 6.4475e-02,  2.6432e-02, -8.3448e-03, -1.0444e-01, -5.3462e-02,\n",
      "           7.7959e-02,  2.4143e-01, -7.3007e-03,  5.7345e-03,  7.0546e-02,\n",
      "          -2.7882e-02,  9.3062e-03, -8.0398e-02,  6.4157e-02,  3.6094e-02,\n",
      "          -1.6507e-01]]], grad_fn=<MulBackward0>)\n",
      "Wdown: torch.Size([32, 8])\n",
      "Parameter containing:\n",
      "tensor([[-3.1355e-02,  3.6585e-02,  3.6894e-02, -1.3828e-02, -5.0864e-02,\n",
      "         -7.7739e-02,  7.8714e-02, -1.0312e-01],\n",
      "        [-1.3010e-01, -1.4110e-01,  1.2179e-01, -1.0945e-01, -1.2249e-01,\n",
      "          1.6907e-01, -8.7172e-02,  3.5724e-02],\n",
      "        [-4.5318e-02, -1.6887e-01, -1.3354e-02,  9.5296e-02, -1.7005e-01,\n",
      "         -1.3798e-01, -8.9512e-02,  1.3186e-01],\n",
      "        [ 8.8651e-02,  1.6334e-02,  1.5033e-01,  3.4891e-02,  2.7176e-02,\n",
      "         -9.8128e-02, -9.6446e-02, -7.1352e-02],\n",
      "        [-1.1127e-01, -1.7228e-01, -1.3319e-01, -4.7356e-02, -1.0979e-01,\n",
      "          5.3924e-02, -1.6958e-01, -8.5660e-02],\n",
      "        [-1.7094e-01, -5.4964e-03,  1.3648e-01, -1.1574e-01, -1.6192e-01,\n",
      "         -1.6018e-03,  5.2023e-02,  8.8789e-02],\n",
      "        [-1.2047e-01,  1.1335e-01,  5.3661e-03, -6.5454e-03, -1.7370e-01,\n",
      "          1.0451e-01,  4.7912e-02,  1.1133e-01],\n",
      "        [ 7.2820e-02, -1.5413e-01, -1.3741e-01, -1.0161e-01, -1.2747e-01,\n",
      "          1.5089e-01,  6.0612e-02, -7.3363e-02],\n",
      "        [-5.2269e-02,  1.4841e-01, -1.7492e-01, -1.3143e-01, -1.1084e-01,\n",
      "          5.1809e-02,  1.5182e-01, -1.5181e-01],\n",
      "        [ 1.6782e-01,  2.9896e-02,  4.0980e-03,  1.6873e-01, -1.6533e-01,\n",
      "          1.1474e-01, -1.6794e-02,  1.5594e-01],\n",
      "        [-1.5010e-01, -1.3560e-02,  7.3077e-02,  1.1726e-01, -6.8219e-03,\n",
      "          4.5938e-02, -9.9155e-02, -2.9751e-03],\n",
      "        [ 1.7348e-01, -1.1838e-01,  5.0580e-02, -1.6478e-01, -6.5153e-03,\n",
      "          1.6758e-01, -1.2350e-01,  1.2695e-01],\n",
      "        [ 3.7887e-02,  5.4085e-02, -7.3148e-03,  1.2091e-03,  3.4779e-02,\n",
      "          1.4121e-01,  3.5768e-02,  7.8466e-02],\n",
      "        [ 9.0600e-02, -1.5027e-01, -1.6800e-01, -7.4179e-02, -1.2019e-02,\n",
      "         -1.2385e-01, -6.3113e-02,  3.7009e-02],\n",
      "        [-4.0653e-02, -1.5005e-01, -1.4039e-01, -4.3561e-02,  8.3764e-02,\n",
      "          1.5913e-01, -9.9894e-02, -1.3029e-01],\n",
      "        [ 5.4227e-02, -4.3747e-02, -1.4250e-01,  1.6453e-01,  5.3544e-02,\n",
      "         -1.7382e-01,  7.5603e-02, -1.1672e-01],\n",
      "        [ 1.3068e-01,  3.2768e-02, -1.4872e-01,  1.8265e-02, -1.9594e-02,\n",
      "          1.5943e-01, -3.5449e-02, -1.5641e-01],\n",
      "        [ 3.1814e-02,  2.8846e-02,  1.0038e-01,  2.6652e-02,  1.7215e-01,\n",
      "          4.1252e-02,  4.1964e-02, -1.3067e-01],\n",
      "        [-1.4907e-01, -1.0314e-01, -8.9474e-02, -4.8484e-02, -1.4959e-01,\n",
      "         -1.1083e-01, -4.8872e-03,  1.8293e-03],\n",
      "        [ 1.6387e-01, -1.1631e-01,  8.6573e-02, -2.1528e-02, -6.8403e-02,\n",
      "         -1.6459e-01, -6.9652e-02,  7.4229e-02],\n",
      "        [-1.6679e-01, -9.3716e-02, -6.1629e-02,  6.1625e-02,  1.7168e-01,\n",
      "         -1.0182e-01,  2.6129e-02,  6.0644e-02],\n",
      "        [-4.2471e-02, -1.2998e-01, -1.3981e-01,  7.7821e-02,  4.2238e-02,\n",
      "          1.4273e-01, -8.6938e-02, -1.0105e-02],\n",
      "        [-1.5584e-01, -1.5044e-01,  5.7489e-02,  1.4627e-01, -5.7655e-03,\n",
      "         -1.2296e-02,  7.3597e-02, -1.9613e-02],\n",
      "        [-3.4707e-02, -1.5434e-01, -3.9184e-02,  3.1919e-02,  1.7107e-01,\n",
      "         -5.2179e-02, -2.2853e-02,  2.9895e-04],\n",
      "        [-3.8735e-02, -1.2177e-01,  3.6174e-02,  5.9803e-02,  4.9718e-02,\n",
      "          2.1142e-02, -6.5383e-03, -1.1767e-01],\n",
      "        [-1.2257e-01,  8.8373e-02,  2.2258e-02, -1.2801e-01, -5.6065e-02,\n",
      "         -9.9842e-02, -1.0121e-01, -8.6156e-02],\n",
      "        [ 1.1428e-01,  1.2171e-01,  2.3687e-02, -2.9700e-02,  1.6325e-01,\n",
      "         -5.5560e-02,  6.2100e-02,  7.3736e-02],\n",
      "        [-1.2759e-01,  6.2463e-02,  1.6956e-01,  2.0842e-02, -1.0427e-01,\n",
      "         -1.4254e-01, -3.8105e-02, -3.3327e-02],\n",
      "        [-8.4661e-02,  8.0418e-02,  7.8576e-02,  5.8173e-02,  1.5175e-01,\n",
      "          8.8992e-02, -1.3159e-02,  1.7578e-01],\n",
      "        [-4.8491e-02, -7.4514e-03,  6.8876e-04,  1.6711e-01,  9.7252e-02,\n",
      "         -4.0158e-02, -4.3876e-02, -1.2246e-01],\n",
      "        [ 7.1733e-03,  3.6569e-02,  4.6691e-02,  5.0983e-03, -6.5416e-05,\n",
      "          1.2602e-01,  1.3041e-01, -2.7197e-02],\n",
      "        [-6.5462e-02, -4.2117e-02, -1.3896e-01,  5.3109e-02, -1.5426e-01,\n",
      "         -1.4133e-01, -1.2919e-01, -1.5776e-01]], requires_grad=True)\n",
      "Wdown spliced: torch.Size([16, 4])\n",
      "tensor([[-1.9594e-02,  1.5943e-01, -3.5449e-02, -1.5641e-01],\n",
      "        [ 1.7215e-01,  4.1252e-02,  4.1964e-02, -1.3067e-01],\n",
      "        [-1.4959e-01, -1.1083e-01, -4.8872e-03,  1.8293e-03],\n",
      "        [-6.8403e-02, -1.6459e-01, -6.9652e-02,  7.4229e-02],\n",
      "        [ 1.7168e-01, -1.0182e-01,  2.6129e-02,  6.0644e-02],\n",
      "        [ 4.2238e-02,  1.4273e-01, -8.6938e-02, -1.0105e-02],\n",
      "        [-5.7655e-03, -1.2296e-02,  7.3597e-02, -1.9613e-02],\n",
      "        [ 1.7107e-01, -5.2179e-02, -2.2853e-02,  2.9895e-04],\n",
      "        [ 4.9718e-02,  2.1142e-02, -6.5383e-03, -1.1767e-01],\n",
      "        [-5.6065e-02, -9.9842e-02, -1.0121e-01, -8.6156e-02],\n",
      "        [ 1.6325e-01, -5.5560e-02,  6.2100e-02,  7.3736e-02],\n",
      "        [-1.0427e-01, -1.4254e-01, -3.8105e-02, -3.3327e-02],\n",
      "        [ 1.5175e-01,  8.8992e-02, -1.3159e-02,  1.7578e-01],\n",
      "        [ 9.7252e-02, -4.0158e-02, -4.3876e-02, -1.2246e-01],\n",
      "        [-6.5416e-05,  1.2602e-01,  1.3041e-01, -2.7197e-02],\n",
      "        [-1.5426e-01, -1.4133e-01, -1.2919e-01, -1.5776e-01]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Bdown: torch.Size([8])\n",
      "Parameter containing:\n",
      "tensor([-0.1412,  0.1029, -0.1046,  0.1465, -0.1325,  0.0145, -0.1405, -0.0524],\n",
      "       requires_grad=True)\n",
      "Bdown spliced: torch.Size([4])\n",
      "tensor([-0.1325,  0.0145, -0.1405, -0.0524], grad_fn=<SliceBackward0>)\n",
      "outputs: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.2237, -0.0903, -0.2036, -0.0964],\n",
      "         [-0.1232,  0.0573, -0.1246, -0.1192],\n",
      "         [-0.1190,  0.0694, -0.1096, -0.0885]]], grad_fn=<AddBackward0>)\n",
      "------------- END MLP.forwardTensor() ------------\n",
      "layer's final residual state:\n",
      "tensor([[[ 1.7452,  1.9489,  1.6797, -1.0096],\n",
      "         [-1.4733,  0.5009, -1.3095, -0.0511],\n",
      "         [-2.2721, -0.9240, -0.5504, -0.3235]]], grad_fn=<AddBackward0>)\n",
      "----------------- END Layer.forwardTensor() --------------------\n",
      "forwardTensor() output: torch.Size([1, 3, 4])\n",
      "tensor([[[ 1.7452,  1.9489,  1.6797, -1.0096],\n",
      "         [-1.4733,  0.5009, -1.3095, -0.0511],\n",
      "         [-2.2721, -0.9240, -0.5504, -0.3235]]], grad_fn=<AddBackward0>)\n",
      "final output: ((tensor([[[ 1.8621,  0.5943,  0.0928, -1.2366, -0.5227,  1.8973,  1.2019,\n",
      "           0.6794],\n",
      "         [-0.0710, -0.8199, -0.2435,  0.4829, -0.3740,  0.5088, -2.9446,\n",
      "           0.3719],\n",
      "         [ 0.2186, -1.4938, -0.6808, -0.8450, -1.1288,  0.0832,  1.2810,\n",
      "          -0.3561]]], grad_fn=<AddBackward0>),), (tensor([[[ 0.2688, -1.1227, -0.8897, -0.2861],\n",
      "         [-0.0445, -1.2577,  0.9135,  0.5683],\n",
      "         [-2.1076, -0.9029, -1.0936, -0.2738]]], grad_fn=<AddBackward0>), tensor([[[ 1.7452,  1.9489,  1.6797, -1.0096],\n",
      "         [-1.4733,  0.5009, -1.3095, -0.0511],\n",
      "         [-2.2721, -0.9240, -0.5504, -0.3235]]], grad_fn=<AddBackward0>)))\n",
      "------------- END Layer.forwardTuple() ------------\n",
      "out: ((tensor([[[ 1.8621,  0.5943,  0.0928, -1.2366, -0.5227,  1.8973,  1.2019,\n",
      "           0.6794],\n",
      "         [-0.0710, -0.8199, -0.2435,  0.4829, -0.3740,  0.5088, -2.9446,\n",
      "           0.3719],\n",
      "         [ 0.2186, -1.4938, -0.6808, -0.8450, -1.1288,  0.0832,  1.2810,\n",
      "          -0.3561]]], grad_fn=<AddBackward0>),), (tensor([[[ 0.2688, -1.1227, -0.8897, -0.2861],\n",
      "         [-0.0445, -1.2577,  0.9135,  0.5683],\n",
      "         [-2.1076, -0.9029, -1.0936, -0.2738]]], grad_fn=<AddBackward0>), tensor([[[ 1.7452,  1.9489,  1.6797, -1.0096],\n",
      "         [-1.4733,  0.5009, -1.3095, -0.0511],\n",
      "         [-2.2721, -0.9240, -0.5504, -0.3235]]], grad_fn=<AddBackward0>)))\n",
      "---------- RESET CONFIG --------\n",
      "model_count:  [1, 2, 4]\n",
      "model_dim_list:  [128, 64, 32]\n",
      "head_dim_list:  [32, 16, 8]\n"
     ]
    }
   ],
   "source": [
    "# Testing our Layer's forwardTuple()\n",
    "verbose = True\n",
    "\n",
    "print(\"--------- Micro Hyperparameters -------\")\n",
    "hold1, hold2, hold3, hold4, hold5 = config.hidden_size, config.num_attention_heads, config.head_dim, config.levels, config.max_position_embeddings\n",
    "config.hidden_size = 8\n",
    "config.num_attention_heads = 2\n",
    "config.head_dim = 4\n",
    "config.levels = 2\n",
    "config.max_position_embeddings = 3\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "print(\"head_dim_list: \", config.head_dim_list)\n",
    "\n",
    "layer = Layer(config)\n",
    "# we need to make sure to send in a tuple of the expected size. above we set hidden_size=4 and levels=2\n",
    "x = ((torch.randn((1,3,8)),),(torch.randn((1,3,4)),torch.randn((1,3,4))))\n",
    "print(f\"x: {x}\")\n",
    "out = layer(x)\n",
    "print(f\"out: {out}\")\n",
    "\n",
    "verbose = False\n",
    "print(\"---------- RESET CONFIG --------\")\n",
    "config.hidden_size = hold1\n",
    "config.num_attention_heads = hold2\n",
    "config.head_dim = hold3\n",
    "config.levels = hold4\n",
    "config.max_position_embeddings = hold5\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "print(\"head_dim_list: \", config.head_dim_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7934c0d0-d213-46c6-b75f-39fba2edea81",
   "metadata": {},
   "source": [
    "# Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2c19d5f-94eb-4347-8922-c88fa3c20735",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputLayer(nn.Module):\n",
    "    def __init__(self, embedding: torch.Tensor, config: Config):\n",
    "        super().__init__()\n",
    "        self.embedding = embedding\n",
    "        self.v = config.vocab_size\n",
    "        self.model_dim_list = config.model_dim_list\n",
    "\n",
    "        # applies RMSNorm to the embedding matrix\n",
    "        self.embedding_norm = RMSNorm(config.hidden_size,\n",
    "                                      eps = config.rms_norm_eps)\n",
    "        \n",
    "        # Applies RMSNorm to the model's final residual state before we use the embedding matrix to get logits\n",
    "        self.final_norm = RMSNorm(config.hidden_size,\n",
    "                                  eps = config.rms_norm_eps)\n",
    "\n",
    "    def forwardTensor(self, x, model=0):\n",
    "        global verbose\n",
    "        if verbose: \n",
    "            print(\"------------- OutputLayer.forwardTensor() ------------\")\n",
    "            print(f\"x: {x.shape}\\n{x}\")\n",
    "\n",
    "        # setting up our splicing logic\n",
    "        d_i = x.shape[-1]\n",
    "        skip = model * d_i\n",
    "        if verbose:\n",
    "            print(f\"d_i: {d_i}\")\n",
    "            print(f\"skip: {skip}\")\n",
    "            print(f\"embedding: {self.embedding.shape}\\n{self.embedding}\")\n",
    "\n",
    "        # splice out our embedding matrix according to what model we're using\n",
    "        sliced_embed = self.embedding[:,skip:skip + d_i]\n",
    "        if verbose: print(f\"sliced_embed: {sliced_embed.shape}\\n{sliced_embed}\")\n",
    "\n",
    "        # normalize our sliced embedding matrix\n",
    "        normed_sliced_embed = self.embedding_norm(sliced_embed)\n",
    "        if verbose: print(f\"normed & sliced embedding: {normed_sliced_embed.shape}\\n{normed_sliced_embed}\")\n",
    "\n",
    "        # normalize the residual state before the final linear layer\n",
    "        x = self.final_norm(x, model)\n",
    "        if verbose: print(f\"normed x: {x.shape}\\n{x}\")\n",
    "\n",
    "        # calculating the final output logits of the model\n",
    "        logits = x @ normed_sliced_embed.t()\n",
    "        if verbose: \n",
    "            print(f\"final logits: {logits.shape}\\n{logits}\")\n",
    "            print(\"------------- END OutputLayer.forwardTensor() ------------\")\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def forwardTuple(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the final embedding classification layer during training.\n",
    "\n",
    "        Parameters:\n",
    "            x (Tuple[Tuple[Tensor]]): \n",
    "                The input tuple of tuples of tensors \n",
    "                first tuple is of length config.levels and second layer of tuples have lengths of config.model_count\n",
    "                tensors are shape (batch size, sequence length, hidden dimension) where hidden dimension changes by which model was used\n",
    "\n",
    "        Returns:\n",
    "            output (Tuple[Tuple[Tensor]]): \n",
    "                The output tuple of tuples of tensors after applying the final embedding classification\n",
    "        \"\"\"\n",
    "        global verbose\n",
    "        if verbose: \n",
    "            print(\"------------- OutputLayer.forwardTuple() ------------\")\n",
    "            print(f\"x:\\n{x}\")\n",
    "            \n",
    "        # forwardTuple() should only be used during training, so we assert input_len == max_position_embeddings\n",
    "        assert x is tuple\n",
    "        input_len = x[0][0].shape[1]\n",
    "        if verbose: print(f\"input_len: {input_len}\")\n",
    "        assert input_len == config.max_position_embeddings\n",
    "\n",
    "        # we could define these from the config but this way the method is more flexible to testing\n",
    "        num_levels = len(x)\n",
    "        models_per_level = [len(x[i]) for i in range(num_levels)]\n",
    "        if verbose: \n",
    "            print(f\"num_levels: {num_levels}\")\n",
    "            print(f\"models_per_level: {models_per_level}\")\n",
    "\n",
    "        # the loop that iterates over levels, aka the different potential sizes of models\n",
    "        out = ()\n",
    "        for i in range(num_levels):\n",
    "            if verbose: print(f\"Level {i} from range({num_levels})\")\n",
    "\n",
    "            # now for the loop that iterates over models in this level\n",
    "            out_lvl = ()\n",
    "            for j in range(models_per_level[i]):\n",
    "                if verbose: print(f\"Model {j} from range({models_per_level[i]})\")\n",
    "\n",
    "                output = self.forwardTensor(x[i][j], model = j)\n",
    "                if verbose: print(f\"forwardTensor() output: {output.shape}\\n{output}\")\n",
    "                \n",
    "                out_lvl += (output,)\n",
    "            \n",
    "            out += (out_lvl,)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"final output: {out}\")\n",
    "            print(\"------------- END Layer.forwardTuple() ------------\")\n",
    "        \n",
    "        return out\n",
    "        \n",
    "    def forward(self, x, model=0):\n",
    "        train = True if type(x) == tuple else False\n",
    "        print(f\"---------- Layer Input: {'Tuple' if train else 'torch.Tensor'} ------------\")\n",
    "        return self.forwardTuple(x) if train else self.forwardTensor(x, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "242486f7-e76a-47dd-8325-94fba4d0d990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Micro Hyperparameters -------\n",
      "model_count:  [1, 2, 4]\n",
      "model_dim_list:  [4, 2, 1]\n",
      "embedding: torch.Size([5, 4])\n",
      "tensor([[-1.0187,  0.9947, -0.0253, -3.0200],\n",
      "        [ 2.4933, -0.1411,  1.8727, -0.3439],\n",
      "        [ 1.6053,  1.1763, -0.3581, -1.0166],\n",
      "        [ 0.5760,  0.5348,  1.8814,  0.2625],\n",
      "        [-1.8697, -1.8058,  1.7278, -0.0814]])\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the big model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[0.0579, 0.5822, 0.7695, 0.3253],\n",
      "         [0.5348, 0.7898, 0.7203, 0.9398],\n",
      "         [0.5933, 0.6965, 0.8708, 0.9116]]])\n",
      "---------- Layer Input: torch.Tensor ------------\n",
      "------------- OutputLayer.forwardTensor() ------------\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[0.0579, 0.5822, 0.7695, 0.3253],\n",
      "         [0.5348, 0.7898, 0.7203, 0.9398],\n",
      "         [0.5933, 0.6965, 0.8708, 0.9116]]])\n",
      "d_i: 4\n",
      "skip: 0\n",
      "embedding: torch.Size([5, 4])\n",
      "tensor([[-1.0187,  0.9947, -0.0253, -3.0200],\n",
      "        [ 2.4933, -0.1411,  1.8727, -0.3439],\n",
      "        [ 1.6053,  1.1763, -0.3581, -1.0166],\n",
      "        [ 0.5760,  0.5348,  1.8814,  0.2625],\n",
      "        [-1.8697, -1.8058,  1.7278, -0.0814]])\n",
      "sliced_embed: torch.Size([5, 4])\n",
      "tensor([[-1.0187,  0.9947, -0.0253, -3.0200],\n",
      "        [ 2.4933, -0.1411,  1.8727, -0.3439],\n",
      "        [ 1.6053,  1.1763, -0.3581, -1.0166],\n",
      "        [ 0.5760,  0.5348,  1.8814,  0.2625],\n",
      "        [-1.8697, -1.8058,  1.7278, -0.0814]])\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([5, 4])\n",
      "tensor([[-1.0187,  0.9947, -0.0253, -3.0200],\n",
      "        [ 2.4933, -0.1411,  1.8727, -0.3439],\n",
      "        [ 1.6053,  1.1763, -0.3581, -1.0166],\n",
      "        [ 0.5760,  0.5348,  1.8814,  0.2625],\n",
      "        [-1.8697, -1.8058,  1.7278, -0.0814]])\n",
      "normed x: torch.Size([5, 4])\n",
      "tensor([[-0.6102,  0.5958, -0.0151, -1.8090],\n",
      "        [ 1.5879, -0.0898,  1.1926, -0.2190],\n",
      "        [ 1.4186,  1.0395, -0.3164, -0.8983],\n",
      "        [ 0.5604,  0.5203,  1.8303,  0.2554],\n",
      "        [-1.1977, -1.1567,  1.1067, -0.0521]])\n",
      "dim: 4\n",
      "skip: 0\n",
      "spliced scale: torch.Size([4])\n",
      "tensor([0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([5, 4])\n",
      "tensor([[-0.6102,  0.5958, -0.0151, -1.8090],\n",
      "        [ 1.5879, -0.0898,  1.1926, -0.2190],\n",
      "        [ 1.4186,  1.0395, -0.3164, -0.8983],\n",
      "        [ 0.5604,  0.5203,  1.8303,  0.2554],\n",
      "        [-1.1977, -1.1567,  1.1067, -0.0521]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "normed & sliced embedding: torch.Size([5, 4])\n",
      "tensor([[-0.6102,  0.5958, -0.0151, -1.8090],\n",
      "        [ 1.5879, -0.0898,  1.1926, -0.2190],\n",
      "        [ 1.4186,  1.0395, -0.3164, -0.8983],\n",
      "        [ 0.5604,  0.5203,  1.8303,  0.2554],\n",
      "        [-1.1977, -1.1567,  1.1067, -0.0521]], grad_fn=<MulBackward0>)\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[0.0579, 0.5822, 0.7695, 0.3253],\n",
      "         [0.5348, 0.7898, 0.7203, 0.9398],\n",
      "         [0.5933, 0.6965, 0.8708, 0.9116]]])\n",
      "normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[0.1136, 1.1417, 1.5090, 0.6378],\n",
      "         [0.7035, 1.0389, 0.9475, 1.2362],\n",
      "         [0.7618, 0.8943, 1.1180, 1.1704]]])\n",
      "dim: 4\n",
      "skip: 0\n",
      "spliced scale: torch.Size([4])\n",
      "tensor([0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[0.1136, 1.1417, 1.5090, 0.6378],\n",
      "         [0.7035, 1.0389, 0.9475, 1.2362],\n",
      "         [0.7618, 0.8943, 1.1180, 1.1704]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[0.1136, 1.1417, 1.5090, 0.6378],\n",
      "         [0.7035, 1.0389, 0.9475, 1.2362],\n",
      "         [0.7618, 0.8943, 1.1180, 1.1704]]], grad_fn=<MulBackward0>)\n",
      "final logits: torch.Size([1, 3, 5])\n",
      "tensor([[[-0.5657,  1.7378,  0.2975,  3.5824,  0.1802],\n",
      "         [-2.0607,  1.8830,  0.6675,  2.9846, -1.0600],\n",
      "         [-2.0662,  2.2064,  0.6050,  3.2375, -0.7704]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "------------- END OutputLayer.forwardTensor() ------------\n",
      "y: torch.Size([1, 3, 5])\n",
      "tensor([[[-0.5657,  1.7378,  0.2975,  3.5824,  0.1802],\n",
      "         [-2.0607,  1.8830,  0.6675,  2.9846, -1.0600],\n",
      "         [-2.0662,  2.2064,  0.6050,  3.2375, -0.7704]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the first sub-model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 3, 2])\n",
      "tensor([[[0.1656, 0.8909],\n",
      "         [0.5840, 0.1227],\n",
      "         [0.5445, 0.4471]]])\n",
      "---------- Layer Input: torch.Tensor ------------\n",
      "------------- OutputLayer.forwardTensor() ------------\n",
      "x: torch.Size([1, 3, 2])\n",
      "tensor([[[0.1656, 0.8909],\n",
      "         [0.5840, 0.1227],\n",
      "         [0.5445, 0.4471]]])\n",
      "d_i: 2\n",
      "skip: 0\n",
      "embedding: torch.Size([5, 4])\n",
      "tensor([[-1.0187,  0.9947, -0.0253, -3.0200],\n",
      "        [ 2.4933, -0.1411,  1.8727, -0.3439],\n",
      "        [ 1.6053,  1.1763, -0.3581, -1.0166],\n",
      "        [ 0.5760,  0.5348,  1.8814,  0.2625],\n",
      "        [-1.8697, -1.8058,  1.7278, -0.0814]])\n",
      "sliced_embed: torch.Size([5, 2])\n",
      "tensor([[-1.0187,  0.9947],\n",
      "        [ 2.4933, -0.1411],\n",
      "        [ 1.6053,  1.1763],\n",
      "        [ 0.5760,  0.5348],\n",
      "        [-1.8697, -1.8058]])\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([5, 2])\n",
      "tensor([[-1.0187,  0.9947],\n",
      "        [ 2.4933, -0.1411],\n",
      "        [ 1.6053,  1.1763],\n",
      "        [ 0.5760,  0.5348],\n",
      "        [-1.8697, -1.8058]])\n",
      "normed x: torch.Size([5, 2])\n",
      "tensor([[-1.0118,  0.9880],\n",
      "        [ 1.4120, -0.0799],\n",
      "        [ 1.1407,  0.8359],\n",
      "        [ 1.0364,  0.9622],\n",
      "        [-1.0173, -0.9824]])\n",
      "dim: 2\n",
      "skip: 0\n",
      "spliced scale: torch.Size([2])\n",
      "tensor([0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([5, 2])\n",
      "tensor([[-1.0118,  0.9880],\n",
      "        [ 1.4120, -0.0799],\n",
      "        [ 1.1407,  0.8359],\n",
      "        [ 1.0364,  0.9622],\n",
      "        [-1.0173, -0.9824]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "normed & sliced embedding: torch.Size([5, 2])\n",
      "tensor([[-1.0118,  0.9880],\n",
      "        [ 1.4120, -0.0799],\n",
      "        [ 1.1407,  0.8359],\n",
      "        [ 1.0364,  0.9622],\n",
      "        [-1.0173, -0.9824]], grad_fn=<MulBackward0>)\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 3, 2])\n",
      "tensor([[[0.1656, 0.8909],\n",
      "         [0.5840, 0.1227],\n",
      "         [0.5445, 0.4471]]])\n",
      "normed x: torch.Size([1, 3, 2])\n",
      "tensor([[[0.2585, 1.3904],\n",
      "         [1.3840, 0.2908],\n",
      "         [1.0929, 0.8975]]])\n",
      "dim: 2\n",
      "skip: 0\n",
      "spliced scale: torch.Size([2])\n",
      "tensor([0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 3, 2])\n",
      "tensor([[[0.2585, 1.3904],\n",
      "         [1.3840, 0.2908],\n",
      "         [1.0929, 0.8975]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "normed x: torch.Size([1, 3, 2])\n",
      "tensor([[[0.2585, 1.3904],\n",
      "         [1.3840, 0.2908],\n",
      "         [1.0929, 0.8975]]], grad_fn=<MulBackward0>)\n",
      "final logits: torch.Size([1, 3, 5])\n",
      "tensor([[[ 1.1122,  0.2539,  1.4571,  1.6058, -1.6289],\n",
      "         [-1.1131,  1.9309,  1.8218,  1.7141, -1.6935],\n",
      "         [-0.2191,  1.4714,  1.9970,  1.9963, -1.9935]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "------------- END OutputLayer.forwardTensor() ------------\n",
      "y: torch.Size([1, 3, 5])\n",
      "tensor([[[ 1.1122,  0.2539,  1.4571,  1.6058, -1.6289],\n",
      "         [-1.1131,  1.9309,  1.8218,  1.7141, -1.6935],\n",
      "         [-0.2191,  1.4714,  1.9970,  1.9963, -1.9935]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the second sub-model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 3, 2])\n",
      "tensor([[[0.9468, 0.7966],\n",
      "         [0.3082, 0.1491],\n",
      "         [0.4589, 0.3697]]])\n",
      "---------- Layer Input: torch.Tensor ------------\n",
      "------------- OutputLayer.forwardTensor() ------------\n",
      "x: torch.Size([1, 3, 2])\n",
      "tensor([[[0.9468, 0.7966],\n",
      "         [0.3082, 0.1491],\n",
      "         [0.4589, 0.3697]]])\n",
      "d_i: 2\n",
      "skip: 2\n",
      "embedding: torch.Size([5, 4])\n",
      "tensor([[-1.0187,  0.9947, -0.0253, -3.0200],\n",
      "        [ 2.4933, -0.1411,  1.8727, -0.3439],\n",
      "        [ 1.6053,  1.1763, -0.3581, -1.0166],\n",
      "        [ 0.5760,  0.5348,  1.8814,  0.2625],\n",
      "        [-1.8697, -1.8058,  1.7278, -0.0814]])\n",
      "sliced_embed: torch.Size([5, 2])\n",
      "tensor([[-0.0253, -3.0200],\n",
      "        [ 1.8727, -0.3439],\n",
      "        [-0.3581, -1.0166],\n",
      "        [ 1.8814,  0.2625],\n",
      "        [ 1.7278, -0.0814]])\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([5, 2])\n",
      "tensor([[-0.0253, -3.0200],\n",
      "        [ 1.8727, -0.3439],\n",
      "        [-0.3581, -1.0166],\n",
      "        [ 1.8814,  0.2625],\n",
      "        [ 1.7278, -0.0814]])\n",
      "normed x: torch.Size([5, 2])\n",
      "tensor([[-0.0118, -1.4142],\n",
      "        [ 1.3910, -0.2554],\n",
      "        [-0.4699, -1.3339],\n",
      "        [ 1.4006,  0.1954],\n",
      "        [ 1.4126, -0.0665]])\n",
      "dim: 2\n",
      "skip: 0\n",
      "spliced scale: torch.Size([2])\n",
      "tensor([0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([5, 2])\n",
      "tensor([[-0.0118, -1.4142],\n",
      "        [ 1.3910, -0.2554],\n",
      "        [-0.4699, -1.3339],\n",
      "        [ 1.4006,  0.1954],\n",
      "        [ 1.4126, -0.0665]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "normed & sliced embedding: torch.Size([5, 2])\n",
      "tensor([[-0.0118, -1.4142],\n",
      "        [ 1.3910, -0.2554],\n",
      "        [-0.4699, -1.3339],\n",
      "        [ 1.4006,  0.1954],\n",
      "        [ 1.4126, -0.0665]], grad_fn=<MulBackward0>)\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 3, 2])\n",
      "tensor([[[0.9468, 0.7966],\n",
      "         [0.3082, 0.1491],\n",
      "         [0.4589, 0.3697]]])\n",
      "normed x: torch.Size([1, 3, 2])\n",
      "tensor([[[1.0822, 0.9105],\n",
      "         [1.2730, 0.6159],\n",
      "         [1.1013, 0.8872]]])\n",
      "dim: 2\n",
      "skip: 2\n",
      "spliced scale: torch.Size([2])\n",
      "tensor([0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 3, 2])\n",
      "tensor([[[1.0822, 0.9105],\n",
      "         [1.2730, 0.6159],\n",
      "         [1.1013, 0.8872]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "normed x: torch.Size([1, 3, 2])\n",
      "tensor([[[1.0822, 0.9105],\n",
      "         [1.2730, 0.6159],\n",
      "         [1.1013, 0.8872]]], grad_fn=<MulBackward0>)\n",
      "final logits: torch.Size([1, 3, 5])\n",
      "tensor([[[-1.3003,  1.2727, -1.7229,  1.6936,  1.4682],\n",
      "         [-0.8860,  1.6135, -1.4197,  1.9034,  1.7574],\n",
      "         [-1.2676,  1.3053, -1.7008,  1.7160,  1.4968]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "------------- END OutputLayer.forwardTensor() ------------\n",
      "y: torch.Size([1, 3, 5])\n",
      "tensor([[[-1.3003,  1.2727, -1.7229,  1.6936,  1.4682],\n",
      "         [-0.8860,  1.6135, -1.4197,  1.9034,  1.7574],\n",
      "         [-1.2676,  1.3053, -1.7008,  1.7160,  1.4968]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "---------- RESET CONFIG --------\n",
      "model_count:  [1, 2, 4]\n",
      "model_dim_list:  [128, 64, 32]\n"
     ]
    }
   ],
   "source": [
    "# Testing our OutputLayer's forwardTensor()\n",
    "verbose = True\n",
    "\n",
    "print(\"--------- Micro Hyperparameters -------\")\n",
    "hold1, hold2 = config.hidden_size, config.vocab_size\n",
    "config.hidden_size = 4\n",
    "config.vocab_size = 5\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "\n",
    "embedding = torch.randn(config.vocab_size, config.hidden_size)\n",
    "print(f\"embedding: {embedding.shape}\\n{embedding}\")\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the big model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,3,config.hidden_size)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "layer = OutputLayer(embedding, config)\n",
    "y = layer(x)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the first sub-model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,3,config.hidden_size//config.split)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "layer = OutputLayer(embedding, config)\n",
    "y = layer(x)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the second sub-model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,3,config.hidden_size//config.split)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "layer = OutputLayer(embedding, config)\n",
    "y = layer(x, model=1)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "verbose = False\n",
    "print(\"---------- RESET CONFIG --------\")\n",
    "config.hidden_size = hold1\n",
    "config.vocab_size = hold2\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b436e615-f928-4e03-9b14-86e916baff38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Micro Hyperparameters -------\n",
      "model_count:  [1, 2]\n",
      "model_dim_list:  [4, 2]\n",
      "embedding: torch.Size([5, 4])\n",
      "tensor([[-0.8400, -0.5197, -1.2188,  0.5423],\n",
      "        [-0.2075,  0.7968, -0.4593, -1.0467],\n",
      "        [ 1.3413, -0.5771, -1.0626,  0.6517],\n",
      "        [-0.7116, -1.1119, -0.8719,  1.4338],\n",
      "        [-1.2388, -0.6028,  1.4139,  0.1831]])\n",
      "x: ((tensor([[[-0.1462,  0.2696, -1.3195, -1.2945],\n",
      "         [-0.9201, -1.2686, -1.1653,  2.0797],\n",
      "         [-0.4185, -0.1431,  0.7092,  1.2014]]]),), (tensor([[[-0.2275, -0.4399],\n",
      "         [ 2.3621,  0.2800],\n",
      "         [-0.2841,  1.2598]]]), tensor([[[-1.0265, -1.4249],\n",
      "         [ 0.9723,  1.2800],\n",
      "         [ 1.3253, -1.7510]]])))\n",
      "---------- Layer Input: Tuple ------------\n",
      "------------- Layer.forwardTuple() ------------\n",
      "x:\n",
      "((tensor([[[-0.1462,  0.2696, -1.3195, -1.2945],\n",
      "         [-0.9201, -1.2686, -1.1653,  2.0797],\n",
      "         [-0.4185, -0.1431,  0.7092,  1.2014]]]),), (tensor([[[-0.2275, -0.4399],\n",
      "         [ 2.3621,  0.2800],\n",
      "         [-0.2841,  1.2598]]]), tensor([[[-1.0265, -1.4249],\n",
      "         [ 0.9723,  1.2800],\n",
      "         [ 1.3253, -1.7510]]])))\n",
      "input_len: 3\n",
      "num_levels: 2\n",
      "models_per_level: [1, 2]\n",
      "Level 0 from range(2)\n",
      "Model 0 from range(1)\n",
      "----------------- Layer.forwardTensor() --------------------\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.1462,  0.2696, -1.3195, -1.2945],\n",
      "         [-0.9201, -1.2686, -1.1653,  2.0797],\n",
      "         [-0.4185, -0.1431,  0.7092,  1.2014]]])\n",
      "normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.1560,  0.2877, -1.4084, -1.3817],\n",
      "         [-0.6450, -0.8893, -0.8169,  1.4580],\n",
      "         [-0.5719, -0.1956,  0.9692,  1.6418]]])\n",
      "dim: 4\n",
      "skip: 0\n",
      "spliced scale: torch.Size([4])\n",
      "tensor([0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.1560,  0.2877, -1.4084, -1.3817],\n",
      "         [-0.6450, -0.8893, -0.8169,  1.4580],\n",
      "         [-0.5719, -0.1956,  0.9692,  1.6418]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "---------- Attention Input: torch.Tensor ------------\n",
      "----------------- MultiQueryAttention.forwardTensor() --------------------\n",
      "x shape: torch.Size([1, 3, 4])\n",
      "d_skip: 0\n",
      "models_in_this_level: 1\n",
      "h_dim: 32\n",
      "h_skip: 0\n",
      "self.Wqkv: torch.Size([4, 192])\n",
      "Parameter containing:\n",
      "tensor([[-3.2097e-01, -1.5344e-02, -1.1510e-01, -3.0874e-01,  3.3117e-01,\n",
      "         -2.5896e-01,  6.9617e-02,  4.6040e-01, -4.1660e-01,  4.1727e-01,\n",
      "          4.1405e-01, -2.2532e-01,  4.6028e-01,  7.0296e-02, -4.5145e-01,\n",
      "         -4.9349e-01,  8.6677e-02,  2.9951e-01,  3.9258e-01, -4.0547e-01,\n",
      "          4.8854e-01, -4.5012e-01,  2.7701e-01,  3.6282e-01, -2.8834e-01,\n",
      "          5.2543e-02, -4.6092e-01,  1.8264e-01, -4.3262e-01, -4.9215e-01,\n",
      "          4.2615e-01, -3.8198e-01,  1.2274e-01,  2.0562e-01,  3.0310e-01,\n",
      "          2.8698e-01, -8.8241e-02, -1.6075e-01,  2.5161e-01,  4.2444e-01,\n",
      "         -8.2624e-02, -4.0507e-01, -1.2071e-01, -4.2426e-01, -2.8454e-01,\n",
      "          2.0912e-01,  2.4156e-01, -3.9879e-01, -2.2591e-01,  3.3003e-01,\n",
      "          3.2374e-01, -1.6253e-02,  2.8714e-01, -3.8530e-01, -3.8436e-01,\n",
      "         -2.4212e-01,  3.6687e-02, -4.5855e-01,  4.8830e-01,  4.7009e-01,\n",
      "         -1.6423e-01, -3.2546e-01,  4.4032e-01,  3.6316e-01, -8.7626e-02,\n",
      "         -4.4174e-01, -4.6600e-01, -4.8568e-03, -4.8418e-01,  6.4661e-02,\n",
      "          4.8494e-01, -1.6592e-01, -2.0181e-01,  4.4469e-01,  9.5466e-02,\n",
      "          4.8366e-01,  4.4880e-01, -1.9130e-01,  1.5374e-01,  2.2177e-01,\n",
      "          4.9499e-01,  4.8626e-01, -3.4601e-01, -3.4760e-01,  8.6108e-02,\n",
      "          4.1357e-01, -2.4427e-01, -2.5268e-01,  2.0449e-01, -4.9356e-01,\n",
      "         -3.6677e-01, -1.0875e-01,  4.0741e-01, -4.2069e-01, -3.3176e-01,\n",
      "          2.1056e-01, -8.8283e-02, -1.0314e-01,  4.6487e-01, -3.7954e-01,\n",
      "         -2.3864e-01, -4.2001e-01, -1.2691e-01,  1.9158e-01,  1.1451e-01,\n",
      "          1.6561e-01,  1.7231e-01,  1.7429e-01, -1.8123e-01,  3.6120e-01,\n",
      "          3.1331e-01, -9.2029e-02,  3.9661e-01,  8.8137e-02,  1.8981e-01,\n",
      "          1.9692e-02,  3.0192e-01, -2.1089e-01, -3.5772e-02, -4.6871e-01,\n",
      "         -4.8495e-01, -3.9506e-01,  1.8029e-01,  3.0469e-01, -6.6937e-02,\n",
      "         -1.2337e-01, -4.9423e-01,  4.1953e-01, -1.5530e-01, -3.9074e-01,\n",
      "         -1.3531e-01, -1.8341e-01,  3.2812e-01,  3.5656e-01,  1.2500e-02,\n",
      "          3.2196e-01, -4.7280e-01,  4.0531e-01, -1.1058e-01,  2.0058e-01,\n",
      "         -3.1400e-01,  9.3957e-02, -2.3539e-02, -4.6383e-01, -2.1440e-01,\n",
      "          3.5762e-01,  2.6374e-01, -1.1291e-01,  1.3419e-01,  2.3561e-01,\n",
      "          2.6260e-01, -2.5277e-01, -4.1158e-01,  2.9288e-01, -3.6784e-01,\n",
      "          2.8182e-02, -4.1021e-01, -2.1270e-01,  1.2341e-01, -4.1593e-01,\n",
      "         -2.2971e-01, -2.8959e-01,  1.1191e-01, -2.0859e-01, -1.2854e-01,\n",
      "         -1.8272e-01, -9.3147e-02, -2.4128e-01, -4.6417e-01, -1.5658e-01,\n",
      "         -2.6369e-01, -2.7113e-02, -1.5909e-01, -2.2127e-01, -1.8380e-01,\n",
      "          3.7399e-01, -3.1944e-01,  2.3862e-02, -2.5048e-01, -2.1036e-01,\n",
      "         -2.7727e-01, -2.9599e-01,  4.8089e-01, -4.5476e-01,  3.0850e-01,\n",
      "          1.0780e-01,  9.1512e-02, -4.3502e-01, -2.0343e-01,  1.3289e-02,\n",
      "         -1.5765e-01,  1.8142e-01],\n",
      "        [-1.4990e-01,  2.7819e-01,  7.4129e-02, -4.4660e-02,  4.3282e-01,\n",
      "         -3.0581e-01,  1.8844e-01,  4.4872e-01, -1.1302e-01,  1.4516e-01,\n",
      "         -4.8995e-05,  4.3458e-01,  5.3082e-02,  4.4512e-01,  1.2523e-01,\n",
      "         -1.1507e-01,  3.1780e-01, -3.1342e-01, -4.4965e-01, -3.3284e-02,\n",
      "          1.5604e-02, -1.6669e-01,  1.8346e-01, -2.4034e-01, -4.7407e-01,\n",
      "         -7.6704e-02,  6.7073e-02,  1.2652e-01, -3.9627e-01,  3.7724e-01,\n",
      "         -1.9206e-01,  3.0627e-01,  4.2555e-02, -3.0911e-01,  4.3753e-01,\n",
      "         -2.9667e-01, -1.4520e-02, -1.0416e-02, -3.2493e-01,  4.7091e-01,\n",
      "          2.3998e-01,  3.4873e-01,  5.5291e-02,  9.5433e-02,  1.9302e-01,\n",
      "          3.8875e-01,  2.3504e-01, -1.4209e-01,  2.1127e-01,  2.3719e-01,\n",
      "          3.7475e-01, -2.1212e-01, -2.0359e-01, -4.3030e-01,  8.8165e-02,\n",
      "         -2.9098e-01,  7.8429e-02, -3.4094e-01,  4.3696e-01,  1.9249e-01,\n",
      "         -4.8272e-01, -3.3708e-01, -2.7591e-01, -2.7684e-01, -3.7848e-01,\n",
      "         -4.9171e-01, -4.6506e-01, -1.9350e-01,  3.7812e-01,  3.6263e-02,\n",
      "         -1.0092e-01, -3.0463e-01, -5.2127e-02, -2.3248e-01, -1.8609e-01,\n",
      "         -4.9422e-01,  4.5411e-01,  2.4784e-01,  1.7444e-01,  2.7136e-01,\n",
      "         -3.0759e-01, -2.0130e-01,  2.6875e-01, -4.1371e-01,  4.6615e-01,\n",
      "          1.8902e-01, -2.4082e-01, -4.6193e-01, -6.2983e-02, -2.6200e-01,\n",
      "          3.7065e-01,  4.9508e-01, -5.5899e-03,  1.6553e-01,  2.9573e-01,\n",
      "         -2.0170e-01, -5.1177e-03,  3.8824e-01, -3.7151e-01, -1.4505e-01,\n",
      "         -3.5091e-01, -4.1308e-01, -4.3970e-01,  2.8640e-01, -7.5188e-02,\n",
      "         -1.1953e-01,  2.8627e-01,  2.9312e-01,  4.1545e-02,  3.4332e-01,\n",
      "          5.6798e-02,  2.4595e-02,  1.1926e-01,  4.9348e-02,  1.0625e-01,\n",
      "          3.0282e-01,  1.3777e-01,  4.6769e-01, -4.9020e-01,  3.9159e-01,\n",
      "          6.4323e-02,  3.1386e-01,  1.8567e-01, -3.6389e-01, -6.6978e-02,\n",
      "          3.4887e-01, -4.4969e-01,  4.6964e-01,  8.3232e-02, -9.8104e-02,\n",
      "         -4.3657e-01, -3.1562e-01, -4.9943e-01,  2.4091e-01, -7.4649e-02,\n",
      "         -8.5931e-02, -4.4864e-01,  2.0309e-01,  1.6673e-01, -3.4258e-01,\n",
      "         -3.1006e-01,  2.3308e-01,  2.1086e-01,  1.3077e-01, -2.9130e-01,\n",
      "         -2.9917e-01, -1.6217e-01,  3.2903e-01, -3.0571e-01, -2.0588e-01,\n",
      "          3.5932e-01, -2.6446e-01, -1.8150e-01, -4.2555e-01, -2.7207e-01,\n",
      "         -3.3697e-01,  5.7231e-03, -9.8906e-03, -2.9698e-01, -4.7332e-01,\n",
      "         -5.7863e-02, -2.1753e-01,  2.9426e-01,  3.6454e-01, -3.2457e-02,\n",
      "          2.5800e-01, -4.3584e-01, -8.4784e-02,  5.4089e-03,  3.8661e-01,\n",
      "         -4.5274e-01, -2.3479e-01,  3.2677e-01,  1.7917e-02,  3.6443e-02,\n",
      "          2.8099e-01,  4.1516e-01, -2.7281e-01, -1.5072e-01, -3.6735e-01,\n",
      "          6.6030e-02, -1.6825e-01,  7.4779e-03, -4.9353e-01,  3.9334e-01,\n",
      "          1.6545e-01,  3.9433e-01,  3.8434e-01,  2.4296e-02, -3.1079e-01,\n",
      "          3.2874e-01,  4.7495e-01],\n",
      "        [-3.6494e-01, -1.9819e-01,  1.2391e-01,  1.4987e-01, -4.8445e-01,\n",
      "         -2.4220e-01,  1.5516e-01, -2.8535e-02,  3.2431e-01, -2.2211e-01,\n",
      "          1.5887e-01,  2.8480e-01,  2.1065e-01, -3.2590e-01, -3.0512e-01,\n",
      "         -4.2174e-01, -2.1729e-01, -2.7202e-01, -4.9488e-01,  4.2156e-01,\n",
      "          4.1862e-01, -2.5510e-01, -1.7890e-01, -2.3873e-01, -1.4193e-01,\n",
      "          5.3084e-02,  1.0497e-01,  4.0349e-01, -4.5228e-01,  5.2142e-02,\n",
      "          4.7570e-01, -1.9244e-01, -2.6906e-01,  3.6564e-02,  2.8004e-01,\n",
      "          1.9182e-01, -3.6188e-01,  3.8665e-01,  2.9734e-01,  1.3272e-01,\n",
      "          2.8950e-01, -1.1843e-01,  1.8197e-01,  1.0729e-01, -1.2266e-01,\n",
      "          3.5865e-03, -3.2721e-01,  4.0710e-01,  2.1084e-01, -4.9324e-01,\n",
      "         -4.5889e-01,  4.4462e-01,  1.4339e-01,  6.2129e-02, -3.7913e-01,\n",
      "         -6.1301e-02, -5.8408e-02,  2.0735e-01, -2.0772e-01, -4.4801e-01,\n",
      "          8.8660e-02, -3.2614e-01,  5.8857e-02, -3.6563e-03, -1.2983e-01,\n",
      "          2.8123e-01, -3.0251e-01, -2.0165e-01, -2.1545e-01,  3.0909e-01,\n",
      "         -1.9715e-01, -3.4939e-01, -3.8061e-01,  4.6676e-01,  4.4117e-01,\n",
      "         -1.2104e-01,  1.7660e-01, -2.4349e-02,  2.1922e-01, -4.1842e-01,\n",
      "         -4.8203e-01,  6.6405e-02, -2.5446e-01, -2.6192e-01, -3.6002e-01,\n",
      "          3.5817e-01, -5.3533e-02, -1.7330e-01, -2.6523e-01, -4.1151e-01,\n",
      "          2.4367e-01, -1.5724e-01,  1.0179e-01,  1.8709e-02,  2.6733e-01,\n",
      "         -3.9686e-01,  1.9645e-01,  1.2850e-01, -1.6648e-01, -2.5955e-02,\n",
      "          2.8239e-01, -3.5465e-01, -8.4585e-02, -1.6311e-01,  2.4559e-01,\n",
      "          3.4939e-01,  2.8058e-01,  2.2911e-01, -1.5656e-01, -3.8567e-01,\n",
      "          4.8251e-02,  5.4339e-02,  2.4236e-03,  4.1418e-02,  1.3403e-01,\n",
      "          4.6479e-01, -3.6504e-01, -3.9382e-01,  8.7580e-02,  3.4661e-01,\n",
      "          3.6698e-01,  3.1494e-01,  2.8518e-01,  3.6290e-01, -1.9336e-01,\n",
      "         -3.9900e-01, -3.0189e-01,  4.7716e-01, -5.7343e-02, -3.9568e-01,\n",
      "          4.7925e-01, -2.9701e-01,  1.6367e-01, -2.7030e-01,  1.7530e-01,\n",
      "         -4.8403e-01, -2.8215e-01, -1.4787e-01, -9.4221e-02,  1.7540e-02,\n",
      "         -2.8544e-01,  3.6671e-01,  4.9354e-01,  2.5809e-01,  4.5043e-01,\n",
      "          4.0915e-01,  3.4615e-01, -2.8886e-01, -1.0766e-01,  2.1052e-01,\n",
      "          6.3936e-02, -1.3789e-01,  3.7345e-01,  2.8598e-01, -4.5961e-01,\n",
      "         -2.7873e-01,  2.4984e-01, -3.3217e-01, -3.3710e-01, -3.4967e-01,\n",
      "         -6.8175e-02,  4.5880e-01,  1.7332e-01, -2.4107e-01, -2.8240e-01,\n",
      "         -2.8811e-01,  2.5626e-01, -4.9741e-01,  8.1723e-02,  2.7604e-01,\n",
      "          4.7153e-01, -3.5504e-01, -3.8385e-01, -3.3069e-01, -2.1225e-01,\n",
      "          1.5297e-01,  4.7800e-01, -1.8034e-02, -3.5942e-01,  4.7031e-01,\n",
      "         -3.1677e-01,  1.2753e-01,  4.5037e-01,  2.8465e-01,  1.2980e-01,\n",
      "         -2.7024e-01,  2.7878e-01, -4.7501e-01,  4.2285e-01,  4.5226e-01,\n",
      "         -7.5834e-02, -2.7833e-01],\n",
      "        [ 3.6166e-01, -2.9039e-01, -1.9532e-01,  2.1463e-01,  3.0336e-01,\n",
      "          3.4968e-01,  1.7080e-01, -3.1044e-01, -2.6154e-01, -1.7996e-01,\n",
      "         -3.9154e-01,  4.5309e-01,  2.6656e-01,  3.2655e-01,  4.4053e-01,\n",
      "         -3.8769e-01,  4.5635e-01, -1.7444e-01,  2.8878e-01,  2.6081e-01,\n",
      "         -3.5004e-01,  1.7178e-01,  4.8178e-01, -2.5152e-01, -1.7255e-01,\n",
      "         -4.2525e-01, -5.4256e-02, -2.4009e-01, -2.3620e-01, -1.2440e-01,\n",
      "          3.0858e-01, -4.4050e-01,  1.7219e-01, -1.5976e-01,  3.5247e-01,\n",
      "          2.9060e-01, -3.8982e-01,  5.3928e-02,  2.4100e-01,  6.9047e-03,\n",
      "          4.1114e-01,  3.0978e-02,  2.3816e-01, -2.6534e-01, -9.1487e-02,\n",
      "         -1.2970e-01, -1.3459e-01,  4.5638e-01,  4.2021e-01, -2.6263e-01,\n",
      "          6.4170e-02,  2.1780e-01, -1.9102e-01, -3.3325e-02, -4.4954e-01,\n",
      "          2.6437e-01, -3.3372e-01,  4.9230e-01, -3.5328e-01, -2.8936e-01,\n",
      "         -1.1743e-01, -1.5401e-02,  3.7282e-01,  1.4672e-01,  7.0350e-02,\n",
      "          3.2993e-01,  2.1661e-01, -1.4682e-01,  4.2327e-01, -2.8551e-01,\n",
      "         -1.3827e-01,  2.2878e-01, -7.2087e-02,  4.5573e-01,  4.1544e-02,\n",
      "          1.9417e-01, -1.5592e-01, -2.4566e-01,  2.5795e-01,  1.7242e-03,\n",
      "          3.9895e-01,  6.4041e-02, -3.9480e-01, -1.9869e-01,  2.9795e-02,\n",
      "          7.1546e-02, -3.7036e-02, -1.2344e-01,  4.1349e-01, -1.3400e-01,\n",
      "          2.2914e-01,  4.0769e-01,  2.6082e-01, -3.1787e-01, -1.5388e-01,\n",
      "          4.6755e-01,  3.8927e-01,  3.0885e-01, -1.5936e-01,  1.8015e-01,\n",
      "         -1.3583e-01, -2.4389e-01, -3.9084e-01,  3.8825e-01, -1.7214e-01,\n",
      "          2.9319e-01,  4.8597e-01,  3.2779e-01,  2.0918e-01,  1.6785e-01,\n",
      "         -4.5960e-01, -4.2303e-01, -2.9061e-01,  4.8491e-01, -1.8013e-01,\n",
      "         -4.4329e-01,  1.1073e-03, -3.2739e-01, -1.9347e-01, -2.4275e-01,\n",
      "          1.8868e-01, -2.4091e-01, -6.2331e-02, -1.4281e-01,  4.4504e-01,\n",
      "         -4.6353e-01,  1.3425e-01, -4.4999e-01, -4.0042e-01,  1.3937e-01,\n",
      "         -2.3713e-01, -2.1600e-01, -4.5663e-01,  2.7168e-02,  2.3453e-01,\n",
      "         -3.6210e-01,  2.8875e-01,  3.8881e-01,  1.2422e-01,  4.6177e-01,\n",
      "          4.4899e-01,  4.2099e-01, -2.2689e-01,  2.3584e-01, -1.8267e-01,\n",
      "          2.2487e-01, -3.3422e-01, -4.5962e-01, -4.3136e-01, -4.5188e-01,\n",
      "         -2.5392e-01,  2.1336e-01,  5.7722e-02, -5.0130e-02, -4.0297e-01,\n",
      "          1.9646e-01, -2.6501e-01,  3.6008e-01,  1.8912e-01, -2.5685e-01,\n",
      "         -3.4995e-01, -2.0231e-01, -4.7444e-01, -7.5289e-02, -4.8484e-01,\n",
      "          2.4525e-01,  2.8355e-01, -4.7662e-01,  2.2617e-01,  4.4863e-01,\n",
      "          2.4027e-01, -1.9471e-01, -2.4254e-01, -1.4281e-01,  4.4090e-01,\n",
      "          1.5124e-01,  3.8078e-01, -7.7263e-02, -2.4661e-01, -3.7154e-01,\n",
      "         -2.3260e-01, -4.9303e-01,  4.7423e-02, -2.7226e-01, -3.4987e-01,\n",
      "          1.2225e-01,  4.7308e-01, -3.5899e-01, -4.5405e-01, -1.4199e-01,\n",
      "         -2.3627e-01,  4.9589e-01]], requires_grad=True)\n",
      "Wq: torch.Size([4, 128])\n",
      "tensor([[-3.2097e-01, -1.5344e-02, -1.1510e-01, -3.0874e-01,  3.3117e-01,\n",
      "         -2.5896e-01,  6.9617e-02,  4.6040e-01, -4.1660e-01,  4.1727e-01,\n",
      "          4.1405e-01, -2.2532e-01,  4.6028e-01,  7.0296e-02, -4.5145e-01,\n",
      "         -4.9349e-01,  8.6677e-02,  2.9951e-01,  3.9258e-01, -4.0547e-01,\n",
      "          4.8854e-01, -4.5012e-01,  2.7701e-01,  3.6282e-01, -2.8834e-01,\n",
      "          5.2543e-02, -4.6092e-01,  1.8264e-01, -4.3262e-01, -4.9215e-01,\n",
      "          4.2615e-01, -3.8198e-01,  1.2274e-01,  2.0562e-01,  3.0310e-01,\n",
      "          2.8698e-01, -8.8241e-02, -1.6075e-01,  2.5161e-01,  4.2444e-01,\n",
      "         -8.2624e-02, -4.0507e-01, -1.2071e-01, -4.2426e-01, -2.8454e-01,\n",
      "          2.0912e-01,  2.4156e-01, -3.9879e-01, -2.2591e-01,  3.3003e-01,\n",
      "          3.2374e-01, -1.6253e-02,  2.8714e-01, -3.8530e-01, -3.8436e-01,\n",
      "         -2.4212e-01,  3.6687e-02, -4.5855e-01,  4.8830e-01,  4.7009e-01,\n",
      "         -1.6423e-01, -3.2546e-01,  4.4032e-01,  3.6316e-01, -8.7626e-02,\n",
      "         -4.4174e-01, -4.6600e-01, -4.8568e-03, -4.8418e-01,  6.4661e-02,\n",
      "          4.8494e-01, -1.6592e-01, -2.0181e-01,  4.4469e-01,  9.5466e-02,\n",
      "          4.8366e-01,  4.4880e-01, -1.9130e-01,  1.5374e-01,  2.2177e-01,\n",
      "          4.9499e-01,  4.8626e-01, -3.4601e-01, -3.4760e-01,  8.6108e-02,\n",
      "          4.1357e-01, -2.4427e-01, -2.5268e-01,  2.0449e-01, -4.9356e-01,\n",
      "         -3.6677e-01, -1.0875e-01,  4.0741e-01, -4.2069e-01, -3.3176e-01,\n",
      "          2.1056e-01, -8.8283e-02, -1.0314e-01,  4.6487e-01, -3.7954e-01,\n",
      "         -2.3864e-01, -4.2001e-01, -1.2691e-01,  1.9158e-01,  1.1451e-01,\n",
      "          1.6561e-01,  1.7231e-01,  1.7429e-01, -1.8123e-01,  3.6120e-01,\n",
      "          3.1331e-01, -9.2029e-02,  3.9661e-01,  8.8137e-02,  1.8981e-01,\n",
      "          1.9692e-02,  3.0192e-01, -2.1089e-01, -3.5772e-02, -4.6871e-01,\n",
      "         -4.8495e-01, -3.9506e-01,  1.8029e-01,  3.0469e-01, -6.6937e-02,\n",
      "         -1.2337e-01, -4.9423e-01,  4.1953e-01],\n",
      "        [-1.4990e-01,  2.7819e-01,  7.4129e-02, -4.4660e-02,  4.3282e-01,\n",
      "         -3.0581e-01,  1.8844e-01,  4.4872e-01, -1.1302e-01,  1.4516e-01,\n",
      "         -4.8995e-05,  4.3458e-01,  5.3082e-02,  4.4512e-01,  1.2523e-01,\n",
      "         -1.1507e-01,  3.1780e-01, -3.1342e-01, -4.4965e-01, -3.3284e-02,\n",
      "          1.5604e-02, -1.6669e-01,  1.8346e-01, -2.4034e-01, -4.7407e-01,\n",
      "         -7.6704e-02,  6.7073e-02,  1.2652e-01, -3.9627e-01,  3.7724e-01,\n",
      "         -1.9206e-01,  3.0627e-01,  4.2555e-02, -3.0911e-01,  4.3753e-01,\n",
      "         -2.9667e-01, -1.4520e-02, -1.0416e-02, -3.2493e-01,  4.7091e-01,\n",
      "          2.3998e-01,  3.4873e-01,  5.5291e-02,  9.5433e-02,  1.9302e-01,\n",
      "          3.8875e-01,  2.3504e-01, -1.4209e-01,  2.1127e-01,  2.3719e-01,\n",
      "          3.7475e-01, -2.1212e-01, -2.0359e-01, -4.3030e-01,  8.8165e-02,\n",
      "         -2.9098e-01,  7.8429e-02, -3.4094e-01,  4.3696e-01,  1.9249e-01,\n",
      "         -4.8272e-01, -3.3708e-01, -2.7591e-01, -2.7684e-01, -3.7848e-01,\n",
      "         -4.9171e-01, -4.6506e-01, -1.9350e-01,  3.7812e-01,  3.6263e-02,\n",
      "         -1.0092e-01, -3.0463e-01, -5.2127e-02, -2.3248e-01, -1.8609e-01,\n",
      "         -4.9422e-01,  4.5411e-01,  2.4784e-01,  1.7444e-01,  2.7136e-01,\n",
      "         -3.0759e-01, -2.0130e-01,  2.6875e-01, -4.1371e-01,  4.6615e-01,\n",
      "          1.8902e-01, -2.4082e-01, -4.6193e-01, -6.2983e-02, -2.6200e-01,\n",
      "          3.7065e-01,  4.9508e-01, -5.5899e-03,  1.6553e-01,  2.9573e-01,\n",
      "         -2.0170e-01, -5.1177e-03,  3.8824e-01, -3.7151e-01, -1.4505e-01,\n",
      "         -3.5091e-01, -4.1308e-01, -4.3970e-01,  2.8640e-01, -7.5188e-02,\n",
      "         -1.1953e-01,  2.8627e-01,  2.9312e-01,  4.1545e-02,  3.4332e-01,\n",
      "          5.6798e-02,  2.4595e-02,  1.1926e-01,  4.9348e-02,  1.0625e-01,\n",
      "          3.0282e-01,  1.3777e-01,  4.6769e-01, -4.9020e-01,  3.9159e-01,\n",
      "          6.4323e-02,  3.1386e-01,  1.8567e-01, -3.6389e-01, -6.6978e-02,\n",
      "          3.4887e-01, -4.4969e-01,  4.6964e-01],\n",
      "        [-3.6494e-01, -1.9819e-01,  1.2391e-01,  1.4987e-01, -4.8445e-01,\n",
      "         -2.4220e-01,  1.5516e-01, -2.8535e-02,  3.2431e-01, -2.2211e-01,\n",
      "          1.5887e-01,  2.8480e-01,  2.1065e-01, -3.2590e-01, -3.0512e-01,\n",
      "         -4.2174e-01, -2.1729e-01, -2.7202e-01, -4.9488e-01,  4.2156e-01,\n",
      "          4.1862e-01, -2.5510e-01, -1.7890e-01, -2.3873e-01, -1.4193e-01,\n",
      "          5.3084e-02,  1.0497e-01,  4.0349e-01, -4.5228e-01,  5.2142e-02,\n",
      "          4.7570e-01, -1.9244e-01, -2.6906e-01,  3.6564e-02,  2.8004e-01,\n",
      "          1.9182e-01, -3.6188e-01,  3.8665e-01,  2.9734e-01,  1.3272e-01,\n",
      "          2.8950e-01, -1.1843e-01,  1.8197e-01,  1.0729e-01, -1.2266e-01,\n",
      "          3.5865e-03, -3.2721e-01,  4.0710e-01,  2.1084e-01, -4.9324e-01,\n",
      "         -4.5889e-01,  4.4462e-01,  1.4339e-01,  6.2129e-02, -3.7913e-01,\n",
      "         -6.1301e-02, -5.8408e-02,  2.0735e-01, -2.0772e-01, -4.4801e-01,\n",
      "          8.8660e-02, -3.2614e-01,  5.8857e-02, -3.6563e-03, -1.2983e-01,\n",
      "          2.8123e-01, -3.0251e-01, -2.0165e-01, -2.1545e-01,  3.0909e-01,\n",
      "         -1.9715e-01, -3.4939e-01, -3.8061e-01,  4.6676e-01,  4.4117e-01,\n",
      "         -1.2104e-01,  1.7660e-01, -2.4349e-02,  2.1922e-01, -4.1842e-01,\n",
      "         -4.8203e-01,  6.6405e-02, -2.5446e-01, -2.6192e-01, -3.6002e-01,\n",
      "          3.5817e-01, -5.3533e-02, -1.7330e-01, -2.6523e-01, -4.1151e-01,\n",
      "          2.4367e-01, -1.5724e-01,  1.0179e-01,  1.8709e-02,  2.6733e-01,\n",
      "         -3.9686e-01,  1.9645e-01,  1.2850e-01, -1.6648e-01, -2.5955e-02,\n",
      "          2.8239e-01, -3.5465e-01, -8.4585e-02, -1.6311e-01,  2.4559e-01,\n",
      "          3.4939e-01,  2.8058e-01,  2.2911e-01, -1.5656e-01, -3.8567e-01,\n",
      "          4.8251e-02,  5.4339e-02,  2.4236e-03,  4.1418e-02,  1.3403e-01,\n",
      "          4.6479e-01, -3.6504e-01, -3.9382e-01,  8.7580e-02,  3.4661e-01,\n",
      "          3.6698e-01,  3.1494e-01,  2.8518e-01,  3.6290e-01, -1.9336e-01,\n",
      "         -3.9900e-01, -3.0189e-01,  4.7716e-01],\n",
      "        [ 3.6166e-01, -2.9039e-01, -1.9532e-01,  2.1463e-01,  3.0336e-01,\n",
      "          3.4968e-01,  1.7080e-01, -3.1044e-01, -2.6154e-01, -1.7996e-01,\n",
      "         -3.9154e-01,  4.5309e-01,  2.6656e-01,  3.2655e-01,  4.4053e-01,\n",
      "         -3.8769e-01,  4.5635e-01, -1.7444e-01,  2.8878e-01,  2.6081e-01,\n",
      "         -3.5004e-01,  1.7178e-01,  4.8178e-01, -2.5152e-01, -1.7255e-01,\n",
      "         -4.2525e-01, -5.4256e-02, -2.4009e-01, -2.3620e-01, -1.2440e-01,\n",
      "          3.0858e-01, -4.4050e-01,  1.7219e-01, -1.5976e-01,  3.5247e-01,\n",
      "          2.9060e-01, -3.8982e-01,  5.3928e-02,  2.4100e-01,  6.9047e-03,\n",
      "          4.1114e-01,  3.0978e-02,  2.3816e-01, -2.6534e-01, -9.1487e-02,\n",
      "         -1.2970e-01, -1.3459e-01,  4.5638e-01,  4.2021e-01, -2.6263e-01,\n",
      "          6.4170e-02,  2.1780e-01, -1.9102e-01, -3.3325e-02, -4.4954e-01,\n",
      "          2.6437e-01, -3.3372e-01,  4.9230e-01, -3.5328e-01, -2.8936e-01,\n",
      "         -1.1743e-01, -1.5401e-02,  3.7282e-01,  1.4672e-01,  7.0350e-02,\n",
      "          3.2993e-01,  2.1661e-01, -1.4682e-01,  4.2327e-01, -2.8551e-01,\n",
      "         -1.3827e-01,  2.2878e-01, -7.2087e-02,  4.5573e-01,  4.1544e-02,\n",
      "          1.9417e-01, -1.5592e-01, -2.4566e-01,  2.5795e-01,  1.7242e-03,\n",
      "          3.9895e-01,  6.4041e-02, -3.9480e-01, -1.9869e-01,  2.9795e-02,\n",
      "          7.1546e-02, -3.7036e-02, -1.2344e-01,  4.1349e-01, -1.3400e-01,\n",
      "          2.2914e-01,  4.0769e-01,  2.6082e-01, -3.1787e-01, -1.5388e-01,\n",
      "          4.6755e-01,  3.8927e-01,  3.0885e-01, -1.5936e-01,  1.8015e-01,\n",
      "         -1.3583e-01, -2.4389e-01, -3.9084e-01,  3.8825e-01, -1.7214e-01,\n",
      "          2.9319e-01,  4.8597e-01,  3.2779e-01,  2.0918e-01,  1.6785e-01,\n",
      "         -4.5960e-01, -4.2303e-01, -2.9061e-01,  4.8491e-01, -1.8013e-01,\n",
      "         -4.4329e-01,  1.1073e-03, -3.2739e-01, -1.9347e-01, -2.4275e-01,\n",
      "          1.8868e-01, -2.4091e-01, -6.2331e-02, -1.4281e-01,  4.4504e-01,\n",
      "         -4.6353e-01,  1.3425e-01, -4.4999e-01]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wk: torch.Size([4, 32])\n",
      "tensor([[-0.1553, -0.3907, -0.1353, -0.1834,  0.3281,  0.3566,  0.0125,  0.3220,\n",
      "         -0.4728,  0.4053, -0.1106,  0.2006, -0.3140,  0.0940, -0.0235, -0.4638,\n",
      "         -0.2144,  0.3576,  0.2637, -0.1129,  0.1342,  0.2356,  0.2626, -0.2528,\n",
      "         -0.4116,  0.2929, -0.3678,  0.0282, -0.4102, -0.2127,  0.1234, -0.4159],\n",
      "        [ 0.0832, -0.0981, -0.4366, -0.3156, -0.4994,  0.2409, -0.0746, -0.0859,\n",
      "         -0.4486,  0.2031,  0.1667, -0.3426, -0.3101,  0.2331,  0.2109,  0.1308,\n",
      "         -0.2913, -0.2992, -0.1622,  0.3290, -0.3057, -0.2059,  0.3593, -0.2645,\n",
      "         -0.1815, -0.4256, -0.2721, -0.3370,  0.0057, -0.0099, -0.2970, -0.4733],\n",
      "        [-0.0573, -0.3957,  0.4792, -0.2970,  0.1637, -0.2703,  0.1753, -0.4840,\n",
      "         -0.2821, -0.1479, -0.0942,  0.0175, -0.2854,  0.3667,  0.4935,  0.2581,\n",
      "          0.4504,  0.4091,  0.3461, -0.2889, -0.1077,  0.2105,  0.0639, -0.1379,\n",
      "          0.3734,  0.2860, -0.4596, -0.2787,  0.2498, -0.3322, -0.3371, -0.3497],\n",
      "        [-0.4004,  0.1394, -0.2371, -0.2160, -0.4566,  0.0272,  0.2345, -0.3621,\n",
      "          0.2887,  0.3888,  0.1242,  0.4618,  0.4490,  0.4210, -0.2269,  0.2358,\n",
      "         -0.1827,  0.2249, -0.3342, -0.4596, -0.4314, -0.4519, -0.2539,  0.2134,\n",
      "          0.0577, -0.0501, -0.4030,  0.1965, -0.2650,  0.3601,  0.1891, -0.2569]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wv: torch.Size([4, 32])\n",
      "tensor([[-0.2297, -0.2896,  0.1119, -0.2086, -0.1285, -0.1827, -0.0931, -0.2413,\n",
      "         -0.4642, -0.1566, -0.2637, -0.0271, -0.1591, -0.2213, -0.1838,  0.3740,\n",
      "         -0.3194,  0.0239, -0.2505, -0.2104, -0.2773, -0.2960,  0.4809, -0.4548,\n",
      "          0.3085,  0.1078,  0.0915, -0.4350, -0.2034,  0.0133, -0.1576,  0.1814],\n",
      "        [-0.0579, -0.2175,  0.2943,  0.3645, -0.0325,  0.2580, -0.4358, -0.0848,\n",
      "          0.0054,  0.3866, -0.4527, -0.2348,  0.3268,  0.0179,  0.0364,  0.2810,\n",
      "          0.4152, -0.2728, -0.1507, -0.3673,  0.0660, -0.1682,  0.0075, -0.4935,\n",
      "          0.3933,  0.1655,  0.3943,  0.3843,  0.0243, -0.3108,  0.3287,  0.4750],\n",
      "        [-0.0682,  0.4588,  0.1733, -0.2411, -0.2824, -0.2881,  0.2563, -0.4974,\n",
      "          0.0817,  0.2760,  0.4715, -0.3550, -0.3838, -0.3307, -0.2122,  0.1530,\n",
      "          0.4780, -0.0180, -0.3594,  0.4703, -0.3168,  0.1275,  0.4504,  0.2846,\n",
      "          0.1298, -0.2702,  0.2788, -0.4750,  0.4229,  0.4523, -0.0758, -0.2783],\n",
      "        [-0.3499, -0.2023, -0.4744, -0.0753, -0.4848,  0.2453,  0.2835, -0.4766,\n",
      "          0.2262,  0.4486,  0.2403, -0.1947, -0.2425, -0.1428,  0.4409,  0.1512,\n",
      "          0.3808, -0.0773, -0.2466, -0.3715, -0.2326, -0.4930,  0.0474, -0.2723,\n",
      "         -0.3499,  0.1222,  0.4731, -0.3590, -0.4541, -0.1420, -0.2363,  0.4959]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wq spliced: torch.Size([4, 128])\n",
      "tensor([[-3.2097e-01, -1.5344e-02, -1.1510e-01, -3.0874e-01,  3.3117e-01,\n",
      "         -2.5896e-01,  6.9617e-02,  4.6040e-01, -4.1660e-01,  4.1727e-01,\n",
      "          4.1405e-01, -2.2532e-01,  4.6028e-01,  7.0296e-02, -4.5145e-01,\n",
      "         -4.9349e-01,  8.6677e-02,  2.9951e-01,  3.9258e-01, -4.0547e-01,\n",
      "          4.8854e-01, -4.5012e-01,  2.7701e-01,  3.6282e-01, -2.8834e-01,\n",
      "          5.2543e-02, -4.6092e-01,  1.8264e-01, -4.3262e-01, -4.9215e-01,\n",
      "          4.2615e-01, -3.8198e-01,  1.2274e-01,  2.0562e-01,  3.0310e-01,\n",
      "          2.8698e-01, -8.8241e-02, -1.6075e-01,  2.5161e-01,  4.2444e-01,\n",
      "         -8.2624e-02, -4.0507e-01, -1.2071e-01, -4.2426e-01, -2.8454e-01,\n",
      "          2.0912e-01,  2.4156e-01, -3.9879e-01, -2.2591e-01,  3.3003e-01,\n",
      "          3.2374e-01, -1.6253e-02,  2.8714e-01, -3.8530e-01, -3.8436e-01,\n",
      "         -2.4212e-01,  3.6687e-02, -4.5855e-01,  4.8830e-01,  4.7009e-01,\n",
      "         -1.6423e-01, -3.2546e-01,  4.4032e-01,  3.6316e-01, -8.7626e-02,\n",
      "         -4.4174e-01, -4.6600e-01, -4.8568e-03, -4.8418e-01,  6.4661e-02,\n",
      "          4.8494e-01, -1.6592e-01, -2.0181e-01,  4.4469e-01,  9.5466e-02,\n",
      "          4.8366e-01,  4.4880e-01, -1.9130e-01,  1.5374e-01,  2.2177e-01,\n",
      "          4.9499e-01,  4.8626e-01, -3.4601e-01, -3.4760e-01,  8.6108e-02,\n",
      "          4.1357e-01, -2.4427e-01, -2.5268e-01,  2.0449e-01, -4.9356e-01,\n",
      "         -3.6677e-01, -1.0875e-01,  4.0741e-01, -4.2069e-01, -3.3176e-01,\n",
      "          2.1056e-01, -8.8283e-02, -1.0314e-01,  4.6487e-01, -3.7954e-01,\n",
      "         -2.3864e-01, -4.2001e-01, -1.2691e-01,  1.9158e-01,  1.1451e-01,\n",
      "          1.6561e-01,  1.7231e-01,  1.7429e-01, -1.8123e-01,  3.6120e-01,\n",
      "          3.1331e-01, -9.2029e-02,  3.9661e-01,  8.8137e-02,  1.8981e-01,\n",
      "          1.9692e-02,  3.0192e-01, -2.1089e-01, -3.5772e-02, -4.6871e-01,\n",
      "         -4.8495e-01, -3.9506e-01,  1.8029e-01,  3.0469e-01, -6.6937e-02,\n",
      "         -1.2337e-01, -4.9423e-01,  4.1953e-01],\n",
      "        [-1.4990e-01,  2.7819e-01,  7.4129e-02, -4.4660e-02,  4.3282e-01,\n",
      "         -3.0581e-01,  1.8844e-01,  4.4872e-01, -1.1302e-01,  1.4516e-01,\n",
      "         -4.8995e-05,  4.3458e-01,  5.3082e-02,  4.4512e-01,  1.2523e-01,\n",
      "         -1.1507e-01,  3.1780e-01, -3.1342e-01, -4.4965e-01, -3.3284e-02,\n",
      "          1.5604e-02, -1.6669e-01,  1.8346e-01, -2.4034e-01, -4.7407e-01,\n",
      "         -7.6704e-02,  6.7073e-02,  1.2652e-01, -3.9627e-01,  3.7724e-01,\n",
      "         -1.9206e-01,  3.0627e-01,  4.2555e-02, -3.0911e-01,  4.3753e-01,\n",
      "         -2.9667e-01, -1.4520e-02, -1.0416e-02, -3.2493e-01,  4.7091e-01,\n",
      "          2.3998e-01,  3.4873e-01,  5.5291e-02,  9.5433e-02,  1.9302e-01,\n",
      "          3.8875e-01,  2.3504e-01, -1.4209e-01,  2.1127e-01,  2.3719e-01,\n",
      "          3.7475e-01, -2.1212e-01, -2.0359e-01, -4.3030e-01,  8.8165e-02,\n",
      "         -2.9098e-01,  7.8429e-02, -3.4094e-01,  4.3696e-01,  1.9249e-01,\n",
      "         -4.8272e-01, -3.3708e-01, -2.7591e-01, -2.7684e-01, -3.7848e-01,\n",
      "         -4.9171e-01, -4.6506e-01, -1.9350e-01,  3.7812e-01,  3.6263e-02,\n",
      "         -1.0092e-01, -3.0463e-01, -5.2127e-02, -2.3248e-01, -1.8609e-01,\n",
      "         -4.9422e-01,  4.5411e-01,  2.4784e-01,  1.7444e-01,  2.7136e-01,\n",
      "         -3.0759e-01, -2.0130e-01,  2.6875e-01, -4.1371e-01,  4.6615e-01,\n",
      "          1.8902e-01, -2.4082e-01, -4.6193e-01, -6.2983e-02, -2.6200e-01,\n",
      "          3.7065e-01,  4.9508e-01, -5.5899e-03,  1.6553e-01,  2.9573e-01,\n",
      "         -2.0170e-01, -5.1177e-03,  3.8824e-01, -3.7151e-01, -1.4505e-01,\n",
      "         -3.5091e-01, -4.1308e-01, -4.3970e-01,  2.8640e-01, -7.5188e-02,\n",
      "         -1.1953e-01,  2.8627e-01,  2.9312e-01,  4.1545e-02,  3.4332e-01,\n",
      "          5.6798e-02,  2.4595e-02,  1.1926e-01,  4.9348e-02,  1.0625e-01,\n",
      "          3.0282e-01,  1.3777e-01,  4.6769e-01, -4.9020e-01,  3.9159e-01,\n",
      "          6.4323e-02,  3.1386e-01,  1.8567e-01, -3.6389e-01, -6.6978e-02,\n",
      "          3.4887e-01, -4.4969e-01,  4.6964e-01],\n",
      "        [-3.6494e-01, -1.9819e-01,  1.2391e-01,  1.4987e-01, -4.8445e-01,\n",
      "         -2.4220e-01,  1.5516e-01, -2.8535e-02,  3.2431e-01, -2.2211e-01,\n",
      "          1.5887e-01,  2.8480e-01,  2.1065e-01, -3.2590e-01, -3.0512e-01,\n",
      "         -4.2174e-01, -2.1729e-01, -2.7202e-01, -4.9488e-01,  4.2156e-01,\n",
      "          4.1862e-01, -2.5510e-01, -1.7890e-01, -2.3873e-01, -1.4193e-01,\n",
      "          5.3084e-02,  1.0497e-01,  4.0349e-01, -4.5228e-01,  5.2142e-02,\n",
      "          4.7570e-01, -1.9244e-01, -2.6906e-01,  3.6564e-02,  2.8004e-01,\n",
      "          1.9182e-01, -3.6188e-01,  3.8665e-01,  2.9734e-01,  1.3272e-01,\n",
      "          2.8950e-01, -1.1843e-01,  1.8197e-01,  1.0729e-01, -1.2266e-01,\n",
      "          3.5865e-03, -3.2721e-01,  4.0710e-01,  2.1084e-01, -4.9324e-01,\n",
      "         -4.5889e-01,  4.4462e-01,  1.4339e-01,  6.2129e-02, -3.7913e-01,\n",
      "         -6.1301e-02, -5.8408e-02,  2.0735e-01, -2.0772e-01, -4.4801e-01,\n",
      "          8.8660e-02, -3.2614e-01,  5.8857e-02, -3.6563e-03, -1.2983e-01,\n",
      "          2.8123e-01, -3.0251e-01, -2.0165e-01, -2.1545e-01,  3.0909e-01,\n",
      "         -1.9715e-01, -3.4939e-01, -3.8061e-01,  4.6676e-01,  4.4117e-01,\n",
      "         -1.2104e-01,  1.7660e-01, -2.4349e-02,  2.1922e-01, -4.1842e-01,\n",
      "         -4.8203e-01,  6.6405e-02, -2.5446e-01, -2.6192e-01, -3.6002e-01,\n",
      "          3.5817e-01, -5.3533e-02, -1.7330e-01, -2.6523e-01, -4.1151e-01,\n",
      "          2.4367e-01, -1.5724e-01,  1.0179e-01,  1.8709e-02,  2.6733e-01,\n",
      "         -3.9686e-01,  1.9645e-01,  1.2850e-01, -1.6648e-01, -2.5955e-02,\n",
      "          2.8239e-01, -3.5465e-01, -8.4585e-02, -1.6311e-01,  2.4559e-01,\n",
      "          3.4939e-01,  2.8058e-01,  2.2911e-01, -1.5656e-01, -3.8567e-01,\n",
      "          4.8251e-02,  5.4339e-02,  2.4236e-03,  4.1418e-02,  1.3403e-01,\n",
      "          4.6479e-01, -3.6504e-01, -3.9382e-01,  8.7580e-02,  3.4661e-01,\n",
      "          3.6698e-01,  3.1494e-01,  2.8518e-01,  3.6290e-01, -1.9336e-01,\n",
      "         -3.9900e-01, -3.0189e-01,  4.7716e-01],\n",
      "        [ 3.6166e-01, -2.9039e-01, -1.9532e-01,  2.1463e-01,  3.0336e-01,\n",
      "          3.4968e-01,  1.7080e-01, -3.1044e-01, -2.6154e-01, -1.7996e-01,\n",
      "         -3.9154e-01,  4.5309e-01,  2.6656e-01,  3.2655e-01,  4.4053e-01,\n",
      "         -3.8769e-01,  4.5635e-01, -1.7444e-01,  2.8878e-01,  2.6081e-01,\n",
      "         -3.5004e-01,  1.7178e-01,  4.8178e-01, -2.5152e-01, -1.7255e-01,\n",
      "         -4.2525e-01, -5.4256e-02, -2.4009e-01, -2.3620e-01, -1.2440e-01,\n",
      "          3.0858e-01, -4.4050e-01,  1.7219e-01, -1.5976e-01,  3.5247e-01,\n",
      "          2.9060e-01, -3.8982e-01,  5.3928e-02,  2.4100e-01,  6.9047e-03,\n",
      "          4.1114e-01,  3.0978e-02,  2.3816e-01, -2.6534e-01, -9.1487e-02,\n",
      "         -1.2970e-01, -1.3459e-01,  4.5638e-01,  4.2021e-01, -2.6263e-01,\n",
      "          6.4170e-02,  2.1780e-01, -1.9102e-01, -3.3325e-02, -4.4954e-01,\n",
      "          2.6437e-01, -3.3372e-01,  4.9230e-01, -3.5328e-01, -2.8936e-01,\n",
      "         -1.1743e-01, -1.5401e-02,  3.7282e-01,  1.4672e-01,  7.0350e-02,\n",
      "          3.2993e-01,  2.1661e-01, -1.4682e-01,  4.2327e-01, -2.8551e-01,\n",
      "         -1.3827e-01,  2.2878e-01, -7.2087e-02,  4.5573e-01,  4.1544e-02,\n",
      "          1.9417e-01, -1.5592e-01, -2.4566e-01,  2.5795e-01,  1.7242e-03,\n",
      "          3.9895e-01,  6.4041e-02, -3.9480e-01, -1.9869e-01,  2.9795e-02,\n",
      "          7.1546e-02, -3.7036e-02, -1.2344e-01,  4.1349e-01, -1.3400e-01,\n",
      "          2.2914e-01,  4.0769e-01,  2.6082e-01, -3.1787e-01, -1.5388e-01,\n",
      "          4.6755e-01,  3.8927e-01,  3.0885e-01, -1.5936e-01,  1.8015e-01,\n",
      "         -1.3583e-01, -2.4389e-01, -3.9084e-01,  3.8825e-01, -1.7214e-01,\n",
      "          2.9319e-01,  4.8597e-01,  3.2779e-01,  2.0918e-01,  1.6785e-01,\n",
      "         -4.5960e-01, -4.2303e-01, -2.9061e-01,  4.8491e-01, -1.8013e-01,\n",
      "         -4.4329e-01,  1.1073e-03, -3.2739e-01, -1.9347e-01, -2.4275e-01,\n",
      "          1.8868e-01, -2.4091e-01, -6.2331e-02, -1.4281e-01,  4.4504e-01,\n",
      "         -4.6353e-01,  1.3425e-01, -4.4999e-01]], grad_fn=<CatBackward0>)\n",
      "Wk spliced: torch.Size([4, 32])\n",
      "tensor([[-0.1553, -0.3907, -0.1353, -0.1834,  0.3281,  0.3566,  0.0125,  0.3220,\n",
      "         -0.4728,  0.4053, -0.1106,  0.2006, -0.3140,  0.0940, -0.0235, -0.4638,\n",
      "         -0.2144,  0.3576,  0.2637, -0.1129,  0.1342,  0.2356,  0.2626, -0.2528,\n",
      "         -0.4116,  0.2929, -0.3678,  0.0282, -0.4102, -0.2127,  0.1234, -0.4159],\n",
      "        [ 0.0832, -0.0981, -0.4366, -0.3156, -0.4994,  0.2409, -0.0746, -0.0859,\n",
      "         -0.4486,  0.2031,  0.1667, -0.3426, -0.3101,  0.2331,  0.2109,  0.1308,\n",
      "         -0.2913, -0.2992, -0.1622,  0.3290, -0.3057, -0.2059,  0.3593, -0.2645,\n",
      "         -0.1815, -0.4256, -0.2721, -0.3370,  0.0057, -0.0099, -0.2970, -0.4733],\n",
      "        [-0.0573, -0.3957,  0.4792, -0.2970,  0.1637, -0.2703,  0.1753, -0.4840,\n",
      "         -0.2821, -0.1479, -0.0942,  0.0175, -0.2854,  0.3667,  0.4935,  0.2581,\n",
      "          0.4504,  0.4091,  0.3461, -0.2889, -0.1077,  0.2105,  0.0639, -0.1379,\n",
      "          0.3734,  0.2860, -0.4596, -0.2787,  0.2498, -0.3322, -0.3371, -0.3497],\n",
      "        [-0.4004,  0.1394, -0.2371, -0.2160, -0.4566,  0.0272,  0.2345, -0.3621,\n",
      "          0.2887,  0.3888,  0.1242,  0.4618,  0.4490,  0.4210, -0.2269,  0.2358,\n",
      "         -0.1827,  0.2249, -0.3342, -0.4596, -0.4314, -0.4519, -0.2539,  0.2134,\n",
      "          0.0577, -0.0501, -0.4030,  0.1965, -0.2650,  0.3601,  0.1891, -0.2569]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Wv spliced: torch.Size([4, 32])\n",
      "tensor([[-0.2297, -0.2896,  0.1119, -0.2086, -0.1285, -0.1827, -0.0931, -0.2413,\n",
      "         -0.4642, -0.1566, -0.2637, -0.0271, -0.1591, -0.2213, -0.1838,  0.3740,\n",
      "         -0.3194,  0.0239, -0.2505, -0.2104, -0.2773, -0.2960,  0.4809, -0.4548,\n",
      "          0.3085,  0.1078,  0.0915, -0.4350, -0.2034,  0.0133, -0.1576,  0.1814],\n",
      "        [-0.0579, -0.2175,  0.2943,  0.3645, -0.0325,  0.2580, -0.4358, -0.0848,\n",
      "          0.0054,  0.3866, -0.4527, -0.2348,  0.3268,  0.0179,  0.0364,  0.2810,\n",
      "          0.4152, -0.2728, -0.1507, -0.3673,  0.0660, -0.1682,  0.0075, -0.4935,\n",
      "          0.3933,  0.1655,  0.3943,  0.3843,  0.0243, -0.3108,  0.3287,  0.4750],\n",
      "        [-0.0682,  0.4588,  0.1733, -0.2411, -0.2824, -0.2881,  0.2563, -0.4974,\n",
      "          0.0817,  0.2760,  0.4715, -0.3550, -0.3838, -0.3307, -0.2122,  0.1530,\n",
      "          0.4780, -0.0180, -0.3594,  0.4703, -0.3168,  0.1275,  0.4504,  0.2846,\n",
      "          0.1298, -0.2702,  0.2788, -0.4750,  0.4229,  0.4523, -0.0758, -0.2783],\n",
      "        [-0.3499, -0.2023, -0.4744, -0.0753, -0.4848,  0.2453,  0.2835, -0.4766,\n",
      "          0.2262,  0.4486,  0.2403, -0.1947, -0.2425, -0.1428,  0.4409,  0.1512,\n",
      "          0.3808, -0.0773, -0.2466, -0.3715, -0.2326, -0.4930,  0.0474, -0.2723,\n",
      "         -0.3499,  0.1222,  0.4731, -0.3590, -0.4541, -0.1420, -0.2363,  0.4959]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Wqkv_spliced: torch.Size([4, 192])\n",
      "tensor([[-3.2097e-01, -1.5344e-02, -1.1510e-01, -3.0874e-01,  3.3117e-01,\n",
      "         -2.5896e-01,  6.9617e-02,  4.6040e-01, -4.1660e-01,  4.1727e-01,\n",
      "          4.1405e-01, -2.2532e-01,  4.6028e-01,  7.0296e-02, -4.5145e-01,\n",
      "         -4.9349e-01,  8.6677e-02,  2.9951e-01,  3.9258e-01, -4.0547e-01,\n",
      "          4.8854e-01, -4.5012e-01,  2.7701e-01,  3.6282e-01, -2.8834e-01,\n",
      "          5.2543e-02, -4.6092e-01,  1.8264e-01, -4.3262e-01, -4.9215e-01,\n",
      "          4.2615e-01, -3.8198e-01,  1.2274e-01,  2.0562e-01,  3.0310e-01,\n",
      "          2.8698e-01, -8.8241e-02, -1.6075e-01,  2.5161e-01,  4.2444e-01,\n",
      "         -8.2624e-02, -4.0507e-01, -1.2071e-01, -4.2426e-01, -2.8454e-01,\n",
      "          2.0912e-01,  2.4156e-01, -3.9879e-01, -2.2591e-01,  3.3003e-01,\n",
      "          3.2374e-01, -1.6253e-02,  2.8714e-01, -3.8530e-01, -3.8436e-01,\n",
      "         -2.4212e-01,  3.6687e-02, -4.5855e-01,  4.8830e-01,  4.7009e-01,\n",
      "         -1.6423e-01, -3.2546e-01,  4.4032e-01,  3.6316e-01, -8.7626e-02,\n",
      "         -4.4174e-01, -4.6600e-01, -4.8568e-03, -4.8418e-01,  6.4661e-02,\n",
      "          4.8494e-01, -1.6592e-01, -2.0181e-01,  4.4469e-01,  9.5466e-02,\n",
      "          4.8366e-01,  4.4880e-01, -1.9130e-01,  1.5374e-01,  2.2177e-01,\n",
      "          4.9499e-01,  4.8626e-01, -3.4601e-01, -3.4760e-01,  8.6108e-02,\n",
      "          4.1357e-01, -2.4427e-01, -2.5268e-01,  2.0449e-01, -4.9356e-01,\n",
      "         -3.6677e-01, -1.0875e-01,  4.0741e-01, -4.2069e-01, -3.3176e-01,\n",
      "          2.1056e-01, -8.8283e-02, -1.0314e-01,  4.6487e-01, -3.7954e-01,\n",
      "         -2.3864e-01, -4.2001e-01, -1.2691e-01,  1.9158e-01,  1.1451e-01,\n",
      "          1.6561e-01,  1.7231e-01,  1.7429e-01, -1.8123e-01,  3.6120e-01,\n",
      "          3.1331e-01, -9.2029e-02,  3.9661e-01,  8.8137e-02,  1.8981e-01,\n",
      "          1.9692e-02,  3.0192e-01, -2.1089e-01, -3.5772e-02, -4.6871e-01,\n",
      "         -4.8495e-01, -3.9506e-01,  1.8029e-01,  3.0469e-01, -6.6937e-02,\n",
      "         -1.2337e-01, -4.9423e-01,  4.1953e-01, -1.5530e-01, -3.9074e-01,\n",
      "         -1.3531e-01, -1.8341e-01,  3.2812e-01,  3.5656e-01,  1.2500e-02,\n",
      "          3.2196e-01, -4.7280e-01,  4.0531e-01, -1.1058e-01,  2.0058e-01,\n",
      "         -3.1400e-01,  9.3957e-02, -2.3539e-02, -4.6383e-01, -2.1440e-01,\n",
      "          3.5762e-01,  2.6374e-01, -1.1291e-01,  1.3419e-01,  2.3561e-01,\n",
      "          2.6260e-01, -2.5277e-01, -4.1158e-01,  2.9288e-01, -3.6784e-01,\n",
      "          2.8182e-02, -4.1021e-01, -2.1270e-01,  1.2341e-01, -4.1593e-01,\n",
      "         -2.2971e-01, -2.8959e-01,  1.1191e-01, -2.0859e-01, -1.2854e-01,\n",
      "         -1.8272e-01, -9.3147e-02, -2.4128e-01, -4.6417e-01, -1.5658e-01,\n",
      "         -2.6369e-01, -2.7113e-02, -1.5909e-01, -2.2127e-01, -1.8380e-01,\n",
      "          3.7399e-01, -3.1944e-01,  2.3862e-02, -2.5048e-01, -2.1036e-01,\n",
      "         -2.7727e-01, -2.9599e-01,  4.8089e-01, -4.5476e-01,  3.0850e-01,\n",
      "          1.0780e-01,  9.1512e-02, -4.3502e-01, -2.0343e-01,  1.3289e-02,\n",
      "         -1.5765e-01,  1.8142e-01],\n",
      "        [-1.4990e-01,  2.7819e-01,  7.4129e-02, -4.4660e-02,  4.3282e-01,\n",
      "         -3.0581e-01,  1.8844e-01,  4.4872e-01, -1.1302e-01,  1.4516e-01,\n",
      "         -4.8995e-05,  4.3458e-01,  5.3082e-02,  4.4512e-01,  1.2523e-01,\n",
      "         -1.1507e-01,  3.1780e-01, -3.1342e-01, -4.4965e-01, -3.3284e-02,\n",
      "          1.5604e-02, -1.6669e-01,  1.8346e-01, -2.4034e-01, -4.7407e-01,\n",
      "         -7.6704e-02,  6.7073e-02,  1.2652e-01, -3.9627e-01,  3.7724e-01,\n",
      "         -1.9206e-01,  3.0627e-01,  4.2555e-02, -3.0911e-01,  4.3753e-01,\n",
      "         -2.9667e-01, -1.4520e-02, -1.0416e-02, -3.2493e-01,  4.7091e-01,\n",
      "          2.3998e-01,  3.4873e-01,  5.5291e-02,  9.5433e-02,  1.9302e-01,\n",
      "          3.8875e-01,  2.3504e-01, -1.4209e-01,  2.1127e-01,  2.3719e-01,\n",
      "          3.7475e-01, -2.1212e-01, -2.0359e-01, -4.3030e-01,  8.8165e-02,\n",
      "         -2.9098e-01,  7.8429e-02, -3.4094e-01,  4.3696e-01,  1.9249e-01,\n",
      "         -4.8272e-01, -3.3708e-01, -2.7591e-01, -2.7684e-01, -3.7848e-01,\n",
      "         -4.9171e-01, -4.6506e-01, -1.9350e-01,  3.7812e-01,  3.6263e-02,\n",
      "         -1.0092e-01, -3.0463e-01, -5.2127e-02, -2.3248e-01, -1.8609e-01,\n",
      "         -4.9422e-01,  4.5411e-01,  2.4784e-01,  1.7444e-01,  2.7136e-01,\n",
      "         -3.0759e-01, -2.0130e-01,  2.6875e-01, -4.1371e-01,  4.6615e-01,\n",
      "          1.8902e-01, -2.4082e-01, -4.6193e-01, -6.2983e-02, -2.6200e-01,\n",
      "          3.7065e-01,  4.9508e-01, -5.5899e-03,  1.6553e-01,  2.9573e-01,\n",
      "         -2.0170e-01, -5.1177e-03,  3.8824e-01, -3.7151e-01, -1.4505e-01,\n",
      "         -3.5091e-01, -4.1308e-01, -4.3970e-01,  2.8640e-01, -7.5188e-02,\n",
      "         -1.1953e-01,  2.8627e-01,  2.9312e-01,  4.1545e-02,  3.4332e-01,\n",
      "          5.6798e-02,  2.4595e-02,  1.1926e-01,  4.9348e-02,  1.0625e-01,\n",
      "          3.0282e-01,  1.3777e-01,  4.6769e-01, -4.9020e-01,  3.9159e-01,\n",
      "          6.4323e-02,  3.1386e-01,  1.8567e-01, -3.6389e-01, -6.6978e-02,\n",
      "          3.4887e-01, -4.4969e-01,  4.6964e-01,  8.3232e-02, -9.8104e-02,\n",
      "         -4.3657e-01, -3.1562e-01, -4.9943e-01,  2.4091e-01, -7.4649e-02,\n",
      "         -8.5931e-02, -4.4864e-01,  2.0309e-01,  1.6673e-01, -3.4258e-01,\n",
      "         -3.1006e-01,  2.3308e-01,  2.1086e-01,  1.3077e-01, -2.9130e-01,\n",
      "         -2.9917e-01, -1.6217e-01,  3.2903e-01, -3.0571e-01, -2.0588e-01,\n",
      "          3.5932e-01, -2.6446e-01, -1.8150e-01, -4.2555e-01, -2.7207e-01,\n",
      "         -3.3697e-01,  5.7231e-03, -9.8906e-03, -2.9698e-01, -4.7332e-01,\n",
      "         -5.7863e-02, -2.1753e-01,  2.9426e-01,  3.6454e-01, -3.2457e-02,\n",
      "          2.5800e-01, -4.3584e-01, -8.4784e-02,  5.4089e-03,  3.8661e-01,\n",
      "         -4.5274e-01, -2.3479e-01,  3.2677e-01,  1.7917e-02,  3.6443e-02,\n",
      "          2.8099e-01,  4.1516e-01, -2.7281e-01, -1.5072e-01, -3.6735e-01,\n",
      "          6.6030e-02, -1.6825e-01,  7.4779e-03, -4.9353e-01,  3.9334e-01,\n",
      "          1.6545e-01,  3.9433e-01,  3.8434e-01,  2.4296e-02, -3.1079e-01,\n",
      "          3.2874e-01,  4.7495e-01],\n",
      "        [-3.6494e-01, -1.9819e-01,  1.2391e-01,  1.4987e-01, -4.8445e-01,\n",
      "         -2.4220e-01,  1.5516e-01, -2.8535e-02,  3.2431e-01, -2.2211e-01,\n",
      "          1.5887e-01,  2.8480e-01,  2.1065e-01, -3.2590e-01, -3.0512e-01,\n",
      "         -4.2174e-01, -2.1729e-01, -2.7202e-01, -4.9488e-01,  4.2156e-01,\n",
      "          4.1862e-01, -2.5510e-01, -1.7890e-01, -2.3873e-01, -1.4193e-01,\n",
      "          5.3084e-02,  1.0497e-01,  4.0349e-01, -4.5228e-01,  5.2142e-02,\n",
      "          4.7570e-01, -1.9244e-01, -2.6906e-01,  3.6564e-02,  2.8004e-01,\n",
      "          1.9182e-01, -3.6188e-01,  3.8665e-01,  2.9734e-01,  1.3272e-01,\n",
      "          2.8950e-01, -1.1843e-01,  1.8197e-01,  1.0729e-01, -1.2266e-01,\n",
      "          3.5865e-03, -3.2721e-01,  4.0710e-01,  2.1084e-01, -4.9324e-01,\n",
      "         -4.5889e-01,  4.4462e-01,  1.4339e-01,  6.2129e-02, -3.7913e-01,\n",
      "         -6.1301e-02, -5.8408e-02,  2.0735e-01, -2.0772e-01, -4.4801e-01,\n",
      "          8.8660e-02, -3.2614e-01,  5.8857e-02, -3.6563e-03, -1.2983e-01,\n",
      "          2.8123e-01, -3.0251e-01, -2.0165e-01, -2.1545e-01,  3.0909e-01,\n",
      "         -1.9715e-01, -3.4939e-01, -3.8061e-01,  4.6676e-01,  4.4117e-01,\n",
      "         -1.2104e-01,  1.7660e-01, -2.4349e-02,  2.1922e-01, -4.1842e-01,\n",
      "         -4.8203e-01,  6.6405e-02, -2.5446e-01, -2.6192e-01, -3.6002e-01,\n",
      "          3.5817e-01, -5.3533e-02, -1.7330e-01, -2.6523e-01, -4.1151e-01,\n",
      "          2.4367e-01, -1.5724e-01,  1.0179e-01,  1.8709e-02,  2.6733e-01,\n",
      "         -3.9686e-01,  1.9645e-01,  1.2850e-01, -1.6648e-01, -2.5955e-02,\n",
      "          2.8239e-01, -3.5465e-01, -8.4585e-02, -1.6311e-01,  2.4559e-01,\n",
      "          3.4939e-01,  2.8058e-01,  2.2911e-01, -1.5656e-01, -3.8567e-01,\n",
      "          4.8251e-02,  5.4339e-02,  2.4236e-03,  4.1418e-02,  1.3403e-01,\n",
      "          4.6479e-01, -3.6504e-01, -3.9382e-01,  8.7580e-02,  3.4661e-01,\n",
      "          3.6698e-01,  3.1494e-01,  2.8518e-01,  3.6290e-01, -1.9336e-01,\n",
      "         -3.9900e-01, -3.0189e-01,  4.7716e-01, -5.7343e-02, -3.9568e-01,\n",
      "          4.7925e-01, -2.9701e-01,  1.6367e-01, -2.7030e-01,  1.7530e-01,\n",
      "         -4.8403e-01, -2.8215e-01, -1.4787e-01, -9.4221e-02,  1.7540e-02,\n",
      "         -2.8544e-01,  3.6671e-01,  4.9354e-01,  2.5809e-01,  4.5043e-01,\n",
      "          4.0915e-01,  3.4615e-01, -2.8886e-01, -1.0766e-01,  2.1052e-01,\n",
      "          6.3936e-02, -1.3789e-01,  3.7345e-01,  2.8598e-01, -4.5961e-01,\n",
      "         -2.7873e-01,  2.4984e-01, -3.3217e-01, -3.3710e-01, -3.4967e-01,\n",
      "         -6.8175e-02,  4.5880e-01,  1.7332e-01, -2.4107e-01, -2.8240e-01,\n",
      "         -2.8811e-01,  2.5626e-01, -4.9741e-01,  8.1723e-02,  2.7604e-01,\n",
      "          4.7153e-01, -3.5504e-01, -3.8385e-01, -3.3069e-01, -2.1225e-01,\n",
      "          1.5297e-01,  4.7800e-01, -1.8034e-02, -3.5942e-01,  4.7031e-01,\n",
      "         -3.1677e-01,  1.2753e-01,  4.5037e-01,  2.8465e-01,  1.2980e-01,\n",
      "         -2.7024e-01,  2.7878e-01, -4.7501e-01,  4.2285e-01,  4.5226e-01,\n",
      "         -7.5834e-02, -2.7833e-01],\n",
      "        [ 3.6166e-01, -2.9039e-01, -1.9532e-01,  2.1463e-01,  3.0336e-01,\n",
      "          3.4968e-01,  1.7080e-01, -3.1044e-01, -2.6154e-01, -1.7996e-01,\n",
      "         -3.9154e-01,  4.5309e-01,  2.6656e-01,  3.2655e-01,  4.4053e-01,\n",
      "         -3.8769e-01,  4.5635e-01, -1.7444e-01,  2.8878e-01,  2.6081e-01,\n",
      "         -3.5004e-01,  1.7178e-01,  4.8178e-01, -2.5152e-01, -1.7255e-01,\n",
      "         -4.2525e-01, -5.4256e-02, -2.4009e-01, -2.3620e-01, -1.2440e-01,\n",
      "          3.0858e-01, -4.4050e-01,  1.7219e-01, -1.5976e-01,  3.5247e-01,\n",
      "          2.9060e-01, -3.8982e-01,  5.3928e-02,  2.4100e-01,  6.9047e-03,\n",
      "          4.1114e-01,  3.0978e-02,  2.3816e-01, -2.6534e-01, -9.1487e-02,\n",
      "         -1.2970e-01, -1.3459e-01,  4.5638e-01,  4.2021e-01, -2.6263e-01,\n",
      "          6.4170e-02,  2.1780e-01, -1.9102e-01, -3.3325e-02, -4.4954e-01,\n",
      "          2.6437e-01, -3.3372e-01,  4.9230e-01, -3.5328e-01, -2.8936e-01,\n",
      "         -1.1743e-01, -1.5401e-02,  3.7282e-01,  1.4672e-01,  7.0350e-02,\n",
      "          3.2993e-01,  2.1661e-01, -1.4682e-01,  4.2327e-01, -2.8551e-01,\n",
      "         -1.3827e-01,  2.2878e-01, -7.2087e-02,  4.5573e-01,  4.1544e-02,\n",
      "          1.9417e-01, -1.5592e-01, -2.4566e-01,  2.5795e-01,  1.7242e-03,\n",
      "          3.9895e-01,  6.4041e-02, -3.9480e-01, -1.9869e-01,  2.9795e-02,\n",
      "          7.1546e-02, -3.7036e-02, -1.2344e-01,  4.1349e-01, -1.3400e-01,\n",
      "          2.2914e-01,  4.0769e-01,  2.6082e-01, -3.1787e-01, -1.5388e-01,\n",
      "          4.6755e-01,  3.8927e-01,  3.0885e-01, -1.5936e-01,  1.8015e-01,\n",
      "         -1.3583e-01, -2.4389e-01, -3.9084e-01,  3.8825e-01, -1.7214e-01,\n",
      "          2.9319e-01,  4.8597e-01,  3.2779e-01,  2.0918e-01,  1.6785e-01,\n",
      "         -4.5960e-01, -4.2303e-01, -2.9061e-01,  4.8491e-01, -1.8013e-01,\n",
      "         -4.4329e-01,  1.1073e-03, -3.2739e-01, -1.9347e-01, -2.4275e-01,\n",
      "          1.8868e-01, -2.4091e-01, -6.2331e-02, -1.4281e-01,  4.4504e-01,\n",
      "         -4.6353e-01,  1.3425e-01, -4.4999e-01, -4.0042e-01,  1.3937e-01,\n",
      "         -2.3713e-01, -2.1600e-01, -4.5663e-01,  2.7168e-02,  2.3453e-01,\n",
      "         -3.6210e-01,  2.8875e-01,  3.8881e-01,  1.2422e-01,  4.6177e-01,\n",
      "          4.4899e-01,  4.2099e-01, -2.2689e-01,  2.3584e-01, -1.8267e-01,\n",
      "          2.2487e-01, -3.3422e-01, -4.5962e-01, -4.3136e-01, -4.5188e-01,\n",
      "         -2.5392e-01,  2.1336e-01,  5.7722e-02, -5.0130e-02, -4.0297e-01,\n",
      "          1.9646e-01, -2.6501e-01,  3.6008e-01,  1.8912e-01, -2.5685e-01,\n",
      "         -3.4995e-01, -2.0231e-01, -4.7444e-01, -7.5289e-02, -4.8484e-01,\n",
      "          2.4525e-01,  2.8355e-01, -4.7662e-01,  2.2617e-01,  4.4863e-01,\n",
      "          2.4027e-01, -1.9471e-01, -2.4254e-01, -1.4281e-01,  4.4090e-01,\n",
      "          1.5124e-01,  3.8078e-01, -7.7263e-02, -2.4661e-01, -3.7154e-01,\n",
      "         -2.3260e-01, -4.9303e-01,  4.7423e-02, -2.7226e-01, -3.4987e-01,\n",
      "          1.2225e-01,  4.7308e-01, -3.5899e-01, -4.5405e-01, -1.4199e-01,\n",
      "         -2.3627e-01,  4.9589e-01]], grad_fn=<CatBackward0>)\n",
      "xqkv: torch.Size([1, 3, 192])\n",
      "tensor([[[ 2.1219e-02,  7.6282e-01,  1.3464e-01, -4.7232e-01,  3.3603e-01,\n",
      "          -1.8964e-01, -4.1116e-01,  5.2642e-01, -6.2913e-02,  5.3815e-01,\n",
      "           2.5264e-01, -8.6697e-01, -7.2154e-01,  1.2491e-01, -7.2481e-02,\n",
      "           1.1736e+00, -2.4659e-01,  4.8723e-01,  1.0736e-01, -9.0042e-01,\n",
      "          -1.7768e-01,  1.4420e-01, -4.0415e-01,  5.5801e-01,  3.4688e-01,\n",
      "           4.8255e-01,  1.8343e-02, -2.2863e-01,  9.1685e-01,  2.8378e-01,\n",
      "          -1.2181e+00,  1.0274e+00,  1.3413e-01,  4.8223e-02, -8.0284e-01,\n",
      "          -8.0184e-01,  1.0579e+00, -5.9700e-01, -8.8453e-01, -1.2718e-01,\n",
      "          -8.9388e-01,  2.8754e-01, -5.5062e-01,  3.0917e-01,  3.9910e-01,\n",
      "           2.5340e-01,  6.7676e-01, -1.1826e+00, -7.8154e-01,  1.0743e+00,\n",
      "           6.1498e-01, -9.8566e-01, -4.1393e-02, -1.0516e-01,  1.2405e+00,\n",
      "          -3.2490e-01,  5.6022e-01, -9.9883e-01,  8.3025e-01,  1.0129e+00,\n",
      "          -7.5889e-02,  4.3441e-01, -7.4613e-01, -3.3389e-01, -9.5903e-03,\n",
      "          -9.2453e-01,  6.5660e-02,  4.3196e-01, -9.7053e-02, -4.0486e-02,\n",
      "           3.6402e-01,  1.1421e-01,  6.5216e-01, -1.4234e+00, -7.4720e-01,\n",
      "          -3.1548e-01,  2.7357e-02,  4.7489e-01, -6.3897e-01,  6.3042e-01,\n",
      "          -3.8073e-02, -3.1581e-01,  1.0352e+00,  5.7862e-01,  5.8660e-01,\n",
      "          -6.1344e-01,  9.5387e-02,  3.2114e-01, -2.4780e-01,  7.6635e-01,\n",
      "          -4.9593e-01, -1.8243e-01, -5.6891e-01,  5.2612e-01, -2.7042e-02,\n",
      "          -1.7797e-01, -8.0226e-01, -4.7991e-01,  2.7524e-01, -1.9489e-01,\n",
      "          -2.7379e-01,  7.8316e-01,  5.5244e-01, -2.5420e-01, -1.4755e-01,\n",
      "          -9.5745e-01, -1.0112e+00, -7.1845e-01, -2.8290e-02,  3.5370e-01,\n",
      "           5.3454e-01,  5.2941e-01,  3.7057e-01, -7.2791e-01,  6.1079e-02,\n",
      "           4.1954e-02,  5.0515e-01,  1.1745e+00,  8.5027e-03,  3.3048e-02,\n",
      "          -6.8340e-01,  4.1248e-02, -2.9024e-01, -4.6604e-01, -3.5143e-01,\n",
      "           1.3221e+00,  1.8741e-01,  1.9402e-02,  6.8222e-01,  3.9745e-01,\n",
      "          -4.5185e-01,  6.5456e-01,  2.0551e-01,  3.5685e-01, -5.9438e-01,\n",
      "           1.1071e+00, -5.6912e-02, -3.3377e-01,  2.6295e-02, -7.9261e-01,\n",
      "          -2.5859e-01, -1.0458e+00, -3.1726e-01, -5.7937e-01, -4.3237e-01,\n",
      "          -1.0288e+00, -1.1353e-01,  1.1542e+00,  6.3875e-01,  2.3187e-01,\n",
      "           3.2322e-01, -1.3726e-01, -5.9374e-01, -5.0166e-01,  1.1832e+00,\n",
      "           1.9762e-02,  7.9928e-02,  6.4459e-04,  1.0876e-01,  7.7608e-01,\n",
      "           5.9874e-01, -3.8406e-01,  4.7865e-01,  5.8100e-01,  1.0784e+00,\n",
      "           1.6966e-01, -8.6358e-01,  1.3724e+00, -3.5363e-01, -8.7299e-01,\n",
      "          -1.0852e+00,  7.0575e-01,  9.9460e-01,  7.0276e-01, -2.7110e-01,\n",
      "          -4.0191e-01, -1.0301e+00,  4.9934e-02,  8.4266e-01, -2.2191e-01,\n",
      "           8.2980e-01,  4.9940e-01, -7.7271e-01, -9.5784e-02,  3.6566e-01,\n",
      "           2.4249e-01, -9.4712e-01,  1.3435e+00,  7.0548e-02, -5.3228e-01,\n",
      "           5.5246e-01, -1.8482e-01],\n",
      "         [ 1.1658e+00, -4.9897e-01, -3.7768e-01,  4.2935e-01,  2.3951e-01,\n",
      "           1.1467e+00, -9.0228e-02, -1.1253e+00, -2.7703e-01, -4.7917e-01,\n",
      "          -9.6767e-01,  1.8677e-01, -1.2756e-01,  3.0114e-01,  1.0714e+00,\n",
      "           1.9994e-01,  5.0430e-01,  5.3442e-02,  9.7198e-01,  3.2700e-01,\n",
      "          -1.1813e+00,  8.9743e-01,  5.0673e-01, -1.9196e-01,  4.7197e-01,\n",
      "          -6.2904e-01,  7.2798e-02, -9.0999e-01,  6.5658e-01, -2.4201e-01,\n",
      "          -4.2789e-02, -5.1101e-01,  3.5383e-01, -1.2052e-01, -2.9950e-01,\n",
      "           3.4572e-01, -2.0287e-01, -1.2429e-01,  2.3515e-01, -7.9092e-01,\n",
      "           2.0279e-01,  9.3051e-02,  2.2726e-01, -2.8571e-01, -2.1303e-02,\n",
      "          -6.7265e-01, -2.9375e-01,  7.1640e-01,  3.9823e-01, -4.0378e-01,\n",
      "          -7.3653e-02,  1.5345e-01, -3.9979e-01,  5.3187e-01, -1.7619e-01,\n",
      "           8.5047e-01, -5.3225e-01,  1.1474e+00, -1.0489e+00, -5.3028e-01,\n",
      "           2.9159e-01,  7.5369e-01,  4.5685e-01,  2.2885e-01,  6.0175e-01,\n",
      "           9.7350e-01,  1.2771e+00,  1.2590e-01,  7.6914e-01, -7.4273e-01,\n",
      "          -2.6357e-01,  9.9691e-01,  3.8237e-01,  2.0305e-01, -1.9592e-01,\n",
      "           5.0954e-01, -1.0649e+00, -4.3528e-01, -5.7308e-02, -4.0033e-02,\n",
      "           9.2971e-01, -9.5504e-02, -3.8355e-01,  5.1643e-01, -1.3255e-01,\n",
      "          -6.2315e-01,  3.6147e-01,  5.3540e-01,  7.4365e-01,  6.9216e-01,\n",
      "           4.1953e-02,  3.5270e-01,  3.9286e-02, -3.5458e-01, -4.9175e-01,\n",
      "           1.0494e+00,  4.6855e-01,  6.6562e-02, -6.5778e-02,  6.5767e-01,\n",
      "           3.7274e-02,  5.7242e-01, -2.7816e-02,  3.2101e-01, -4.5860e-01,\n",
      "           1.4152e-01,  1.1358e-01, -8.2356e-02,  5.1283e-01,  2.1487e-02,\n",
      "          -9.6209e-01, -6.2366e-01, -7.8756e-01,  5.7241e-01, -5.8904e-01,\n",
      "          -1.3080e+00, -1.7440e-02, -4.3551e-01,  1.0541e-01, -6.8301e-01,\n",
      "           2.3090e-01, -6.3282e-01, -6.0527e-01, -3.7759e-01,  9.0956e-01,\n",
      "          -5.8053e-01,  1.1611e+00, -1.7342e+00, -5.1080e-01,  8.6573e-01,\n",
      "          -2.6170e-01,  3.2671e-01, -5.6693e-01, -1.8381e-01,  2.5705e-01,\n",
      "          -2.6375e-01,  1.3554e+00,  2.4563e-01,  1.8112e-01,  8.3420e-01,\n",
      "           1.3661e+00,  4.6318e-02, -9.0632e-01,  3.1588e-01, -2.3695e-01,\n",
      "           2.8992e-02, -7.9596e-01, -6.5391e-01, -3.5563e-01, -7.9968e-01,\n",
      "          -9.1137e-01,  8.2195e-01,  2.0596e-01, -1.1716e-01,  2.6718e-01,\n",
      "           7.9564e-01, -3.3097e-01,  9.4233e-01,  7.3562e-01,  6.0040e-01,\n",
      "          -2.5488e-01, -2.8951e-01, -1.1672e+00, -1.0249e-01, -3.6440e-01,\n",
      "           4.8135e-01,  6.5175e-01, -5.7510e-02,  5.5757e-01,  1.8574e-01,\n",
      "           5.3783e-01,  2.3246e-01, -2.2803e-01,  1.8872e-01,  9.0235e-01,\n",
      "          -3.9559e-01,  1.4931e-03,  1.2931e-01,  2.2968e-01, -4.6352e-01,\n",
      "           3.9774e-02, -4.8245e-01, -6.1561e-01,  1.0276e-01, -1.1649e+00,\n",
      "           1.8233e-01,  5.2265e-02, -1.9655e-01, -8.9782e-01, -3.0866e-01,\n",
      "          -4.7320e-01,  4.1095e-01],\n",
      "         [ 4.5295e-01, -7.1447e-01, -1.4924e-01,  6.8292e-01, -2.4553e-01,\n",
      "           5.4726e-01,  3.5411e-01, -8.8838e-01,  1.4529e-01, -7.7775e-01,\n",
      "          -7.2563e-01,  1.0638e+00,  3.6817e-01,  9.3008e-02,  6.6122e-01,\n",
      "          -7.4051e-01,  4.2689e-01, -6.6001e-01, -1.4211e-01,  1.0751e+00,\n",
      "          -4.5140e-01,  3.2481e-01,  4.2327e-01, -8.0481e-01, -1.6321e-01,\n",
      "          -6.6176e-01,  2.6314e-01, -1.3232e-01, -5.0121e-01,  5.3984e-02,\n",
      "           7.6150e-01, -7.5114e-01, -5.6592e-02, -2.8400e-01,  5.9117e-01,\n",
      "           5.5690e-01, -9.3741e-01,  5.5724e-01,  6.0350e-01, -1.9487e-01,\n",
      "           9.5589e-01,  9.9531e-02,  6.2559e-01, -1.0766e-01, -1.4410e-01,\n",
      "          -4.0510e-01, -7.2220e-01,  1.3997e+00,  9.8210e-01, -1.1443e+00,\n",
      "          -5.9784e-01,  8.3928e-01, -2.9904e-01,  3.1001e-01, -9.0291e-01,\n",
      "           5.7000e-01, -6.4082e-01,  1.3381e+00, -1.1460e+00, -1.2158e+00,\n",
      "           8.1467e-02, -8.9317e-02,  4.7127e-01,  8.3781e-02,  1.1381e-01,\n",
      "           1.1630e+00,  4.1989e-01, -3.9586e-01,  6.8905e-01, -2.1325e-01,\n",
      "          -6.7567e-01,  1.9144e-01, -3.6162e-01,  9.9172e-01,  4.7758e-01,\n",
      "           2.1522e-02, -4.3030e-01, -3.6597e-01,  5.1392e-01, -5.8260e-01,\n",
      "          -3.5124e-02, -6.9227e-02, -7.4946e-01, -3.0034e-01, -4.4043e-01,\n",
      "           1.9110e-01,  7.4108e-02, -1.3576e-01,  3.1716e-01, -2.8532e-01,\n",
      "           7.4962e-01,  4.8230e-01,  2.9494e-01, -2.9551e-01,  1.3836e-01,\n",
      "           3.0200e-01,  8.8097e-01,  6.1464e-01, -6.1618e-01,  5.1604e-01,\n",
      "           2.5580e-01, -4.2314e-01, -5.6506e-01,  3.1374e-01, -9.5378e-02,\n",
      "           7.4865e-01,  9.1525e-01,  6.0320e-01,  2.8721e-01, -3.7193e-01,\n",
      "          -8.9807e-01, -5.9402e-01, -7.2491e-01,  7.7619e-01, -2.9516e-01,\n",
      "          -3.4780e-01, -5.5159e-01, -8.9003e-01, -1.1642e-01,  1.2886e-01,\n",
      "           9.3021e-01,  7.4275e-02,  3.4636e-02,  1.4171e-02,  5.9464e-01,\n",
      "          -1.1454e+00,  2.9842e-01, -6.0810e-01, -6.4043e-01,  8.7984e-02,\n",
      "           2.3794e-01, -4.7585e-01, -6.8102e-01, -4.6840e-01,  5.6239e-01,\n",
      "          -1.2309e+00,  5.5874e-01,  2.2350e-01,  1.4325e-01,  7.2740e-01,\n",
      "           7.0071e-01,  9.4725e-01,  7.8050e-02,  8.7701e-01,  3.1623e-01,\n",
      "           6.1971e-01, -3.3235e-01, -1.0343e+00, -8.2948e-01, -6.3232e-01,\n",
      "          -5.7536e-01,  4.1292e-01,  7.2758e-01,  1.1060e-01, -8.4345e-01,\n",
      "           1.0218e-01,  4.0546e-02,  3.9281e-01, -2.8722e-02, -4.3014e-01,\n",
      "          -4.9791e-01,  3.2068e-01, -7.3248e-01, -3.0925e-01, -9.8983e-01,\n",
      "           1.7745e-01,  8.5239e-01, -1.1100e+00,  7.1492e-01,  1.0180e+00,\n",
      "           1.0908e+00, -6.0233e-01, -7.4314e-01, -4.3193e-01,  6.1612e-01,\n",
      "           1.2771e-01,  1.1899e+00, -1.0462e-01, -5.8048e-01,  3.7981e-02,\n",
      "          -5.4322e-01, -4.8366e-01,  2.3786e-01,  1.8550e-01, -7.0196e-01,\n",
      "          -1.5522e-01,  9.1741e-01, -8.7612e-01, -2.2403e-01,  2.5839e-01,\n",
      "          -4.3553e-01,  3.4773e-01]]], grad_fn=<UnsafeViewBackward0>)\n",
      "xq: torch.Size([1, 3, 128])\n",
      "tensor([[[ 0.0212,  0.7628,  0.1346, -0.4723,  0.3360, -0.1896, -0.4112,\n",
      "           0.5264, -0.0629,  0.5382,  0.2526, -0.8670, -0.7215,  0.1249,\n",
      "          -0.0725,  1.1736, -0.2466,  0.4872,  0.1074, -0.9004, -0.1777,\n",
      "           0.1442, -0.4041,  0.5580,  0.3469,  0.4825,  0.0183, -0.2286,\n",
      "           0.9168,  0.2838, -1.2181,  1.0274,  0.1341,  0.0482, -0.8028,\n",
      "          -0.8018,  1.0579, -0.5970, -0.8845, -0.1272, -0.8939,  0.2875,\n",
      "          -0.5506,  0.3092,  0.3991,  0.2534,  0.6768, -1.1826, -0.7815,\n",
      "           1.0743,  0.6150, -0.9857, -0.0414, -0.1052,  1.2405, -0.3249,\n",
      "           0.5602, -0.9988,  0.8302,  1.0129, -0.0759,  0.4344, -0.7461,\n",
      "          -0.3339, -0.0096, -0.9245,  0.0657,  0.4320, -0.0971, -0.0405,\n",
      "           0.3640,  0.1142,  0.6522, -1.4234, -0.7472, -0.3155,  0.0274,\n",
      "           0.4749, -0.6390,  0.6304, -0.0381, -0.3158,  1.0352,  0.5786,\n",
      "           0.5866, -0.6134,  0.0954,  0.3211, -0.2478,  0.7664, -0.4959,\n",
      "          -0.1824, -0.5689,  0.5261, -0.0270, -0.1780, -0.8023, -0.4799,\n",
      "           0.2752, -0.1949, -0.2738,  0.7832,  0.5524, -0.2542, -0.1475,\n",
      "          -0.9574, -1.0112, -0.7184, -0.0283,  0.3537,  0.5345,  0.5294,\n",
      "           0.3706, -0.7279,  0.0611,  0.0420,  0.5051,  1.1745,  0.0085,\n",
      "           0.0330, -0.6834,  0.0412, -0.2902, -0.4660, -0.3514,  1.3221,\n",
      "           0.1874,  0.0194],\n",
      "         [ 1.1658, -0.4990, -0.3777,  0.4294,  0.2395,  1.1467, -0.0902,\n",
      "          -1.1253, -0.2770, -0.4792, -0.9677,  0.1868, -0.1276,  0.3011,\n",
      "           1.0714,  0.1999,  0.5043,  0.0534,  0.9720,  0.3270, -1.1813,\n",
      "           0.8974,  0.5067, -0.1920,  0.4720, -0.6290,  0.0728, -0.9100,\n",
      "           0.6566, -0.2420, -0.0428, -0.5110,  0.3538, -0.1205, -0.2995,\n",
      "           0.3457, -0.2029, -0.1243,  0.2351, -0.7909,  0.2028,  0.0931,\n",
      "           0.2273, -0.2857, -0.0213, -0.6727, -0.2938,  0.7164,  0.3982,\n",
      "          -0.4038, -0.0737,  0.1535, -0.3998,  0.5319, -0.1762,  0.8505,\n",
      "          -0.5323,  1.1474, -1.0489, -0.5303,  0.2916,  0.7537,  0.4568,\n",
      "           0.2288,  0.6017,  0.9735,  1.2771,  0.1259,  0.7691, -0.7427,\n",
      "          -0.2636,  0.9969,  0.3824,  0.2030, -0.1959,  0.5095, -1.0649,\n",
      "          -0.4353, -0.0573, -0.0400,  0.9297, -0.0955, -0.3835,  0.5164,\n",
      "          -0.1325, -0.6232,  0.3615,  0.5354,  0.7436,  0.6922,  0.0420,\n",
      "           0.3527,  0.0393, -0.3546, -0.4917,  1.0494,  0.4685,  0.0666,\n",
      "          -0.0658,  0.6577,  0.0373,  0.5724, -0.0278,  0.3210, -0.4586,\n",
      "           0.1415,  0.1136, -0.0824,  0.5128,  0.0215, -0.9621, -0.6237,\n",
      "          -0.7876,  0.5724, -0.5890, -1.3080, -0.0174, -0.4355,  0.1054,\n",
      "          -0.6830,  0.2309, -0.6328, -0.6053, -0.3776,  0.9096, -0.5805,\n",
      "           1.1611, -1.7342],\n",
      "         [ 0.4529, -0.7145, -0.1492,  0.6829, -0.2455,  0.5473,  0.3541,\n",
      "          -0.8884,  0.1453, -0.7777, -0.7256,  1.0638,  0.3682,  0.0930,\n",
      "           0.6612, -0.7405,  0.4269, -0.6600, -0.1421,  1.0751, -0.4514,\n",
      "           0.3248,  0.4233, -0.8048, -0.1632, -0.6618,  0.2631, -0.1323,\n",
      "          -0.5012,  0.0540,  0.7615, -0.7511, -0.0566, -0.2840,  0.5912,\n",
      "           0.5569, -0.9374,  0.5572,  0.6035, -0.1949,  0.9559,  0.0995,\n",
      "           0.6256, -0.1077, -0.1441, -0.4051, -0.7222,  1.3997,  0.9821,\n",
      "          -1.1443, -0.5978,  0.8393, -0.2990,  0.3100, -0.9029,  0.5700,\n",
      "          -0.6408,  1.3381, -1.1460, -1.2158,  0.0815, -0.0893,  0.4713,\n",
      "           0.0838,  0.1138,  1.1630,  0.4199, -0.3959,  0.6890, -0.2132,\n",
      "          -0.6757,  0.1914, -0.3616,  0.9917,  0.4776,  0.0215, -0.4303,\n",
      "          -0.3660,  0.5139, -0.5826, -0.0351, -0.0692, -0.7495, -0.3003,\n",
      "          -0.4404,  0.1911,  0.0741, -0.1358,  0.3172, -0.2853,  0.7496,\n",
      "           0.4823,  0.2949, -0.2955,  0.1384,  0.3020,  0.8810,  0.6146,\n",
      "          -0.6162,  0.5160,  0.2558, -0.4231, -0.5651,  0.3137, -0.0954,\n",
      "           0.7486,  0.9153,  0.6032,  0.2872, -0.3719, -0.8981, -0.5940,\n",
      "          -0.7249,  0.7762, -0.2952, -0.3478, -0.5516, -0.8900, -0.1164,\n",
      "           0.1289,  0.9302,  0.0743,  0.0346,  0.0142,  0.5946, -1.1454,\n",
      "           0.2984, -0.6081]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xk: torch.Size([1, 3, 32])\n",
      "tensor([[[ 6.8222e-01,  3.9745e-01, -4.5185e-01,  6.5456e-01,  2.0551e-01,\n",
      "           3.5685e-01, -5.9438e-01,  1.1071e+00, -5.6912e-02, -3.3377e-01,\n",
      "           2.6295e-02, -7.9261e-01, -2.5859e-01, -1.0458e+00, -3.1726e-01,\n",
      "          -5.7937e-01, -4.3237e-01, -1.0288e+00, -1.1353e-01,  1.1542e+00,\n",
      "           6.3875e-01,  2.3187e-01,  3.2322e-01, -1.3726e-01, -5.9374e-01,\n",
      "          -5.0166e-01,  1.1832e+00,  1.9762e-02,  7.9928e-02,  6.4459e-04,\n",
      "           1.0876e-01,  7.7608e-01],\n",
      "         [-5.1080e-01,  8.6573e-01, -2.6170e-01,  3.2671e-01, -5.6693e-01,\n",
      "          -1.8381e-01,  2.5705e-01, -2.6375e-01,  1.3554e+00,  2.4563e-01,\n",
      "           1.8112e-01,  8.3420e-01,  1.3661e+00,  4.6318e-02, -9.0632e-01,\n",
      "           3.1588e-01, -2.3695e-01,  2.8992e-02, -7.9596e-01, -6.5391e-01,\n",
      "          -3.5563e-01, -7.9968e-01, -9.1137e-01,  8.2195e-01,  2.0596e-01,\n",
      "          -1.1716e-01,  2.6718e-01,  7.9564e-01, -3.3097e-01,  9.4233e-01,\n",
      "           7.3562e-01,  6.0040e-01],\n",
      "         [-6.4043e-01,  8.7984e-02,  2.3794e-01, -4.7585e-01, -6.8102e-01,\n",
      "          -4.6840e-01,  5.6239e-01, -1.2309e+00,  5.5874e-01,  2.2350e-01,\n",
      "           1.4325e-01,  7.2740e-01,  7.0071e-01,  9.4725e-01,  7.8050e-02,\n",
      "           8.7701e-01,  3.1623e-01,  6.1971e-01, -3.3235e-01, -1.0343e+00,\n",
      "          -8.2948e-01, -6.3232e-01, -5.7536e-01,  4.1292e-01,  7.2758e-01,\n",
      "           1.1060e-01, -8.4345e-01,  1.0218e-01,  4.0546e-02,  3.9281e-01,\n",
      "          -2.8722e-02, -4.3014e-01]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xv: torch.Size([1, 3, 32])\n",
      "tensor([[[ 0.5987, -0.3841,  0.4786,  0.5810,  1.0784,  0.1697, -0.8636,\n",
      "           1.3724, -0.3536, -0.8730, -1.0852,  0.7057,  0.9946,  0.7028,\n",
      "          -0.2711, -0.4019, -1.0301,  0.0499,  0.8427, -0.2219,  0.8298,\n",
      "           0.4994, -0.7727, -0.0958,  0.3657,  0.2425, -0.9471,  1.3435,\n",
      "           0.0705, -0.5323,  0.5525, -0.1848],\n",
      "         [-0.2549, -0.2895, -1.1672, -0.1025, -0.3644,  0.4813,  0.6517,\n",
      "          -0.0575,  0.5576,  0.1857,  0.5378,  0.2325, -0.2280,  0.1887,\n",
      "           0.9023, -0.3956,  0.0015,  0.1293,  0.2297, -0.4635,  0.0398,\n",
      "          -0.4825, -0.6156,  0.1028, -1.1649,  0.1823,  0.0523, -0.1966,\n",
      "          -0.8978, -0.3087, -0.4732,  0.4109],\n",
      "         [-0.4979,  0.3207, -0.7325, -0.3093, -0.9898,  0.1774,  0.8524,\n",
      "          -1.1100,  0.7149,  1.0180,  1.0908, -0.6023, -0.7431, -0.4319,\n",
      "           0.6161,  0.1277,  1.1899, -0.1046, -0.5805,  0.0380, -0.5432,\n",
      "          -0.4837,  0.2379,  0.1855, -0.7020, -0.1552,  0.9174, -0.8761,\n",
      "          -0.2240,  0.2584, -0.4355,  0.3477]]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "xq reshaped: torch.Size([1, 3, 4, 32])\n",
      "tensor([[[[ 0.0212,  0.7628,  0.1346, -0.4723,  0.3360, -0.1896, -0.4112,\n",
      "            0.5264, -0.0629,  0.5382,  0.2526, -0.8670, -0.7215,  0.1249,\n",
      "           -0.0725,  1.1736, -0.2466,  0.4872,  0.1074, -0.9004, -0.1777,\n",
      "            0.1442, -0.4041,  0.5580,  0.3469,  0.4825,  0.0183, -0.2286,\n",
      "            0.9168,  0.2838, -1.2181,  1.0274],\n",
      "          [ 0.1341,  0.0482, -0.8028, -0.8018,  1.0579, -0.5970, -0.8845,\n",
      "           -0.1272, -0.8939,  0.2875, -0.5506,  0.3092,  0.3991,  0.2534,\n",
      "            0.6768, -1.1826, -0.7815,  1.0743,  0.6150, -0.9857, -0.0414,\n",
      "           -0.1052,  1.2405, -0.3249,  0.5602, -0.9988,  0.8302,  1.0129,\n",
      "           -0.0759,  0.4344, -0.7461, -0.3339],\n",
      "          [-0.0096, -0.9245,  0.0657,  0.4320, -0.0971, -0.0405,  0.3640,\n",
      "            0.1142,  0.6522, -1.4234, -0.7472, -0.3155,  0.0274,  0.4749,\n",
      "           -0.6390,  0.6304, -0.0381, -0.3158,  1.0352,  0.5786,  0.5866,\n",
      "           -0.6134,  0.0954,  0.3211, -0.2478,  0.7664, -0.4959, -0.1824,\n",
      "           -0.5689,  0.5261, -0.0270, -0.1780],\n",
      "          [-0.8023, -0.4799,  0.2752, -0.1949, -0.2738,  0.7832,  0.5524,\n",
      "           -0.2542, -0.1475, -0.9574, -1.0112, -0.7184, -0.0283,  0.3537,\n",
      "            0.5345,  0.5294,  0.3706, -0.7279,  0.0611,  0.0420,  0.5051,\n",
      "            1.1745,  0.0085,  0.0330, -0.6834,  0.0412, -0.2902, -0.4660,\n",
      "           -0.3514,  1.3221,  0.1874,  0.0194]],\n",
      "\n",
      "         [[ 1.1658, -0.4990, -0.3777,  0.4294,  0.2395,  1.1467, -0.0902,\n",
      "           -1.1253, -0.2770, -0.4792, -0.9677,  0.1868, -0.1276,  0.3011,\n",
      "            1.0714,  0.1999,  0.5043,  0.0534,  0.9720,  0.3270, -1.1813,\n",
      "            0.8974,  0.5067, -0.1920,  0.4720, -0.6290,  0.0728, -0.9100,\n",
      "            0.6566, -0.2420, -0.0428, -0.5110],\n",
      "          [ 0.3538, -0.1205, -0.2995,  0.3457, -0.2029, -0.1243,  0.2351,\n",
      "           -0.7909,  0.2028,  0.0931,  0.2273, -0.2857, -0.0213, -0.6727,\n",
      "           -0.2938,  0.7164,  0.3982, -0.4038, -0.0737,  0.1535, -0.3998,\n",
      "            0.5319, -0.1762,  0.8505, -0.5323,  1.1474, -1.0489, -0.5303,\n",
      "            0.2916,  0.7537,  0.4568,  0.2288],\n",
      "          [ 0.6017,  0.9735,  1.2771,  0.1259,  0.7691, -0.7427, -0.2636,\n",
      "            0.9969,  0.3824,  0.2030, -0.1959,  0.5095, -1.0649, -0.4353,\n",
      "           -0.0573, -0.0400,  0.9297, -0.0955, -0.3835,  0.5164, -0.1325,\n",
      "           -0.6232,  0.3615,  0.5354,  0.7436,  0.6922,  0.0420,  0.3527,\n",
      "            0.0393, -0.3546, -0.4917,  1.0494],\n",
      "          [ 0.4685,  0.0666, -0.0658,  0.6577,  0.0373,  0.5724, -0.0278,\n",
      "            0.3210, -0.4586,  0.1415,  0.1136, -0.0824,  0.5128,  0.0215,\n",
      "           -0.9621, -0.6237, -0.7876,  0.5724, -0.5890, -1.3080, -0.0174,\n",
      "           -0.4355,  0.1054, -0.6830,  0.2309, -0.6328, -0.6053, -0.3776,\n",
      "            0.9096, -0.5805,  1.1611, -1.7342]],\n",
      "\n",
      "         [[ 0.4529, -0.7145, -0.1492,  0.6829, -0.2455,  0.5473,  0.3541,\n",
      "           -0.8884,  0.1453, -0.7777, -0.7256,  1.0638,  0.3682,  0.0930,\n",
      "            0.6612, -0.7405,  0.4269, -0.6600, -0.1421,  1.0751, -0.4514,\n",
      "            0.3248,  0.4233, -0.8048, -0.1632, -0.6618,  0.2631, -0.1323,\n",
      "           -0.5012,  0.0540,  0.7615, -0.7511],\n",
      "          [-0.0566, -0.2840,  0.5912,  0.5569, -0.9374,  0.5572,  0.6035,\n",
      "           -0.1949,  0.9559,  0.0995,  0.6256, -0.1077, -0.1441, -0.4051,\n",
      "           -0.7222,  1.3997,  0.9821, -1.1443, -0.5978,  0.8393, -0.2990,\n",
      "            0.3100, -0.9029,  0.5700, -0.6408,  1.3381, -1.1460, -1.2158,\n",
      "            0.0815, -0.0893,  0.4713,  0.0838],\n",
      "          [ 0.1138,  1.1630,  0.4199, -0.3959,  0.6890, -0.2132, -0.6757,\n",
      "            0.1914, -0.3616,  0.9917,  0.4776,  0.0215, -0.4303, -0.3660,\n",
      "            0.5139, -0.5826, -0.0351, -0.0692, -0.7495, -0.3003, -0.4404,\n",
      "            0.1911,  0.0741, -0.1358,  0.3172, -0.2853,  0.7496,  0.4823,\n",
      "            0.2949, -0.2955,  0.1384,  0.3020],\n",
      "          [ 0.8810,  0.6146, -0.6162,  0.5160,  0.2558, -0.4231, -0.5651,\n",
      "            0.3137, -0.0954,  0.7486,  0.9153,  0.6032,  0.2872, -0.3719,\n",
      "           -0.8981, -0.5940, -0.7249,  0.7762, -0.2952, -0.3478, -0.5516,\n",
      "           -0.8900, -0.1164,  0.1289,  0.9302,  0.0743,  0.0346,  0.0142,\n",
      "            0.5946, -1.1454,  0.2984, -0.6081]]]], grad_fn=<ViewBackward0>)\n",
      "xk reshaped: torch.Size([1, 3, 1, 32])\n",
      "tensor([[[[ 6.8222e-01,  3.9745e-01, -4.5185e-01,  6.5456e-01,  2.0551e-01,\n",
      "            3.5685e-01, -5.9438e-01,  1.1071e+00, -5.6912e-02, -3.3377e-01,\n",
      "            2.6295e-02, -7.9261e-01, -2.5859e-01, -1.0458e+00, -3.1726e-01,\n",
      "           -5.7937e-01, -4.3237e-01, -1.0288e+00, -1.1353e-01,  1.1542e+00,\n",
      "            6.3875e-01,  2.3187e-01,  3.2322e-01, -1.3726e-01, -5.9374e-01,\n",
      "           -5.0166e-01,  1.1832e+00,  1.9762e-02,  7.9928e-02,  6.4459e-04,\n",
      "            1.0876e-01,  7.7608e-01]],\n",
      "\n",
      "         [[-5.1080e-01,  8.6573e-01, -2.6170e-01,  3.2671e-01, -5.6693e-01,\n",
      "           -1.8381e-01,  2.5705e-01, -2.6375e-01,  1.3554e+00,  2.4563e-01,\n",
      "            1.8112e-01,  8.3420e-01,  1.3661e+00,  4.6318e-02, -9.0632e-01,\n",
      "            3.1588e-01, -2.3695e-01,  2.8992e-02, -7.9596e-01, -6.5391e-01,\n",
      "           -3.5563e-01, -7.9968e-01, -9.1137e-01,  8.2195e-01,  2.0596e-01,\n",
      "           -1.1716e-01,  2.6718e-01,  7.9564e-01, -3.3097e-01,  9.4233e-01,\n",
      "            7.3562e-01,  6.0040e-01]],\n",
      "\n",
      "         [[-6.4043e-01,  8.7984e-02,  2.3794e-01, -4.7585e-01, -6.8102e-01,\n",
      "           -4.6840e-01,  5.6239e-01, -1.2309e+00,  5.5874e-01,  2.2350e-01,\n",
      "            1.4325e-01,  7.2740e-01,  7.0071e-01,  9.4725e-01,  7.8050e-02,\n",
      "            8.7701e-01,  3.1623e-01,  6.1971e-01, -3.3235e-01, -1.0343e+00,\n",
      "           -8.2948e-01, -6.3232e-01, -5.7536e-01,  4.1292e-01,  7.2758e-01,\n",
      "            1.1060e-01, -8.4345e-01,  1.0218e-01,  4.0546e-02,  3.9281e-01,\n",
      "           -2.8722e-02, -4.3014e-01]]]], grad_fn=<ViewBackward0>)\n",
      "xv reshaped: torch.Size([1, 3, 1, 32])\n",
      "tensor([[[[ 0.5987, -0.3841,  0.4786,  0.5810,  1.0784,  0.1697, -0.8636,\n",
      "            1.3724, -0.3536, -0.8730, -1.0852,  0.7057,  0.9946,  0.7028,\n",
      "           -0.2711, -0.4019, -1.0301,  0.0499,  0.8427, -0.2219,  0.8298,\n",
      "            0.4994, -0.7727, -0.0958,  0.3657,  0.2425, -0.9471,  1.3435,\n",
      "            0.0705, -0.5323,  0.5525, -0.1848]],\n",
      "\n",
      "         [[-0.2549, -0.2895, -1.1672, -0.1025, -0.3644,  0.4813,  0.6517,\n",
      "           -0.0575,  0.5576,  0.1857,  0.5378,  0.2325, -0.2280,  0.1887,\n",
      "            0.9023, -0.3956,  0.0015,  0.1293,  0.2297, -0.4635,  0.0398,\n",
      "           -0.4825, -0.6156,  0.1028, -1.1649,  0.1823,  0.0523, -0.1966,\n",
      "           -0.8978, -0.3087, -0.4732,  0.4109]],\n",
      "\n",
      "         [[-0.4979,  0.3207, -0.7325, -0.3093, -0.9898,  0.1774,  0.8524,\n",
      "           -1.1100,  0.7149,  1.0180,  1.0908, -0.6023, -0.7431, -0.4319,\n",
      "            0.6161,  0.1277,  1.1899, -0.1046, -0.5805,  0.0380, -0.5432,\n",
      "           -0.4837,  0.2379,  0.1855, -0.7020, -0.1552,  0.9174, -0.8761,\n",
      "           -0.2240,  0.2584, -0.4355,  0.3477]]]], grad_fn=<ViewBackward0>)\n",
      "rotated xq: torch.Size([1, 3, 4, 32])\n",
      "tensor([[[[ 2.1219e-02,  7.6282e-01,  1.3464e-01, -4.7232e-01,  3.3603e-01,\n",
      "           -1.8964e-01, -4.1116e-01,  5.2642e-01, -6.2913e-02,  5.3815e-01,\n",
      "            2.5264e-01, -8.6697e-01, -7.2154e-01,  1.2491e-01, -7.2481e-02,\n",
      "            1.1736e+00, -2.4659e-01,  4.8723e-01,  1.0736e-01, -9.0042e-01,\n",
      "           -1.7768e-01,  1.4420e-01, -4.0415e-01,  5.5801e-01,  3.4688e-01,\n",
      "            4.8255e-01,  1.8343e-02, -2.2863e-01,  9.1685e-01,  2.8378e-01,\n",
      "           -1.2181e+00,  1.0274e+00],\n",
      "          [ 1.3413e-01,  4.8223e-02, -8.0284e-01, -8.0184e-01,  1.0579e+00,\n",
      "           -5.9700e-01, -8.8453e-01, -1.2718e-01, -8.9388e-01,  2.8754e-01,\n",
      "           -5.5062e-01,  3.0917e-01,  3.9910e-01,  2.5340e-01,  6.7676e-01,\n",
      "           -1.1826e+00, -7.8154e-01,  1.0743e+00,  6.1498e-01, -9.8566e-01,\n",
      "           -4.1393e-02, -1.0516e-01,  1.2405e+00, -3.2490e-01,  5.6022e-01,\n",
      "           -9.9883e-01,  8.3025e-01,  1.0129e+00, -7.5889e-02,  4.3441e-01,\n",
      "           -7.4613e-01, -3.3389e-01],\n",
      "          [-9.5903e-03, -9.2453e-01,  6.5660e-02,  4.3196e-01, -9.7053e-02,\n",
      "           -4.0486e-02,  3.6402e-01,  1.1421e-01,  6.5216e-01, -1.4234e+00,\n",
      "           -7.4720e-01, -3.1548e-01,  2.7357e-02,  4.7489e-01, -6.3897e-01,\n",
      "            6.3042e-01, -3.8073e-02, -3.1581e-01,  1.0352e+00,  5.7862e-01,\n",
      "            5.8660e-01, -6.1344e-01,  9.5387e-02,  3.2114e-01, -2.4780e-01,\n",
      "            7.6635e-01, -4.9593e-01, -1.8243e-01, -5.6891e-01,  5.2612e-01,\n",
      "           -2.7042e-02, -1.7797e-01],\n",
      "          [-8.0226e-01, -4.7991e-01,  2.7524e-01, -1.9489e-01, -2.7379e-01,\n",
      "            7.8316e-01,  5.5244e-01, -2.5420e-01, -1.4755e-01, -9.5745e-01,\n",
      "           -1.0112e+00, -7.1845e-01, -2.8290e-02,  3.5370e-01,  5.3454e-01,\n",
      "            5.2941e-01,  3.7057e-01, -7.2791e-01,  6.1079e-02,  4.1954e-02,\n",
      "            5.0515e-01,  1.1745e+00,  8.5027e-03,  3.3048e-02, -6.8340e-01,\n",
      "            4.1248e-02, -2.9024e-01, -4.6604e-01, -3.5143e-01,  1.3221e+00,\n",
      "            1.8741e-01,  1.9402e-02]],\n",
      "\n",
      "         [[ 2.0550e-01, -4.0155e-01, -8.3775e-01,  2.5790e-01,  5.9501e-01,\n",
      "            9.0377e-01, -1.7844e-01, -1.0898e+00, -3.2276e-01, -4.3070e-01,\n",
      "           -9.7023e-01,  2.2497e-01, -1.4825e-01,  3.0679e-01,  1.0720e+00,\n",
      "            2.0674e-01,  1.2534e+00, -3.0097e-01,  6.2094e-01,  4.7409e-01,\n",
      "           -1.0483e+00,  1.1417e+00,  4.8277e-01, -3.3988e-01,  4.4196e-01,\n",
      "           -6.6317e-01,  1.8295e-02, -9.0131e-01,  6.5222e-01, -2.3480e-01,\n",
      "           -2.3731e-02, -5.0830e-01],\n",
      "          [-1.4392e-01,  1.8701e-01, -2.1411e-01,  2.5262e-01, -6.8484e-02,\n",
      "           -2.4576e-01,  2.6261e-01, -8.9697e-01,  2.5492e-01,  6.8307e-03,\n",
      "            2.8586e-01, -2.6310e-01, -3.0512e-02, -6.9034e-01, -3.0183e-01,\n",
      "            7.1328e-01,  5.1290e-01, -3.7761e-01, -2.2200e-01,  2.8151e-01,\n",
      "           -4.4305e-01,  4.8779e-01, -1.3181e-01,  7.3776e-01, -5.0935e-01,\n",
      "            1.1511e+00, -1.0345e+00, -5.4186e-01,  2.9077e-01,  7.3752e-01,\n",
      "            4.5155e-01,  2.3838e-01],\n",
      "          [-4.5720e-01,  7.7746e-01,  1.2849e+00, -9.6509e-02,  7.7223e-01,\n",
      "           -5.7555e-01, -3.2336e-01,  9.1688e-01,  3.0621e-01,  1.5062e-01,\n",
      "           -1.9797e-01,  4.9422e-01, -1.0656e+00, -4.2675e-01, -4.8555e-02,\n",
      "           -5.4024e-02,  1.0087e+00,  5.9362e-01,  3.5643e-01,  5.2272e-01,\n",
      "            1.1321e-01, -7.8019e-01,  3.0914e-01,  6.6319e-01,  7.7811e-01,\n",
      "            7.0543e-01,  3.0875e-02,  3.7387e-01,  5.5955e-03, -3.6480e-01,\n",
      "           -4.9269e-01,  1.0488e+00],\n",
      "          [ 9.1587e-01, -3.4142e-01,  2.5841e-01,  1.1354e+00,  4.0849e-02,\n",
      "            6.5871e-01, -4.6023e-02,  4.0897e-01, -4.7936e-01,  1.8853e-01,\n",
      "            1.4742e-01, -6.6365e-02,  4.8381e-01,  3.5246e-02, -9.8258e-01,\n",
      "           -6.0048e-01, -3.1253e-02,  4.6423e-01, -5.3340e-01, -9.2424e-01,\n",
      "           -4.9836e-03, -2.8884e-01,  9.8824e-02, -6.3426e-01,  1.8396e-01,\n",
      "           -6.2044e-01, -5.9793e-01, -3.8073e-01,  9.2532e-01, -5.7986e-01,\n",
      "            1.1438e+00, -1.7423e+00]],\n",
      "\n",
      "         [[-5.7667e-01,  6.0766e-01,  6.3806e-02, -3.4894e-01,  6.8801e-02,\n",
      "            3.3852e-01,  1.8457e-01, -6.4487e-01,  1.7482e-01, -6.7014e-01,\n",
      "           -7.5058e-01,  1.0711e+00,  3.9911e-01,  9.0344e-02,  6.3372e-01,\n",
      "           -7.2021e-01,  2.3421e-01, -7.5949e-01, -1.9595e-01,  1.2250e+00,\n",
      "           -5.0923e-01,  5.3889e-01,  5.2009e-01, -1.0105e+00, -1.3110e-01,\n",
      "           -7.7054e-01,  1.8004e-01, -4.2236e-02, -4.7694e-01,  5.8332e-02,\n",
      "            7.8453e-01, -7.7062e-01],\n",
      "          [-8.6947e-01,  1.1213e+00,  7.9440e-01, -2.5656e-01, -5.7933e-01,\n",
      "            3.5416e-01,  8.8013e-01, -3.3821e-01,  1.0641e+00, -1.0153e-01,\n",
      "            7.5026e-01, -4.8666e-03, -1.4896e-01, -4.0041e-01, -7.3850e-01,\n",
      "            1.3969e+00, -4.6016e-01, -3.6447e-01,  2.7537e-01,  9.7402e-01,\n",
      "           -7.9533e-01,  5.3028e-01, -6.3626e-01,  4.9849e-01, -4.3814e-01,\n",
      "            1.3380e+00, -1.0686e+00, -1.2205e+00,  7.2197e-02, -1.0842e-01,\n",
      "            4.4529e-01,  1.2108e-01],\n",
      "          [-1.5424e-02,  1.5157e-01,  8.5728e-01, -3.8890e-02,  8.1612e-01,\n",
      "           -2.7698e-01, -6.5919e-01,  2.2045e-01, -4.1743e-01,  1.0232e+00,\n",
      "            3.9043e-01, -1.9183e-02, -4.4809e-01, -3.5155e-01,  5.0867e-01,\n",
      "           -5.9045e-01,  1.1810e-01,  1.1552e+00,  5.5430e-02, -4.9537e-01,\n",
      "            5.2074e-02,  7.2619e-02, -1.6580e-01, -8.0506e-02,  2.3900e-01,\n",
      "           -1.3394e-01,  7.9848e-01,  4.8240e-01,  2.6715e-01, -3.1253e-01,\n",
      "            1.5654e-01,  2.8636e-01],\n",
      "          [ 2.9254e-01, -7.3063e-01,  4.1428e-04,  6.0291e-01,  5.3238e-01,\n",
      "            3.0040e-02, -4.8916e-01,  2.6869e-01, -2.7828e-01,  7.2914e-01,\n",
      "            9.0558e-01,  5.9986e-01,  2.4905e-01, -3.1721e-01, -9.0811e-01,\n",
      "           -5.7759e-01,  1.1027e+00,  6.6816e-01, -6.8323e-01,  1.5417e-01,\n",
      "           -2.9369e-01, -9.8504e-01, -3.0589e-01,  2.0699e-01,  8.9272e-01,\n",
      "            1.8530e-01,  1.3714e-01,  6.4934e-02,  6.1160e-01, -1.1617e+00,\n",
      "            2.6630e-01, -6.2372e-01]]]], grad_fn=<TransposeBackward0>)\n",
      "rotated xk: torch.Size([1, 3, 1, 32])\n",
      "tensor([[[[ 6.8222e-01,  3.9745e-01, -4.5185e-01,  6.5456e-01,  2.0551e-01,\n",
      "            3.5685e-01, -5.9438e-01,  1.1071e+00, -5.6912e-02, -3.3377e-01,\n",
      "            2.6295e-02, -7.9261e-01, -2.5859e-01, -1.0458e+00, -3.1726e-01,\n",
      "           -5.7937e-01, -4.3237e-01, -1.0288e+00, -1.1353e-01,  1.1542e+00,\n",
      "            6.3875e-01,  2.3187e-01,  3.2322e-01, -1.3726e-01, -5.9374e-01,\n",
      "           -5.0166e-01,  1.1832e+00,  1.9762e-02,  7.9928e-02,  6.4459e-04,\n",
      "            1.0876e-01,  7.7608e-01]],\n",
      "\n",
      "         [[-7.6602e-02,  6.1375e-01,  2.0298e-01,  5.6574e-01, -4.2822e-01,\n",
      "            9.1958e-03,  4.1421e-01, -3.7069e-01,  1.3281e+00,  2.5371e-01,\n",
      "            1.6582e-01,  7.9992e-01,  1.3759e+00,  2.3961e-02, -9.1926e-01,\n",
      "            3.0784e-01, -5.5785e-01,  6.1126e-01, -8.1291e-01, -4.6290e-01,\n",
      "           -5.1430e-01, -8.2048e-01, -8.5153e-01,  7.7958e-01,  3.4025e-01,\n",
      "           -9.8431e-02,  2.7694e-01,  8.3010e-01, -2.8762e-01,  9.4317e-01,\n",
      "            7.1939e-01,  6.0456e-01]],\n",
      "\n",
      "         [[-2.1036e-02, -6.1191e-01,  4.0248e-01,  4.5612e-01, -5.8964e-02,\n",
      "           -1.2792e-01,  7.2754e-01, -1.2962e+00,  4.0305e-01,  2.0447e-01,\n",
      "            2.3701e-01,  7.1621e-01,  6.9675e-01,  9.2756e-01,  7.9022e-02,\n",
      "            8.8817e-01, -7.1394e-01,  1.3173e-01,  7.1257e-02, -1.0432e+00,\n",
      "           -1.0716e+00, -7.7644e-01, -3.4353e-01,  7.3914e-02,  8.2409e-01,\n",
      "            1.4275e-01, -8.2205e-01,  1.6309e-01,  8.4752e-02,  4.3728e-01,\n",
      "           -2.5928e-02, -4.0660e-01]]]], grad_fn=<TransposeBackward0>)\n",
      "repeat_interleaved xk: torch.Size([1, 3, 4, 32])\n",
      "tensor([[[[ 6.8222e-01,  3.9745e-01, -4.5185e-01,  6.5456e-01,  2.0551e-01,\n",
      "            3.5685e-01, -5.9438e-01,  1.1071e+00, -5.6912e-02, -3.3377e-01,\n",
      "            2.6295e-02, -7.9261e-01, -2.5859e-01, -1.0458e+00, -3.1726e-01,\n",
      "           -5.7937e-01, -4.3237e-01, -1.0288e+00, -1.1353e-01,  1.1542e+00,\n",
      "            6.3875e-01,  2.3187e-01,  3.2322e-01, -1.3726e-01, -5.9374e-01,\n",
      "           -5.0166e-01,  1.1832e+00,  1.9762e-02,  7.9928e-02,  6.4459e-04,\n",
      "            1.0876e-01,  7.7608e-01],\n",
      "          [ 6.8222e-01,  3.9745e-01, -4.5185e-01,  6.5456e-01,  2.0551e-01,\n",
      "            3.5685e-01, -5.9438e-01,  1.1071e+00, -5.6912e-02, -3.3377e-01,\n",
      "            2.6295e-02, -7.9261e-01, -2.5859e-01, -1.0458e+00, -3.1726e-01,\n",
      "           -5.7937e-01, -4.3237e-01, -1.0288e+00, -1.1353e-01,  1.1542e+00,\n",
      "            6.3875e-01,  2.3187e-01,  3.2322e-01, -1.3726e-01, -5.9374e-01,\n",
      "           -5.0166e-01,  1.1832e+00,  1.9762e-02,  7.9928e-02,  6.4459e-04,\n",
      "            1.0876e-01,  7.7608e-01],\n",
      "          [ 6.8222e-01,  3.9745e-01, -4.5185e-01,  6.5456e-01,  2.0551e-01,\n",
      "            3.5685e-01, -5.9438e-01,  1.1071e+00, -5.6912e-02, -3.3377e-01,\n",
      "            2.6295e-02, -7.9261e-01, -2.5859e-01, -1.0458e+00, -3.1726e-01,\n",
      "           -5.7937e-01, -4.3237e-01, -1.0288e+00, -1.1353e-01,  1.1542e+00,\n",
      "            6.3875e-01,  2.3187e-01,  3.2322e-01, -1.3726e-01, -5.9374e-01,\n",
      "           -5.0166e-01,  1.1832e+00,  1.9762e-02,  7.9928e-02,  6.4459e-04,\n",
      "            1.0876e-01,  7.7608e-01],\n",
      "          [ 6.8222e-01,  3.9745e-01, -4.5185e-01,  6.5456e-01,  2.0551e-01,\n",
      "            3.5685e-01, -5.9438e-01,  1.1071e+00, -5.6912e-02, -3.3377e-01,\n",
      "            2.6295e-02, -7.9261e-01, -2.5859e-01, -1.0458e+00, -3.1726e-01,\n",
      "           -5.7937e-01, -4.3237e-01, -1.0288e+00, -1.1353e-01,  1.1542e+00,\n",
      "            6.3875e-01,  2.3187e-01,  3.2322e-01, -1.3726e-01, -5.9374e-01,\n",
      "           -5.0166e-01,  1.1832e+00,  1.9762e-02,  7.9928e-02,  6.4459e-04,\n",
      "            1.0876e-01,  7.7608e-01]],\n",
      "\n",
      "         [[-7.6602e-02,  6.1375e-01,  2.0298e-01,  5.6574e-01, -4.2822e-01,\n",
      "            9.1958e-03,  4.1421e-01, -3.7069e-01,  1.3281e+00,  2.5371e-01,\n",
      "            1.6582e-01,  7.9992e-01,  1.3759e+00,  2.3961e-02, -9.1926e-01,\n",
      "            3.0784e-01, -5.5785e-01,  6.1126e-01, -8.1291e-01, -4.6290e-01,\n",
      "           -5.1430e-01, -8.2048e-01, -8.5153e-01,  7.7958e-01,  3.4025e-01,\n",
      "           -9.8431e-02,  2.7694e-01,  8.3010e-01, -2.8762e-01,  9.4317e-01,\n",
      "            7.1939e-01,  6.0456e-01],\n",
      "          [-7.6602e-02,  6.1375e-01,  2.0298e-01,  5.6574e-01, -4.2822e-01,\n",
      "            9.1958e-03,  4.1421e-01, -3.7069e-01,  1.3281e+00,  2.5371e-01,\n",
      "            1.6582e-01,  7.9992e-01,  1.3759e+00,  2.3961e-02, -9.1926e-01,\n",
      "            3.0784e-01, -5.5785e-01,  6.1126e-01, -8.1291e-01, -4.6290e-01,\n",
      "           -5.1430e-01, -8.2048e-01, -8.5153e-01,  7.7958e-01,  3.4025e-01,\n",
      "           -9.8431e-02,  2.7694e-01,  8.3010e-01, -2.8762e-01,  9.4317e-01,\n",
      "            7.1939e-01,  6.0456e-01],\n",
      "          [-7.6602e-02,  6.1375e-01,  2.0298e-01,  5.6574e-01, -4.2822e-01,\n",
      "            9.1958e-03,  4.1421e-01, -3.7069e-01,  1.3281e+00,  2.5371e-01,\n",
      "            1.6582e-01,  7.9992e-01,  1.3759e+00,  2.3961e-02, -9.1926e-01,\n",
      "            3.0784e-01, -5.5785e-01,  6.1126e-01, -8.1291e-01, -4.6290e-01,\n",
      "           -5.1430e-01, -8.2048e-01, -8.5153e-01,  7.7958e-01,  3.4025e-01,\n",
      "           -9.8431e-02,  2.7694e-01,  8.3010e-01, -2.8762e-01,  9.4317e-01,\n",
      "            7.1939e-01,  6.0456e-01],\n",
      "          [-7.6602e-02,  6.1375e-01,  2.0298e-01,  5.6574e-01, -4.2822e-01,\n",
      "            9.1958e-03,  4.1421e-01, -3.7069e-01,  1.3281e+00,  2.5371e-01,\n",
      "            1.6582e-01,  7.9992e-01,  1.3759e+00,  2.3961e-02, -9.1926e-01,\n",
      "            3.0784e-01, -5.5785e-01,  6.1126e-01, -8.1291e-01, -4.6290e-01,\n",
      "           -5.1430e-01, -8.2048e-01, -8.5153e-01,  7.7958e-01,  3.4025e-01,\n",
      "           -9.8431e-02,  2.7694e-01,  8.3010e-01, -2.8762e-01,  9.4317e-01,\n",
      "            7.1939e-01,  6.0456e-01]],\n",
      "\n",
      "         [[-2.1036e-02, -6.1191e-01,  4.0248e-01,  4.5612e-01, -5.8964e-02,\n",
      "           -1.2792e-01,  7.2754e-01, -1.2962e+00,  4.0305e-01,  2.0447e-01,\n",
      "            2.3701e-01,  7.1621e-01,  6.9675e-01,  9.2756e-01,  7.9022e-02,\n",
      "            8.8817e-01, -7.1394e-01,  1.3173e-01,  7.1257e-02, -1.0432e+00,\n",
      "           -1.0716e+00, -7.7644e-01, -3.4353e-01,  7.3914e-02,  8.2409e-01,\n",
      "            1.4275e-01, -8.2205e-01,  1.6309e-01,  8.4752e-02,  4.3728e-01,\n",
      "           -2.5928e-02, -4.0660e-01],\n",
      "          [-2.1036e-02, -6.1191e-01,  4.0248e-01,  4.5612e-01, -5.8964e-02,\n",
      "           -1.2792e-01,  7.2754e-01, -1.2962e+00,  4.0305e-01,  2.0447e-01,\n",
      "            2.3701e-01,  7.1621e-01,  6.9675e-01,  9.2756e-01,  7.9022e-02,\n",
      "            8.8817e-01, -7.1394e-01,  1.3173e-01,  7.1257e-02, -1.0432e+00,\n",
      "           -1.0716e+00, -7.7644e-01, -3.4353e-01,  7.3914e-02,  8.2409e-01,\n",
      "            1.4275e-01, -8.2205e-01,  1.6309e-01,  8.4752e-02,  4.3728e-01,\n",
      "           -2.5928e-02, -4.0660e-01],\n",
      "          [-2.1036e-02, -6.1191e-01,  4.0248e-01,  4.5612e-01, -5.8964e-02,\n",
      "           -1.2792e-01,  7.2754e-01, -1.2962e+00,  4.0305e-01,  2.0447e-01,\n",
      "            2.3701e-01,  7.1621e-01,  6.9675e-01,  9.2756e-01,  7.9022e-02,\n",
      "            8.8817e-01, -7.1394e-01,  1.3173e-01,  7.1257e-02, -1.0432e+00,\n",
      "           -1.0716e+00, -7.7644e-01, -3.4353e-01,  7.3914e-02,  8.2409e-01,\n",
      "            1.4275e-01, -8.2205e-01,  1.6309e-01,  8.4752e-02,  4.3728e-01,\n",
      "           -2.5928e-02, -4.0660e-01],\n",
      "          [-2.1036e-02, -6.1191e-01,  4.0248e-01,  4.5612e-01, -5.8964e-02,\n",
      "           -1.2792e-01,  7.2754e-01, -1.2962e+00,  4.0305e-01,  2.0447e-01,\n",
      "            2.3701e-01,  7.1621e-01,  6.9675e-01,  9.2756e-01,  7.9022e-02,\n",
      "            8.8817e-01, -7.1394e-01,  1.3173e-01,  7.1257e-02, -1.0432e+00,\n",
      "           -1.0716e+00, -7.7644e-01, -3.4353e-01,  7.3914e-02,  8.2409e-01,\n",
      "            1.4275e-01, -8.2205e-01,  1.6309e-01,  8.4752e-02,  4.3728e-01,\n",
      "           -2.5928e-02, -4.0660e-01]]]], grad_fn=<ViewBackward0>)\n",
      "repeat_interleaved xv: torch.Size([1, 3, 4, 32])\n",
      "tensor([[[[ 0.5987, -0.3841,  0.4786,  0.5810,  1.0784,  0.1697, -0.8636,\n",
      "            1.3724, -0.3536, -0.8730, -1.0852,  0.7057,  0.9946,  0.7028,\n",
      "           -0.2711, -0.4019, -1.0301,  0.0499,  0.8427, -0.2219,  0.8298,\n",
      "            0.4994, -0.7727, -0.0958,  0.3657,  0.2425, -0.9471,  1.3435,\n",
      "            0.0705, -0.5323,  0.5525, -0.1848],\n",
      "          [ 0.5987, -0.3841,  0.4786,  0.5810,  1.0784,  0.1697, -0.8636,\n",
      "            1.3724, -0.3536, -0.8730, -1.0852,  0.7057,  0.9946,  0.7028,\n",
      "           -0.2711, -0.4019, -1.0301,  0.0499,  0.8427, -0.2219,  0.8298,\n",
      "            0.4994, -0.7727, -0.0958,  0.3657,  0.2425, -0.9471,  1.3435,\n",
      "            0.0705, -0.5323,  0.5525, -0.1848],\n",
      "          [ 0.5987, -0.3841,  0.4786,  0.5810,  1.0784,  0.1697, -0.8636,\n",
      "            1.3724, -0.3536, -0.8730, -1.0852,  0.7057,  0.9946,  0.7028,\n",
      "           -0.2711, -0.4019, -1.0301,  0.0499,  0.8427, -0.2219,  0.8298,\n",
      "            0.4994, -0.7727, -0.0958,  0.3657,  0.2425, -0.9471,  1.3435,\n",
      "            0.0705, -0.5323,  0.5525, -0.1848],\n",
      "          [ 0.5987, -0.3841,  0.4786,  0.5810,  1.0784,  0.1697, -0.8636,\n",
      "            1.3724, -0.3536, -0.8730, -1.0852,  0.7057,  0.9946,  0.7028,\n",
      "           -0.2711, -0.4019, -1.0301,  0.0499,  0.8427, -0.2219,  0.8298,\n",
      "            0.4994, -0.7727, -0.0958,  0.3657,  0.2425, -0.9471,  1.3435,\n",
      "            0.0705, -0.5323,  0.5525, -0.1848]],\n",
      "\n",
      "         [[-0.2549, -0.2895, -1.1672, -0.1025, -0.3644,  0.4813,  0.6517,\n",
      "           -0.0575,  0.5576,  0.1857,  0.5378,  0.2325, -0.2280,  0.1887,\n",
      "            0.9023, -0.3956,  0.0015,  0.1293,  0.2297, -0.4635,  0.0398,\n",
      "           -0.4825, -0.6156,  0.1028, -1.1649,  0.1823,  0.0523, -0.1966,\n",
      "           -0.8978, -0.3087, -0.4732,  0.4109],\n",
      "          [-0.2549, -0.2895, -1.1672, -0.1025, -0.3644,  0.4813,  0.6517,\n",
      "           -0.0575,  0.5576,  0.1857,  0.5378,  0.2325, -0.2280,  0.1887,\n",
      "            0.9023, -0.3956,  0.0015,  0.1293,  0.2297, -0.4635,  0.0398,\n",
      "           -0.4825, -0.6156,  0.1028, -1.1649,  0.1823,  0.0523, -0.1966,\n",
      "           -0.8978, -0.3087, -0.4732,  0.4109],\n",
      "          [-0.2549, -0.2895, -1.1672, -0.1025, -0.3644,  0.4813,  0.6517,\n",
      "           -0.0575,  0.5576,  0.1857,  0.5378,  0.2325, -0.2280,  0.1887,\n",
      "            0.9023, -0.3956,  0.0015,  0.1293,  0.2297, -0.4635,  0.0398,\n",
      "           -0.4825, -0.6156,  0.1028, -1.1649,  0.1823,  0.0523, -0.1966,\n",
      "           -0.8978, -0.3087, -0.4732,  0.4109],\n",
      "          [-0.2549, -0.2895, -1.1672, -0.1025, -0.3644,  0.4813,  0.6517,\n",
      "           -0.0575,  0.5576,  0.1857,  0.5378,  0.2325, -0.2280,  0.1887,\n",
      "            0.9023, -0.3956,  0.0015,  0.1293,  0.2297, -0.4635,  0.0398,\n",
      "           -0.4825, -0.6156,  0.1028, -1.1649,  0.1823,  0.0523, -0.1966,\n",
      "           -0.8978, -0.3087, -0.4732,  0.4109]],\n",
      "\n",
      "         [[-0.4979,  0.3207, -0.7325, -0.3093, -0.9898,  0.1774,  0.8524,\n",
      "           -1.1100,  0.7149,  1.0180,  1.0908, -0.6023, -0.7431, -0.4319,\n",
      "            0.6161,  0.1277,  1.1899, -0.1046, -0.5805,  0.0380, -0.5432,\n",
      "           -0.4837,  0.2379,  0.1855, -0.7020, -0.1552,  0.9174, -0.8761,\n",
      "           -0.2240,  0.2584, -0.4355,  0.3477],\n",
      "          [-0.4979,  0.3207, -0.7325, -0.3093, -0.9898,  0.1774,  0.8524,\n",
      "           -1.1100,  0.7149,  1.0180,  1.0908, -0.6023, -0.7431, -0.4319,\n",
      "            0.6161,  0.1277,  1.1899, -0.1046, -0.5805,  0.0380, -0.5432,\n",
      "           -0.4837,  0.2379,  0.1855, -0.7020, -0.1552,  0.9174, -0.8761,\n",
      "           -0.2240,  0.2584, -0.4355,  0.3477],\n",
      "          [-0.4979,  0.3207, -0.7325, -0.3093, -0.9898,  0.1774,  0.8524,\n",
      "           -1.1100,  0.7149,  1.0180,  1.0908, -0.6023, -0.7431, -0.4319,\n",
      "            0.6161,  0.1277,  1.1899, -0.1046, -0.5805,  0.0380, -0.5432,\n",
      "           -0.4837,  0.2379,  0.1855, -0.7020, -0.1552,  0.9174, -0.8761,\n",
      "           -0.2240,  0.2584, -0.4355,  0.3477],\n",
      "          [-0.4979,  0.3207, -0.7325, -0.3093, -0.9898,  0.1774,  0.8524,\n",
      "           -1.1100,  0.7149,  1.0180,  1.0908, -0.6023, -0.7431, -0.4319,\n",
      "            0.6161,  0.1277,  1.1899, -0.1046, -0.5805,  0.0380, -0.5432,\n",
      "           -0.4837,  0.2379,  0.1855, -0.7020, -0.1552,  0.9174, -0.8761,\n",
      "           -0.2240,  0.2584, -0.4355,  0.3477]]]], grad_fn=<ViewBackward0>)\n",
      "transposed xq: torch.Size([1, 4, 3, 32])\n",
      "tensor([[[[ 2.1219e-02,  7.6282e-01,  1.3464e-01, -4.7232e-01,  3.3603e-01,\n",
      "           -1.8964e-01, -4.1116e-01,  5.2642e-01, -6.2913e-02,  5.3815e-01,\n",
      "            2.5264e-01, -8.6697e-01, -7.2154e-01,  1.2491e-01, -7.2481e-02,\n",
      "            1.1736e+00, -2.4659e-01,  4.8723e-01,  1.0736e-01, -9.0042e-01,\n",
      "           -1.7768e-01,  1.4420e-01, -4.0415e-01,  5.5801e-01,  3.4688e-01,\n",
      "            4.8255e-01,  1.8343e-02, -2.2863e-01,  9.1685e-01,  2.8378e-01,\n",
      "           -1.2181e+00,  1.0274e+00],\n",
      "          [ 2.0550e-01, -4.0155e-01, -8.3775e-01,  2.5790e-01,  5.9501e-01,\n",
      "            9.0377e-01, -1.7844e-01, -1.0898e+00, -3.2276e-01, -4.3070e-01,\n",
      "           -9.7023e-01,  2.2497e-01, -1.4825e-01,  3.0679e-01,  1.0720e+00,\n",
      "            2.0674e-01,  1.2534e+00, -3.0097e-01,  6.2094e-01,  4.7409e-01,\n",
      "           -1.0483e+00,  1.1417e+00,  4.8277e-01, -3.3988e-01,  4.4196e-01,\n",
      "           -6.6317e-01,  1.8295e-02, -9.0131e-01,  6.5222e-01, -2.3480e-01,\n",
      "           -2.3731e-02, -5.0830e-01],\n",
      "          [-5.7667e-01,  6.0766e-01,  6.3806e-02, -3.4894e-01,  6.8801e-02,\n",
      "            3.3852e-01,  1.8457e-01, -6.4487e-01,  1.7482e-01, -6.7014e-01,\n",
      "           -7.5058e-01,  1.0711e+00,  3.9911e-01,  9.0344e-02,  6.3372e-01,\n",
      "           -7.2021e-01,  2.3421e-01, -7.5949e-01, -1.9595e-01,  1.2250e+00,\n",
      "           -5.0923e-01,  5.3889e-01,  5.2009e-01, -1.0105e+00, -1.3110e-01,\n",
      "           -7.7054e-01,  1.8004e-01, -4.2236e-02, -4.7694e-01,  5.8332e-02,\n",
      "            7.8453e-01, -7.7062e-01]],\n",
      "\n",
      "         [[ 1.3413e-01,  4.8223e-02, -8.0284e-01, -8.0184e-01,  1.0579e+00,\n",
      "           -5.9700e-01, -8.8453e-01, -1.2718e-01, -8.9388e-01,  2.8754e-01,\n",
      "           -5.5062e-01,  3.0917e-01,  3.9910e-01,  2.5340e-01,  6.7676e-01,\n",
      "           -1.1826e+00, -7.8154e-01,  1.0743e+00,  6.1498e-01, -9.8566e-01,\n",
      "           -4.1393e-02, -1.0516e-01,  1.2405e+00, -3.2490e-01,  5.6022e-01,\n",
      "           -9.9883e-01,  8.3025e-01,  1.0129e+00, -7.5889e-02,  4.3441e-01,\n",
      "           -7.4613e-01, -3.3389e-01],\n",
      "          [-1.4392e-01,  1.8701e-01, -2.1411e-01,  2.5262e-01, -6.8484e-02,\n",
      "           -2.4576e-01,  2.6261e-01, -8.9697e-01,  2.5492e-01,  6.8307e-03,\n",
      "            2.8586e-01, -2.6310e-01, -3.0512e-02, -6.9034e-01, -3.0183e-01,\n",
      "            7.1328e-01,  5.1290e-01, -3.7761e-01, -2.2200e-01,  2.8151e-01,\n",
      "           -4.4305e-01,  4.8779e-01, -1.3181e-01,  7.3776e-01, -5.0935e-01,\n",
      "            1.1511e+00, -1.0345e+00, -5.4186e-01,  2.9077e-01,  7.3752e-01,\n",
      "            4.5155e-01,  2.3838e-01],\n",
      "          [-8.6947e-01,  1.1213e+00,  7.9440e-01, -2.5656e-01, -5.7933e-01,\n",
      "            3.5416e-01,  8.8013e-01, -3.3821e-01,  1.0641e+00, -1.0153e-01,\n",
      "            7.5026e-01, -4.8666e-03, -1.4896e-01, -4.0041e-01, -7.3850e-01,\n",
      "            1.3969e+00, -4.6016e-01, -3.6447e-01,  2.7537e-01,  9.7402e-01,\n",
      "           -7.9533e-01,  5.3028e-01, -6.3626e-01,  4.9849e-01, -4.3814e-01,\n",
      "            1.3380e+00, -1.0686e+00, -1.2205e+00,  7.2197e-02, -1.0842e-01,\n",
      "            4.4529e-01,  1.2108e-01]],\n",
      "\n",
      "         [[-9.5903e-03, -9.2453e-01,  6.5660e-02,  4.3196e-01, -9.7053e-02,\n",
      "           -4.0486e-02,  3.6402e-01,  1.1421e-01,  6.5216e-01, -1.4234e+00,\n",
      "           -7.4720e-01, -3.1548e-01,  2.7357e-02,  4.7489e-01, -6.3897e-01,\n",
      "            6.3042e-01, -3.8073e-02, -3.1581e-01,  1.0352e+00,  5.7862e-01,\n",
      "            5.8660e-01, -6.1344e-01,  9.5387e-02,  3.2114e-01, -2.4780e-01,\n",
      "            7.6635e-01, -4.9593e-01, -1.8243e-01, -5.6891e-01,  5.2612e-01,\n",
      "           -2.7042e-02, -1.7797e-01],\n",
      "          [-4.5720e-01,  7.7746e-01,  1.2849e+00, -9.6509e-02,  7.7223e-01,\n",
      "           -5.7555e-01, -3.2336e-01,  9.1688e-01,  3.0621e-01,  1.5062e-01,\n",
      "           -1.9797e-01,  4.9422e-01, -1.0656e+00, -4.2675e-01, -4.8555e-02,\n",
      "           -5.4024e-02,  1.0087e+00,  5.9362e-01,  3.5643e-01,  5.2272e-01,\n",
      "            1.1321e-01, -7.8019e-01,  3.0914e-01,  6.6319e-01,  7.7811e-01,\n",
      "            7.0543e-01,  3.0875e-02,  3.7387e-01,  5.5955e-03, -3.6480e-01,\n",
      "           -4.9269e-01,  1.0488e+00],\n",
      "          [-1.5424e-02,  1.5157e-01,  8.5728e-01, -3.8890e-02,  8.1612e-01,\n",
      "           -2.7698e-01, -6.5919e-01,  2.2045e-01, -4.1743e-01,  1.0232e+00,\n",
      "            3.9043e-01, -1.9183e-02, -4.4809e-01, -3.5155e-01,  5.0867e-01,\n",
      "           -5.9045e-01,  1.1810e-01,  1.1552e+00,  5.5430e-02, -4.9537e-01,\n",
      "            5.2074e-02,  7.2619e-02, -1.6580e-01, -8.0506e-02,  2.3900e-01,\n",
      "           -1.3394e-01,  7.9848e-01,  4.8240e-01,  2.6715e-01, -3.1253e-01,\n",
      "            1.5654e-01,  2.8636e-01]],\n",
      "\n",
      "         [[-8.0226e-01, -4.7991e-01,  2.7524e-01, -1.9489e-01, -2.7379e-01,\n",
      "            7.8316e-01,  5.5244e-01, -2.5420e-01, -1.4755e-01, -9.5745e-01,\n",
      "           -1.0112e+00, -7.1845e-01, -2.8290e-02,  3.5370e-01,  5.3454e-01,\n",
      "            5.2941e-01,  3.7057e-01, -7.2791e-01,  6.1079e-02,  4.1954e-02,\n",
      "            5.0515e-01,  1.1745e+00,  8.5027e-03,  3.3048e-02, -6.8340e-01,\n",
      "            4.1248e-02, -2.9024e-01, -4.6604e-01, -3.5143e-01,  1.3221e+00,\n",
      "            1.8741e-01,  1.9402e-02],\n",
      "          [ 9.1587e-01, -3.4142e-01,  2.5841e-01,  1.1354e+00,  4.0849e-02,\n",
      "            6.5871e-01, -4.6023e-02,  4.0897e-01, -4.7936e-01,  1.8853e-01,\n",
      "            1.4742e-01, -6.6365e-02,  4.8381e-01,  3.5246e-02, -9.8258e-01,\n",
      "           -6.0048e-01, -3.1253e-02,  4.6423e-01, -5.3340e-01, -9.2424e-01,\n",
      "           -4.9836e-03, -2.8884e-01,  9.8824e-02, -6.3426e-01,  1.8396e-01,\n",
      "           -6.2044e-01, -5.9793e-01, -3.8073e-01,  9.2532e-01, -5.7986e-01,\n",
      "            1.1438e+00, -1.7423e+00],\n",
      "          [ 2.9254e-01, -7.3063e-01,  4.1428e-04,  6.0291e-01,  5.3238e-01,\n",
      "            3.0040e-02, -4.8916e-01,  2.6869e-01, -2.7828e-01,  7.2914e-01,\n",
      "            9.0558e-01,  5.9986e-01,  2.4905e-01, -3.1721e-01, -9.0811e-01,\n",
      "           -5.7759e-01,  1.1027e+00,  6.6816e-01, -6.8323e-01,  1.5417e-01,\n",
      "           -2.9369e-01, -9.8504e-01, -3.0589e-01,  2.0699e-01,  8.9272e-01,\n",
      "            1.8530e-01,  1.3714e-01,  6.4934e-02,  6.1160e-01, -1.1617e+00,\n",
      "            2.6630e-01, -6.2372e-01]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xk: torch.Size([1, 4, 3, 32])\n",
      "tensor([[[[ 6.8222e-01,  3.9745e-01, -4.5185e-01,  6.5456e-01,  2.0551e-01,\n",
      "            3.5685e-01, -5.9438e-01,  1.1071e+00, -5.6912e-02, -3.3377e-01,\n",
      "            2.6295e-02, -7.9261e-01, -2.5859e-01, -1.0458e+00, -3.1726e-01,\n",
      "           -5.7937e-01, -4.3237e-01, -1.0288e+00, -1.1353e-01,  1.1542e+00,\n",
      "            6.3875e-01,  2.3187e-01,  3.2322e-01, -1.3726e-01, -5.9374e-01,\n",
      "           -5.0166e-01,  1.1832e+00,  1.9762e-02,  7.9928e-02,  6.4459e-04,\n",
      "            1.0876e-01,  7.7608e-01],\n",
      "          [-7.6602e-02,  6.1375e-01,  2.0298e-01,  5.6574e-01, -4.2822e-01,\n",
      "            9.1958e-03,  4.1421e-01, -3.7069e-01,  1.3281e+00,  2.5371e-01,\n",
      "            1.6582e-01,  7.9992e-01,  1.3759e+00,  2.3961e-02, -9.1926e-01,\n",
      "            3.0784e-01, -5.5785e-01,  6.1126e-01, -8.1291e-01, -4.6290e-01,\n",
      "           -5.1430e-01, -8.2048e-01, -8.5153e-01,  7.7958e-01,  3.4025e-01,\n",
      "           -9.8431e-02,  2.7694e-01,  8.3010e-01, -2.8762e-01,  9.4317e-01,\n",
      "            7.1939e-01,  6.0456e-01],\n",
      "          [-2.1036e-02, -6.1191e-01,  4.0248e-01,  4.5612e-01, -5.8964e-02,\n",
      "           -1.2792e-01,  7.2754e-01, -1.2962e+00,  4.0305e-01,  2.0447e-01,\n",
      "            2.3701e-01,  7.1621e-01,  6.9675e-01,  9.2756e-01,  7.9022e-02,\n",
      "            8.8817e-01, -7.1394e-01,  1.3173e-01,  7.1257e-02, -1.0432e+00,\n",
      "           -1.0716e+00, -7.7644e-01, -3.4353e-01,  7.3914e-02,  8.2409e-01,\n",
      "            1.4275e-01, -8.2205e-01,  1.6309e-01,  8.4752e-02,  4.3728e-01,\n",
      "           -2.5928e-02, -4.0660e-01]],\n",
      "\n",
      "         [[ 6.8222e-01,  3.9745e-01, -4.5185e-01,  6.5456e-01,  2.0551e-01,\n",
      "            3.5685e-01, -5.9438e-01,  1.1071e+00, -5.6912e-02, -3.3377e-01,\n",
      "            2.6295e-02, -7.9261e-01, -2.5859e-01, -1.0458e+00, -3.1726e-01,\n",
      "           -5.7937e-01, -4.3237e-01, -1.0288e+00, -1.1353e-01,  1.1542e+00,\n",
      "            6.3875e-01,  2.3187e-01,  3.2322e-01, -1.3726e-01, -5.9374e-01,\n",
      "           -5.0166e-01,  1.1832e+00,  1.9762e-02,  7.9928e-02,  6.4459e-04,\n",
      "            1.0876e-01,  7.7608e-01],\n",
      "          [-7.6602e-02,  6.1375e-01,  2.0298e-01,  5.6574e-01, -4.2822e-01,\n",
      "            9.1958e-03,  4.1421e-01, -3.7069e-01,  1.3281e+00,  2.5371e-01,\n",
      "            1.6582e-01,  7.9992e-01,  1.3759e+00,  2.3961e-02, -9.1926e-01,\n",
      "            3.0784e-01, -5.5785e-01,  6.1126e-01, -8.1291e-01, -4.6290e-01,\n",
      "           -5.1430e-01, -8.2048e-01, -8.5153e-01,  7.7958e-01,  3.4025e-01,\n",
      "           -9.8431e-02,  2.7694e-01,  8.3010e-01, -2.8762e-01,  9.4317e-01,\n",
      "            7.1939e-01,  6.0456e-01],\n",
      "          [-2.1036e-02, -6.1191e-01,  4.0248e-01,  4.5612e-01, -5.8964e-02,\n",
      "           -1.2792e-01,  7.2754e-01, -1.2962e+00,  4.0305e-01,  2.0447e-01,\n",
      "            2.3701e-01,  7.1621e-01,  6.9675e-01,  9.2756e-01,  7.9022e-02,\n",
      "            8.8817e-01, -7.1394e-01,  1.3173e-01,  7.1257e-02, -1.0432e+00,\n",
      "           -1.0716e+00, -7.7644e-01, -3.4353e-01,  7.3914e-02,  8.2409e-01,\n",
      "            1.4275e-01, -8.2205e-01,  1.6309e-01,  8.4752e-02,  4.3728e-01,\n",
      "           -2.5928e-02, -4.0660e-01]],\n",
      "\n",
      "         [[ 6.8222e-01,  3.9745e-01, -4.5185e-01,  6.5456e-01,  2.0551e-01,\n",
      "            3.5685e-01, -5.9438e-01,  1.1071e+00, -5.6912e-02, -3.3377e-01,\n",
      "            2.6295e-02, -7.9261e-01, -2.5859e-01, -1.0458e+00, -3.1726e-01,\n",
      "           -5.7937e-01, -4.3237e-01, -1.0288e+00, -1.1353e-01,  1.1542e+00,\n",
      "            6.3875e-01,  2.3187e-01,  3.2322e-01, -1.3726e-01, -5.9374e-01,\n",
      "           -5.0166e-01,  1.1832e+00,  1.9762e-02,  7.9928e-02,  6.4459e-04,\n",
      "            1.0876e-01,  7.7608e-01],\n",
      "          [-7.6602e-02,  6.1375e-01,  2.0298e-01,  5.6574e-01, -4.2822e-01,\n",
      "            9.1958e-03,  4.1421e-01, -3.7069e-01,  1.3281e+00,  2.5371e-01,\n",
      "            1.6582e-01,  7.9992e-01,  1.3759e+00,  2.3961e-02, -9.1926e-01,\n",
      "            3.0784e-01, -5.5785e-01,  6.1126e-01, -8.1291e-01, -4.6290e-01,\n",
      "           -5.1430e-01, -8.2048e-01, -8.5153e-01,  7.7958e-01,  3.4025e-01,\n",
      "           -9.8431e-02,  2.7694e-01,  8.3010e-01, -2.8762e-01,  9.4317e-01,\n",
      "            7.1939e-01,  6.0456e-01],\n",
      "          [-2.1036e-02, -6.1191e-01,  4.0248e-01,  4.5612e-01, -5.8964e-02,\n",
      "           -1.2792e-01,  7.2754e-01, -1.2962e+00,  4.0305e-01,  2.0447e-01,\n",
      "            2.3701e-01,  7.1621e-01,  6.9675e-01,  9.2756e-01,  7.9022e-02,\n",
      "            8.8817e-01, -7.1394e-01,  1.3173e-01,  7.1257e-02, -1.0432e+00,\n",
      "           -1.0716e+00, -7.7644e-01, -3.4353e-01,  7.3914e-02,  8.2409e-01,\n",
      "            1.4275e-01, -8.2205e-01,  1.6309e-01,  8.4752e-02,  4.3728e-01,\n",
      "           -2.5928e-02, -4.0660e-01]],\n",
      "\n",
      "         [[ 6.8222e-01,  3.9745e-01, -4.5185e-01,  6.5456e-01,  2.0551e-01,\n",
      "            3.5685e-01, -5.9438e-01,  1.1071e+00, -5.6912e-02, -3.3377e-01,\n",
      "            2.6295e-02, -7.9261e-01, -2.5859e-01, -1.0458e+00, -3.1726e-01,\n",
      "           -5.7937e-01, -4.3237e-01, -1.0288e+00, -1.1353e-01,  1.1542e+00,\n",
      "            6.3875e-01,  2.3187e-01,  3.2322e-01, -1.3726e-01, -5.9374e-01,\n",
      "           -5.0166e-01,  1.1832e+00,  1.9762e-02,  7.9928e-02,  6.4459e-04,\n",
      "            1.0876e-01,  7.7608e-01],\n",
      "          [-7.6602e-02,  6.1375e-01,  2.0298e-01,  5.6574e-01, -4.2822e-01,\n",
      "            9.1958e-03,  4.1421e-01, -3.7069e-01,  1.3281e+00,  2.5371e-01,\n",
      "            1.6582e-01,  7.9992e-01,  1.3759e+00,  2.3961e-02, -9.1926e-01,\n",
      "            3.0784e-01, -5.5785e-01,  6.1126e-01, -8.1291e-01, -4.6290e-01,\n",
      "           -5.1430e-01, -8.2048e-01, -8.5153e-01,  7.7958e-01,  3.4025e-01,\n",
      "           -9.8431e-02,  2.7694e-01,  8.3010e-01, -2.8762e-01,  9.4317e-01,\n",
      "            7.1939e-01,  6.0456e-01],\n",
      "          [-2.1036e-02, -6.1191e-01,  4.0248e-01,  4.5612e-01, -5.8964e-02,\n",
      "           -1.2792e-01,  7.2754e-01, -1.2962e+00,  4.0305e-01,  2.0447e-01,\n",
      "            2.3701e-01,  7.1621e-01,  6.9675e-01,  9.2756e-01,  7.9022e-02,\n",
      "            8.8817e-01, -7.1394e-01,  1.3173e-01,  7.1257e-02, -1.0432e+00,\n",
      "           -1.0716e+00, -7.7644e-01, -3.4353e-01,  7.3914e-02,  8.2409e-01,\n",
      "            1.4275e-01, -8.2205e-01,  1.6309e-01,  8.4752e-02,  4.3728e-01,\n",
      "           -2.5928e-02, -4.0660e-01]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xv: torch.Size([1, 4, 3, 32])\n",
      "tensor([[[[ 0.5987, -0.3841,  0.4786,  0.5810,  1.0784,  0.1697, -0.8636,\n",
      "            1.3724, -0.3536, -0.8730, -1.0852,  0.7057,  0.9946,  0.7028,\n",
      "           -0.2711, -0.4019, -1.0301,  0.0499,  0.8427, -0.2219,  0.8298,\n",
      "            0.4994, -0.7727, -0.0958,  0.3657,  0.2425, -0.9471,  1.3435,\n",
      "            0.0705, -0.5323,  0.5525, -0.1848],\n",
      "          [-0.2549, -0.2895, -1.1672, -0.1025, -0.3644,  0.4813,  0.6517,\n",
      "           -0.0575,  0.5576,  0.1857,  0.5378,  0.2325, -0.2280,  0.1887,\n",
      "            0.9023, -0.3956,  0.0015,  0.1293,  0.2297, -0.4635,  0.0398,\n",
      "           -0.4825, -0.6156,  0.1028, -1.1649,  0.1823,  0.0523, -0.1966,\n",
      "           -0.8978, -0.3087, -0.4732,  0.4109],\n",
      "          [-0.4979,  0.3207, -0.7325, -0.3093, -0.9898,  0.1774,  0.8524,\n",
      "           -1.1100,  0.7149,  1.0180,  1.0908, -0.6023, -0.7431, -0.4319,\n",
      "            0.6161,  0.1277,  1.1899, -0.1046, -0.5805,  0.0380, -0.5432,\n",
      "           -0.4837,  0.2379,  0.1855, -0.7020, -0.1552,  0.9174, -0.8761,\n",
      "           -0.2240,  0.2584, -0.4355,  0.3477]],\n",
      "\n",
      "         [[ 0.5987, -0.3841,  0.4786,  0.5810,  1.0784,  0.1697, -0.8636,\n",
      "            1.3724, -0.3536, -0.8730, -1.0852,  0.7057,  0.9946,  0.7028,\n",
      "           -0.2711, -0.4019, -1.0301,  0.0499,  0.8427, -0.2219,  0.8298,\n",
      "            0.4994, -0.7727, -0.0958,  0.3657,  0.2425, -0.9471,  1.3435,\n",
      "            0.0705, -0.5323,  0.5525, -0.1848],\n",
      "          [-0.2549, -0.2895, -1.1672, -0.1025, -0.3644,  0.4813,  0.6517,\n",
      "           -0.0575,  0.5576,  0.1857,  0.5378,  0.2325, -0.2280,  0.1887,\n",
      "            0.9023, -0.3956,  0.0015,  0.1293,  0.2297, -0.4635,  0.0398,\n",
      "           -0.4825, -0.6156,  0.1028, -1.1649,  0.1823,  0.0523, -0.1966,\n",
      "           -0.8978, -0.3087, -0.4732,  0.4109],\n",
      "          [-0.4979,  0.3207, -0.7325, -0.3093, -0.9898,  0.1774,  0.8524,\n",
      "           -1.1100,  0.7149,  1.0180,  1.0908, -0.6023, -0.7431, -0.4319,\n",
      "            0.6161,  0.1277,  1.1899, -0.1046, -0.5805,  0.0380, -0.5432,\n",
      "           -0.4837,  0.2379,  0.1855, -0.7020, -0.1552,  0.9174, -0.8761,\n",
      "           -0.2240,  0.2584, -0.4355,  0.3477]],\n",
      "\n",
      "         [[ 0.5987, -0.3841,  0.4786,  0.5810,  1.0784,  0.1697, -0.8636,\n",
      "            1.3724, -0.3536, -0.8730, -1.0852,  0.7057,  0.9946,  0.7028,\n",
      "           -0.2711, -0.4019, -1.0301,  0.0499,  0.8427, -0.2219,  0.8298,\n",
      "            0.4994, -0.7727, -0.0958,  0.3657,  0.2425, -0.9471,  1.3435,\n",
      "            0.0705, -0.5323,  0.5525, -0.1848],\n",
      "          [-0.2549, -0.2895, -1.1672, -0.1025, -0.3644,  0.4813,  0.6517,\n",
      "           -0.0575,  0.5576,  0.1857,  0.5378,  0.2325, -0.2280,  0.1887,\n",
      "            0.9023, -0.3956,  0.0015,  0.1293,  0.2297, -0.4635,  0.0398,\n",
      "           -0.4825, -0.6156,  0.1028, -1.1649,  0.1823,  0.0523, -0.1966,\n",
      "           -0.8978, -0.3087, -0.4732,  0.4109],\n",
      "          [-0.4979,  0.3207, -0.7325, -0.3093, -0.9898,  0.1774,  0.8524,\n",
      "           -1.1100,  0.7149,  1.0180,  1.0908, -0.6023, -0.7431, -0.4319,\n",
      "            0.6161,  0.1277,  1.1899, -0.1046, -0.5805,  0.0380, -0.5432,\n",
      "           -0.4837,  0.2379,  0.1855, -0.7020, -0.1552,  0.9174, -0.8761,\n",
      "           -0.2240,  0.2584, -0.4355,  0.3477]],\n",
      "\n",
      "         [[ 0.5987, -0.3841,  0.4786,  0.5810,  1.0784,  0.1697, -0.8636,\n",
      "            1.3724, -0.3536, -0.8730, -1.0852,  0.7057,  0.9946,  0.7028,\n",
      "           -0.2711, -0.4019, -1.0301,  0.0499,  0.8427, -0.2219,  0.8298,\n",
      "            0.4994, -0.7727, -0.0958,  0.3657,  0.2425, -0.9471,  1.3435,\n",
      "            0.0705, -0.5323,  0.5525, -0.1848],\n",
      "          [-0.2549, -0.2895, -1.1672, -0.1025, -0.3644,  0.4813,  0.6517,\n",
      "           -0.0575,  0.5576,  0.1857,  0.5378,  0.2325, -0.2280,  0.1887,\n",
      "            0.9023, -0.3956,  0.0015,  0.1293,  0.2297, -0.4635,  0.0398,\n",
      "           -0.4825, -0.6156,  0.1028, -1.1649,  0.1823,  0.0523, -0.1966,\n",
      "           -0.8978, -0.3087, -0.4732,  0.4109],\n",
      "          [-0.4979,  0.3207, -0.7325, -0.3093, -0.9898,  0.1774,  0.8524,\n",
      "           -1.1100,  0.7149,  1.0180,  1.0908, -0.6023, -0.7431, -0.4319,\n",
      "            0.6161,  0.1277,  1.1899, -0.1046, -0.5805,  0.0380, -0.5432,\n",
      "           -0.4837,  0.2379,  0.1855, -0.7020, -0.1552,  0.9174, -0.8761,\n",
      "           -0.2240,  0.2584, -0.4355,  0.3477]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "scores: torch.Size([1, 4, 3, 3])\n",
      "tensor([[[[-0.1296, -0.0518,  0.0233],\n",
      "          [-0.2016, -1.0221,  0.0433],\n",
      "          [ 0.1086, -0.1792, -0.1866]],\n",
      "\n",
      "         [[-0.1096, -0.3296, -0.0517],\n",
      "          [-0.2563,  0.2502,  0.2570],\n",
      "          [-0.3941,  0.5054,  0.4029]],\n",
      "\n",
      "         [[-0.0258, -0.0046,  0.2276],\n",
      "          [ 0.0394, -0.0318, -0.4680],\n",
      "          [ 0.0077, -0.0943, -0.2922]],\n",
      "\n",
      "         [[-0.0143, -0.4446, -0.1629],\n",
      "          [-0.1292,  0.0747,  0.3372],\n",
      "          [-0.1937,  0.2398,  0.1227]]]], grad_fn=<MulBackward0>)\n",
      "mask: torch.Size([1, 1, 3, 3])\n",
      "tensor([[[[ 0.0000e+00, -2.3820e+38, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]]]])\n",
      "masked scores: torch.Size([1, 4, 3, 3])\n",
      "tensor([[[[-1.2955e-01, -2.3820e+38, -2.3820e+38],\n",
      "          [-2.0162e-01, -1.0221e+00, -2.3820e+38],\n",
      "          [ 1.0858e-01, -1.7916e-01, -1.8657e-01]],\n",
      "\n",
      "         [[-1.0957e-01, -2.3820e+38, -2.3820e+38],\n",
      "          [-2.5632e-01,  2.5025e-01, -2.3820e+38],\n",
      "          [-3.9408e-01,  5.0537e-01,  4.0294e-01]],\n",
      "\n",
      "         [[-2.5832e-02, -2.3820e+38, -2.3820e+38],\n",
      "          [ 3.9381e-02, -3.1831e-02, -2.3820e+38],\n",
      "          [ 7.6987e-03, -9.4310e-02, -2.9219e-01]],\n",
      "\n",
      "         [[-1.4252e-02, -2.3820e+38, -2.3820e+38],\n",
      "          [-1.2923e-01,  7.4693e-02, -2.3820e+38],\n",
      "          [-1.9366e-01,  2.3981e-01,  1.2270e-01]]]], grad_fn=<AddBackward0>)\n",
      "softmaxed scores: torch.Size([1, 4, 3, 3])\n",
      "tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [0.6943, 0.3057, 0.0000],\n",
      "          [0.4009, 0.3007, 0.2984]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.3760, 0.6240, 0.0000],\n",
      "          [0.1761, 0.4330, 0.3908]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.5178, 0.4822, 0.0000],\n",
      "          [0.3782, 0.3415, 0.2802]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.4492, 0.5508, 0.0000],\n",
      "          [0.2554, 0.3941, 0.3505]]]], grad_fn=<SoftmaxBackward0>)\n",
      "attention: torch.Size([1, 4, 3, 32])\n",
      "tensor([[[[ 5.9874e-01, -3.8406e-01,  4.7865e-01,  5.8100e-01,  1.0784e+00,\n",
      "            1.6966e-01, -8.6358e-01,  1.3724e+00, -3.5363e-01, -8.7299e-01,\n",
      "           -1.0852e+00,  7.0575e-01,  9.9460e-01,  7.0276e-01, -2.7110e-01,\n",
      "           -4.0191e-01, -1.0301e+00,  4.9934e-02,  8.4266e-01, -2.2191e-01,\n",
      "            8.2980e-01,  4.9940e-01, -7.7271e-01, -9.5784e-02,  3.6566e-01,\n",
      "            2.4249e-01, -9.4712e-01,  1.3435e+00,  7.0548e-02, -5.3228e-01,\n",
      "            5.5246e-01, -1.8482e-01],\n",
      "          [ 3.3783e-01, -3.5516e-01, -2.4415e-02,  3.7208e-01,  6.3738e-01,\n",
      "            2.6493e-01, -4.0041e-01,  9.3531e-01, -7.5115e-02, -5.4938e-01,\n",
      "           -5.8914e-01,  5.6108e-01,  6.2090e-01,  5.4564e-01,  8.7572e-02,\n",
      "           -3.9998e-01, -7.1476e-01,  7.4197e-02,  6.5530e-01, -2.9576e-01,\n",
      "            5.8832e-01,  1.9929e-01, -7.2469e-01, -3.5096e-02, -1.0218e-01,\n",
      "            2.2410e-01, -6.4165e-01,  8.7277e-01, -2.2544e-01, -4.6393e-01,\n",
      "            2.3896e-01, -2.7163e-03],\n",
      "          [ 1.4806e-02, -1.4531e-01, -3.7764e-01,  1.0981e-01,  2.7358e-02,\n",
      "            2.6570e-01,  1.0413e-01,  2.0162e-01,  2.3923e-01,  9.6752e-03,\n",
      "            5.2172e-02,  1.7307e-01,  1.0839e-01,  2.0958e-01,  3.4649e-01,\n",
      "           -2.4195e-01, -5.7387e-02,  2.7676e-02,  2.3364e-01, -2.1699e-01,\n",
      "            1.8250e-01, -8.9188e-02, -4.2388e-01,  4.7857e-02, -4.1315e-01,\n",
      "            1.0571e-01, -9.0195e-02,  2.1805e-01, -3.0852e-01, -2.2908e-01,\n",
      "           -5.0770e-02,  1.5324e-01]],\n",
      "\n",
      "         [[ 5.9874e-01, -3.8406e-01,  4.7865e-01,  5.8100e-01,  1.0784e+00,\n",
      "            1.6966e-01, -8.6358e-01,  1.3724e+00, -3.5363e-01, -8.7299e-01,\n",
      "           -1.0852e+00,  7.0575e-01,  9.9460e-01,  7.0276e-01, -2.7110e-01,\n",
      "           -4.0191e-01, -1.0301e+00,  4.9934e-02,  8.4266e-01, -2.2191e-01,\n",
      "            8.2980e-01,  4.9940e-01, -7.7271e-01, -9.5784e-02,  3.6566e-01,\n",
      "            2.4249e-01, -9.4712e-01,  1.3435e+00,  7.0548e-02, -5.3228e-01,\n",
      "            5.5246e-01, -1.8482e-01],\n",
      "          [ 6.6078e-02, -3.2506e-01, -5.4836e-01,  1.5450e-01,  1.7808e-01,\n",
      "            3.6415e-01,  8.1985e-02,  4.8012e-01,  2.1496e-01, -2.1234e-01,\n",
      "           -7.2446e-02,  4.1042e-01,  2.3168e-01,  3.8200e-01,  4.6113e-01,\n",
      "           -3.9797e-01, -3.8637e-01,  9.9467e-02,  4.6016e-01, -3.7268e-01,\n",
      "            3.3682e-01, -1.1328e-01, -6.7468e-01,  2.8110e-02, -5.8943e-01,\n",
      "            2.0495e-01, -3.2350e-01,  3.8251e-01, -5.3372e-01, -3.9274e-01,\n",
      "           -8.7552e-02,  1.8694e-01],\n",
      "          [-1.9951e-01, -6.7675e-02, -7.0738e-01, -6.2913e-02, -3.5471e-01,\n",
      "            3.0767e-01,  4.6325e-01, -2.1701e-01,  4.5857e-01,  3.2454e-01,\n",
      "            4.6807e-01, -1.0452e-02, -2.1400e-01,  3.6688e-02,  5.8378e-01,\n",
      "           -1.9217e-01,  2.8428e-01,  2.3899e-02,  2.1006e-02, -2.2495e-01,\n",
      "           -4.8932e-02, -3.0998e-01, -3.0971e-01,  1.0013e-01, -7.1438e-01,\n",
      "            6.0994e-02,  2.1437e-01, -1.9089e-01, -4.6390e-01, -1.2642e-01,\n",
      "           -2.7781e-01,  2.8130e-01]],\n",
      "\n",
      "         [[ 5.9874e-01, -3.8406e-01,  4.7865e-01,  5.8100e-01,  1.0784e+00,\n",
      "            1.6966e-01, -8.6358e-01,  1.3724e+00, -3.5363e-01, -8.7299e-01,\n",
      "           -1.0852e+00,  7.0575e-01,  9.9460e-01,  7.0276e-01, -2.7110e-01,\n",
      "           -4.0191e-01, -1.0301e+00,  4.9934e-02,  8.4266e-01, -2.2191e-01,\n",
      "            8.2980e-01,  4.9940e-01, -7.7271e-01, -9.5784e-02,  3.6566e-01,\n",
      "            2.4249e-01, -9.4712e-01,  1.3435e+00,  7.0548e-02, -5.3228e-01,\n",
      "            5.5246e-01, -1.8482e-01],\n",
      "          [ 1.8712e-01, -3.3847e-01, -3.1498e-01,  2.5141e-01,  3.8267e-01,\n",
      "            3.1996e-01, -1.3288e-01,  6.8287e-01,  8.5755e-02, -3.6247e-01,\n",
      "           -3.0259e-01,  4.7753e-01,  4.0504e-01,  4.5489e-01,  2.9474e-01,\n",
      "           -3.9887e-01, -5.3264e-01,  8.8211e-02,  5.4708e-01, -3.3842e-01,\n",
      "            4.4884e-01,  2.5945e-02, -6.9696e-01, -4.3042e-05, -3.7240e-01,\n",
      "            2.1348e-01, -4.6521e-01,  6.0088e-01, -3.9640e-01, -4.2445e-01,\n",
      "            5.7882e-02,  1.0246e-01],\n",
      "          [-1.2298e-04, -1.5428e-01, -4.2287e-01,  9.8081e-02,  6.0356e-03,\n",
      "            2.7830e-01,  1.3483e-01,  1.8837e-01,  2.5702e-01,  1.8523e-02,\n",
      "            7.8902e-02,  1.7754e-01,  9.0055e-02,  2.0922e-01,  3.7831e-01,\n",
      "           -2.5134e-01, -5.5642e-02,  3.3736e-02,  2.3450e-01, -2.3160e-01,\n",
      "            1.7521e-01, -1.1143e-01, -4.3587e-01,  5.0852e-02, -4.5628e-01,\n",
      "            1.1049e-01, -8.3293e-02,  1.9551e-01, -3.4274e-01, -2.3434e-01,\n",
      "           -7.4711e-02,  1.6790e-01]],\n",
      "\n",
      "         [[ 5.9874e-01, -3.8406e-01,  4.7865e-01,  5.8100e-01,  1.0784e+00,\n",
      "            1.6966e-01, -8.6358e-01,  1.3724e+00, -3.5363e-01, -8.7299e-01,\n",
      "           -1.0852e+00,  7.0575e-01,  9.9460e-01,  7.0276e-01, -2.7110e-01,\n",
      "           -4.0191e-01, -1.0301e+00,  4.9934e-02,  8.4266e-01, -2.2191e-01,\n",
      "            8.2980e-01,  4.9940e-01, -7.7271e-01, -9.5784e-02,  3.6566e-01,\n",
      "            2.4249e-01, -9.4712e-01,  1.3435e+00,  7.0548e-02, -5.3228e-01,\n",
      "            5.5246e-01, -1.8482e-01],\n",
      "          [ 1.2856e-01, -3.3198e-01, -4.2788e-01,  2.0453e-01,  2.8369e-01,\n",
      "            3.4134e-01, -2.8933e-02,  5.8478e-01,  1.4826e-01, -2.8984e-01,\n",
      "           -1.9125e-01,  4.4506e-01,  3.2117e-01,  4.1963e-01,  3.7524e-01,\n",
      "           -3.9843e-01, -4.6188e-01,  9.3656e-02,  5.0503e-01, -3.5499e-01,\n",
      "            3.9465e-01, -4.1409e-02, -6.8618e-01,  1.3577e-02, -4.7740e-01,\n",
      "            2.0935e-01, -3.9666e-01,  4.9524e-01, -4.6283e-01, -4.0911e-01,\n",
      "           -1.2477e-02,  1.4333e-01],\n",
      "          [-1.2201e-01, -9.9791e-02, -5.9440e-01, -3.6842e-04, -2.1506e-01,\n",
      "            2.9521e-01,  3.3499e-01, -6.1154e-02,  3.7996e-01,  2.0700e-01,\n",
      "            3.1705e-01,  6.0762e-02, -9.6260e-02,  1.0249e-01,  5.0227e-01,\n",
      "           -2.1379e-01,  1.5453e-01,  2.7043e-02,  1.0230e-01, -2.2602e-01,\n",
      "            3.7240e-02, -2.3207e-01, -3.5660e-01,  8.1044e-02, -6.1168e-01,\n",
      "            7.9384e-02,  1.0021e-01, -4.1340e-02, -4.1429e-01, -1.6703e-01,\n",
      "           -1.9799e-01,  2.3661e-01]]]], grad_fn=<UnsafeViewBackward0>)\n",
      "reshaped attention: torch.Size([1, 3, 128])\n",
      "tensor([[[ 5.9874e-01, -3.8406e-01,  4.7865e-01,  5.8100e-01,  1.0784e+00,\n",
      "           1.6966e-01, -8.6358e-01,  1.3724e+00, -3.5363e-01, -8.7299e-01,\n",
      "          -1.0852e+00,  7.0575e-01,  9.9460e-01,  7.0276e-01, -2.7110e-01,\n",
      "          -4.0191e-01, -1.0301e+00,  4.9934e-02,  8.4266e-01, -2.2191e-01,\n",
      "           8.2980e-01,  4.9940e-01, -7.7271e-01, -9.5784e-02,  3.6566e-01,\n",
      "           2.4249e-01, -9.4712e-01,  1.3435e+00,  7.0548e-02, -5.3228e-01,\n",
      "           5.5246e-01, -1.8482e-01,  5.9874e-01, -3.8406e-01,  4.7865e-01,\n",
      "           5.8100e-01,  1.0784e+00,  1.6966e-01, -8.6358e-01,  1.3724e+00,\n",
      "          -3.5363e-01, -8.7299e-01, -1.0852e+00,  7.0575e-01,  9.9460e-01,\n",
      "           7.0276e-01, -2.7110e-01, -4.0191e-01, -1.0301e+00,  4.9934e-02,\n",
      "           8.4266e-01, -2.2191e-01,  8.2980e-01,  4.9940e-01, -7.7271e-01,\n",
      "          -9.5784e-02,  3.6566e-01,  2.4249e-01, -9.4712e-01,  1.3435e+00,\n",
      "           7.0548e-02, -5.3228e-01,  5.5246e-01, -1.8482e-01,  5.9874e-01,\n",
      "          -3.8406e-01,  4.7865e-01,  5.8100e-01,  1.0784e+00,  1.6966e-01,\n",
      "          -8.6358e-01,  1.3724e+00, -3.5363e-01, -8.7299e-01, -1.0852e+00,\n",
      "           7.0575e-01,  9.9460e-01,  7.0276e-01, -2.7110e-01, -4.0191e-01,\n",
      "          -1.0301e+00,  4.9934e-02,  8.4266e-01, -2.2191e-01,  8.2980e-01,\n",
      "           4.9940e-01, -7.7271e-01, -9.5784e-02,  3.6566e-01,  2.4249e-01,\n",
      "          -9.4712e-01,  1.3435e+00,  7.0548e-02, -5.3228e-01,  5.5246e-01,\n",
      "          -1.8482e-01,  5.9874e-01, -3.8406e-01,  4.7865e-01,  5.8100e-01,\n",
      "           1.0784e+00,  1.6966e-01, -8.6358e-01,  1.3724e+00, -3.5363e-01,\n",
      "          -8.7299e-01, -1.0852e+00,  7.0575e-01,  9.9460e-01,  7.0276e-01,\n",
      "          -2.7110e-01, -4.0191e-01, -1.0301e+00,  4.9934e-02,  8.4266e-01,\n",
      "          -2.2191e-01,  8.2980e-01,  4.9940e-01, -7.7271e-01, -9.5784e-02,\n",
      "           3.6566e-01,  2.4249e-01, -9.4712e-01,  1.3435e+00,  7.0548e-02,\n",
      "          -5.3228e-01,  5.5246e-01, -1.8482e-01],\n",
      "         [ 3.3783e-01, -3.5516e-01, -2.4415e-02,  3.7208e-01,  6.3738e-01,\n",
      "           2.6493e-01, -4.0041e-01,  9.3531e-01, -7.5115e-02, -5.4938e-01,\n",
      "          -5.8914e-01,  5.6108e-01,  6.2090e-01,  5.4564e-01,  8.7572e-02,\n",
      "          -3.9998e-01, -7.1476e-01,  7.4197e-02,  6.5530e-01, -2.9576e-01,\n",
      "           5.8832e-01,  1.9929e-01, -7.2469e-01, -3.5096e-02, -1.0218e-01,\n",
      "           2.2410e-01, -6.4165e-01,  8.7277e-01, -2.2544e-01, -4.6393e-01,\n",
      "           2.3896e-01, -2.7163e-03,  6.6078e-02, -3.2506e-01, -5.4836e-01,\n",
      "           1.5450e-01,  1.7808e-01,  3.6415e-01,  8.1985e-02,  4.8012e-01,\n",
      "           2.1496e-01, -2.1234e-01, -7.2446e-02,  4.1042e-01,  2.3168e-01,\n",
      "           3.8200e-01,  4.6113e-01, -3.9797e-01, -3.8637e-01,  9.9467e-02,\n",
      "           4.6016e-01, -3.7268e-01,  3.3682e-01, -1.1328e-01, -6.7468e-01,\n",
      "           2.8110e-02, -5.8943e-01,  2.0495e-01, -3.2350e-01,  3.8251e-01,\n",
      "          -5.3372e-01, -3.9274e-01, -8.7552e-02,  1.8694e-01,  1.8712e-01,\n",
      "          -3.3847e-01, -3.1498e-01,  2.5141e-01,  3.8267e-01,  3.1996e-01,\n",
      "          -1.3288e-01,  6.8287e-01,  8.5755e-02, -3.6247e-01, -3.0259e-01,\n",
      "           4.7753e-01,  4.0504e-01,  4.5489e-01,  2.9474e-01, -3.9887e-01,\n",
      "          -5.3264e-01,  8.8211e-02,  5.4708e-01, -3.3842e-01,  4.4884e-01,\n",
      "           2.5945e-02, -6.9696e-01, -4.3042e-05, -3.7240e-01,  2.1348e-01,\n",
      "          -4.6521e-01,  6.0088e-01, -3.9640e-01, -4.2445e-01,  5.7882e-02,\n",
      "           1.0246e-01,  1.2856e-01, -3.3198e-01, -4.2788e-01,  2.0453e-01,\n",
      "           2.8369e-01,  3.4134e-01, -2.8933e-02,  5.8478e-01,  1.4826e-01,\n",
      "          -2.8984e-01, -1.9125e-01,  4.4506e-01,  3.2117e-01,  4.1963e-01,\n",
      "           3.7524e-01, -3.9843e-01, -4.6188e-01,  9.3656e-02,  5.0503e-01,\n",
      "          -3.5499e-01,  3.9465e-01, -4.1409e-02, -6.8618e-01,  1.3577e-02,\n",
      "          -4.7740e-01,  2.0935e-01, -3.9666e-01,  4.9524e-01, -4.6283e-01,\n",
      "          -4.0911e-01, -1.2477e-02,  1.4333e-01],\n",
      "         [ 1.4806e-02, -1.4531e-01, -3.7764e-01,  1.0981e-01,  2.7358e-02,\n",
      "           2.6570e-01,  1.0413e-01,  2.0162e-01,  2.3923e-01,  9.6752e-03,\n",
      "           5.2172e-02,  1.7307e-01,  1.0839e-01,  2.0958e-01,  3.4649e-01,\n",
      "          -2.4195e-01, -5.7387e-02,  2.7676e-02,  2.3364e-01, -2.1699e-01,\n",
      "           1.8250e-01, -8.9188e-02, -4.2388e-01,  4.7857e-02, -4.1315e-01,\n",
      "           1.0571e-01, -9.0195e-02,  2.1805e-01, -3.0852e-01, -2.2908e-01,\n",
      "          -5.0770e-02,  1.5324e-01, -1.9951e-01, -6.7675e-02, -7.0738e-01,\n",
      "          -6.2913e-02, -3.5471e-01,  3.0767e-01,  4.6325e-01, -2.1701e-01,\n",
      "           4.5857e-01,  3.2454e-01,  4.6807e-01, -1.0452e-02, -2.1400e-01,\n",
      "           3.6688e-02,  5.8378e-01, -1.9217e-01,  2.8428e-01,  2.3899e-02,\n",
      "           2.1006e-02, -2.2495e-01, -4.8932e-02, -3.0998e-01, -3.0971e-01,\n",
      "           1.0013e-01, -7.1438e-01,  6.0994e-02,  2.1437e-01, -1.9089e-01,\n",
      "          -4.6390e-01, -1.2642e-01, -2.7781e-01,  2.8130e-01, -1.2298e-04,\n",
      "          -1.5428e-01, -4.2287e-01,  9.8081e-02,  6.0356e-03,  2.7830e-01,\n",
      "           1.3483e-01,  1.8837e-01,  2.5702e-01,  1.8523e-02,  7.8902e-02,\n",
      "           1.7754e-01,  9.0055e-02,  2.0922e-01,  3.7831e-01, -2.5134e-01,\n",
      "          -5.5642e-02,  3.3736e-02,  2.3450e-01, -2.3160e-01,  1.7521e-01,\n",
      "          -1.1143e-01, -4.3587e-01,  5.0852e-02, -4.5628e-01,  1.1049e-01,\n",
      "          -8.3293e-02,  1.9551e-01, -3.4274e-01, -2.3434e-01, -7.4711e-02,\n",
      "           1.6790e-01, -1.2201e-01, -9.9791e-02, -5.9440e-01, -3.6842e-04,\n",
      "          -2.1506e-01,  2.9521e-01,  3.3499e-01, -6.1154e-02,  3.7996e-01,\n",
      "           2.0700e-01,  3.1705e-01,  6.0762e-02, -9.6260e-02,  1.0249e-01,\n",
      "           5.0227e-01, -2.1379e-01,  1.5453e-01,  2.7043e-02,  1.0230e-01,\n",
      "          -2.2602e-01,  3.7240e-02, -2.3207e-01, -3.5660e-01,  8.1044e-02,\n",
      "          -6.1168e-01,  7.9384e-02,  1.0021e-01, -4.1340e-02, -4.1429e-01,\n",
      "          -1.6703e-01, -1.9799e-01,  2.3661e-01]]], grad_fn=<ViewBackward0>)\n",
      "self.Wo: torch.Size([128, 4])\n",
      "Parameter containing:\n",
      "tensor([[-0.0079,  0.0757,  0.0749,  0.0462],\n",
      "        [ 0.0460, -0.0546,  0.0502, -0.0218],\n",
      "        [-0.0014, -0.0434, -0.0075,  0.0641],\n",
      "        [ 0.0671, -0.0485, -0.0218,  0.0060],\n",
      "        [ 0.0309,  0.0527,  0.0630, -0.0393],\n",
      "        [ 0.0365, -0.0782,  0.0736, -0.0832],\n",
      "        [-0.0593,  0.0654, -0.0045, -0.0195],\n",
      "        [-0.0409,  0.0541, -0.0687,  0.0108],\n",
      "        [ 0.0058,  0.0848,  0.0145,  0.0689],\n",
      "        [ 0.0026, -0.0821,  0.0007, -0.0826],\n",
      "        [ 0.0520,  0.0327, -0.0099,  0.0035],\n",
      "        [-0.0113, -0.0376, -0.0707,  0.0649],\n",
      "        [ 0.0840,  0.0234, -0.0376,  0.0552],\n",
      "        [ 0.0851, -0.0157,  0.0635,  0.0348],\n",
      "        [-0.0805,  0.0577, -0.0686,  0.0417],\n",
      "        [ 0.0427, -0.0689, -0.0836, -0.0075],\n",
      "        [-0.0264,  0.0699,  0.0436, -0.0784],\n",
      "        [-0.0033,  0.0120, -0.0278,  0.0845],\n",
      "        [ 0.0211,  0.0219, -0.0064,  0.0622],\n",
      "        [ 0.0116, -0.0601, -0.0351,  0.0438],\n",
      "        [ 0.0529, -0.0174,  0.0657, -0.0547],\n",
      "        [ 0.0491,  0.0206,  0.0715,  0.0095],\n",
      "        [-0.0055, -0.0811,  0.0734, -0.0154],\n",
      "        [-0.0363,  0.0509,  0.0742,  0.0262],\n",
      "        [-0.0246, -0.0447,  0.0577, -0.0400],\n",
      "        [ 0.0389,  0.0033,  0.0518, -0.0246],\n",
      "        [-0.0094,  0.0168,  0.0019,  0.0794],\n",
      "        [ 0.0543, -0.0045, -0.0197, -0.0204],\n",
      "        [ 0.0335,  0.0870, -0.0398,  0.0823],\n",
      "        [-0.0363,  0.0104, -0.0642, -0.0105],\n",
      "        [ 0.0579,  0.0792,  0.0132, -0.0749],\n",
      "        [-0.0677,  0.0264,  0.0142,  0.0011],\n",
      "        [ 0.0827,  0.0024, -0.0406,  0.0068],\n",
      "        [-0.0444, -0.0071, -0.0194,  0.0284],\n",
      "        [ 0.0008, -0.0484, -0.0254,  0.0153],\n",
      "        [-0.0116,  0.0578,  0.0749, -0.0239],\n",
      "        [ 0.0216, -0.0516, -0.0301, -0.0757],\n",
      "        [-0.0307,  0.0636, -0.0593, -0.0068],\n",
      "        [-0.0570, -0.0767, -0.0239, -0.0010],\n",
      "        [ 0.0309, -0.0854,  0.0383,  0.0767],\n",
      "        [-0.0815, -0.0579,  0.0037,  0.0585],\n",
      "        [-0.0627, -0.0090, -0.0607,  0.0304],\n",
      "        [-0.0340, -0.0255, -0.0437,  0.0423],\n",
      "        [-0.0613, -0.0223, -0.0705,  0.0365],\n",
      "        [-0.0433, -0.0027, -0.0013,  0.0185],\n",
      "        [-0.0285,  0.0573,  0.0227,  0.0772],\n",
      "        [ 0.0165,  0.0765,  0.0751, -0.0699],\n",
      "        [-0.0342, -0.0433,  0.0554, -0.0370],\n",
      "        [-0.0358, -0.0385,  0.0709, -0.0257],\n",
      "        [ 0.0039,  0.0320, -0.0702, -0.0271],\n",
      "        [-0.0589, -0.0542, -0.0375, -0.0203],\n",
      "        [-0.0016, -0.0322,  0.0168,  0.0031],\n",
      "        [ 0.0460,  0.0677, -0.0761, -0.0260],\n",
      "        [ 0.0072,  0.0692, -0.0434,  0.0824],\n",
      "        [-0.0196, -0.0401, -0.0728, -0.0267],\n",
      "        [-0.0574,  0.0809,  0.0224,  0.0883],\n",
      "        [-0.0287,  0.0293, -0.0042,  0.0625],\n",
      "        [-0.0607,  0.0780, -0.0487, -0.0470],\n",
      "        [ 0.0747,  0.0287,  0.0232, -0.0343],\n",
      "        [-0.0560,  0.0061, -0.0046,  0.0253],\n",
      "        [-0.0705, -0.0466, -0.0435,  0.0327],\n",
      "        [-0.0161,  0.0762, -0.0001, -0.0089],\n",
      "        [ 0.0236,  0.0732,  0.0577, -0.0353],\n",
      "        [ 0.0087, -0.0584,  0.0357, -0.0773],\n",
      "        [-0.0670, -0.0439, -0.0023,  0.0128],\n",
      "        [ 0.0493,  0.0366,  0.0490,  0.0567],\n",
      "        [ 0.0221,  0.0582, -0.0552, -0.0022],\n",
      "        [ 0.0296,  0.0399,  0.0145, -0.0225],\n",
      "        [ 0.0603, -0.0110, -0.0349,  0.0332],\n",
      "        [-0.0803, -0.0851,  0.0214, -0.0223],\n",
      "        [-0.0452,  0.0069,  0.0462, -0.0463],\n",
      "        [-0.0643, -0.0237, -0.0822,  0.0790],\n",
      "        [ 0.0303,  0.0251, -0.0693,  0.0408],\n",
      "        [ 0.0185,  0.0506,  0.0176,  0.0865],\n",
      "        [-0.0782, -0.0380, -0.0346, -0.0512],\n",
      "        [-0.0452, -0.0101,  0.0294,  0.0438],\n",
      "        [-0.0496,  0.0350,  0.0099,  0.0243],\n",
      "        [-0.0701, -0.0437, -0.0308,  0.0103],\n",
      "        [-0.0842, -0.0148, -0.0684, -0.0078],\n",
      "        [-0.0701, -0.0414,  0.0810, -0.0062],\n",
      "        [-0.0146,  0.0005,  0.0533,  0.0344],\n",
      "        [ 0.0085,  0.0402,  0.0674, -0.0017],\n",
      "        [-0.0191, -0.0351,  0.0825,  0.0132],\n",
      "        [-0.0615,  0.0387,  0.0302, -0.0637],\n",
      "        [ 0.0858, -0.0366,  0.0106,  0.0591],\n",
      "        [-0.0817, -0.0733, -0.0238,  0.0241],\n",
      "        [-0.0592, -0.0499, -0.0003,  0.0668],\n",
      "        [-0.0703, -0.0642,  0.0084, -0.0529],\n",
      "        [ 0.0316, -0.0261, -0.0091,  0.0567],\n",
      "        [-0.0310, -0.0424,  0.0673,  0.0006],\n",
      "        [-0.0438, -0.0581,  0.0070,  0.0240],\n",
      "        [ 0.0063, -0.0861, -0.0742,  0.0079],\n",
      "        [-0.0030,  0.0009,  0.0563, -0.0348],\n",
      "        [ 0.0190,  0.0472, -0.0052, -0.0382],\n",
      "        [-0.0308, -0.0811, -0.0136,  0.0697],\n",
      "        [ 0.0380, -0.0230, -0.0244, -0.0466],\n",
      "        [-0.0628, -0.0160, -0.0716,  0.0577],\n",
      "        [ 0.0544, -0.0765,  0.0275, -0.0681],\n",
      "        [ 0.0318,  0.0341,  0.0013, -0.0111],\n",
      "        [-0.0799,  0.0441, -0.0169, -0.0087],\n",
      "        [-0.0725,  0.0369, -0.0701,  0.0023],\n",
      "        [ 0.0854, -0.0224, -0.0534, -0.0877],\n",
      "        [ 0.0283, -0.0878, -0.0075, -0.0721],\n",
      "        [-0.0322,  0.0820,  0.0631,  0.0723],\n",
      "        [ 0.0064,  0.0323,  0.0624,  0.0100],\n",
      "        [ 0.0456,  0.0870, -0.0688,  0.0531],\n",
      "        [ 0.0388, -0.0071,  0.0018, -0.0191],\n",
      "        [ 0.0644, -0.0875, -0.0701,  0.0224],\n",
      "        [-0.0434, -0.0629, -0.0451, -0.0093],\n",
      "        [ 0.0819,  0.0873,  0.0458, -0.0539],\n",
      "        [ 0.0413, -0.0432, -0.0012, -0.0310],\n",
      "        [ 0.0508,  0.0221, -0.0217, -0.0304],\n",
      "        [-0.0855,  0.0643,  0.0014, -0.0692],\n",
      "        [-0.0104,  0.0773,  0.0658, -0.0545],\n",
      "        [ 0.0044, -0.0092, -0.0831,  0.0229],\n",
      "        [ 0.0332, -0.0195, -0.0683,  0.0217],\n",
      "        [-0.0628, -0.0243, -0.0171,  0.0723],\n",
      "        [-0.0556, -0.0618, -0.0275,  0.0095],\n",
      "        [ 0.0450,  0.0690, -0.0316,  0.0098],\n",
      "        [-0.0299, -0.0863,  0.0291, -0.0333],\n",
      "        [ 0.0593, -0.0456,  0.0017,  0.0872],\n",
      "        [ 0.0687,  0.0754,  0.0252, -0.0636],\n",
      "        [ 0.0395, -0.0375, -0.0368,  0.0031],\n",
      "        [ 0.0429,  0.0580, -0.0204, -0.0299],\n",
      "        [-0.0047, -0.0014, -0.0368, -0.0616],\n",
      "        [-0.0147,  0.0073, -0.0368,  0.0620],\n",
      "        [ 0.0585, -0.0117, -0.0788,  0.0392],\n",
      "        [ 0.0628,  0.0042,  0.0403,  0.0434]], requires_grad=True)\n",
      "spliced Wo: torch.Size([128, 4])\n",
      "tensor([[-0.0079,  0.0757,  0.0749,  0.0462],\n",
      "        [ 0.0460, -0.0546,  0.0502, -0.0218],\n",
      "        [-0.0014, -0.0434, -0.0075,  0.0641],\n",
      "        [ 0.0671, -0.0485, -0.0218,  0.0060],\n",
      "        [ 0.0309,  0.0527,  0.0630, -0.0393],\n",
      "        [ 0.0365, -0.0782,  0.0736, -0.0832],\n",
      "        [-0.0593,  0.0654, -0.0045, -0.0195],\n",
      "        [-0.0409,  0.0541, -0.0687,  0.0108],\n",
      "        [ 0.0058,  0.0848,  0.0145,  0.0689],\n",
      "        [ 0.0026, -0.0821,  0.0007, -0.0826],\n",
      "        [ 0.0520,  0.0327, -0.0099,  0.0035],\n",
      "        [-0.0113, -0.0376, -0.0707,  0.0649],\n",
      "        [ 0.0840,  0.0234, -0.0376,  0.0552],\n",
      "        [ 0.0851, -0.0157,  0.0635,  0.0348],\n",
      "        [-0.0805,  0.0577, -0.0686,  0.0417],\n",
      "        [ 0.0427, -0.0689, -0.0836, -0.0075],\n",
      "        [-0.0264,  0.0699,  0.0436, -0.0784],\n",
      "        [-0.0033,  0.0120, -0.0278,  0.0845],\n",
      "        [ 0.0211,  0.0219, -0.0064,  0.0622],\n",
      "        [ 0.0116, -0.0601, -0.0351,  0.0438],\n",
      "        [ 0.0529, -0.0174,  0.0657, -0.0547],\n",
      "        [ 0.0491,  0.0206,  0.0715,  0.0095],\n",
      "        [-0.0055, -0.0811,  0.0734, -0.0154],\n",
      "        [-0.0363,  0.0509,  0.0742,  0.0262],\n",
      "        [-0.0246, -0.0447,  0.0577, -0.0400],\n",
      "        [ 0.0389,  0.0033,  0.0518, -0.0246],\n",
      "        [-0.0094,  0.0168,  0.0019,  0.0794],\n",
      "        [ 0.0543, -0.0045, -0.0197, -0.0204],\n",
      "        [ 0.0335,  0.0870, -0.0398,  0.0823],\n",
      "        [-0.0363,  0.0104, -0.0642, -0.0105],\n",
      "        [ 0.0579,  0.0792,  0.0132, -0.0749],\n",
      "        [-0.0677,  0.0264,  0.0142,  0.0011],\n",
      "        [ 0.0827,  0.0024, -0.0406,  0.0068],\n",
      "        [-0.0444, -0.0071, -0.0194,  0.0284],\n",
      "        [ 0.0008, -0.0484, -0.0254,  0.0153],\n",
      "        [-0.0116,  0.0578,  0.0749, -0.0239],\n",
      "        [ 0.0216, -0.0516, -0.0301, -0.0757],\n",
      "        [-0.0307,  0.0636, -0.0593, -0.0068],\n",
      "        [-0.0570, -0.0767, -0.0239, -0.0010],\n",
      "        [ 0.0309, -0.0854,  0.0383,  0.0767],\n",
      "        [-0.0815, -0.0579,  0.0037,  0.0585],\n",
      "        [-0.0627, -0.0090, -0.0607,  0.0304],\n",
      "        [-0.0340, -0.0255, -0.0437,  0.0423],\n",
      "        [-0.0613, -0.0223, -0.0705,  0.0365],\n",
      "        [-0.0433, -0.0027, -0.0013,  0.0185],\n",
      "        [-0.0285,  0.0573,  0.0227,  0.0772],\n",
      "        [ 0.0165,  0.0765,  0.0751, -0.0699],\n",
      "        [-0.0342, -0.0433,  0.0554, -0.0370],\n",
      "        [-0.0358, -0.0385,  0.0709, -0.0257],\n",
      "        [ 0.0039,  0.0320, -0.0702, -0.0271],\n",
      "        [-0.0589, -0.0542, -0.0375, -0.0203],\n",
      "        [-0.0016, -0.0322,  0.0168,  0.0031],\n",
      "        [ 0.0460,  0.0677, -0.0761, -0.0260],\n",
      "        [ 0.0072,  0.0692, -0.0434,  0.0824],\n",
      "        [-0.0196, -0.0401, -0.0728, -0.0267],\n",
      "        [-0.0574,  0.0809,  0.0224,  0.0883],\n",
      "        [-0.0287,  0.0293, -0.0042,  0.0625],\n",
      "        [-0.0607,  0.0780, -0.0487, -0.0470],\n",
      "        [ 0.0747,  0.0287,  0.0232, -0.0343],\n",
      "        [-0.0560,  0.0061, -0.0046,  0.0253],\n",
      "        [-0.0705, -0.0466, -0.0435,  0.0327],\n",
      "        [-0.0161,  0.0762, -0.0001, -0.0089],\n",
      "        [ 0.0236,  0.0732,  0.0577, -0.0353],\n",
      "        [ 0.0087, -0.0584,  0.0357, -0.0773],\n",
      "        [-0.0670, -0.0439, -0.0023,  0.0128],\n",
      "        [ 0.0493,  0.0366,  0.0490,  0.0567],\n",
      "        [ 0.0221,  0.0582, -0.0552, -0.0022],\n",
      "        [ 0.0296,  0.0399,  0.0145, -0.0225],\n",
      "        [ 0.0603, -0.0110, -0.0349,  0.0332],\n",
      "        [-0.0803, -0.0851,  0.0214, -0.0223],\n",
      "        [-0.0452,  0.0069,  0.0462, -0.0463],\n",
      "        [-0.0643, -0.0237, -0.0822,  0.0790],\n",
      "        [ 0.0303,  0.0251, -0.0693,  0.0408],\n",
      "        [ 0.0185,  0.0506,  0.0176,  0.0865],\n",
      "        [-0.0782, -0.0380, -0.0346, -0.0512],\n",
      "        [-0.0452, -0.0101,  0.0294,  0.0438],\n",
      "        [-0.0496,  0.0350,  0.0099,  0.0243],\n",
      "        [-0.0701, -0.0437, -0.0308,  0.0103],\n",
      "        [-0.0842, -0.0148, -0.0684, -0.0078],\n",
      "        [-0.0701, -0.0414,  0.0810, -0.0062],\n",
      "        [-0.0146,  0.0005,  0.0533,  0.0344],\n",
      "        [ 0.0085,  0.0402,  0.0674, -0.0017],\n",
      "        [-0.0191, -0.0351,  0.0825,  0.0132],\n",
      "        [-0.0615,  0.0387,  0.0302, -0.0637],\n",
      "        [ 0.0858, -0.0366,  0.0106,  0.0591],\n",
      "        [-0.0817, -0.0733, -0.0238,  0.0241],\n",
      "        [-0.0592, -0.0499, -0.0003,  0.0668],\n",
      "        [-0.0703, -0.0642,  0.0084, -0.0529],\n",
      "        [ 0.0316, -0.0261, -0.0091,  0.0567],\n",
      "        [-0.0310, -0.0424,  0.0673,  0.0006],\n",
      "        [-0.0438, -0.0581,  0.0070,  0.0240],\n",
      "        [ 0.0063, -0.0861, -0.0742,  0.0079],\n",
      "        [-0.0030,  0.0009,  0.0563, -0.0348],\n",
      "        [ 0.0190,  0.0472, -0.0052, -0.0382],\n",
      "        [-0.0308, -0.0811, -0.0136,  0.0697],\n",
      "        [ 0.0380, -0.0230, -0.0244, -0.0466],\n",
      "        [-0.0628, -0.0160, -0.0716,  0.0577],\n",
      "        [ 0.0544, -0.0765,  0.0275, -0.0681],\n",
      "        [ 0.0318,  0.0341,  0.0013, -0.0111],\n",
      "        [-0.0799,  0.0441, -0.0169, -0.0087],\n",
      "        [-0.0725,  0.0369, -0.0701,  0.0023],\n",
      "        [ 0.0854, -0.0224, -0.0534, -0.0877],\n",
      "        [ 0.0283, -0.0878, -0.0075, -0.0721],\n",
      "        [-0.0322,  0.0820,  0.0631,  0.0723],\n",
      "        [ 0.0064,  0.0323,  0.0624,  0.0100],\n",
      "        [ 0.0456,  0.0870, -0.0688,  0.0531],\n",
      "        [ 0.0388, -0.0071,  0.0018, -0.0191],\n",
      "        [ 0.0644, -0.0875, -0.0701,  0.0224],\n",
      "        [-0.0434, -0.0629, -0.0451, -0.0093],\n",
      "        [ 0.0819,  0.0873,  0.0458, -0.0539],\n",
      "        [ 0.0413, -0.0432, -0.0012, -0.0310],\n",
      "        [ 0.0508,  0.0221, -0.0217, -0.0304],\n",
      "        [-0.0855,  0.0643,  0.0014, -0.0692],\n",
      "        [-0.0104,  0.0773,  0.0658, -0.0545],\n",
      "        [ 0.0044, -0.0092, -0.0831,  0.0229],\n",
      "        [ 0.0332, -0.0195, -0.0683,  0.0217],\n",
      "        [-0.0628, -0.0243, -0.0171,  0.0723],\n",
      "        [-0.0556, -0.0618, -0.0275,  0.0095],\n",
      "        [ 0.0450,  0.0690, -0.0316,  0.0098],\n",
      "        [-0.0299, -0.0863,  0.0291, -0.0333],\n",
      "        [ 0.0593, -0.0456,  0.0017,  0.0872],\n",
      "        [ 0.0687,  0.0754,  0.0252, -0.0636],\n",
      "        [ 0.0395, -0.0375, -0.0368,  0.0031],\n",
      "        [ 0.0429,  0.0580, -0.0204, -0.0299],\n",
      "        [-0.0047, -0.0014, -0.0368, -0.0616],\n",
      "        [-0.0147,  0.0073, -0.0368,  0.0620],\n",
      "        [ 0.0585, -0.0117, -0.0788,  0.0392],\n",
      "        [ 0.0628,  0.0042,  0.0403,  0.0434]], grad_fn=<CatBackward0>)\n",
      "projected output: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.3299,  0.0616, -0.4732,  0.8327],\n",
      "         [ 0.1484,  0.0641, -0.1063,  0.1522],\n",
      "         [-0.0509,  0.0417,  0.0814, -0.2066]]], grad_fn=<UnsafeViewBackward0>)\n",
      "----------------- END MultiQueryAttention.forwardTensor() --------------------\n",
      "x in layer after MQA & resid connection and before MLP:\n",
      "tensor([[[ 0.1837,  0.3312, -1.7927, -0.4619],\n",
      "         [-0.7717, -1.2045, -1.2717,  2.2320],\n",
      "         [-0.4694, -0.1014,  0.7906,  0.9948]]], grad_fn=<AddBackward0>)\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.1837,  0.3312, -1.7927, -0.4619],\n",
      "         [-0.7717, -1.2045, -1.2717,  2.2320],\n",
      "         [-0.4694, -0.1014,  0.7906,  0.9948]]], grad_fn=<AddBackward0>)\n",
      "normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.1944,  0.3505, -1.8975, -0.4888],\n",
      "         [-0.5249, -0.8193, -0.8650,  1.5182],\n",
      "         [-0.6911, -0.1493,  1.1640,  1.4646]]], grad_fn=<MulBackward0>)\n",
      "dim: 4\n",
      "skip: 0\n",
      "spliced scale: torch.Size([4])\n",
      "tensor([0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.1944,  0.3505, -1.8975, -0.4888],\n",
      "         [-0.5249, -0.8193, -0.8650,  1.5182],\n",
      "         [-0.6911, -0.1493,  1.1640,  1.4646]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "---------- MLP Input: torch.Tensor ------------\n",
      "------------- MLP.forwardTensor() ------------\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.1944,  0.3505, -1.8975, -0.4888],\n",
      "         [-0.5249, -0.8193, -0.8650,  1.5182],\n",
      "         [-0.6911, -0.1493,  1.1640,  1.4646]]], grad_fn=<MulBackward0>)\n",
      "d_dim: 4\n",
      "d_skip: 0\n",
      "i_dim: 16\n",
      "i_skip: 0\n",
      "Wgate: torch.Size([4, 16])\n",
      "Parameter containing:\n",
      "tensor([[-0.4711, -0.2294,  0.3826,  0.2413,  0.0221, -0.1754, -0.4362,  0.4694,\n",
      "         -0.2813,  0.0714, -0.1090,  0.2667, -0.4410, -0.4892, -0.3989,  0.2439],\n",
      "        [ 0.2152, -0.2464, -0.1261, -0.0181,  0.3651,  0.3879,  0.2535, -0.3582,\n",
      "          0.0528,  0.2415,  0.1849,  0.3567, -0.0384,  0.4340, -0.3725,  0.3105],\n",
      "        [-0.2154,  0.2649,  0.3705, -0.4557,  0.1628, -0.2772, -0.1900,  0.4409,\n",
      "          0.3852, -0.0915, -0.4160,  0.0106,  0.2830,  0.1615,  0.0159,  0.4351],\n",
      "        [ 0.0065, -0.4006,  0.0773, -0.0895, -0.2334, -0.0861,  0.0079,  0.1160,\n",
      "          0.4893, -0.4880,  0.0563, -0.4693, -0.4824,  0.1443, -0.1036,  0.1029]],\n",
      "       requires_grad=True)\n",
      "Wgate spliced: torch.Size([4, 16])\n",
      "tensor([[-0.4711, -0.2294,  0.3826,  0.2413,  0.0221, -0.1754, -0.4362,  0.4694,\n",
      "         -0.2813,  0.0714, -0.1090,  0.2667, -0.4410, -0.4892, -0.3989,  0.2439],\n",
      "        [ 0.2152, -0.2464, -0.1261, -0.0181,  0.3651,  0.3879,  0.2535, -0.3582,\n",
      "          0.0528,  0.2415,  0.1849,  0.3567, -0.0384,  0.4340, -0.3725,  0.3105],\n",
      "        [-0.2154,  0.2649,  0.3705, -0.4557,  0.1628, -0.2772, -0.1900,  0.4409,\n",
      "          0.3852, -0.0915, -0.4160,  0.0106,  0.2830,  0.1615,  0.0159,  0.4351],\n",
      "        [ 0.0065, -0.4006,  0.0773, -0.0895, -0.2334, -0.0861,  0.0079,  0.1160,\n",
      "          0.4893, -0.4880,  0.0563, -0.4693, -0.4824,  0.1443, -0.1036,  0.1029]],\n",
      "       grad_fn=<AliasBackward0>)\n",
      "Bgate: torch.Size([16])\n",
      "Parameter containing:\n",
      "tensor([ 0.1358,  0.2152,  0.2748, -0.3331, -0.1813,  0.0264,  0.4762, -0.4854,\n",
      "        -0.2410, -0.1014, -0.0148,  0.3180, -0.3845, -0.0458,  0.2054,  0.4083],\n",
      "       requires_grad=True)\n",
      "Bgate spliced: torch.Size([16])\n",
      "tensor([ 0.1358,  0.2152,  0.2748, -0.3331, -0.1813,  0.0264,  0.4762, -0.4854,\n",
      "        -0.2410, -0.1014, -0.0148,  0.3180, -0.3845, -0.0458,  0.2054,  0.4083],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xgate: torch.Size([1, 3, 16])\n",
      "tensor([[[ 0.5251, -0.2225, -0.4357,  0.6159, -0.2438,  0.6964,  0.8370,\n",
      "          -1.4130, -1.2472,  0.4093,  0.7906,  0.7042, -0.7848, -0.3658,\n",
      "           0.0178, -0.3113],\n",
      "         [ 0.4030, -0.2998, -0.0258, -0.1867, -0.9872, -0.0903,  0.6737,\n",
      "          -0.6435,  0.2731, -0.9986,  0.3362, -0.8359, -1.0986, -0.0651,\n",
      "           0.5490, -0.1944],\n",
      "         [ 0.1881,  0.1321,  0.5736, -1.1588, -0.4035, -0.3591,  0.5300,\n",
      "          -0.0731,  1.1105, -1.0081, -0.3688, -0.5946, -0.4511,  0.6269,\n",
      "           0.4034,  0.8505]]], grad_fn=<AddBackward0>)\n",
      "GeLU'ed Xgate: torch.Size([1, 3, 16])\n",
      "tensor([[[ 0.3677, -0.0916, -0.1444,  0.4503, -0.0984,  0.5271,  0.6685,\n",
      "          -0.1114, -0.1324,  0.2697,  0.6209,  0.5347, -0.1697, -0.1307,\n",
      "           0.0090, -0.1176],\n",
      "         [ 0.2646, -0.1146, -0.0127, -0.0795, -0.1597, -0.0419,  0.5051,\n",
      "          -0.1673,  0.1659, -0.1588,  0.2124, -0.1685, -0.1494, -0.0309,\n",
      "           0.3889, -0.0822],\n",
      "         [ 0.1081,  0.0730,  0.4112, -0.1428, -0.1385, -0.1292,  0.3721,\n",
      "          -0.0344,  0.9624, -0.1580, -0.1313, -0.1641, -0.1470,  0.4606,\n",
      "           0.2649,  0.6825]]], grad_fn=<GeluBackward0>)\n",
      "Wup: torch.Size([4, 16])\n",
      "Parameter containing:\n",
      "tensor([[ 0.1108,  0.3057,  0.1222,  0.4416,  0.1918,  0.4734,  0.3690, -0.1281,\n",
      "         -0.3413, -0.3456,  0.0640, -0.0212, -0.4335, -0.2666, -0.1306,  0.3194],\n",
      "        [-0.2082, -0.3736,  0.3005,  0.1933, -0.4231, -0.1596,  0.4303, -0.1602,\n",
      "          0.3515, -0.4712, -0.0116,  0.4635,  0.3738, -0.3957, -0.0436, -0.2250],\n",
      "        [-0.3199,  0.3075,  0.1510,  0.2626, -0.4115, -0.2712,  0.0805, -0.4850,\n",
      "         -0.3383, -0.3757, -0.2884, -0.4095, -0.4162, -0.1790,  0.4855, -0.1969],\n",
      "        [ 0.1019,  0.3763, -0.1884, -0.0300,  0.1595,  0.4114, -0.3694, -0.1927,\n",
      "         -0.0307,  0.0315,  0.1099, -0.3822, -0.0468,  0.1110, -0.2350,  0.2922]],\n",
      "       requires_grad=True)\n",
      "Wup spliced: torch.Size([4, 16])\n",
      "tensor([[ 0.1108,  0.3057,  0.1222,  0.4416,  0.1918,  0.4734,  0.3690, -0.1281,\n",
      "         -0.3413, -0.3456,  0.0640, -0.0212, -0.4335, -0.2666, -0.1306,  0.3194],\n",
      "        [-0.2082, -0.3736,  0.3005,  0.1933, -0.4231, -0.1596,  0.4303, -0.1602,\n",
      "          0.3515, -0.4712, -0.0116,  0.4635,  0.3738, -0.3957, -0.0436, -0.2250],\n",
      "        [-0.3199,  0.3075,  0.1510,  0.2626, -0.4115, -0.2712,  0.0805, -0.4850,\n",
      "         -0.3383, -0.3757, -0.2884, -0.4095, -0.4162, -0.1790,  0.4855, -0.1969],\n",
      "        [ 0.1019,  0.3763, -0.1884, -0.0300,  0.1595,  0.4114, -0.3694, -0.1927,\n",
      "         -0.0307,  0.0315,  0.1099, -0.3822, -0.0468,  0.1110, -0.2350,  0.2922]],\n",
      "       grad_fn=<AliasBackward0>)\n",
      "Bup: torch.Size([16])\n",
      "Parameter containing:\n",
      "tensor([ 0.2493, -0.0109, -0.0733, -0.2945, -0.1111,  0.2164, -0.0506,  0.2475,\n",
      "        -0.1203, -0.3330,  0.2976, -0.1972,  0.0952, -0.0163, -0.4097,  0.3306],\n",
      "       requires_grad=True)\n",
      "Bup spliced: torch.Size([16])\n",
      "tensor([ 0.2493, -0.0109, -0.0733, -0.2945, -0.1111,  0.2164, -0.0506,  0.2475,\n",
      "        -0.1203, -0.3330,  0.2976, -0.1972,  0.0952, -0.0163, -0.4097,  0.3306],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xup: torch.Size([1, 3, 16])\n",
      "tensor([[[ 0.7552, -0.8498, -0.1386, -0.6245,  0.4806,  0.5660,  0.1997,\n",
      "           1.1809,  0.5935,  0.1321,  0.7994,  0.9250,  0.9544,  0.0786,\n",
      "          -1.2568,  0.5446],\n",
      "         [ 0.7931,  0.4401, -0.8004, -0.9573,  0.7329,  0.9578, -1.2273,\n",
      "           0.5730,  0.0170,  0.6074,  0.6900, -0.7919,  0.3055,  0.7712,\n",
      "          -1.0822,  0.9612],\n",
      "         [-0.0194,  0.7426, -0.3029, -0.3668, -0.4258,  0.2000, -0.8171,\n",
      "          -0.4868, -0.3757, -0.4149,  0.0805, -1.2882, -0.2140,  0.1812,\n",
      "          -0.0919,  0.3422]]], grad_fn=<AddBackward0>)\n",
      "Xfuse: torch.Size([1, 3, 16])\n",
      "tensor([[[ 0.2777,  0.0779,  0.0200, -0.2812, -0.0473,  0.2983,  0.1335,\n",
      "          -0.1315, -0.0786,  0.0356,  0.4964,  0.4946, -0.1620, -0.0103,\n",
      "          -0.0113, -0.0640],\n",
      "         [ 0.2098, -0.0504,  0.0101,  0.0761, -0.1171, -0.0401, -0.6199,\n",
      "          -0.0959,  0.0028, -0.0964,  0.1465,  0.1335, -0.0456, -0.0238,\n",
      "          -0.4209, -0.0790],\n",
      "         [-0.0021,  0.0542, -0.1246,  0.0524,  0.0590, -0.0258, -0.3040,\n",
      "           0.0168, -0.3615,  0.0655, -0.0106,  0.2114,  0.0315,  0.0834,\n",
      "          -0.0244,  0.2336]]], grad_fn=<MulBackward0>)\n",
      "Wdown: torch.Size([16, 4])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0279, -0.2160, -0.1735,  0.1518],\n",
      "        [ 0.1084, -0.0126, -0.1621, -0.0115],\n",
      "        [-0.2072, -0.2054,  0.0458,  0.1223],\n",
      "        [-0.2420, -0.0571,  0.2103, -0.2155],\n",
      "        [-0.2034, -0.1359,  0.0111, -0.0423],\n",
      "        [ 0.1759, -0.2092, -0.2173, -0.0898],\n",
      "        [-0.2167, -0.1977,  0.2118,  0.1070],\n",
      "        [-0.1025,  0.1586, -0.2437, -0.1566],\n",
      "        [-0.1593,  0.2311, -0.2201, -0.1509],\n",
      "        [-0.2218, -0.0825, -0.0995,  0.2268],\n",
      "        [-0.1547, -0.2075, -0.0174,  0.0752],\n",
      "        [ 0.0670,  0.1068, -0.2171, -0.0432],\n",
      "        [ 0.2249, -0.1833, -0.0199, -0.0505],\n",
      "        [-0.0075, -0.1590,  0.1165,  0.0302],\n",
      "        [ 0.1515,  0.2459,  0.0666,  0.1779],\n",
      "        [ 0.0220,  0.1881,  0.0740, -0.2126]], requires_grad=True)\n",
      "Wdown spliced: torch.Size([16, 4])\n",
      "tensor([[ 0.0279, -0.2160, -0.1735,  0.1518],\n",
      "        [ 0.1084, -0.0126, -0.1621, -0.0115],\n",
      "        [-0.2072, -0.2054,  0.0458,  0.1223],\n",
      "        [-0.2420, -0.0571,  0.2103, -0.2155],\n",
      "        [-0.2034, -0.1359,  0.0111, -0.0423],\n",
      "        [ 0.1759, -0.2092, -0.2173, -0.0898],\n",
      "        [-0.2167, -0.1977,  0.2118,  0.1070],\n",
      "        [-0.1025,  0.1586, -0.2437, -0.1566],\n",
      "        [-0.1593,  0.2311, -0.2201, -0.1509],\n",
      "        [-0.2218, -0.0825, -0.0995,  0.2268],\n",
      "        [-0.1547, -0.2075, -0.0174,  0.0752],\n",
      "        [ 0.0670,  0.1068, -0.2171, -0.0432],\n",
      "        [ 0.2249, -0.1833, -0.0199, -0.0505],\n",
      "        [-0.0075, -0.1590,  0.1165,  0.0302],\n",
      "        [ 0.1515,  0.2459,  0.0666,  0.1779],\n",
      "        [ 0.0220,  0.1881,  0.0740, -0.2126]], grad_fn=<AliasBackward0>)\n",
      "Bdown: torch.Size([4])\n",
      "Parameter containing:\n",
      "tensor([-0.0769,  0.1017,  0.2368,  0.0229], requires_grad=True)\n",
      "Bdown spliced: torch.Size([4])\n",
      "tensor([-0.0769,  0.1017,  0.2368,  0.0229], grad_fn=<SliceBackward0>)\n",
      "outputs: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.0287, -0.1054,  0.0070,  0.1926],\n",
      "         [-0.0046,  0.0684,  0.0662, -0.0762],\n",
      "         [ 0.0564,  0.1389,  0.2236, -0.0335]]], grad_fn=<AddBackward0>)\n",
      "------------- END MLP.forwardTensor() ------------\n",
      "layer's final residual state:\n",
      "tensor([[[ 0.1550,  0.2258, -1.7857, -0.2692],\n",
      "         [-0.7763, -1.1361, -1.2054,  2.1558],\n",
      "         [-0.4130,  0.0375,  1.0142,  0.9613]]], grad_fn=<AddBackward0>)\n",
      "----------------- END Layer.forwardTensor() --------------------\n",
      "forwardTensor() output: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.1550,  0.2258, -1.7857, -0.2692],\n",
      "         [-0.7763, -1.1361, -1.2054,  2.1558],\n",
      "         [-0.4130,  0.0375,  1.0142,  0.9613]]], grad_fn=<AddBackward0>)\n",
      "Level 1 from range(2)\n",
      "Model 0 from range(2)\n",
      "----------------- Layer.forwardTensor() --------------------\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 3, 2])\n",
      "tensor([[[-0.2275, -0.4399],\n",
      "         [ 2.3621,  0.2800],\n",
      "         [-0.2841,  1.2598]]])\n",
      "normed x: torch.Size([1, 3, 2])\n",
      "tensor([[[-0.6496, -1.2562],\n",
      "         [ 1.4044,  0.1665],\n",
      "         [-0.3111,  1.3796]]])\n",
      "dim: 2\n",
      "skip: 0\n",
      "spliced scale: torch.Size([2])\n",
      "tensor([0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 3, 2])\n",
      "tensor([[[-0.6496, -1.2562],\n",
      "         [ 1.4044,  0.1665],\n",
      "         [-0.3111,  1.3796]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "---------- Attention Input: torch.Tensor ------------\n",
      "----------------- MultiQueryAttention.forwardTensor() --------------------\n",
      "x shape: torch.Size([1, 3, 2])\n",
      "d_skip: 0\n",
      "models_in_this_level: 2\n",
      "h_dim: 16\n",
      "h_skip: 0\n",
      "self.Wqkv: torch.Size([4, 192])\n",
      "Parameter containing:\n",
      "tensor([[-3.2097e-01, -1.5344e-02, -1.1510e-01, -3.0874e-01,  3.3117e-01,\n",
      "         -2.5896e-01,  6.9617e-02,  4.6040e-01, -4.1660e-01,  4.1727e-01,\n",
      "          4.1405e-01, -2.2532e-01,  4.6028e-01,  7.0296e-02, -4.5145e-01,\n",
      "         -4.9349e-01,  8.6677e-02,  2.9951e-01,  3.9258e-01, -4.0547e-01,\n",
      "          4.8854e-01, -4.5012e-01,  2.7701e-01,  3.6282e-01, -2.8834e-01,\n",
      "          5.2543e-02, -4.6092e-01,  1.8264e-01, -4.3262e-01, -4.9215e-01,\n",
      "          4.2615e-01, -3.8198e-01,  1.2274e-01,  2.0562e-01,  3.0310e-01,\n",
      "          2.8698e-01, -8.8241e-02, -1.6075e-01,  2.5161e-01,  4.2444e-01,\n",
      "         -8.2624e-02, -4.0507e-01, -1.2071e-01, -4.2426e-01, -2.8454e-01,\n",
      "          2.0912e-01,  2.4156e-01, -3.9879e-01, -2.2591e-01,  3.3003e-01,\n",
      "          3.2374e-01, -1.6253e-02,  2.8714e-01, -3.8530e-01, -3.8436e-01,\n",
      "         -2.4212e-01,  3.6687e-02, -4.5855e-01,  4.8830e-01,  4.7009e-01,\n",
      "         -1.6423e-01, -3.2546e-01,  4.4032e-01,  3.6316e-01, -8.7626e-02,\n",
      "         -4.4174e-01, -4.6600e-01, -4.8568e-03, -4.8418e-01,  6.4661e-02,\n",
      "          4.8494e-01, -1.6592e-01, -2.0181e-01,  4.4469e-01,  9.5466e-02,\n",
      "          4.8366e-01,  4.4880e-01, -1.9130e-01,  1.5374e-01,  2.2177e-01,\n",
      "          4.9499e-01,  4.8626e-01, -3.4601e-01, -3.4760e-01,  8.6108e-02,\n",
      "          4.1357e-01, -2.4427e-01, -2.5268e-01,  2.0449e-01, -4.9356e-01,\n",
      "         -3.6677e-01, -1.0875e-01,  4.0741e-01, -4.2069e-01, -3.3176e-01,\n",
      "          2.1056e-01, -8.8283e-02, -1.0314e-01,  4.6487e-01, -3.7954e-01,\n",
      "         -2.3864e-01, -4.2001e-01, -1.2691e-01,  1.9158e-01,  1.1451e-01,\n",
      "          1.6561e-01,  1.7231e-01,  1.7429e-01, -1.8123e-01,  3.6120e-01,\n",
      "          3.1331e-01, -9.2029e-02,  3.9661e-01,  8.8137e-02,  1.8981e-01,\n",
      "          1.9692e-02,  3.0192e-01, -2.1089e-01, -3.5772e-02, -4.6871e-01,\n",
      "         -4.8495e-01, -3.9506e-01,  1.8029e-01,  3.0469e-01, -6.6937e-02,\n",
      "         -1.2337e-01, -4.9423e-01,  4.1953e-01, -1.5530e-01, -3.9074e-01,\n",
      "         -1.3531e-01, -1.8341e-01,  3.2812e-01,  3.5656e-01,  1.2500e-02,\n",
      "          3.2196e-01, -4.7280e-01,  4.0531e-01, -1.1058e-01,  2.0058e-01,\n",
      "         -3.1400e-01,  9.3957e-02, -2.3539e-02, -4.6383e-01, -2.1440e-01,\n",
      "          3.5762e-01,  2.6374e-01, -1.1291e-01,  1.3419e-01,  2.3561e-01,\n",
      "          2.6260e-01, -2.5277e-01, -4.1158e-01,  2.9288e-01, -3.6784e-01,\n",
      "          2.8182e-02, -4.1021e-01, -2.1270e-01,  1.2341e-01, -4.1593e-01,\n",
      "         -2.2971e-01, -2.8959e-01,  1.1191e-01, -2.0859e-01, -1.2854e-01,\n",
      "         -1.8272e-01, -9.3147e-02, -2.4128e-01, -4.6417e-01, -1.5658e-01,\n",
      "         -2.6369e-01, -2.7113e-02, -1.5909e-01, -2.2127e-01, -1.8380e-01,\n",
      "          3.7399e-01, -3.1944e-01,  2.3862e-02, -2.5048e-01, -2.1036e-01,\n",
      "         -2.7727e-01, -2.9599e-01,  4.8089e-01, -4.5476e-01,  3.0850e-01,\n",
      "          1.0780e-01,  9.1512e-02, -4.3502e-01, -2.0343e-01,  1.3289e-02,\n",
      "         -1.5765e-01,  1.8142e-01],\n",
      "        [-1.4990e-01,  2.7819e-01,  7.4129e-02, -4.4660e-02,  4.3282e-01,\n",
      "         -3.0581e-01,  1.8844e-01,  4.4872e-01, -1.1302e-01,  1.4516e-01,\n",
      "         -4.8995e-05,  4.3458e-01,  5.3082e-02,  4.4512e-01,  1.2523e-01,\n",
      "         -1.1507e-01,  3.1780e-01, -3.1342e-01, -4.4965e-01, -3.3284e-02,\n",
      "          1.5604e-02, -1.6669e-01,  1.8346e-01, -2.4034e-01, -4.7407e-01,\n",
      "         -7.6704e-02,  6.7073e-02,  1.2652e-01, -3.9627e-01,  3.7724e-01,\n",
      "         -1.9206e-01,  3.0627e-01,  4.2555e-02, -3.0911e-01,  4.3753e-01,\n",
      "         -2.9667e-01, -1.4520e-02, -1.0416e-02, -3.2493e-01,  4.7091e-01,\n",
      "          2.3998e-01,  3.4873e-01,  5.5291e-02,  9.5433e-02,  1.9302e-01,\n",
      "          3.8875e-01,  2.3504e-01, -1.4209e-01,  2.1127e-01,  2.3719e-01,\n",
      "          3.7475e-01, -2.1212e-01, -2.0359e-01, -4.3030e-01,  8.8165e-02,\n",
      "         -2.9098e-01,  7.8429e-02, -3.4094e-01,  4.3696e-01,  1.9249e-01,\n",
      "         -4.8272e-01, -3.3708e-01, -2.7591e-01, -2.7684e-01, -3.7848e-01,\n",
      "         -4.9171e-01, -4.6506e-01, -1.9350e-01,  3.7812e-01,  3.6263e-02,\n",
      "         -1.0092e-01, -3.0463e-01, -5.2127e-02, -2.3248e-01, -1.8609e-01,\n",
      "         -4.9422e-01,  4.5411e-01,  2.4784e-01,  1.7444e-01,  2.7136e-01,\n",
      "         -3.0759e-01, -2.0130e-01,  2.6875e-01, -4.1371e-01,  4.6615e-01,\n",
      "          1.8902e-01, -2.4082e-01, -4.6193e-01, -6.2983e-02, -2.6200e-01,\n",
      "          3.7065e-01,  4.9508e-01, -5.5899e-03,  1.6553e-01,  2.9573e-01,\n",
      "         -2.0170e-01, -5.1177e-03,  3.8824e-01, -3.7151e-01, -1.4505e-01,\n",
      "         -3.5091e-01, -4.1308e-01, -4.3970e-01,  2.8640e-01, -7.5188e-02,\n",
      "         -1.1953e-01,  2.8627e-01,  2.9312e-01,  4.1545e-02,  3.4332e-01,\n",
      "          5.6798e-02,  2.4595e-02,  1.1926e-01,  4.9348e-02,  1.0625e-01,\n",
      "          3.0282e-01,  1.3777e-01,  4.6769e-01, -4.9020e-01,  3.9159e-01,\n",
      "          6.4323e-02,  3.1386e-01,  1.8567e-01, -3.6389e-01, -6.6978e-02,\n",
      "          3.4887e-01, -4.4969e-01,  4.6964e-01,  8.3232e-02, -9.8104e-02,\n",
      "         -4.3657e-01, -3.1562e-01, -4.9943e-01,  2.4091e-01, -7.4649e-02,\n",
      "         -8.5931e-02, -4.4864e-01,  2.0309e-01,  1.6673e-01, -3.4258e-01,\n",
      "         -3.1006e-01,  2.3308e-01,  2.1086e-01,  1.3077e-01, -2.9130e-01,\n",
      "         -2.9917e-01, -1.6217e-01,  3.2903e-01, -3.0571e-01, -2.0588e-01,\n",
      "          3.5932e-01, -2.6446e-01, -1.8150e-01, -4.2555e-01, -2.7207e-01,\n",
      "         -3.3697e-01,  5.7231e-03, -9.8906e-03, -2.9698e-01, -4.7332e-01,\n",
      "         -5.7863e-02, -2.1753e-01,  2.9426e-01,  3.6454e-01, -3.2457e-02,\n",
      "          2.5800e-01, -4.3584e-01, -8.4784e-02,  5.4089e-03,  3.8661e-01,\n",
      "         -4.5274e-01, -2.3479e-01,  3.2677e-01,  1.7917e-02,  3.6443e-02,\n",
      "          2.8099e-01,  4.1516e-01, -2.7281e-01, -1.5072e-01, -3.6735e-01,\n",
      "          6.6030e-02, -1.6825e-01,  7.4779e-03, -4.9353e-01,  3.9334e-01,\n",
      "          1.6545e-01,  3.9433e-01,  3.8434e-01,  2.4296e-02, -3.1079e-01,\n",
      "          3.2874e-01,  4.7495e-01],\n",
      "        [-3.6494e-01, -1.9819e-01,  1.2391e-01,  1.4987e-01, -4.8445e-01,\n",
      "         -2.4220e-01,  1.5516e-01, -2.8535e-02,  3.2431e-01, -2.2211e-01,\n",
      "          1.5887e-01,  2.8480e-01,  2.1065e-01, -3.2590e-01, -3.0512e-01,\n",
      "         -4.2174e-01, -2.1729e-01, -2.7202e-01, -4.9488e-01,  4.2156e-01,\n",
      "          4.1862e-01, -2.5510e-01, -1.7890e-01, -2.3873e-01, -1.4193e-01,\n",
      "          5.3084e-02,  1.0497e-01,  4.0349e-01, -4.5228e-01,  5.2142e-02,\n",
      "          4.7570e-01, -1.9244e-01, -2.6906e-01,  3.6564e-02,  2.8004e-01,\n",
      "          1.9182e-01, -3.6188e-01,  3.8665e-01,  2.9734e-01,  1.3272e-01,\n",
      "          2.8950e-01, -1.1843e-01,  1.8197e-01,  1.0729e-01, -1.2266e-01,\n",
      "          3.5865e-03, -3.2721e-01,  4.0710e-01,  2.1084e-01, -4.9324e-01,\n",
      "         -4.5889e-01,  4.4462e-01,  1.4339e-01,  6.2129e-02, -3.7913e-01,\n",
      "         -6.1301e-02, -5.8408e-02,  2.0735e-01, -2.0772e-01, -4.4801e-01,\n",
      "          8.8660e-02, -3.2614e-01,  5.8857e-02, -3.6563e-03, -1.2983e-01,\n",
      "          2.8123e-01, -3.0251e-01, -2.0165e-01, -2.1545e-01,  3.0909e-01,\n",
      "         -1.9715e-01, -3.4939e-01, -3.8061e-01,  4.6676e-01,  4.4117e-01,\n",
      "         -1.2104e-01,  1.7660e-01, -2.4349e-02,  2.1922e-01, -4.1842e-01,\n",
      "         -4.8203e-01,  6.6405e-02, -2.5446e-01, -2.6192e-01, -3.6002e-01,\n",
      "          3.5817e-01, -5.3533e-02, -1.7330e-01, -2.6523e-01, -4.1151e-01,\n",
      "          2.4367e-01, -1.5724e-01,  1.0179e-01,  1.8709e-02,  2.6733e-01,\n",
      "         -3.9686e-01,  1.9645e-01,  1.2850e-01, -1.6648e-01, -2.5955e-02,\n",
      "          2.8239e-01, -3.5465e-01, -8.4585e-02, -1.6311e-01,  2.4559e-01,\n",
      "          3.4939e-01,  2.8058e-01,  2.2911e-01, -1.5656e-01, -3.8567e-01,\n",
      "          4.8251e-02,  5.4339e-02,  2.4236e-03,  4.1418e-02,  1.3403e-01,\n",
      "          4.6479e-01, -3.6504e-01, -3.9382e-01,  8.7580e-02,  3.4661e-01,\n",
      "          3.6698e-01,  3.1494e-01,  2.8518e-01,  3.6290e-01, -1.9336e-01,\n",
      "         -3.9900e-01, -3.0189e-01,  4.7716e-01, -5.7343e-02, -3.9568e-01,\n",
      "          4.7925e-01, -2.9701e-01,  1.6367e-01, -2.7030e-01,  1.7530e-01,\n",
      "         -4.8403e-01, -2.8215e-01, -1.4787e-01, -9.4221e-02,  1.7540e-02,\n",
      "         -2.8544e-01,  3.6671e-01,  4.9354e-01,  2.5809e-01,  4.5043e-01,\n",
      "          4.0915e-01,  3.4615e-01, -2.8886e-01, -1.0766e-01,  2.1052e-01,\n",
      "          6.3936e-02, -1.3789e-01,  3.7345e-01,  2.8598e-01, -4.5961e-01,\n",
      "         -2.7873e-01,  2.4984e-01, -3.3217e-01, -3.3710e-01, -3.4967e-01,\n",
      "         -6.8175e-02,  4.5880e-01,  1.7332e-01, -2.4107e-01, -2.8240e-01,\n",
      "         -2.8811e-01,  2.5626e-01, -4.9741e-01,  8.1723e-02,  2.7604e-01,\n",
      "          4.7153e-01, -3.5504e-01, -3.8385e-01, -3.3069e-01, -2.1225e-01,\n",
      "          1.5297e-01,  4.7800e-01, -1.8034e-02, -3.5942e-01,  4.7031e-01,\n",
      "         -3.1677e-01,  1.2753e-01,  4.5037e-01,  2.8465e-01,  1.2980e-01,\n",
      "         -2.7024e-01,  2.7878e-01, -4.7501e-01,  4.2285e-01,  4.5226e-01,\n",
      "         -7.5834e-02, -2.7833e-01],\n",
      "        [ 3.6166e-01, -2.9039e-01, -1.9532e-01,  2.1463e-01,  3.0336e-01,\n",
      "          3.4968e-01,  1.7080e-01, -3.1044e-01, -2.6154e-01, -1.7996e-01,\n",
      "         -3.9154e-01,  4.5309e-01,  2.6656e-01,  3.2655e-01,  4.4053e-01,\n",
      "         -3.8769e-01,  4.5635e-01, -1.7444e-01,  2.8878e-01,  2.6081e-01,\n",
      "         -3.5004e-01,  1.7178e-01,  4.8178e-01, -2.5152e-01, -1.7255e-01,\n",
      "         -4.2525e-01, -5.4256e-02, -2.4009e-01, -2.3620e-01, -1.2440e-01,\n",
      "          3.0858e-01, -4.4050e-01,  1.7219e-01, -1.5976e-01,  3.5247e-01,\n",
      "          2.9060e-01, -3.8982e-01,  5.3928e-02,  2.4100e-01,  6.9047e-03,\n",
      "          4.1114e-01,  3.0978e-02,  2.3816e-01, -2.6534e-01, -9.1487e-02,\n",
      "         -1.2970e-01, -1.3459e-01,  4.5638e-01,  4.2021e-01, -2.6263e-01,\n",
      "          6.4170e-02,  2.1780e-01, -1.9102e-01, -3.3325e-02, -4.4954e-01,\n",
      "          2.6437e-01, -3.3372e-01,  4.9230e-01, -3.5328e-01, -2.8936e-01,\n",
      "         -1.1743e-01, -1.5401e-02,  3.7282e-01,  1.4672e-01,  7.0350e-02,\n",
      "          3.2993e-01,  2.1661e-01, -1.4682e-01,  4.2327e-01, -2.8551e-01,\n",
      "         -1.3827e-01,  2.2878e-01, -7.2087e-02,  4.5573e-01,  4.1544e-02,\n",
      "          1.9417e-01, -1.5592e-01, -2.4566e-01,  2.5795e-01,  1.7242e-03,\n",
      "          3.9895e-01,  6.4041e-02, -3.9480e-01, -1.9869e-01,  2.9795e-02,\n",
      "          7.1546e-02, -3.7036e-02, -1.2344e-01,  4.1349e-01, -1.3400e-01,\n",
      "          2.2914e-01,  4.0769e-01,  2.6082e-01, -3.1787e-01, -1.5388e-01,\n",
      "          4.6755e-01,  3.8927e-01,  3.0885e-01, -1.5936e-01,  1.8015e-01,\n",
      "         -1.3583e-01, -2.4389e-01, -3.9084e-01,  3.8825e-01, -1.7214e-01,\n",
      "          2.9319e-01,  4.8597e-01,  3.2779e-01,  2.0918e-01,  1.6785e-01,\n",
      "         -4.5960e-01, -4.2303e-01, -2.9061e-01,  4.8491e-01, -1.8013e-01,\n",
      "         -4.4329e-01,  1.1073e-03, -3.2739e-01, -1.9347e-01, -2.4275e-01,\n",
      "          1.8868e-01, -2.4091e-01, -6.2331e-02, -1.4281e-01,  4.4504e-01,\n",
      "         -4.6353e-01,  1.3425e-01, -4.4999e-01, -4.0042e-01,  1.3937e-01,\n",
      "         -2.3713e-01, -2.1600e-01, -4.5663e-01,  2.7168e-02,  2.3453e-01,\n",
      "         -3.6210e-01,  2.8875e-01,  3.8881e-01,  1.2422e-01,  4.6177e-01,\n",
      "          4.4899e-01,  4.2099e-01, -2.2689e-01,  2.3584e-01, -1.8267e-01,\n",
      "          2.2487e-01, -3.3422e-01, -4.5962e-01, -4.3136e-01, -4.5188e-01,\n",
      "         -2.5392e-01,  2.1336e-01,  5.7722e-02, -5.0130e-02, -4.0297e-01,\n",
      "          1.9646e-01, -2.6501e-01,  3.6008e-01,  1.8912e-01, -2.5685e-01,\n",
      "         -3.4995e-01, -2.0231e-01, -4.7444e-01, -7.5289e-02, -4.8484e-01,\n",
      "          2.4525e-01,  2.8355e-01, -4.7662e-01,  2.2617e-01,  4.4863e-01,\n",
      "          2.4027e-01, -1.9471e-01, -2.4254e-01, -1.4281e-01,  4.4090e-01,\n",
      "          1.5124e-01,  3.8078e-01, -7.7263e-02, -2.4661e-01, -3.7154e-01,\n",
      "         -2.3260e-01, -4.9303e-01,  4.7423e-02, -2.7226e-01, -3.4987e-01,\n",
      "          1.2225e-01,  4.7308e-01, -3.5899e-01, -4.5405e-01, -1.4199e-01,\n",
      "         -2.3627e-01,  4.9589e-01]], requires_grad=True)\n",
      "Wq: torch.Size([4, 128])\n",
      "tensor([[-3.2097e-01, -1.5344e-02, -1.1510e-01, -3.0874e-01,  3.3117e-01,\n",
      "         -2.5896e-01,  6.9617e-02,  4.6040e-01, -4.1660e-01,  4.1727e-01,\n",
      "          4.1405e-01, -2.2532e-01,  4.6028e-01,  7.0296e-02, -4.5145e-01,\n",
      "         -4.9349e-01,  8.6677e-02,  2.9951e-01,  3.9258e-01, -4.0547e-01,\n",
      "          4.8854e-01, -4.5012e-01,  2.7701e-01,  3.6282e-01, -2.8834e-01,\n",
      "          5.2543e-02, -4.6092e-01,  1.8264e-01, -4.3262e-01, -4.9215e-01,\n",
      "          4.2615e-01, -3.8198e-01,  1.2274e-01,  2.0562e-01,  3.0310e-01,\n",
      "          2.8698e-01, -8.8241e-02, -1.6075e-01,  2.5161e-01,  4.2444e-01,\n",
      "         -8.2624e-02, -4.0507e-01, -1.2071e-01, -4.2426e-01, -2.8454e-01,\n",
      "          2.0912e-01,  2.4156e-01, -3.9879e-01, -2.2591e-01,  3.3003e-01,\n",
      "          3.2374e-01, -1.6253e-02,  2.8714e-01, -3.8530e-01, -3.8436e-01,\n",
      "         -2.4212e-01,  3.6687e-02, -4.5855e-01,  4.8830e-01,  4.7009e-01,\n",
      "         -1.6423e-01, -3.2546e-01,  4.4032e-01,  3.6316e-01, -8.7626e-02,\n",
      "         -4.4174e-01, -4.6600e-01, -4.8568e-03, -4.8418e-01,  6.4661e-02,\n",
      "          4.8494e-01, -1.6592e-01, -2.0181e-01,  4.4469e-01,  9.5466e-02,\n",
      "          4.8366e-01,  4.4880e-01, -1.9130e-01,  1.5374e-01,  2.2177e-01,\n",
      "          4.9499e-01,  4.8626e-01, -3.4601e-01, -3.4760e-01,  8.6108e-02,\n",
      "          4.1357e-01, -2.4427e-01, -2.5268e-01,  2.0449e-01, -4.9356e-01,\n",
      "         -3.6677e-01, -1.0875e-01,  4.0741e-01, -4.2069e-01, -3.3176e-01,\n",
      "          2.1056e-01, -8.8283e-02, -1.0314e-01,  4.6487e-01, -3.7954e-01,\n",
      "         -2.3864e-01, -4.2001e-01, -1.2691e-01,  1.9158e-01,  1.1451e-01,\n",
      "          1.6561e-01,  1.7231e-01,  1.7429e-01, -1.8123e-01,  3.6120e-01,\n",
      "          3.1331e-01, -9.2029e-02,  3.9661e-01,  8.8137e-02,  1.8981e-01,\n",
      "          1.9692e-02,  3.0192e-01, -2.1089e-01, -3.5772e-02, -4.6871e-01,\n",
      "         -4.8495e-01, -3.9506e-01,  1.8029e-01,  3.0469e-01, -6.6937e-02,\n",
      "         -1.2337e-01, -4.9423e-01,  4.1953e-01],\n",
      "        [-1.4990e-01,  2.7819e-01,  7.4129e-02, -4.4660e-02,  4.3282e-01,\n",
      "         -3.0581e-01,  1.8844e-01,  4.4872e-01, -1.1302e-01,  1.4516e-01,\n",
      "         -4.8995e-05,  4.3458e-01,  5.3082e-02,  4.4512e-01,  1.2523e-01,\n",
      "         -1.1507e-01,  3.1780e-01, -3.1342e-01, -4.4965e-01, -3.3284e-02,\n",
      "          1.5604e-02, -1.6669e-01,  1.8346e-01, -2.4034e-01, -4.7407e-01,\n",
      "         -7.6704e-02,  6.7073e-02,  1.2652e-01, -3.9627e-01,  3.7724e-01,\n",
      "         -1.9206e-01,  3.0627e-01,  4.2555e-02, -3.0911e-01,  4.3753e-01,\n",
      "         -2.9667e-01, -1.4520e-02, -1.0416e-02, -3.2493e-01,  4.7091e-01,\n",
      "          2.3998e-01,  3.4873e-01,  5.5291e-02,  9.5433e-02,  1.9302e-01,\n",
      "          3.8875e-01,  2.3504e-01, -1.4209e-01,  2.1127e-01,  2.3719e-01,\n",
      "          3.7475e-01, -2.1212e-01, -2.0359e-01, -4.3030e-01,  8.8165e-02,\n",
      "         -2.9098e-01,  7.8429e-02, -3.4094e-01,  4.3696e-01,  1.9249e-01,\n",
      "         -4.8272e-01, -3.3708e-01, -2.7591e-01, -2.7684e-01, -3.7848e-01,\n",
      "         -4.9171e-01, -4.6506e-01, -1.9350e-01,  3.7812e-01,  3.6263e-02,\n",
      "         -1.0092e-01, -3.0463e-01, -5.2127e-02, -2.3248e-01, -1.8609e-01,\n",
      "         -4.9422e-01,  4.5411e-01,  2.4784e-01,  1.7444e-01,  2.7136e-01,\n",
      "         -3.0759e-01, -2.0130e-01,  2.6875e-01, -4.1371e-01,  4.6615e-01,\n",
      "          1.8902e-01, -2.4082e-01, -4.6193e-01, -6.2983e-02, -2.6200e-01,\n",
      "          3.7065e-01,  4.9508e-01, -5.5899e-03,  1.6553e-01,  2.9573e-01,\n",
      "         -2.0170e-01, -5.1177e-03,  3.8824e-01, -3.7151e-01, -1.4505e-01,\n",
      "         -3.5091e-01, -4.1308e-01, -4.3970e-01,  2.8640e-01, -7.5188e-02,\n",
      "         -1.1953e-01,  2.8627e-01,  2.9312e-01,  4.1545e-02,  3.4332e-01,\n",
      "          5.6798e-02,  2.4595e-02,  1.1926e-01,  4.9348e-02,  1.0625e-01,\n",
      "          3.0282e-01,  1.3777e-01,  4.6769e-01, -4.9020e-01,  3.9159e-01,\n",
      "          6.4323e-02,  3.1386e-01,  1.8567e-01, -3.6389e-01, -6.6978e-02,\n",
      "          3.4887e-01, -4.4969e-01,  4.6964e-01],\n",
      "        [-3.6494e-01, -1.9819e-01,  1.2391e-01,  1.4987e-01, -4.8445e-01,\n",
      "         -2.4220e-01,  1.5516e-01, -2.8535e-02,  3.2431e-01, -2.2211e-01,\n",
      "          1.5887e-01,  2.8480e-01,  2.1065e-01, -3.2590e-01, -3.0512e-01,\n",
      "         -4.2174e-01, -2.1729e-01, -2.7202e-01, -4.9488e-01,  4.2156e-01,\n",
      "          4.1862e-01, -2.5510e-01, -1.7890e-01, -2.3873e-01, -1.4193e-01,\n",
      "          5.3084e-02,  1.0497e-01,  4.0349e-01, -4.5228e-01,  5.2142e-02,\n",
      "          4.7570e-01, -1.9244e-01, -2.6906e-01,  3.6564e-02,  2.8004e-01,\n",
      "          1.9182e-01, -3.6188e-01,  3.8665e-01,  2.9734e-01,  1.3272e-01,\n",
      "          2.8950e-01, -1.1843e-01,  1.8197e-01,  1.0729e-01, -1.2266e-01,\n",
      "          3.5865e-03, -3.2721e-01,  4.0710e-01,  2.1084e-01, -4.9324e-01,\n",
      "         -4.5889e-01,  4.4462e-01,  1.4339e-01,  6.2129e-02, -3.7913e-01,\n",
      "         -6.1301e-02, -5.8408e-02,  2.0735e-01, -2.0772e-01, -4.4801e-01,\n",
      "          8.8660e-02, -3.2614e-01,  5.8857e-02, -3.6563e-03, -1.2983e-01,\n",
      "          2.8123e-01, -3.0251e-01, -2.0165e-01, -2.1545e-01,  3.0909e-01,\n",
      "         -1.9715e-01, -3.4939e-01, -3.8061e-01,  4.6676e-01,  4.4117e-01,\n",
      "         -1.2104e-01,  1.7660e-01, -2.4349e-02,  2.1922e-01, -4.1842e-01,\n",
      "         -4.8203e-01,  6.6405e-02, -2.5446e-01, -2.6192e-01, -3.6002e-01,\n",
      "          3.5817e-01, -5.3533e-02, -1.7330e-01, -2.6523e-01, -4.1151e-01,\n",
      "          2.4367e-01, -1.5724e-01,  1.0179e-01,  1.8709e-02,  2.6733e-01,\n",
      "         -3.9686e-01,  1.9645e-01,  1.2850e-01, -1.6648e-01, -2.5955e-02,\n",
      "          2.8239e-01, -3.5465e-01, -8.4585e-02, -1.6311e-01,  2.4559e-01,\n",
      "          3.4939e-01,  2.8058e-01,  2.2911e-01, -1.5656e-01, -3.8567e-01,\n",
      "          4.8251e-02,  5.4339e-02,  2.4236e-03,  4.1418e-02,  1.3403e-01,\n",
      "          4.6479e-01, -3.6504e-01, -3.9382e-01,  8.7580e-02,  3.4661e-01,\n",
      "          3.6698e-01,  3.1494e-01,  2.8518e-01,  3.6290e-01, -1.9336e-01,\n",
      "         -3.9900e-01, -3.0189e-01,  4.7716e-01],\n",
      "        [ 3.6166e-01, -2.9039e-01, -1.9532e-01,  2.1463e-01,  3.0336e-01,\n",
      "          3.4968e-01,  1.7080e-01, -3.1044e-01, -2.6154e-01, -1.7996e-01,\n",
      "         -3.9154e-01,  4.5309e-01,  2.6656e-01,  3.2655e-01,  4.4053e-01,\n",
      "         -3.8769e-01,  4.5635e-01, -1.7444e-01,  2.8878e-01,  2.6081e-01,\n",
      "         -3.5004e-01,  1.7178e-01,  4.8178e-01, -2.5152e-01, -1.7255e-01,\n",
      "         -4.2525e-01, -5.4256e-02, -2.4009e-01, -2.3620e-01, -1.2440e-01,\n",
      "          3.0858e-01, -4.4050e-01,  1.7219e-01, -1.5976e-01,  3.5247e-01,\n",
      "          2.9060e-01, -3.8982e-01,  5.3928e-02,  2.4100e-01,  6.9047e-03,\n",
      "          4.1114e-01,  3.0978e-02,  2.3816e-01, -2.6534e-01, -9.1487e-02,\n",
      "         -1.2970e-01, -1.3459e-01,  4.5638e-01,  4.2021e-01, -2.6263e-01,\n",
      "          6.4170e-02,  2.1780e-01, -1.9102e-01, -3.3325e-02, -4.4954e-01,\n",
      "          2.6437e-01, -3.3372e-01,  4.9230e-01, -3.5328e-01, -2.8936e-01,\n",
      "         -1.1743e-01, -1.5401e-02,  3.7282e-01,  1.4672e-01,  7.0350e-02,\n",
      "          3.2993e-01,  2.1661e-01, -1.4682e-01,  4.2327e-01, -2.8551e-01,\n",
      "         -1.3827e-01,  2.2878e-01, -7.2087e-02,  4.5573e-01,  4.1544e-02,\n",
      "          1.9417e-01, -1.5592e-01, -2.4566e-01,  2.5795e-01,  1.7242e-03,\n",
      "          3.9895e-01,  6.4041e-02, -3.9480e-01, -1.9869e-01,  2.9795e-02,\n",
      "          7.1546e-02, -3.7036e-02, -1.2344e-01,  4.1349e-01, -1.3400e-01,\n",
      "          2.2914e-01,  4.0769e-01,  2.6082e-01, -3.1787e-01, -1.5388e-01,\n",
      "          4.6755e-01,  3.8927e-01,  3.0885e-01, -1.5936e-01,  1.8015e-01,\n",
      "         -1.3583e-01, -2.4389e-01, -3.9084e-01,  3.8825e-01, -1.7214e-01,\n",
      "          2.9319e-01,  4.8597e-01,  3.2779e-01,  2.0918e-01,  1.6785e-01,\n",
      "         -4.5960e-01, -4.2303e-01, -2.9061e-01,  4.8491e-01, -1.8013e-01,\n",
      "         -4.4329e-01,  1.1073e-03, -3.2739e-01, -1.9347e-01, -2.4275e-01,\n",
      "          1.8868e-01, -2.4091e-01, -6.2331e-02, -1.4281e-01,  4.4504e-01,\n",
      "         -4.6353e-01,  1.3425e-01, -4.4999e-01]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wk: torch.Size([4, 32])\n",
      "tensor([[-0.1553, -0.3907, -0.1353, -0.1834,  0.3281,  0.3566,  0.0125,  0.3220,\n",
      "         -0.4728,  0.4053, -0.1106,  0.2006, -0.3140,  0.0940, -0.0235, -0.4638,\n",
      "         -0.2144,  0.3576,  0.2637, -0.1129,  0.1342,  0.2356,  0.2626, -0.2528,\n",
      "         -0.4116,  0.2929, -0.3678,  0.0282, -0.4102, -0.2127,  0.1234, -0.4159],\n",
      "        [ 0.0832, -0.0981, -0.4366, -0.3156, -0.4994,  0.2409, -0.0746, -0.0859,\n",
      "         -0.4486,  0.2031,  0.1667, -0.3426, -0.3101,  0.2331,  0.2109,  0.1308,\n",
      "         -0.2913, -0.2992, -0.1622,  0.3290, -0.3057, -0.2059,  0.3593, -0.2645,\n",
      "         -0.1815, -0.4256, -0.2721, -0.3370,  0.0057, -0.0099, -0.2970, -0.4733],\n",
      "        [-0.0573, -0.3957,  0.4792, -0.2970,  0.1637, -0.2703,  0.1753, -0.4840,\n",
      "         -0.2821, -0.1479, -0.0942,  0.0175, -0.2854,  0.3667,  0.4935,  0.2581,\n",
      "          0.4504,  0.4091,  0.3461, -0.2889, -0.1077,  0.2105,  0.0639, -0.1379,\n",
      "          0.3734,  0.2860, -0.4596, -0.2787,  0.2498, -0.3322, -0.3371, -0.3497],\n",
      "        [-0.4004,  0.1394, -0.2371, -0.2160, -0.4566,  0.0272,  0.2345, -0.3621,\n",
      "          0.2887,  0.3888,  0.1242,  0.4618,  0.4490,  0.4210, -0.2269,  0.2358,\n",
      "         -0.1827,  0.2249, -0.3342, -0.4596, -0.4314, -0.4519, -0.2539,  0.2134,\n",
      "          0.0577, -0.0501, -0.4030,  0.1965, -0.2650,  0.3601,  0.1891, -0.2569]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wv: torch.Size([4, 32])\n",
      "tensor([[-0.2297, -0.2896,  0.1119, -0.2086, -0.1285, -0.1827, -0.0931, -0.2413,\n",
      "         -0.4642, -0.1566, -0.2637, -0.0271, -0.1591, -0.2213, -0.1838,  0.3740,\n",
      "         -0.3194,  0.0239, -0.2505, -0.2104, -0.2773, -0.2960,  0.4809, -0.4548,\n",
      "          0.3085,  0.1078,  0.0915, -0.4350, -0.2034,  0.0133, -0.1576,  0.1814],\n",
      "        [-0.0579, -0.2175,  0.2943,  0.3645, -0.0325,  0.2580, -0.4358, -0.0848,\n",
      "          0.0054,  0.3866, -0.4527, -0.2348,  0.3268,  0.0179,  0.0364,  0.2810,\n",
      "          0.4152, -0.2728, -0.1507, -0.3673,  0.0660, -0.1682,  0.0075, -0.4935,\n",
      "          0.3933,  0.1655,  0.3943,  0.3843,  0.0243, -0.3108,  0.3287,  0.4750],\n",
      "        [-0.0682,  0.4588,  0.1733, -0.2411, -0.2824, -0.2881,  0.2563, -0.4974,\n",
      "          0.0817,  0.2760,  0.4715, -0.3550, -0.3838, -0.3307, -0.2122,  0.1530,\n",
      "          0.4780, -0.0180, -0.3594,  0.4703, -0.3168,  0.1275,  0.4504,  0.2846,\n",
      "          0.1298, -0.2702,  0.2788, -0.4750,  0.4229,  0.4523, -0.0758, -0.2783],\n",
      "        [-0.3499, -0.2023, -0.4744, -0.0753, -0.4848,  0.2453,  0.2835, -0.4766,\n",
      "          0.2262,  0.4486,  0.2403, -0.1947, -0.2425, -0.1428,  0.4409,  0.1512,\n",
      "          0.3808, -0.0773, -0.2466, -0.3715, -0.2326, -0.4930,  0.0474, -0.2723,\n",
      "         -0.3499,  0.1222,  0.4731, -0.3590, -0.4541, -0.1420, -0.2363,  0.4959]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wq spliced: torch.Size([2, 64])\n",
      "tensor([[-3.2097e-01, -1.5344e-02, -1.1510e-01, -3.0874e-01,  3.3117e-01,\n",
      "         -2.5896e-01,  6.9617e-02,  4.6040e-01, -4.1660e-01,  4.1727e-01,\n",
      "          4.1405e-01, -2.2532e-01,  4.6028e-01,  7.0296e-02, -4.5145e-01,\n",
      "         -4.9349e-01,  1.2274e-01,  2.0562e-01,  3.0310e-01,  2.8698e-01,\n",
      "         -8.8241e-02, -1.6075e-01,  2.5161e-01,  4.2444e-01, -8.2624e-02,\n",
      "         -4.0507e-01, -1.2071e-01, -4.2426e-01, -2.8454e-01,  2.0912e-01,\n",
      "          2.4156e-01, -3.9879e-01, -8.7626e-02, -4.4174e-01, -4.6600e-01,\n",
      "         -4.8568e-03, -4.8418e-01,  6.4661e-02,  4.8494e-01, -1.6592e-01,\n",
      "         -2.0181e-01,  4.4469e-01,  9.5466e-02,  4.8366e-01,  4.4880e-01,\n",
      "         -1.9130e-01,  1.5374e-01,  2.2177e-01, -8.8283e-02, -1.0314e-01,\n",
      "          4.6487e-01, -3.7954e-01, -2.3864e-01, -4.2001e-01, -1.2691e-01,\n",
      "          1.9158e-01,  1.1451e-01,  1.6561e-01,  1.7231e-01,  1.7429e-01,\n",
      "         -1.8123e-01,  3.6120e-01,  3.1331e-01, -9.2029e-02],\n",
      "        [-1.4990e-01,  2.7819e-01,  7.4129e-02, -4.4660e-02,  4.3282e-01,\n",
      "         -3.0581e-01,  1.8844e-01,  4.4872e-01, -1.1302e-01,  1.4516e-01,\n",
      "         -4.8995e-05,  4.3458e-01,  5.3082e-02,  4.4512e-01,  1.2523e-01,\n",
      "         -1.1507e-01,  4.2555e-02, -3.0911e-01,  4.3753e-01, -2.9667e-01,\n",
      "         -1.4520e-02, -1.0416e-02, -3.2493e-01,  4.7091e-01,  2.3998e-01,\n",
      "          3.4873e-01,  5.5291e-02,  9.5433e-02,  1.9302e-01,  3.8875e-01,\n",
      "          2.3504e-01, -1.4209e-01, -3.7848e-01, -4.9171e-01, -4.6506e-01,\n",
      "         -1.9350e-01,  3.7812e-01,  3.6263e-02, -1.0092e-01, -3.0463e-01,\n",
      "         -5.2127e-02, -2.3248e-01, -1.8609e-01, -4.9422e-01,  4.5411e-01,\n",
      "          2.4784e-01,  1.7444e-01,  2.7136e-01, -5.1177e-03,  3.8824e-01,\n",
      "         -3.7151e-01, -1.4505e-01, -3.5091e-01, -4.1308e-01, -4.3970e-01,\n",
      "          2.8640e-01, -7.5188e-02, -1.1953e-01,  2.8627e-01,  2.9312e-01,\n",
      "          4.1545e-02,  3.4332e-01,  5.6798e-02,  2.4595e-02]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Wk spliced: torch.Size([2, 16])\n",
      "tensor([[-0.1553, -0.3907, -0.1353, -0.1834,  0.3281,  0.3566,  0.0125,  0.3220,\n",
      "         -0.4728,  0.4053, -0.1106,  0.2006, -0.3140,  0.0940, -0.0235, -0.4638],\n",
      "        [ 0.0832, -0.0981, -0.4366, -0.3156, -0.4994,  0.2409, -0.0746, -0.0859,\n",
      "         -0.4486,  0.2031,  0.1667, -0.3426, -0.3101,  0.2331,  0.2109,  0.1308]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Wv spliced: torch.Size([2, 16])\n",
      "tensor([[-0.2297, -0.2896,  0.1119, -0.2086, -0.1285, -0.1827, -0.0931, -0.2413,\n",
      "         -0.4642, -0.1566, -0.2637, -0.0271, -0.1591, -0.2213, -0.1838,  0.3740],\n",
      "        [-0.0579, -0.2175,  0.2943,  0.3645, -0.0325,  0.2580, -0.4358, -0.0848,\n",
      "          0.0054,  0.3866, -0.4527, -0.2348,  0.3268,  0.0179,  0.0364,  0.2810]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Wqkv_spliced: torch.Size([2, 96])\n",
      "tensor([[-3.2097e-01, -1.5344e-02, -1.1510e-01, -3.0874e-01,  3.3117e-01,\n",
      "         -2.5896e-01,  6.9617e-02,  4.6040e-01, -4.1660e-01,  4.1727e-01,\n",
      "          4.1405e-01, -2.2532e-01,  4.6028e-01,  7.0296e-02, -4.5145e-01,\n",
      "         -4.9349e-01,  1.2274e-01,  2.0562e-01,  3.0310e-01,  2.8698e-01,\n",
      "         -8.8241e-02, -1.6075e-01,  2.5161e-01,  4.2444e-01, -8.2624e-02,\n",
      "         -4.0507e-01, -1.2071e-01, -4.2426e-01, -2.8454e-01,  2.0912e-01,\n",
      "          2.4156e-01, -3.9879e-01, -8.7626e-02, -4.4174e-01, -4.6600e-01,\n",
      "         -4.8568e-03, -4.8418e-01,  6.4661e-02,  4.8494e-01, -1.6592e-01,\n",
      "         -2.0181e-01,  4.4469e-01,  9.5466e-02,  4.8366e-01,  4.4880e-01,\n",
      "         -1.9130e-01,  1.5374e-01,  2.2177e-01, -8.8283e-02, -1.0314e-01,\n",
      "          4.6487e-01, -3.7954e-01, -2.3864e-01, -4.2001e-01, -1.2691e-01,\n",
      "          1.9158e-01,  1.1451e-01,  1.6561e-01,  1.7231e-01,  1.7429e-01,\n",
      "         -1.8123e-01,  3.6120e-01,  3.1331e-01, -9.2029e-02, -1.5530e-01,\n",
      "         -3.9074e-01, -1.3531e-01, -1.8341e-01,  3.2812e-01,  3.5656e-01,\n",
      "          1.2500e-02,  3.2196e-01, -4.7280e-01,  4.0531e-01, -1.1058e-01,\n",
      "          2.0058e-01, -3.1400e-01,  9.3957e-02, -2.3539e-02, -4.6383e-01,\n",
      "         -2.2971e-01, -2.8959e-01,  1.1191e-01, -2.0859e-01, -1.2854e-01,\n",
      "         -1.8272e-01, -9.3147e-02, -2.4128e-01, -4.6417e-01, -1.5658e-01,\n",
      "         -2.6369e-01, -2.7113e-02, -1.5909e-01, -2.2127e-01, -1.8380e-01,\n",
      "          3.7399e-01],\n",
      "        [-1.4990e-01,  2.7819e-01,  7.4129e-02, -4.4660e-02,  4.3282e-01,\n",
      "         -3.0581e-01,  1.8844e-01,  4.4872e-01, -1.1302e-01,  1.4516e-01,\n",
      "         -4.8995e-05,  4.3458e-01,  5.3082e-02,  4.4512e-01,  1.2523e-01,\n",
      "         -1.1507e-01,  4.2555e-02, -3.0911e-01,  4.3753e-01, -2.9667e-01,\n",
      "         -1.4520e-02, -1.0416e-02, -3.2493e-01,  4.7091e-01,  2.3998e-01,\n",
      "          3.4873e-01,  5.5291e-02,  9.5433e-02,  1.9302e-01,  3.8875e-01,\n",
      "          2.3504e-01, -1.4209e-01, -3.7848e-01, -4.9171e-01, -4.6506e-01,\n",
      "         -1.9350e-01,  3.7812e-01,  3.6263e-02, -1.0092e-01, -3.0463e-01,\n",
      "         -5.2127e-02, -2.3248e-01, -1.8609e-01, -4.9422e-01,  4.5411e-01,\n",
      "          2.4784e-01,  1.7444e-01,  2.7136e-01, -5.1177e-03,  3.8824e-01,\n",
      "         -3.7151e-01, -1.4505e-01, -3.5091e-01, -4.1308e-01, -4.3970e-01,\n",
      "          2.8640e-01, -7.5188e-02, -1.1953e-01,  2.8627e-01,  2.9312e-01,\n",
      "          4.1545e-02,  3.4332e-01,  5.6798e-02,  2.4595e-02,  8.3232e-02,\n",
      "         -9.8104e-02, -4.3657e-01, -3.1562e-01, -4.9943e-01,  2.4091e-01,\n",
      "         -7.4649e-02, -8.5931e-02, -4.4864e-01,  2.0309e-01,  1.6673e-01,\n",
      "         -3.4258e-01, -3.1006e-01,  2.3308e-01,  2.1086e-01,  1.3077e-01,\n",
      "         -5.7863e-02, -2.1753e-01,  2.9426e-01,  3.6454e-01, -3.2457e-02,\n",
      "          2.5800e-01, -4.3584e-01, -8.4784e-02,  5.4089e-03,  3.8661e-01,\n",
      "         -4.5274e-01, -2.3479e-01,  3.2677e-01,  1.7917e-02,  3.6443e-02,\n",
      "          2.8099e-01]], grad_fn=<CatBackward0>)\n",
      "xqkv: torch.Size([1, 3, 96])\n",
      "tensor([[[ 0.3968, -0.3395, -0.0184,  0.2566, -0.7588,  0.5524, -0.2819,\n",
      "          -0.8627,  0.4126, -0.4534, -0.2689, -0.3996, -0.3657, -0.6048,\n",
      "           0.1359,  0.4651, -0.1332,  0.2547, -0.7465,  0.1863,  0.0756,\n",
      "           0.1175,  0.2447, -0.8673, -0.2478, -0.1750,  0.0090,  0.1557,\n",
      "          -0.0576, -0.6242, -0.4522,  0.4375,  0.5324,  0.9046,  0.8869,\n",
      "           0.2462, -0.1605, -0.0876, -0.1882,  0.4905,  0.1966,  0.0032,\n",
      "           0.1718,  0.3067, -0.8620, -0.1871, -0.3190, -0.4849,  0.0638,\n",
      "          -0.4207,  0.1647,  0.4287,  0.5958,  0.7917,  0.6348, -0.4842,\n",
      "           0.0201,  0.0426, -0.4715, -0.4814,  0.0655, -0.6659, -0.2749,\n",
      "           0.0289, -0.0037,  0.3771,  0.6363,  0.5156,  0.4143, -0.5342,\n",
      "           0.0857, -0.1012,  0.8707, -0.5184, -0.1376,  0.3001,  0.5935,\n",
      "          -0.3538, -0.2496,  0.1370,  0.2219,  0.4614, -0.4423, -0.3224,\n",
      "           0.1243, -0.2054,  0.6080,  0.2632,  0.2947, -0.3840,  0.7400,\n",
      "           0.3126, -0.3071,  0.1212,  0.0736, -0.5959],\n",
      "         [-0.4757,  0.0248, -0.1493, -0.4410,  0.5371, -0.4146,  0.1291,\n",
      "           0.7213, -0.6039,  0.6102,  0.5815, -0.2441,  0.6552,  0.1728,\n",
      "          -0.6132, -0.7122,  0.1795,  0.2373,  0.4985,  0.3536, -0.1263,\n",
      "          -0.2275,  0.2993,  0.6745, -0.0761, -0.5108, -0.1603, -0.5799,\n",
      "          -0.3675,  0.3584,  0.3784, -0.5837, -0.1861, -0.7022, -0.7319,\n",
      "          -0.0390, -0.6170,  0.0968,  0.6642, -0.2837, -0.2921,  0.5858,\n",
      "           0.1031,  0.5970,  0.7059, -0.2274,  0.2449,  0.3566, -0.1248,\n",
      "          -0.0802,  0.5910, -0.5572, -0.3936, -0.6586, -0.2514,  0.3167,\n",
      "           0.1483,  0.2127,  0.2896,  0.2936, -0.2476,  0.5644,  0.4495,\n",
      "          -0.1251, -0.2042, -0.5651, -0.2627, -0.3101,  0.3777,  0.5408,\n",
      "           0.0051,  0.4379, -0.7387,  0.6030, -0.1275,  0.2247, -0.4926,\n",
      "           0.1708,  0.0020, -0.6296, -0.3322, -0.4429,  0.2062, -0.2322,\n",
      "          -0.1859, -0.2136, -0.2034, -0.3530, -0.6510, -0.1555, -0.4457,\n",
      "          -0.0772, -0.1690, -0.3078, -0.2521,  0.5720],\n",
      "         [-0.1069,  0.3886,  0.1381,  0.0344,  0.4941, -0.3413,  0.2383,\n",
      "           0.4758, -0.0263,  0.0704, -0.1289,  0.6696, -0.0700,  0.5922,\n",
      "           0.3132, -0.0052,  0.0205, -0.4904,  0.5093, -0.4986,  0.0074,\n",
      "           0.0356, -0.5265,  0.5176,  0.3568,  0.6071,  0.1138,  0.2636,\n",
      "           0.3548,  0.4712,  0.2491, -0.0720, -0.4949, -0.5409, -0.4966,\n",
      "          -0.2654,  0.6723,  0.0299, -0.2901, -0.3686, -0.0091, -0.4591,\n",
      "          -0.2864, -0.8323,  0.4869,  0.4014,  0.1928,  0.3054,  0.0204,\n",
      "           0.5677, -0.6572, -0.0820, -0.4099, -0.4392, -0.5671,  0.3355,\n",
      "          -0.1394, -0.2164,  0.3413,  0.3502,  0.1137,  0.3613, -0.0191,\n",
      "           0.0626,  0.1631, -0.0138, -0.5602, -0.3784, -0.7911,  0.2214,\n",
      "          -0.1069, -0.2187, -0.4718,  0.1541,  0.2644, -0.5350, -0.3301,\n",
      "           0.2923,  0.2982,  0.3247, -0.0084, -0.2100,  0.3711,  0.5678,\n",
      "          -0.0048,  0.4128, -0.5723, -0.0419,  0.1519,  0.5821, -0.5426,\n",
      "          -0.3155,  0.5003,  0.0936,  0.1075,  0.2713]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "xq: torch.Size([1, 3, 64])\n",
      "tensor([[[ 0.3968, -0.3395, -0.0184,  0.2566, -0.7588,  0.5524, -0.2819,\n",
      "          -0.8627,  0.4126, -0.4534, -0.2689, -0.3996, -0.3657, -0.6048,\n",
      "           0.1359,  0.4651, -0.1332,  0.2547, -0.7465,  0.1863,  0.0756,\n",
      "           0.1175,  0.2447, -0.8673, -0.2478, -0.1750,  0.0090,  0.1557,\n",
      "          -0.0576, -0.6242, -0.4522,  0.4375,  0.5324,  0.9046,  0.8869,\n",
      "           0.2462, -0.1605, -0.0876, -0.1882,  0.4905,  0.1966,  0.0032,\n",
      "           0.1718,  0.3067, -0.8620, -0.1871, -0.3190, -0.4849,  0.0638,\n",
      "          -0.4207,  0.1647,  0.4287,  0.5958,  0.7917,  0.6348, -0.4842,\n",
      "           0.0201,  0.0426, -0.4715, -0.4814,  0.0655, -0.6659, -0.2749,\n",
      "           0.0289],\n",
      "         [-0.4757,  0.0248, -0.1493, -0.4410,  0.5371, -0.4146,  0.1291,\n",
      "           0.7213, -0.6039,  0.6102,  0.5815, -0.2441,  0.6552,  0.1728,\n",
      "          -0.6132, -0.7122,  0.1795,  0.2373,  0.4985,  0.3536, -0.1263,\n",
      "          -0.2275,  0.2993,  0.6745, -0.0761, -0.5108, -0.1603, -0.5799,\n",
      "          -0.3675,  0.3584,  0.3784, -0.5837, -0.1861, -0.7022, -0.7319,\n",
      "          -0.0390, -0.6170,  0.0968,  0.6642, -0.2837, -0.2921,  0.5858,\n",
      "           0.1031,  0.5970,  0.7059, -0.2274,  0.2449,  0.3566, -0.1248,\n",
      "          -0.0802,  0.5910, -0.5572, -0.3936, -0.6586, -0.2514,  0.3167,\n",
      "           0.1483,  0.2127,  0.2896,  0.2936, -0.2476,  0.5644,  0.4495,\n",
      "          -0.1251],\n",
      "         [-0.1069,  0.3886,  0.1381,  0.0344,  0.4941, -0.3413,  0.2383,\n",
      "           0.4758, -0.0263,  0.0704, -0.1289,  0.6696, -0.0700,  0.5922,\n",
      "           0.3132, -0.0052,  0.0205, -0.4904,  0.5093, -0.4986,  0.0074,\n",
      "           0.0356, -0.5265,  0.5176,  0.3568,  0.6071,  0.1138,  0.2636,\n",
      "           0.3548,  0.4712,  0.2491, -0.0720, -0.4949, -0.5409, -0.4966,\n",
      "          -0.2654,  0.6723,  0.0299, -0.2901, -0.3686, -0.0091, -0.4591,\n",
      "          -0.2864, -0.8323,  0.4869,  0.4014,  0.1928,  0.3054,  0.0204,\n",
      "           0.5677, -0.6572, -0.0820, -0.4099, -0.4392, -0.5671,  0.3355,\n",
      "          -0.1394, -0.2164,  0.3413,  0.3502,  0.1137,  0.3613, -0.0191,\n",
      "           0.0626]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xk: torch.Size([1, 3, 16])\n",
      "tensor([[[-0.0037,  0.3771,  0.6363,  0.5156,  0.4143, -0.5342,  0.0857,\n",
      "          -0.1012,  0.8707, -0.5184, -0.1376,  0.3001,  0.5935, -0.3538,\n",
      "          -0.2496,  0.1370],\n",
      "         [-0.2042, -0.5651, -0.2627, -0.3101,  0.3777,  0.5408,  0.0051,\n",
      "           0.4379, -0.7387,  0.6030, -0.1275,  0.2247, -0.4926,  0.1708,\n",
      "           0.0020, -0.6296],\n",
      "         [ 0.1631, -0.0138, -0.5602, -0.3784, -0.7911,  0.2214, -0.1069,\n",
      "          -0.2187, -0.4718,  0.1541,  0.2644, -0.5350, -0.3301,  0.2923,\n",
      "           0.2982,  0.3247]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xv: torch.Size([1, 3, 16])\n",
      "tensor([[[ 0.2219,  0.4614, -0.4423, -0.3224,  0.1243, -0.2054,  0.6080,\n",
      "           0.2632,  0.2947, -0.3840,  0.7400,  0.3126, -0.3071,  0.1212,\n",
      "           0.0736, -0.5959],\n",
      "         [-0.3322, -0.4429,  0.2062, -0.2322, -0.1859, -0.2136, -0.2034,\n",
      "          -0.3530, -0.6510, -0.1555, -0.4457, -0.0772, -0.1690, -0.3078,\n",
      "          -0.2521,  0.5720],\n",
      "         [-0.0084, -0.2100,  0.3711,  0.5678, -0.0048,  0.4128, -0.5723,\n",
      "          -0.0419,  0.1519,  0.5821, -0.5426, -0.3155,  0.5003,  0.0936,\n",
      "           0.1075,  0.2713]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xq reshaped: torch.Size([1, 3, 4, 16])\n",
      "tensor([[[[ 0.3968, -0.3395, -0.0184,  0.2566, -0.7588,  0.5524, -0.2819,\n",
      "           -0.8627,  0.4126, -0.4534, -0.2689, -0.3996, -0.3657, -0.6048,\n",
      "            0.1359,  0.4651],\n",
      "          [-0.1332,  0.2547, -0.7465,  0.1863,  0.0756,  0.1175,  0.2447,\n",
      "           -0.8673, -0.2478, -0.1750,  0.0090,  0.1557, -0.0576, -0.6242,\n",
      "           -0.4522,  0.4375],\n",
      "          [ 0.5324,  0.9046,  0.8869,  0.2462, -0.1605, -0.0876, -0.1882,\n",
      "            0.4905,  0.1966,  0.0032,  0.1718,  0.3067, -0.8620, -0.1871,\n",
      "           -0.3190, -0.4849],\n",
      "          [ 0.0638, -0.4207,  0.1647,  0.4287,  0.5958,  0.7917,  0.6348,\n",
      "           -0.4842,  0.0201,  0.0426, -0.4715, -0.4814,  0.0655, -0.6659,\n",
      "           -0.2749,  0.0289]],\n",
      "\n",
      "         [[-0.4757,  0.0248, -0.1493, -0.4410,  0.5371, -0.4146,  0.1291,\n",
      "            0.7213, -0.6039,  0.6102,  0.5815, -0.2441,  0.6552,  0.1728,\n",
      "           -0.6132, -0.7122],\n",
      "          [ 0.1795,  0.2373,  0.4985,  0.3536, -0.1263, -0.2275,  0.2993,\n",
      "            0.6745, -0.0761, -0.5108, -0.1603, -0.5799, -0.3675,  0.3584,\n",
      "            0.3784, -0.5837],\n",
      "          [-0.1861, -0.7022, -0.7319, -0.0390, -0.6170,  0.0968,  0.6642,\n",
      "           -0.2837, -0.2921,  0.5858,  0.1031,  0.5970,  0.7059, -0.2274,\n",
      "            0.2449,  0.3566],\n",
      "          [-0.1248, -0.0802,  0.5910, -0.5572, -0.3936, -0.6586, -0.2514,\n",
      "            0.3167,  0.1483,  0.2127,  0.2896,  0.2936, -0.2476,  0.5644,\n",
      "            0.4495, -0.1251]],\n",
      "\n",
      "         [[-0.1069,  0.3886,  0.1381,  0.0344,  0.4941, -0.3413,  0.2383,\n",
      "            0.4758, -0.0263,  0.0704, -0.1289,  0.6696, -0.0700,  0.5922,\n",
      "            0.3132, -0.0052],\n",
      "          [ 0.0205, -0.4904,  0.5093, -0.4986,  0.0074,  0.0356, -0.5265,\n",
      "            0.5176,  0.3568,  0.6071,  0.1138,  0.2636,  0.3548,  0.4712,\n",
      "            0.2491, -0.0720],\n",
      "          [-0.4949, -0.5409, -0.4966, -0.2654,  0.6723,  0.0299, -0.2901,\n",
      "           -0.3686, -0.0091, -0.4591, -0.2864, -0.8323,  0.4869,  0.4014,\n",
      "            0.1928,  0.3054],\n",
      "          [ 0.0204,  0.5677, -0.6572, -0.0820, -0.4099, -0.4392, -0.5671,\n",
      "            0.3355, -0.1394, -0.2164,  0.3413,  0.3502,  0.1137,  0.3613,\n",
      "           -0.0191,  0.0626]]]], grad_fn=<ViewBackward0>)\n",
      "xk reshaped: torch.Size([1, 3, 1, 16])\n",
      "tensor([[[[-0.0037,  0.3771,  0.6363,  0.5156,  0.4143, -0.5342,  0.0857,\n",
      "           -0.1012,  0.8707, -0.5184, -0.1376,  0.3001,  0.5935, -0.3538,\n",
      "           -0.2496,  0.1370]],\n",
      "\n",
      "         [[-0.2042, -0.5651, -0.2627, -0.3101,  0.3777,  0.5408,  0.0051,\n",
      "            0.4379, -0.7387,  0.6030, -0.1275,  0.2247, -0.4926,  0.1708,\n",
      "            0.0020, -0.6296]],\n",
      "\n",
      "         [[ 0.1631, -0.0138, -0.5602, -0.3784, -0.7911,  0.2214, -0.1069,\n",
      "           -0.2187, -0.4718,  0.1541,  0.2644, -0.5350, -0.3301,  0.2923,\n",
      "            0.2982,  0.3247]]]], grad_fn=<ViewBackward0>)\n",
      "xv reshaped: torch.Size([1, 3, 1, 16])\n",
      "tensor([[[[ 0.2219,  0.4614, -0.4423, -0.3224,  0.1243, -0.2054,  0.6080,\n",
      "            0.2632,  0.2947, -0.3840,  0.7400,  0.3126, -0.3071,  0.1212,\n",
      "            0.0736, -0.5959]],\n",
      "\n",
      "         [[-0.3322, -0.4429,  0.2062, -0.2322, -0.1859, -0.2136, -0.2034,\n",
      "           -0.3530, -0.6510, -0.1555, -0.4457, -0.0772, -0.1690, -0.3078,\n",
      "           -0.2521,  0.5720]],\n",
      "\n",
      "         [[-0.0084, -0.2100,  0.3711,  0.5678, -0.0048,  0.4128, -0.5723,\n",
      "           -0.0419,  0.1519,  0.5821, -0.5426, -0.3155,  0.5003,  0.0936,\n",
      "            0.1075,  0.2713]]]], grad_fn=<ViewBackward0>)\n",
      "rotated xq: torch.Size([1, 3, 4, 16])\n",
      "tensor([[[[ 0.3968, -0.3395, -0.0184,  0.2566, -0.7588,  0.5524, -0.2819,\n",
      "           -0.8627,  0.4126, -0.4534, -0.2689, -0.3996, -0.3657, -0.6048,\n",
      "            0.1359,  0.4651],\n",
      "          [-0.1332,  0.2547, -0.7465,  0.1863,  0.0756,  0.1175,  0.2447,\n",
      "           -0.8673, -0.2478, -0.1750,  0.0090,  0.1557, -0.0576, -0.6242,\n",
      "           -0.4522,  0.4375],\n",
      "          [ 0.5324,  0.9046,  0.8869,  0.2462, -0.1605, -0.0876, -0.1882,\n",
      "            0.4905,  0.1966,  0.0032,  0.1718,  0.3067, -0.8620, -0.1871,\n",
      "           -0.3190, -0.4849],\n",
      "          [ 0.0638, -0.4207,  0.1647,  0.4287,  0.5958,  0.7917,  0.6348,\n",
      "           -0.4842,  0.0201,  0.0426, -0.4715, -0.4814,  0.0655, -0.6659,\n",
      "           -0.2749,  0.0289]],\n",
      "\n",
      "         [[ 0.2511, -0.3044, -0.3227, -0.3909,  0.4690, -0.4236,  0.1485,\n",
      "            0.7338, -0.7266,  0.5294,  0.5062, -0.3182,  0.7056,  0.1493,\n",
      "           -0.6088, -0.6993],\n",
      "          [ 0.1610,  0.4731,  0.5236,  0.4507, -0.0890, -0.2473,  0.2871,\n",
      "            0.6847,  0.1099, -0.3056,  0.0027, -0.5082, -0.3782,  0.3451,\n",
      "            0.3876, -0.5716],\n",
      "          [ 0.1453, -0.9064, -0.7276, -0.1440, -0.6844,  0.1095,  0.6562,\n",
      "           -0.2900, -0.3144,  0.1212, -0.1296,  0.5806,  0.6408, -0.2216,\n",
      "            0.2658,  0.3515],\n",
      "          [-0.1922, -0.1813,  0.4716, -0.6003, -0.3669, -0.6893, -0.2655,\n",
      "            0.3189, -0.0249,  0.1372,  0.4591,  0.1904, -0.2857,  0.5265,\n",
      "            0.4413, -0.1195]],\n",
      "\n",
      "         [[ 0.0684,  0.1041,  0.1876, -0.2009,  0.4981, -0.4056,  0.2180,\n",
      "            0.4757, -0.0863,  0.3809, -0.0223,  0.6397,  0.0296,  0.5502,\n",
      "            0.3276,  0.0117],\n",
      "          [-0.3329, -0.7593,  0.3435, -0.5592, -0.0632, -0.0175, -0.5412,\n",
      "            0.5198, -0.1298, -0.1805,  0.3929,  0.0735,  0.3492,  0.4723,\n",
      "            0.2153, -0.0535],\n",
      "          [ 0.2142,  0.1808, -0.2312,  0.0410,  0.5622, -0.0153, -0.3017,\n",
      "           -0.3793, -0.4462, -0.6860, -0.5246, -0.8726,  0.6107,  0.4022,\n",
      "            0.1741,  0.2921],\n",
      "          [ 0.1182,  0.4402, -0.7318, -0.1988, -0.4243, -0.4770, -0.5648,\n",
      "            0.3331,  0.0765,  0.4187, -0.1132,  0.2997,  0.0300,  0.3097,\n",
      "           -0.0549,  0.0745]]]], grad_fn=<TransposeBackward0>)\n",
      "rotated xk: torch.Size([1, 3, 1, 16])\n",
      "tensor([[[[-0.0037,  0.3771,  0.6363,  0.5156,  0.4143, -0.5342,  0.0857,\n",
      "           -0.1012,  0.8707, -0.5184, -0.1376,  0.3001,  0.5935, -0.3538,\n",
      "           -0.2496,  0.1370]],\n",
      "\n",
      "         [[ 0.5112, -0.7996, -0.2100, -0.3450,  0.4250,  0.5304,  0.0051,\n",
      "            0.4490, -0.5710,  0.2089, -0.2029,  0.1663, -0.4524,  0.2009,\n",
      "            0.0022, -0.6217]],\n",
      "\n",
      "         [[ 0.3612, -0.1449, -0.6081, -0.1684, -0.7097,  0.1872, -0.1255,\n",
      "           -0.2301,  0.3447,  0.0540, -0.1179, -0.6333, -0.4807,  0.3153,\n",
      "            0.2909,  0.3167]]]], grad_fn=<TransposeBackward0>)\n",
      "repeat_interleaved xk: torch.Size([1, 3, 4, 16])\n",
      "tensor([[[[-0.0037,  0.3771,  0.6363,  0.5156,  0.4143, -0.5342,  0.0857,\n",
      "           -0.1012,  0.8707, -0.5184, -0.1376,  0.3001,  0.5935, -0.3538,\n",
      "           -0.2496,  0.1370],\n",
      "          [-0.0037,  0.3771,  0.6363,  0.5156,  0.4143, -0.5342,  0.0857,\n",
      "           -0.1012,  0.8707, -0.5184, -0.1376,  0.3001,  0.5935, -0.3538,\n",
      "           -0.2496,  0.1370],\n",
      "          [-0.0037,  0.3771,  0.6363,  0.5156,  0.4143, -0.5342,  0.0857,\n",
      "           -0.1012,  0.8707, -0.5184, -0.1376,  0.3001,  0.5935, -0.3538,\n",
      "           -0.2496,  0.1370],\n",
      "          [-0.0037,  0.3771,  0.6363,  0.5156,  0.4143, -0.5342,  0.0857,\n",
      "           -0.1012,  0.8707, -0.5184, -0.1376,  0.3001,  0.5935, -0.3538,\n",
      "           -0.2496,  0.1370]],\n",
      "\n",
      "         [[ 0.5112, -0.7996, -0.2100, -0.3450,  0.4250,  0.5304,  0.0051,\n",
      "            0.4490, -0.5710,  0.2089, -0.2029,  0.1663, -0.4524,  0.2009,\n",
      "            0.0022, -0.6217],\n",
      "          [ 0.5112, -0.7996, -0.2100, -0.3450,  0.4250,  0.5304,  0.0051,\n",
      "            0.4490, -0.5710,  0.2089, -0.2029,  0.1663, -0.4524,  0.2009,\n",
      "            0.0022, -0.6217],\n",
      "          [ 0.5112, -0.7996, -0.2100, -0.3450,  0.4250,  0.5304,  0.0051,\n",
      "            0.4490, -0.5710,  0.2089, -0.2029,  0.1663, -0.4524,  0.2009,\n",
      "            0.0022, -0.6217],\n",
      "          [ 0.5112, -0.7996, -0.2100, -0.3450,  0.4250,  0.5304,  0.0051,\n",
      "            0.4490, -0.5710,  0.2089, -0.2029,  0.1663, -0.4524,  0.2009,\n",
      "            0.0022, -0.6217]],\n",
      "\n",
      "         [[ 0.3612, -0.1449, -0.6081, -0.1684, -0.7097,  0.1872, -0.1255,\n",
      "           -0.2301,  0.3447,  0.0540, -0.1179, -0.6333, -0.4807,  0.3153,\n",
      "            0.2909,  0.3167],\n",
      "          [ 0.3612, -0.1449, -0.6081, -0.1684, -0.7097,  0.1872, -0.1255,\n",
      "           -0.2301,  0.3447,  0.0540, -0.1179, -0.6333, -0.4807,  0.3153,\n",
      "            0.2909,  0.3167],\n",
      "          [ 0.3612, -0.1449, -0.6081, -0.1684, -0.7097,  0.1872, -0.1255,\n",
      "           -0.2301,  0.3447,  0.0540, -0.1179, -0.6333, -0.4807,  0.3153,\n",
      "            0.2909,  0.3167],\n",
      "          [ 0.3612, -0.1449, -0.6081, -0.1684, -0.7097,  0.1872, -0.1255,\n",
      "           -0.2301,  0.3447,  0.0540, -0.1179, -0.6333, -0.4807,  0.3153,\n",
      "            0.2909,  0.3167]]]], grad_fn=<ViewBackward0>)\n",
      "repeat_interleaved xv: torch.Size([1, 3, 4, 16])\n",
      "tensor([[[[ 0.2219,  0.4614, -0.4423, -0.3224,  0.1243, -0.2054,  0.6080,\n",
      "            0.2632,  0.2947, -0.3840,  0.7400,  0.3126, -0.3071,  0.1212,\n",
      "            0.0736, -0.5959],\n",
      "          [ 0.2219,  0.4614, -0.4423, -0.3224,  0.1243, -0.2054,  0.6080,\n",
      "            0.2632,  0.2947, -0.3840,  0.7400,  0.3126, -0.3071,  0.1212,\n",
      "            0.0736, -0.5959],\n",
      "          [ 0.2219,  0.4614, -0.4423, -0.3224,  0.1243, -0.2054,  0.6080,\n",
      "            0.2632,  0.2947, -0.3840,  0.7400,  0.3126, -0.3071,  0.1212,\n",
      "            0.0736, -0.5959],\n",
      "          [ 0.2219,  0.4614, -0.4423, -0.3224,  0.1243, -0.2054,  0.6080,\n",
      "            0.2632,  0.2947, -0.3840,  0.7400,  0.3126, -0.3071,  0.1212,\n",
      "            0.0736, -0.5959]],\n",
      "\n",
      "         [[-0.3322, -0.4429,  0.2062, -0.2322, -0.1859, -0.2136, -0.2034,\n",
      "           -0.3530, -0.6510, -0.1555, -0.4457, -0.0772, -0.1690, -0.3078,\n",
      "           -0.2521,  0.5720],\n",
      "          [-0.3322, -0.4429,  0.2062, -0.2322, -0.1859, -0.2136, -0.2034,\n",
      "           -0.3530, -0.6510, -0.1555, -0.4457, -0.0772, -0.1690, -0.3078,\n",
      "           -0.2521,  0.5720],\n",
      "          [-0.3322, -0.4429,  0.2062, -0.2322, -0.1859, -0.2136, -0.2034,\n",
      "           -0.3530, -0.6510, -0.1555, -0.4457, -0.0772, -0.1690, -0.3078,\n",
      "           -0.2521,  0.5720],\n",
      "          [-0.3322, -0.4429,  0.2062, -0.2322, -0.1859, -0.2136, -0.2034,\n",
      "           -0.3530, -0.6510, -0.1555, -0.4457, -0.0772, -0.1690, -0.3078,\n",
      "           -0.2521,  0.5720]],\n",
      "\n",
      "         [[-0.0084, -0.2100,  0.3711,  0.5678, -0.0048,  0.4128, -0.5723,\n",
      "           -0.0419,  0.1519,  0.5821, -0.5426, -0.3155,  0.5003,  0.0936,\n",
      "            0.1075,  0.2713],\n",
      "          [-0.0084, -0.2100,  0.3711,  0.5678, -0.0048,  0.4128, -0.5723,\n",
      "           -0.0419,  0.1519,  0.5821, -0.5426, -0.3155,  0.5003,  0.0936,\n",
      "            0.1075,  0.2713],\n",
      "          [-0.0084, -0.2100,  0.3711,  0.5678, -0.0048,  0.4128, -0.5723,\n",
      "           -0.0419,  0.1519,  0.5821, -0.5426, -0.3155,  0.5003,  0.0936,\n",
      "            0.1075,  0.2713],\n",
      "          [-0.0084, -0.2100,  0.3711,  0.5678, -0.0048,  0.4128, -0.5723,\n",
      "           -0.0419,  0.1519,  0.5821, -0.5426, -0.3155,  0.5003,  0.0936,\n",
      "            0.1075,  0.2713]]]], grad_fn=<ViewBackward0>)\n",
      "transposed xq: torch.Size([1, 4, 3, 16])\n",
      "tensor([[[[ 0.3968, -0.3395, -0.0184,  0.2566, -0.7588,  0.5524, -0.2819,\n",
      "           -0.8627,  0.4126, -0.4534, -0.2689, -0.3996, -0.3657, -0.6048,\n",
      "            0.1359,  0.4651],\n",
      "          [ 0.2511, -0.3044, -0.3227, -0.3909,  0.4690, -0.4236,  0.1485,\n",
      "            0.7338, -0.7266,  0.5294,  0.5062, -0.3182,  0.7056,  0.1493,\n",
      "           -0.6088, -0.6993],\n",
      "          [ 0.0684,  0.1041,  0.1876, -0.2009,  0.4981, -0.4056,  0.2180,\n",
      "            0.4757, -0.0863,  0.3809, -0.0223,  0.6397,  0.0296,  0.5502,\n",
      "            0.3276,  0.0117]],\n",
      "\n",
      "         [[-0.1332,  0.2547, -0.7465,  0.1863,  0.0756,  0.1175,  0.2447,\n",
      "           -0.8673, -0.2478, -0.1750,  0.0090,  0.1557, -0.0576, -0.6242,\n",
      "           -0.4522,  0.4375],\n",
      "          [ 0.1610,  0.4731,  0.5236,  0.4507, -0.0890, -0.2473,  0.2871,\n",
      "            0.6847,  0.1099, -0.3056,  0.0027, -0.5082, -0.3782,  0.3451,\n",
      "            0.3876, -0.5716],\n",
      "          [-0.3329, -0.7593,  0.3435, -0.5592, -0.0632, -0.0175, -0.5412,\n",
      "            0.5198, -0.1298, -0.1805,  0.3929,  0.0735,  0.3492,  0.4723,\n",
      "            0.2153, -0.0535]],\n",
      "\n",
      "         [[ 0.5324,  0.9046,  0.8869,  0.2462, -0.1605, -0.0876, -0.1882,\n",
      "            0.4905,  0.1966,  0.0032,  0.1718,  0.3067, -0.8620, -0.1871,\n",
      "           -0.3190, -0.4849],\n",
      "          [ 0.1453, -0.9064, -0.7276, -0.1440, -0.6844,  0.1095,  0.6562,\n",
      "           -0.2900, -0.3144,  0.1212, -0.1296,  0.5806,  0.6408, -0.2216,\n",
      "            0.2658,  0.3515],\n",
      "          [ 0.2142,  0.1808, -0.2312,  0.0410,  0.5622, -0.0153, -0.3017,\n",
      "           -0.3793, -0.4462, -0.6860, -0.5246, -0.8726,  0.6107,  0.4022,\n",
      "            0.1741,  0.2921]],\n",
      "\n",
      "         [[ 0.0638, -0.4207,  0.1647,  0.4287,  0.5958,  0.7917,  0.6348,\n",
      "           -0.4842,  0.0201,  0.0426, -0.4715, -0.4814,  0.0655, -0.6659,\n",
      "           -0.2749,  0.0289],\n",
      "          [-0.1922, -0.1813,  0.4716, -0.6003, -0.3669, -0.6893, -0.2655,\n",
      "            0.3189, -0.0249,  0.1372,  0.4591,  0.1904, -0.2857,  0.5265,\n",
      "            0.4413, -0.1195],\n",
      "          [ 0.1182,  0.4402, -0.7318, -0.1988, -0.4243, -0.4770, -0.5648,\n",
      "            0.3331,  0.0765,  0.4187, -0.1132,  0.2997,  0.0300,  0.3097,\n",
      "           -0.0549,  0.0745]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xk: torch.Size([1, 4, 3, 16])\n",
      "tensor([[[[-0.0037,  0.3771,  0.6363,  0.5156,  0.4143, -0.5342,  0.0857,\n",
      "           -0.1012,  0.8707, -0.5184, -0.1376,  0.3001,  0.5935, -0.3538,\n",
      "           -0.2496,  0.1370],\n",
      "          [ 0.5112, -0.7996, -0.2100, -0.3450,  0.4250,  0.5304,  0.0051,\n",
      "            0.4490, -0.5710,  0.2089, -0.2029,  0.1663, -0.4524,  0.2009,\n",
      "            0.0022, -0.6217],\n",
      "          [ 0.3612, -0.1449, -0.6081, -0.1684, -0.7097,  0.1872, -0.1255,\n",
      "           -0.2301,  0.3447,  0.0540, -0.1179, -0.6333, -0.4807,  0.3153,\n",
      "            0.2909,  0.3167]],\n",
      "\n",
      "         [[-0.0037,  0.3771,  0.6363,  0.5156,  0.4143, -0.5342,  0.0857,\n",
      "           -0.1012,  0.8707, -0.5184, -0.1376,  0.3001,  0.5935, -0.3538,\n",
      "           -0.2496,  0.1370],\n",
      "          [ 0.5112, -0.7996, -0.2100, -0.3450,  0.4250,  0.5304,  0.0051,\n",
      "            0.4490, -0.5710,  0.2089, -0.2029,  0.1663, -0.4524,  0.2009,\n",
      "            0.0022, -0.6217],\n",
      "          [ 0.3612, -0.1449, -0.6081, -0.1684, -0.7097,  0.1872, -0.1255,\n",
      "           -0.2301,  0.3447,  0.0540, -0.1179, -0.6333, -0.4807,  0.3153,\n",
      "            0.2909,  0.3167]],\n",
      "\n",
      "         [[-0.0037,  0.3771,  0.6363,  0.5156,  0.4143, -0.5342,  0.0857,\n",
      "           -0.1012,  0.8707, -0.5184, -0.1376,  0.3001,  0.5935, -0.3538,\n",
      "           -0.2496,  0.1370],\n",
      "          [ 0.5112, -0.7996, -0.2100, -0.3450,  0.4250,  0.5304,  0.0051,\n",
      "            0.4490, -0.5710,  0.2089, -0.2029,  0.1663, -0.4524,  0.2009,\n",
      "            0.0022, -0.6217],\n",
      "          [ 0.3612, -0.1449, -0.6081, -0.1684, -0.7097,  0.1872, -0.1255,\n",
      "           -0.2301,  0.3447,  0.0540, -0.1179, -0.6333, -0.4807,  0.3153,\n",
      "            0.2909,  0.3167]],\n",
      "\n",
      "         [[-0.0037,  0.3771,  0.6363,  0.5156,  0.4143, -0.5342,  0.0857,\n",
      "           -0.1012,  0.8707, -0.5184, -0.1376,  0.3001,  0.5935, -0.3538,\n",
      "           -0.2496,  0.1370],\n",
      "          [ 0.5112, -0.7996, -0.2100, -0.3450,  0.4250,  0.5304,  0.0051,\n",
      "            0.4490, -0.5710,  0.2089, -0.2029,  0.1663, -0.4524,  0.2009,\n",
      "            0.0022, -0.6217],\n",
      "          [ 0.3612, -0.1449, -0.6081, -0.1684, -0.7097,  0.1872, -0.1255,\n",
      "           -0.2301,  0.3447,  0.0540, -0.1179, -0.6333, -0.4807,  0.3153,\n",
      "            0.2909,  0.3167]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xv: torch.Size([1, 4, 3, 16])\n",
      "tensor([[[[ 0.2219,  0.4614, -0.4423, -0.3224,  0.1243, -0.2054,  0.6080,\n",
      "            0.2632,  0.2947, -0.3840,  0.7400,  0.3126, -0.3071,  0.1212,\n",
      "            0.0736, -0.5959],\n",
      "          [-0.3322, -0.4429,  0.2062, -0.2322, -0.1859, -0.2136, -0.2034,\n",
      "           -0.3530, -0.6510, -0.1555, -0.4457, -0.0772, -0.1690, -0.3078,\n",
      "           -0.2521,  0.5720],\n",
      "          [-0.0084, -0.2100,  0.3711,  0.5678, -0.0048,  0.4128, -0.5723,\n",
      "           -0.0419,  0.1519,  0.5821, -0.5426, -0.3155,  0.5003,  0.0936,\n",
      "            0.1075,  0.2713]],\n",
      "\n",
      "         [[ 0.2219,  0.4614, -0.4423, -0.3224,  0.1243, -0.2054,  0.6080,\n",
      "            0.2632,  0.2947, -0.3840,  0.7400,  0.3126, -0.3071,  0.1212,\n",
      "            0.0736, -0.5959],\n",
      "          [-0.3322, -0.4429,  0.2062, -0.2322, -0.1859, -0.2136, -0.2034,\n",
      "           -0.3530, -0.6510, -0.1555, -0.4457, -0.0772, -0.1690, -0.3078,\n",
      "           -0.2521,  0.5720],\n",
      "          [-0.0084, -0.2100,  0.3711,  0.5678, -0.0048,  0.4128, -0.5723,\n",
      "           -0.0419,  0.1519,  0.5821, -0.5426, -0.3155,  0.5003,  0.0936,\n",
      "            0.1075,  0.2713]],\n",
      "\n",
      "         [[ 0.2219,  0.4614, -0.4423, -0.3224,  0.1243, -0.2054,  0.6080,\n",
      "            0.2632,  0.2947, -0.3840,  0.7400,  0.3126, -0.3071,  0.1212,\n",
      "            0.0736, -0.5959],\n",
      "          [-0.3322, -0.4429,  0.2062, -0.2322, -0.1859, -0.2136, -0.2034,\n",
      "           -0.3530, -0.6510, -0.1555, -0.4457, -0.0772, -0.1690, -0.3078,\n",
      "           -0.2521,  0.5720],\n",
      "          [-0.0084, -0.2100,  0.3711,  0.5678, -0.0048,  0.4128, -0.5723,\n",
      "           -0.0419,  0.1519,  0.5821, -0.5426, -0.3155,  0.5003,  0.0936,\n",
      "            0.1075,  0.2713]],\n",
      "\n",
      "         [[ 0.2219,  0.4614, -0.4423, -0.3224,  0.1243, -0.2054,  0.6080,\n",
      "            0.2632,  0.2947, -0.3840,  0.7400,  0.3126, -0.3071,  0.1212,\n",
      "            0.0736, -0.5959],\n",
      "          [-0.3322, -0.4429,  0.2062, -0.2322, -0.1859, -0.2136, -0.2034,\n",
      "           -0.3530, -0.6510, -0.1555, -0.4457, -0.0772, -0.1690, -0.3078,\n",
      "           -0.2521,  0.5720],\n",
      "          [-0.0084, -0.2100,  0.3711,  0.5678, -0.0048,  0.4128, -0.5723,\n",
      "           -0.0419,  0.1519,  0.5821, -0.5426, -0.3155,  0.5003,  0.0936,\n",
      "            0.1075,  0.2713]]]], grad_fn=<TransposeBackward0>)\n",
      "scores: torch.Size([1, 4, 3, 3])\n",
      "tensor([[[[-0.0042, -0.1539,  0.4027],\n",
      "          [-0.2034,  0.3483, -0.2434],\n",
      "          [ 0.0284,  0.1308, -0.1976]],\n",
      "\n",
      "         [[ 0.0187, -0.1791,  0.0296],\n",
      "          [ 0.0934, -0.0091, -0.0041],\n",
      "          [-0.1358,  0.1729, -0.0639]],\n",
      "\n",
      "         [[ 0.1877, -0.0150, -0.1372],\n",
      "          [-0.2100,  0.1043,  0.1244],\n",
      "          [ 0.0473, -0.0614,  0.0770]],\n",
      "\n",
      "         [[ 0.0893,  0.1191, -0.0685],\n",
      "          [-0.1242,  0.0167,  0.0201],\n",
      "          [-0.1296, -0.0593,  0.1567]]]], grad_fn=<MulBackward0>)\n",
      "mask: torch.Size([1, 1, 3, 3])\n",
      "tensor([[[[ 0.0000e+00, -2.3820e+38, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]]]])\n",
      "masked scores: torch.Size([1, 4, 3, 3])\n",
      "tensor([[[[-4.2295e-03, -2.3820e+38, -2.3820e+38],\n",
      "          [-2.0342e-01,  3.4830e-01, -2.3820e+38],\n",
      "          [ 2.8377e-02,  1.3079e-01, -1.9758e-01]],\n",
      "\n",
      "         [[ 1.8675e-02, -2.3820e+38, -2.3820e+38],\n",
      "          [ 9.3380e-02, -9.1133e-03, -2.3820e+38],\n",
      "          [-1.3576e-01,  1.7293e-01, -6.3912e-02]],\n",
      "\n",
      "         [[ 1.8767e-01, -2.3820e+38, -2.3820e+38],\n",
      "          [-2.1001e-01,  1.0428e-01, -2.3820e+38],\n",
      "          [ 4.7284e-02, -6.1373e-02,  7.7046e-02]],\n",
      "\n",
      "         [[ 8.9285e-02, -2.3820e+38, -2.3820e+38],\n",
      "          [-1.2425e-01,  1.6666e-02, -2.3820e+38],\n",
      "          [-1.2961e-01, -5.9266e-02,  1.5672e-01]]]], grad_fn=<AddBackward0>)\n",
      "softmaxed scores: torch.Size([1, 4, 3, 3])\n",
      "tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [0.3655, 0.6345, 0.0000],\n",
      "          [0.3442, 0.3813, 0.2746]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.5256, 0.4744, 0.0000],\n",
      "          [0.2910, 0.3963, 0.3127]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.4221, 0.5779, 0.0000],\n",
      "          [0.3416, 0.3064, 0.3519]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.4648, 0.5352, 0.0000],\n",
      "          [0.2937, 0.3151, 0.3911]]]], grad_fn=<SoftmaxBackward0>)\n",
      "attention: torch.Size([1, 4, 3, 16])\n",
      "tensor([[[[ 2.2190e-01,  4.6137e-01, -4.4234e-01, -3.2245e-01,  1.2427e-01,\n",
      "           -2.0541e-01,  6.0801e-01,  2.6323e-01,  2.9472e-01, -3.8395e-01,\n",
      "            7.4002e-01,  3.1255e-01, -3.0714e-01,  1.2122e-01,  7.3611e-02,\n",
      "           -5.9591e-01],\n",
      "          [-1.2972e-01, -1.1243e-01, -3.0848e-02, -2.6521e-01, -7.2559e-02,\n",
      "           -2.1064e-01,  9.3158e-02, -1.2776e-01, -3.0535e-01, -2.3901e-01,\n",
      "           -1.2358e-02,  6.5263e-02, -2.1950e-01, -1.5098e-01, -1.3304e-01,\n",
      "            1.4517e-01],\n",
      "          [-5.2600e-02, -6.7744e-02,  2.8262e-02, -4.3629e-02, -2.9434e-02,\n",
      "           -3.8826e-02, -2.5416e-02, -5.5486e-02, -1.0507e-01, -3.1631e-02,\n",
      "           -6.4209e-02, -8.4675e-03, -3.2794e-02, -4.9937e-02, -4.1267e-02,\n",
      "            8.7489e-02]],\n",
      "\n",
      "         [[ 2.2190e-01,  4.6137e-01, -4.4234e-01, -3.2245e-01,  1.2427e-01,\n",
      "           -2.0541e-01,  6.0801e-01,  2.6323e-01,  2.9472e-01, -3.8395e-01,\n",
      "            7.4002e-01,  3.1255e-01, -3.0714e-01,  1.2122e-01,  7.3611e-02,\n",
      "           -5.9591e-01],\n",
      "          [-4.0981e-02,  3.2381e-02, -1.3470e-01, -2.7966e-01, -2.2886e-02,\n",
      "           -2.0932e-01,  2.2309e-01, -2.9088e-02, -1.5392e-01, -2.7559e-01,\n",
      "            1.7752e-01,  1.2767e-01, -2.4162e-01, -8.2288e-02, -8.0886e-02,\n",
      "           -4.1853e-02],\n",
      "          [-6.9691e-02, -1.0691e-01,  6.9015e-02, -8.3173e-03, -3.9007e-02,\n",
      "           -1.5368e-02, -8.2604e-02, -7.6363e-02, -1.2470e-01,  8.6433e-03,\n",
      "           -1.3091e-01, -3.8267e-02,  7.6935e-05, -5.7423e-02, -4.4858e-02,\n",
      "            1.3808e-01]],\n",
      "\n",
      "         [[ 2.2190e-01,  4.6137e-01, -4.4234e-01, -3.2245e-01,  1.2427e-01,\n",
      "           -2.0541e-01,  6.0801e-01,  2.6323e-01,  2.9472e-01, -3.8395e-01,\n",
      "            7.4002e-01,  3.1255e-01, -3.0714e-01,  1.2122e-01,  7.3611e-02,\n",
      "           -5.9591e-01],\n",
      "          [-9.8353e-02, -6.1242e-02, -6.7555e-02, -2.7032e-01, -5.5001e-02,\n",
      "           -2.1017e-01,  1.3908e-01, -9.2885e-02, -2.5183e-01, -2.5194e-01,\n",
      "            5.4756e-02,  8.7322e-02, -2.2732e-01, -1.2670e-01, -1.1460e-01,\n",
      "            7.9066e-02],\n",
      "          [-2.8949e-02, -5.2023e-02,  4.2679e-02,  1.8508e-02, -1.6207e-02,\n",
      "            9.6259e-03, -5.6028e-02, -3.2985e-02, -4.5358e-02,  2.6026e-02,\n",
      "           -7.4721e-02, -2.7900e-02,  1.9349e-02, -1.9974e-02, -1.4277e-02,\n",
      "            6.7192e-02]],\n",
      "\n",
      "         [[ 2.2190e-01,  4.6137e-01, -4.4234e-01, -3.2245e-01,  1.2427e-01,\n",
      "           -2.0541e-01,  6.0801e-01,  2.6323e-01,  2.9472e-01, -3.8395e-01,\n",
      "            7.4002e-01,  3.1255e-01, -3.0714e-01,  1.2122e-01,  7.3611e-02,\n",
      "           -5.9591e-01],\n",
      "          [-7.4656e-02, -2.2573e-02, -9.5286e-02, -2.7417e-01, -4.1736e-02,\n",
      "           -2.0982e-01,  1.7378e-01, -6.6535e-02, -2.1139e-01, -2.6171e-01,\n",
      "            1.0546e-01,  1.0399e-01, -2.3323e-01, -1.0836e-01, -1.0068e-01,\n",
      "            2.9123e-02],\n",
      "          [-4.2792e-02, -8.6196e-02,  8.0194e-02,  5.4176e-02, -2.3963e-02,\n",
      "            3.3776e-02, -1.0933e-01, -5.0301e-02, -5.9182e-02,  6.5864e-02,\n",
      "           -1.3529e-01, -5.5898e-02,  5.2189e-02, -2.4790e-02, -1.5783e-02,\n",
      "            1.1133e-01]]]], grad_fn=<UnsafeViewBackward0>)\n",
      "reshaped attention: torch.Size([1, 3, 64])\n",
      "tensor([[[ 2.2190e-01,  4.6137e-01, -4.4234e-01, -3.2245e-01,  1.2427e-01,\n",
      "          -2.0541e-01,  6.0801e-01,  2.6323e-01,  2.9472e-01, -3.8395e-01,\n",
      "           7.4002e-01,  3.1255e-01, -3.0714e-01,  1.2122e-01,  7.3611e-02,\n",
      "          -5.9591e-01,  2.2190e-01,  4.6137e-01, -4.4234e-01, -3.2245e-01,\n",
      "           1.2427e-01, -2.0541e-01,  6.0801e-01,  2.6323e-01,  2.9472e-01,\n",
      "          -3.8395e-01,  7.4002e-01,  3.1255e-01, -3.0714e-01,  1.2122e-01,\n",
      "           7.3611e-02, -5.9591e-01,  2.2190e-01,  4.6137e-01, -4.4234e-01,\n",
      "          -3.2245e-01,  1.2427e-01, -2.0541e-01,  6.0801e-01,  2.6323e-01,\n",
      "           2.9472e-01, -3.8395e-01,  7.4002e-01,  3.1255e-01, -3.0714e-01,\n",
      "           1.2122e-01,  7.3611e-02, -5.9591e-01,  2.2190e-01,  4.6137e-01,\n",
      "          -4.4234e-01, -3.2245e-01,  1.2427e-01, -2.0541e-01,  6.0801e-01,\n",
      "           2.6323e-01,  2.9472e-01, -3.8395e-01,  7.4002e-01,  3.1255e-01,\n",
      "          -3.0714e-01,  1.2122e-01,  7.3611e-02, -5.9591e-01],\n",
      "         [-1.2972e-01, -1.1243e-01, -3.0848e-02, -2.6521e-01, -7.2559e-02,\n",
      "          -2.1064e-01,  9.3158e-02, -1.2776e-01, -3.0535e-01, -2.3901e-01,\n",
      "          -1.2358e-02,  6.5263e-02, -2.1950e-01, -1.5098e-01, -1.3304e-01,\n",
      "           1.4517e-01, -4.0981e-02,  3.2381e-02, -1.3470e-01, -2.7966e-01,\n",
      "          -2.2886e-02, -2.0932e-01,  2.2309e-01, -2.9088e-02, -1.5392e-01,\n",
      "          -2.7559e-01,  1.7752e-01,  1.2767e-01, -2.4162e-01, -8.2288e-02,\n",
      "          -8.0886e-02, -4.1853e-02, -9.8353e-02, -6.1242e-02, -6.7555e-02,\n",
      "          -2.7032e-01, -5.5001e-02, -2.1017e-01,  1.3908e-01, -9.2885e-02,\n",
      "          -2.5183e-01, -2.5194e-01,  5.4756e-02,  8.7322e-02, -2.2732e-01,\n",
      "          -1.2670e-01, -1.1460e-01,  7.9066e-02, -7.4656e-02, -2.2573e-02,\n",
      "          -9.5286e-02, -2.7417e-01, -4.1736e-02, -2.0982e-01,  1.7378e-01,\n",
      "          -6.6535e-02, -2.1139e-01, -2.6171e-01,  1.0546e-01,  1.0399e-01,\n",
      "          -2.3323e-01, -1.0836e-01, -1.0068e-01,  2.9123e-02],\n",
      "         [-5.2600e-02, -6.7744e-02,  2.8262e-02, -4.3629e-02, -2.9434e-02,\n",
      "          -3.8826e-02, -2.5416e-02, -5.5486e-02, -1.0507e-01, -3.1631e-02,\n",
      "          -6.4209e-02, -8.4675e-03, -3.2794e-02, -4.9937e-02, -4.1267e-02,\n",
      "           8.7489e-02, -6.9691e-02, -1.0691e-01,  6.9015e-02, -8.3173e-03,\n",
      "          -3.9007e-02, -1.5368e-02, -8.2604e-02, -7.6363e-02, -1.2470e-01,\n",
      "           8.6433e-03, -1.3091e-01, -3.8267e-02,  7.6935e-05, -5.7423e-02,\n",
      "          -4.4858e-02,  1.3808e-01, -2.8949e-02, -5.2023e-02,  4.2679e-02,\n",
      "           1.8508e-02, -1.6207e-02,  9.6259e-03, -5.6028e-02, -3.2985e-02,\n",
      "          -4.5358e-02,  2.6026e-02, -7.4721e-02, -2.7900e-02,  1.9349e-02,\n",
      "          -1.9974e-02, -1.4277e-02,  6.7192e-02, -4.2792e-02, -8.6196e-02,\n",
      "           8.0194e-02,  5.4176e-02, -2.3963e-02,  3.3776e-02, -1.0933e-01,\n",
      "          -5.0301e-02, -5.9182e-02,  6.5864e-02, -1.3529e-01, -5.5898e-02,\n",
      "           5.2189e-02, -2.4790e-02, -1.5783e-02,  1.1133e-01]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "self.Wo: torch.Size([128, 4])\n",
      "Parameter containing:\n",
      "tensor([[-0.0079,  0.0757,  0.0749,  0.0462],\n",
      "        [ 0.0460, -0.0546,  0.0502, -0.0218],\n",
      "        [-0.0014, -0.0434, -0.0075,  0.0641],\n",
      "        [ 0.0671, -0.0485, -0.0218,  0.0060],\n",
      "        [ 0.0309,  0.0527,  0.0630, -0.0393],\n",
      "        [ 0.0365, -0.0782,  0.0736, -0.0832],\n",
      "        [-0.0593,  0.0654, -0.0045, -0.0195],\n",
      "        [-0.0409,  0.0541, -0.0687,  0.0108],\n",
      "        [ 0.0058,  0.0848,  0.0145,  0.0689],\n",
      "        [ 0.0026, -0.0821,  0.0007, -0.0826],\n",
      "        [ 0.0520,  0.0327, -0.0099,  0.0035],\n",
      "        [-0.0113, -0.0376, -0.0707,  0.0649],\n",
      "        [ 0.0840,  0.0234, -0.0376,  0.0552],\n",
      "        [ 0.0851, -0.0157,  0.0635,  0.0348],\n",
      "        [-0.0805,  0.0577, -0.0686,  0.0417],\n",
      "        [ 0.0427, -0.0689, -0.0836, -0.0075],\n",
      "        [-0.0264,  0.0699,  0.0436, -0.0784],\n",
      "        [-0.0033,  0.0120, -0.0278,  0.0845],\n",
      "        [ 0.0211,  0.0219, -0.0064,  0.0622],\n",
      "        [ 0.0116, -0.0601, -0.0351,  0.0438],\n",
      "        [ 0.0529, -0.0174,  0.0657, -0.0547],\n",
      "        [ 0.0491,  0.0206,  0.0715,  0.0095],\n",
      "        [-0.0055, -0.0811,  0.0734, -0.0154],\n",
      "        [-0.0363,  0.0509,  0.0742,  0.0262],\n",
      "        [-0.0246, -0.0447,  0.0577, -0.0400],\n",
      "        [ 0.0389,  0.0033,  0.0518, -0.0246],\n",
      "        [-0.0094,  0.0168,  0.0019,  0.0794],\n",
      "        [ 0.0543, -0.0045, -0.0197, -0.0204],\n",
      "        [ 0.0335,  0.0870, -0.0398,  0.0823],\n",
      "        [-0.0363,  0.0104, -0.0642, -0.0105],\n",
      "        [ 0.0579,  0.0792,  0.0132, -0.0749],\n",
      "        [-0.0677,  0.0264,  0.0142,  0.0011],\n",
      "        [ 0.0827,  0.0024, -0.0406,  0.0068],\n",
      "        [-0.0444, -0.0071, -0.0194,  0.0284],\n",
      "        [ 0.0008, -0.0484, -0.0254,  0.0153],\n",
      "        [-0.0116,  0.0578,  0.0749, -0.0239],\n",
      "        [ 0.0216, -0.0516, -0.0301, -0.0757],\n",
      "        [-0.0307,  0.0636, -0.0593, -0.0068],\n",
      "        [-0.0570, -0.0767, -0.0239, -0.0010],\n",
      "        [ 0.0309, -0.0854,  0.0383,  0.0767],\n",
      "        [-0.0815, -0.0579,  0.0037,  0.0585],\n",
      "        [-0.0627, -0.0090, -0.0607,  0.0304],\n",
      "        [-0.0340, -0.0255, -0.0437,  0.0423],\n",
      "        [-0.0613, -0.0223, -0.0705,  0.0365],\n",
      "        [-0.0433, -0.0027, -0.0013,  0.0185],\n",
      "        [-0.0285,  0.0573,  0.0227,  0.0772],\n",
      "        [ 0.0165,  0.0765,  0.0751, -0.0699],\n",
      "        [-0.0342, -0.0433,  0.0554, -0.0370],\n",
      "        [-0.0358, -0.0385,  0.0709, -0.0257],\n",
      "        [ 0.0039,  0.0320, -0.0702, -0.0271],\n",
      "        [-0.0589, -0.0542, -0.0375, -0.0203],\n",
      "        [-0.0016, -0.0322,  0.0168,  0.0031],\n",
      "        [ 0.0460,  0.0677, -0.0761, -0.0260],\n",
      "        [ 0.0072,  0.0692, -0.0434,  0.0824],\n",
      "        [-0.0196, -0.0401, -0.0728, -0.0267],\n",
      "        [-0.0574,  0.0809,  0.0224,  0.0883],\n",
      "        [-0.0287,  0.0293, -0.0042,  0.0625],\n",
      "        [-0.0607,  0.0780, -0.0487, -0.0470],\n",
      "        [ 0.0747,  0.0287,  0.0232, -0.0343],\n",
      "        [-0.0560,  0.0061, -0.0046,  0.0253],\n",
      "        [-0.0705, -0.0466, -0.0435,  0.0327],\n",
      "        [-0.0161,  0.0762, -0.0001, -0.0089],\n",
      "        [ 0.0236,  0.0732,  0.0577, -0.0353],\n",
      "        [ 0.0087, -0.0584,  0.0357, -0.0773],\n",
      "        [-0.0670, -0.0439, -0.0023,  0.0128],\n",
      "        [ 0.0493,  0.0366,  0.0490,  0.0567],\n",
      "        [ 0.0221,  0.0582, -0.0552, -0.0022],\n",
      "        [ 0.0296,  0.0399,  0.0145, -0.0225],\n",
      "        [ 0.0603, -0.0110, -0.0349,  0.0332],\n",
      "        [-0.0803, -0.0851,  0.0214, -0.0223],\n",
      "        [-0.0452,  0.0069,  0.0462, -0.0463],\n",
      "        [-0.0643, -0.0237, -0.0822,  0.0790],\n",
      "        [ 0.0303,  0.0251, -0.0693,  0.0408],\n",
      "        [ 0.0185,  0.0506,  0.0176,  0.0865],\n",
      "        [-0.0782, -0.0380, -0.0346, -0.0512],\n",
      "        [-0.0452, -0.0101,  0.0294,  0.0438],\n",
      "        [-0.0496,  0.0350,  0.0099,  0.0243],\n",
      "        [-0.0701, -0.0437, -0.0308,  0.0103],\n",
      "        [-0.0842, -0.0148, -0.0684, -0.0078],\n",
      "        [-0.0701, -0.0414,  0.0810, -0.0062],\n",
      "        [-0.0146,  0.0005,  0.0533,  0.0344],\n",
      "        [ 0.0085,  0.0402,  0.0674, -0.0017],\n",
      "        [-0.0191, -0.0351,  0.0825,  0.0132],\n",
      "        [-0.0615,  0.0387,  0.0302, -0.0637],\n",
      "        [ 0.0858, -0.0366,  0.0106,  0.0591],\n",
      "        [-0.0817, -0.0733, -0.0238,  0.0241],\n",
      "        [-0.0592, -0.0499, -0.0003,  0.0668],\n",
      "        [-0.0703, -0.0642,  0.0084, -0.0529],\n",
      "        [ 0.0316, -0.0261, -0.0091,  0.0567],\n",
      "        [-0.0310, -0.0424,  0.0673,  0.0006],\n",
      "        [-0.0438, -0.0581,  0.0070,  0.0240],\n",
      "        [ 0.0063, -0.0861, -0.0742,  0.0079],\n",
      "        [-0.0030,  0.0009,  0.0563, -0.0348],\n",
      "        [ 0.0190,  0.0472, -0.0052, -0.0382],\n",
      "        [-0.0308, -0.0811, -0.0136,  0.0697],\n",
      "        [ 0.0380, -0.0230, -0.0244, -0.0466],\n",
      "        [-0.0628, -0.0160, -0.0716,  0.0577],\n",
      "        [ 0.0544, -0.0765,  0.0275, -0.0681],\n",
      "        [ 0.0318,  0.0341,  0.0013, -0.0111],\n",
      "        [-0.0799,  0.0441, -0.0169, -0.0087],\n",
      "        [-0.0725,  0.0369, -0.0701,  0.0023],\n",
      "        [ 0.0854, -0.0224, -0.0534, -0.0877],\n",
      "        [ 0.0283, -0.0878, -0.0075, -0.0721],\n",
      "        [-0.0322,  0.0820,  0.0631,  0.0723],\n",
      "        [ 0.0064,  0.0323,  0.0624,  0.0100],\n",
      "        [ 0.0456,  0.0870, -0.0688,  0.0531],\n",
      "        [ 0.0388, -0.0071,  0.0018, -0.0191],\n",
      "        [ 0.0644, -0.0875, -0.0701,  0.0224],\n",
      "        [-0.0434, -0.0629, -0.0451, -0.0093],\n",
      "        [ 0.0819,  0.0873,  0.0458, -0.0539],\n",
      "        [ 0.0413, -0.0432, -0.0012, -0.0310],\n",
      "        [ 0.0508,  0.0221, -0.0217, -0.0304],\n",
      "        [-0.0855,  0.0643,  0.0014, -0.0692],\n",
      "        [-0.0104,  0.0773,  0.0658, -0.0545],\n",
      "        [ 0.0044, -0.0092, -0.0831,  0.0229],\n",
      "        [ 0.0332, -0.0195, -0.0683,  0.0217],\n",
      "        [-0.0628, -0.0243, -0.0171,  0.0723],\n",
      "        [-0.0556, -0.0618, -0.0275,  0.0095],\n",
      "        [ 0.0450,  0.0690, -0.0316,  0.0098],\n",
      "        [-0.0299, -0.0863,  0.0291, -0.0333],\n",
      "        [ 0.0593, -0.0456,  0.0017,  0.0872],\n",
      "        [ 0.0687,  0.0754,  0.0252, -0.0636],\n",
      "        [ 0.0395, -0.0375, -0.0368,  0.0031],\n",
      "        [ 0.0429,  0.0580, -0.0204, -0.0299],\n",
      "        [-0.0047, -0.0014, -0.0368, -0.0616],\n",
      "        [-0.0147,  0.0073, -0.0368,  0.0620],\n",
      "        [ 0.0585, -0.0117, -0.0788,  0.0392],\n",
      "        [ 0.0628,  0.0042,  0.0403,  0.0434]], requires_grad=True)\n",
      "spliced Wo: torch.Size([64, 2])\n",
      "tensor([[-0.0079,  0.0757],\n",
      "        [ 0.0460, -0.0546],\n",
      "        [-0.0014, -0.0434],\n",
      "        [ 0.0671, -0.0485],\n",
      "        [ 0.0309,  0.0527],\n",
      "        [ 0.0365, -0.0782],\n",
      "        [-0.0593,  0.0654],\n",
      "        [-0.0409,  0.0541],\n",
      "        [ 0.0058,  0.0848],\n",
      "        [ 0.0026, -0.0821],\n",
      "        [ 0.0520,  0.0327],\n",
      "        [-0.0113, -0.0376],\n",
      "        [ 0.0840,  0.0234],\n",
      "        [ 0.0851, -0.0157],\n",
      "        [-0.0805,  0.0577],\n",
      "        [ 0.0427, -0.0689],\n",
      "        [ 0.0827,  0.0024],\n",
      "        [-0.0444, -0.0071],\n",
      "        [ 0.0008, -0.0484],\n",
      "        [-0.0116,  0.0578],\n",
      "        [ 0.0216, -0.0516],\n",
      "        [-0.0307,  0.0636],\n",
      "        [-0.0570, -0.0767],\n",
      "        [ 0.0309, -0.0854],\n",
      "        [-0.0815, -0.0579],\n",
      "        [-0.0627, -0.0090],\n",
      "        [-0.0340, -0.0255],\n",
      "        [-0.0613, -0.0223],\n",
      "        [-0.0433, -0.0027],\n",
      "        [-0.0285,  0.0573],\n",
      "        [ 0.0165,  0.0765],\n",
      "        [-0.0342, -0.0433],\n",
      "        [-0.0670, -0.0439],\n",
      "        [ 0.0493,  0.0366],\n",
      "        [ 0.0221,  0.0582],\n",
      "        [ 0.0296,  0.0399],\n",
      "        [ 0.0603, -0.0110],\n",
      "        [-0.0803, -0.0851],\n",
      "        [-0.0452,  0.0069],\n",
      "        [-0.0643, -0.0237],\n",
      "        [ 0.0303,  0.0251],\n",
      "        [ 0.0185,  0.0506],\n",
      "        [-0.0782, -0.0380],\n",
      "        [-0.0452, -0.0101],\n",
      "        [-0.0496,  0.0350],\n",
      "        [-0.0701, -0.0437],\n",
      "        [-0.0842, -0.0148],\n",
      "        [-0.0701, -0.0414],\n",
      "        [-0.0628, -0.0160],\n",
      "        [ 0.0544, -0.0765],\n",
      "        [ 0.0318,  0.0341],\n",
      "        [-0.0799,  0.0441],\n",
      "        [-0.0725,  0.0369],\n",
      "        [ 0.0854, -0.0224],\n",
      "        [ 0.0283, -0.0878],\n",
      "        [-0.0322,  0.0820],\n",
      "        [ 0.0064,  0.0323],\n",
      "        [ 0.0456,  0.0870],\n",
      "        [ 0.0388, -0.0071],\n",
      "        [ 0.0644, -0.0875],\n",
      "        [-0.0434, -0.0629],\n",
      "        [ 0.0819,  0.0873],\n",
      "        [ 0.0413, -0.0432],\n",
      "        [ 0.0508,  0.0221]], grad_fn=<CatBackward0>)\n",
      "projected output: torch.Size([1, 3, 2])\n",
      "tensor([[[-0.1176, -0.0676],\n",
      "         [-0.0091, -0.1246],\n",
      "         [ 0.0064,  0.0185]]], grad_fn=<UnsafeViewBackward0>)\n",
      "----------------- END MultiQueryAttention.forwardTensor() --------------------\n",
      "x in layer after MQA & resid connection and before MLP:\n",
      "tensor([[[-0.3451, -0.5075],\n",
      "         [ 2.3530,  0.1554],\n",
      "         [-0.2777,  1.2783]]], grad_fn=<AddBackward0>)\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 3, 2])\n",
      "tensor([[[-0.3451, -0.5075],\n",
      "         [ 2.3530,  0.1554],\n",
      "         [-0.2777,  1.2783]]], grad_fn=<AddBackward0>)\n",
      "normed x: torch.Size([1, 3, 2])\n",
      "tensor([[[-0.7952, -1.1694],\n",
      "         [ 1.4111,  0.0932],\n",
      "         [-0.3003,  1.3820]]], grad_fn=<MulBackward0>)\n",
      "dim: 2\n",
      "skip: 0\n",
      "spliced scale: torch.Size([2])\n",
      "tensor([0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 3, 2])\n",
      "tensor([[[-0.7952, -1.1694],\n",
      "         [ 1.4111,  0.0932],\n",
      "         [-0.3003,  1.3820]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "---------- MLP Input: torch.Tensor ------------\n",
      "------------- MLP.forwardTensor() ------------\n",
      "x: torch.Size([1, 3, 2])\n",
      "tensor([[[-0.7952, -1.1694],\n",
      "         [ 1.4111,  0.0932],\n",
      "         [-0.3003,  1.3820]]], grad_fn=<MulBackward0>)\n",
      "d_dim: 2\n",
      "d_skip: 0\n",
      "i_dim: 8\n",
      "i_skip: 0\n",
      "Wgate: torch.Size([4, 16])\n",
      "Parameter containing:\n",
      "tensor([[-0.4711, -0.2294,  0.3826,  0.2413,  0.0221, -0.1754, -0.4362,  0.4694,\n",
      "         -0.2813,  0.0714, -0.1090,  0.2667, -0.4410, -0.4892, -0.3989,  0.2439],\n",
      "        [ 0.2152, -0.2464, -0.1261, -0.0181,  0.3651,  0.3879,  0.2535, -0.3582,\n",
      "          0.0528,  0.2415,  0.1849,  0.3567, -0.0384,  0.4340, -0.3725,  0.3105],\n",
      "        [-0.2154,  0.2649,  0.3705, -0.4557,  0.1628, -0.2772, -0.1900,  0.4409,\n",
      "          0.3852, -0.0915, -0.4160,  0.0106,  0.2830,  0.1615,  0.0159,  0.4351],\n",
      "        [ 0.0065, -0.4006,  0.0773, -0.0895, -0.2334, -0.0861,  0.0079,  0.1160,\n",
      "          0.4893, -0.4880,  0.0563, -0.4693, -0.4824,  0.1443, -0.1036,  0.1029]],\n",
      "       requires_grad=True)\n",
      "Wgate spliced: torch.Size([2, 8])\n",
      "tensor([[-0.4711, -0.2294,  0.3826,  0.2413,  0.0221, -0.1754, -0.4362,  0.4694],\n",
      "        [ 0.2152, -0.2464, -0.1261, -0.0181,  0.3651,  0.3879,  0.2535, -0.3582]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Bgate: torch.Size([16])\n",
      "Parameter containing:\n",
      "tensor([ 0.1358,  0.2152,  0.2748, -0.3331, -0.1813,  0.0264,  0.4762, -0.4854,\n",
      "        -0.2410, -0.1014, -0.0148,  0.3180, -0.3845, -0.0458,  0.2054,  0.4083],\n",
      "       requires_grad=True)\n",
      "Bgate spliced: torch.Size([8])\n",
      "tensor([ 0.1358,  0.2152,  0.2748, -0.3331, -0.1813,  0.0264,  0.4762, -0.4854],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xgate: torch.Size([1, 3, 8])\n",
      "tensor([[[ 0.2588,  0.6858,  0.1181, -0.5039, -0.6259, -0.2877,  0.5265,\n",
      "          -0.4397],\n",
      "         [-0.5089, -0.1315,  0.8030,  0.0057, -0.1160, -0.1849, -0.1157,\n",
      "           0.1436],\n",
      "         [ 0.5747, -0.0564, -0.0144, -0.4305,  0.3166,  0.6152,  0.9575,\n",
      "          -1.1214]]], grad_fn=<AddBackward0>)\n",
      "GeLU'ed Xgate: torch.Size([1, 3, 8])\n",
      "tensor([[[ 0.1559,  0.5168,  0.0646, -0.1548, -0.1663, -0.1113,  0.3689,\n",
      "          -0.1451],\n",
      "         [-0.1554, -0.0589,  0.6336,  0.0029, -0.0527, -0.0789, -0.0525,\n",
      "           0.0800],\n",
      "         [ 0.4122, -0.0269, -0.0071, -0.1435,  0.1976,  0.4496,  0.7956,\n",
      "          -0.1470]]], grad_fn=<GeluBackward0>)\n",
      "Wup: torch.Size([4, 16])\n",
      "Parameter containing:\n",
      "tensor([[ 0.1108,  0.3057,  0.1222,  0.4416,  0.1918,  0.4734,  0.3690, -0.1281,\n",
      "         -0.3413, -0.3456,  0.0640, -0.0212, -0.4335, -0.2666, -0.1306,  0.3194],\n",
      "        [-0.2082, -0.3736,  0.3005,  0.1933, -0.4231, -0.1596,  0.4303, -0.1602,\n",
      "          0.3515, -0.4712, -0.0116,  0.4635,  0.3738, -0.3957, -0.0436, -0.2250],\n",
      "        [-0.3199,  0.3075,  0.1510,  0.2626, -0.4115, -0.2712,  0.0805, -0.4850,\n",
      "         -0.3383, -0.3757, -0.2884, -0.4095, -0.4162, -0.1790,  0.4855, -0.1969],\n",
      "        [ 0.1019,  0.3763, -0.1884, -0.0300,  0.1595,  0.4114, -0.3694, -0.1927,\n",
      "         -0.0307,  0.0315,  0.1099, -0.3822, -0.0468,  0.1110, -0.2350,  0.2922]],\n",
      "       requires_grad=True)\n",
      "Wup spliced: torch.Size([2, 8])\n",
      "tensor([[ 0.1108,  0.3057,  0.1222,  0.4416,  0.1918,  0.4734,  0.3690, -0.1281],\n",
      "        [-0.2082, -0.3736,  0.3005,  0.1933, -0.4231, -0.1596,  0.4303, -0.1602]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Bup: torch.Size([16])\n",
      "Parameter containing:\n",
      "tensor([ 0.2493, -0.0109, -0.0733, -0.2945, -0.1111,  0.2164, -0.0506,  0.2475,\n",
      "        -0.1203, -0.3330,  0.2976, -0.1972,  0.0952, -0.0163, -0.4097,  0.3306],\n",
      "       requires_grad=True)\n",
      "Bup spliced: torch.Size([8])\n",
      "tensor([ 0.2493, -0.0109, -0.0733, -0.2945, -0.1111,  0.2164, -0.0506,  0.2475],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xup: torch.Size([1, 3, 8])\n",
      "tensor([[[ 0.4046,  0.1829, -0.5220, -0.8717,  0.2311,  0.0266, -0.8472,\n",
      "           0.5367],\n",
      "         [ 0.3863,  0.3856,  0.1272,  0.3468,  0.1201,  0.8696,  0.5102,\n",
      "           0.0518],\n",
      "         [-0.0717, -0.6191,  0.3053, -0.1599, -0.7534, -0.1463,  0.4333,\n",
      "           0.0646]]], grad_fn=<AddBackward0>)\n",
      "Xfuse: torch.Size([1, 3, 8])\n",
      "tensor([[[ 0.0631,  0.0945, -0.0337,  0.1349, -0.0384, -0.0030, -0.3126,\n",
      "          -0.0779],\n",
      "         [-0.0600, -0.0227,  0.0806,  0.0010, -0.0063, -0.0686, -0.0268,\n",
      "           0.0041],\n",
      "         [-0.0295,  0.0167, -0.0022,  0.0230, -0.1489, -0.0658,  0.3447,\n",
      "          -0.0095]]], grad_fn=<MulBackward0>)\n",
      "Wdown: torch.Size([16, 4])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0279, -0.2160, -0.1735,  0.1518],\n",
      "        [ 0.1084, -0.0126, -0.1621, -0.0115],\n",
      "        [-0.2072, -0.2054,  0.0458,  0.1223],\n",
      "        [-0.2420, -0.0571,  0.2103, -0.2155],\n",
      "        [-0.2034, -0.1359,  0.0111, -0.0423],\n",
      "        [ 0.1759, -0.2092, -0.2173, -0.0898],\n",
      "        [-0.2167, -0.1977,  0.2118,  0.1070],\n",
      "        [-0.1025,  0.1586, -0.2437, -0.1566],\n",
      "        [-0.1593,  0.2311, -0.2201, -0.1509],\n",
      "        [-0.2218, -0.0825, -0.0995,  0.2268],\n",
      "        [-0.1547, -0.2075, -0.0174,  0.0752],\n",
      "        [ 0.0670,  0.1068, -0.2171, -0.0432],\n",
      "        [ 0.2249, -0.1833, -0.0199, -0.0505],\n",
      "        [-0.0075, -0.1590,  0.1165,  0.0302],\n",
      "        [ 0.1515,  0.2459,  0.0666,  0.1779],\n",
      "        [ 0.0220,  0.1881,  0.0740, -0.2126]], requires_grad=True)\n",
      "Wdown spliced: torch.Size([8, 2])\n",
      "tensor([[ 0.0279, -0.2160],\n",
      "        [ 0.1084, -0.0126],\n",
      "        [-0.2072, -0.2054],\n",
      "        [-0.2420, -0.0571],\n",
      "        [-0.2034, -0.1359],\n",
      "        [ 0.1759, -0.2092],\n",
      "        [-0.2167, -0.1977],\n",
      "        [-0.1025,  0.1586]], grad_fn=<SliceBackward0>)\n",
      "Bdown: torch.Size([4])\n",
      "Parameter containing:\n",
      "tensor([-0.0769,  0.1017,  0.2368,  0.0229], requires_grad=True)\n",
      "Bdown spliced: torch.Size([2])\n",
      "tensor([-0.0769,  0.1017], grad_fn=<SliceBackward0>)\n",
      "outputs: torch.Size([1, 3, 2])\n",
      "tensor([[[-0.0076,  0.1414],\n",
      "         [-0.1034,  0.1195],\n",
      "         [-0.1361,  0.0713]]], grad_fn=<AddBackward0>)\n",
      "------------- END MLP.forwardTensor() ------------\n",
      "layer's final residual state:\n",
      "tensor([[[-0.3527, -0.3661],\n",
      "         [ 2.2496,  0.2749],\n",
      "         [-0.4138,  1.3496]]], grad_fn=<AddBackward0>)\n",
      "----------------- END Layer.forwardTensor() --------------------\n",
      "forwardTensor() output: torch.Size([1, 3, 2])\n",
      "tensor([[[-0.3527, -0.3661],\n",
      "         [ 2.2496,  0.2749],\n",
      "         [-0.4138,  1.3496]]], grad_fn=<AddBackward0>)\n",
      "Model 1 from range(2)\n",
      "----------------- Layer.forwardTensor() --------------------\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 3, 2])\n",
      "tensor([[[-1.0265, -1.4249],\n",
      "         [ 0.9723,  1.2800],\n",
      "         [ 1.3253, -1.7510]]])\n",
      "normed x: torch.Size([1, 3, 2])\n",
      "tensor([[[-0.8266, -1.1475],\n",
      "         [ 0.8555,  1.1261],\n",
      "         [ 0.8535, -1.1276]]])\n",
      "dim: 2\n",
      "skip: 2\n",
      "spliced scale: torch.Size([2])\n",
      "tensor([0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 3, 2])\n",
      "tensor([[[-0.8266, -1.1475],\n",
      "         [ 0.8555,  1.1261],\n",
      "         [ 0.8535, -1.1276]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "---------- Attention Input: torch.Tensor ------------\n",
      "----------------- MultiQueryAttention.forwardTensor() --------------------\n",
      "x shape: torch.Size([1, 3, 2])\n",
      "d_skip: 2\n",
      "models_in_this_level: 2\n",
      "h_dim: 16\n",
      "h_skip: 16\n",
      "self.Wqkv: torch.Size([4, 192])\n",
      "Parameter containing:\n",
      "tensor([[-3.2097e-01, -1.5344e-02, -1.1510e-01, -3.0874e-01,  3.3117e-01,\n",
      "         -2.5896e-01,  6.9617e-02,  4.6040e-01, -4.1660e-01,  4.1727e-01,\n",
      "          4.1405e-01, -2.2532e-01,  4.6028e-01,  7.0296e-02, -4.5145e-01,\n",
      "         -4.9349e-01,  8.6677e-02,  2.9951e-01,  3.9258e-01, -4.0547e-01,\n",
      "          4.8854e-01, -4.5012e-01,  2.7701e-01,  3.6282e-01, -2.8834e-01,\n",
      "          5.2543e-02, -4.6092e-01,  1.8264e-01, -4.3262e-01, -4.9215e-01,\n",
      "          4.2615e-01, -3.8198e-01,  1.2274e-01,  2.0562e-01,  3.0310e-01,\n",
      "          2.8698e-01, -8.8241e-02, -1.6075e-01,  2.5161e-01,  4.2444e-01,\n",
      "         -8.2624e-02, -4.0507e-01, -1.2071e-01, -4.2426e-01, -2.8454e-01,\n",
      "          2.0912e-01,  2.4156e-01, -3.9879e-01, -2.2591e-01,  3.3003e-01,\n",
      "          3.2374e-01, -1.6253e-02,  2.8714e-01, -3.8530e-01, -3.8436e-01,\n",
      "         -2.4212e-01,  3.6687e-02, -4.5855e-01,  4.8830e-01,  4.7009e-01,\n",
      "         -1.6423e-01, -3.2546e-01,  4.4032e-01,  3.6316e-01, -8.7626e-02,\n",
      "         -4.4174e-01, -4.6600e-01, -4.8568e-03, -4.8418e-01,  6.4661e-02,\n",
      "          4.8494e-01, -1.6592e-01, -2.0181e-01,  4.4469e-01,  9.5466e-02,\n",
      "          4.8366e-01,  4.4880e-01, -1.9130e-01,  1.5374e-01,  2.2177e-01,\n",
      "          4.9499e-01,  4.8626e-01, -3.4601e-01, -3.4760e-01,  8.6108e-02,\n",
      "          4.1357e-01, -2.4427e-01, -2.5268e-01,  2.0449e-01, -4.9356e-01,\n",
      "         -3.6677e-01, -1.0875e-01,  4.0741e-01, -4.2069e-01, -3.3176e-01,\n",
      "          2.1056e-01, -8.8283e-02, -1.0314e-01,  4.6487e-01, -3.7954e-01,\n",
      "         -2.3864e-01, -4.2001e-01, -1.2691e-01,  1.9158e-01,  1.1451e-01,\n",
      "          1.6561e-01,  1.7231e-01,  1.7429e-01, -1.8123e-01,  3.6120e-01,\n",
      "          3.1331e-01, -9.2029e-02,  3.9661e-01,  8.8137e-02,  1.8981e-01,\n",
      "          1.9692e-02,  3.0192e-01, -2.1089e-01, -3.5772e-02, -4.6871e-01,\n",
      "         -4.8495e-01, -3.9506e-01,  1.8029e-01,  3.0469e-01, -6.6937e-02,\n",
      "         -1.2337e-01, -4.9423e-01,  4.1953e-01, -1.5530e-01, -3.9074e-01,\n",
      "         -1.3531e-01, -1.8341e-01,  3.2812e-01,  3.5656e-01,  1.2500e-02,\n",
      "          3.2196e-01, -4.7280e-01,  4.0531e-01, -1.1058e-01,  2.0058e-01,\n",
      "         -3.1400e-01,  9.3957e-02, -2.3539e-02, -4.6383e-01, -2.1440e-01,\n",
      "          3.5762e-01,  2.6374e-01, -1.1291e-01,  1.3419e-01,  2.3561e-01,\n",
      "          2.6260e-01, -2.5277e-01, -4.1158e-01,  2.9288e-01, -3.6784e-01,\n",
      "          2.8182e-02, -4.1021e-01, -2.1270e-01,  1.2341e-01, -4.1593e-01,\n",
      "         -2.2971e-01, -2.8959e-01,  1.1191e-01, -2.0859e-01, -1.2854e-01,\n",
      "         -1.8272e-01, -9.3147e-02, -2.4128e-01, -4.6417e-01, -1.5658e-01,\n",
      "         -2.6369e-01, -2.7113e-02, -1.5909e-01, -2.2127e-01, -1.8380e-01,\n",
      "          3.7399e-01, -3.1944e-01,  2.3862e-02, -2.5048e-01, -2.1036e-01,\n",
      "         -2.7727e-01, -2.9599e-01,  4.8089e-01, -4.5476e-01,  3.0850e-01,\n",
      "          1.0780e-01,  9.1512e-02, -4.3502e-01, -2.0343e-01,  1.3289e-02,\n",
      "         -1.5765e-01,  1.8142e-01],\n",
      "        [-1.4990e-01,  2.7819e-01,  7.4129e-02, -4.4660e-02,  4.3282e-01,\n",
      "         -3.0581e-01,  1.8844e-01,  4.4872e-01, -1.1302e-01,  1.4516e-01,\n",
      "         -4.8995e-05,  4.3458e-01,  5.3082e-02,  4.4512e-01,  1.2523e-01,\n",
      "         -1.1507e-01,  3.1780e-01, -3.1342e-01, -4.4965e-01, -3.3284e-02,\n",
      "          1.5604e-02, -1.6669e-01,  1.8346e-01, -2.4034e-01, -4.7407e-01,\n",
      "         -7.6704e-02,  6.7073e-02,  1.2652e-01, -3.9627e-01,  3.7724e-01,\n",
      "         -1.9206e-01,  3.0627e-01,  4.2555e-02, -3.0911e-01,  4.3753e-01,\n",
      "         -2.9667e-01, -1.4520e-02, -1.0416e-02, -3.2493e-01,  4.7091e-01,\n",
      "          2.3998e-01,  3.4873e-01,  5.5291e-02,  9.5433e-02,  1.9302e-01,\n",
      "          3.8875e-01,  2.3504e-01, -1.4209e-01,  2.1127e-01,  2.3719e-01,\n",
      "          3.7475e-01, -2.1212e-01, -2.0359e-01, -4.3030e-01,  8.8165e-02,\n",
      "         -2.9098e-01,  7.8429e-02, -3.4094e-01,  4.3696e-01,  1.9249e-01,\n",
      "         -4.8272e-01, -3.3708e-01, -2.7591e-01, -2.7684e-01, -3.7848e-01,\n",
      "         -4.9171e-01, -4.6506e-01, -1.9350e-01,  3.7812e-01,  3.6263e-02,\n",
      "         -1.0092e-01, -3.0463e-01, -5.2127e-02, -2.3248e-01, -1.8609e-01,\n",
      "         -4.9422e-01,  4.5411e-01,  2.4784e-01,  1.7444e-01,  2.7136e-01,\n",
      "         -3.0759e-01, -2.0130e-01,  2.6875e-01, -4.1371e-01,  4.6615e-01,\n",
      "          1.8902e-01, -2.4082e-01, -4.6193e-01, -6.2983e-02, -2.6200e-01,\n",
      "          3.7065e-01,  4.9508e-01, -5.5899e-03,  1.6553e-01,  2.9573e-01,\n",
      "         -2.0170e-01, -5.1177e-03,  3.8824e-01, -3.7151e-01, -1.4505e-01,\n",
      "         -3.5091e-01, -4.1308e-01, -4.3970e-01,  2.8640e-01, -7.5188e-02,\n",
      "         -1.1953e-01,  2.8627e-01,  2.9312e-01,  4.1545e-02,  3.4332e-01,\n",
      "          5.6798e-02,  2.4595e-02,  1.1926e-01,  4.9348e-02,  1.0625e-01,\n",
      "          3.0282e-01,  1.3777e-01,  4.6769e-01, -4.9020e-01,  3.9159e-01,\n",
      "          6.4323e-02,  3.1386e-01,  1.8567e-01, -3.6389e-01, -6.6978e-02,\n",
      "          3.4887e-01, -4.4969e-01,  4.6964e-01,  8.3232e-02, -9.8104e-02,\n",
      "         -4.3657e-01, -3.1562e-01, -4.9943e-01,  2.4091e-01, -7.4649e-02,\n",
      "         -8.5931e-02, -4.4864e-01,  2.0309e-01,  1.6673e-01, -3.4258e-01,\n",
      "         -3.1006e-01,  2.3308e-01,  2.1086e-01,  1.3077e-01, -2.9130e-01,\n",
      "         -2.9917e-01, -1.6217e-01,  3.2903e-01, -3.0571e-01, -2.0588e-01,\n",
      "          3.5932e-01, -2.6446e-01, -1.8150e-01, -4.2555e-01, -2.7207e-01,\n",
      "         -3.3697e-01,  5.7231e-03, -9.8906e-03, -2.9698e-01, -4.7332e-01,\n",
      "         -5.7863e-02, -2.1753e-01,  2.9426e-01,  3.6454e-01, -3.2457e-02,\n",
      "          2.5800e-01, -4.3584e-01, -8.4784e-02,  5.4089e-03,  3.8661e-01,\n",
      "         -4.5274e-01, -2.3479e-01,  3.2677e-01,  1.7917e-02,  3.6443e-02,\n",
      "          2.8099e-01,  4.1516e-01, -2.7281e-01, -1.5072e-01, -3.6735e-01,\n",
      "          6.6030e-02, -1.6825e-01,  7.4779e-03, -4.9353e-01,  3.9334e-01,\n",
      "          1.6545e-01,  3.9433e-01,  3.8434e-01,  2.4296e-02, -3.1079e-01,\n",
      "          3.2874e-01,  4.7495e-01],\n",
      "        [-3.6494e-01, -1.9819e-01,  1.2391e-01,  1.4987e-01, -4.8445e-01,\n",
      "         -2.4220e-01,  1.5516e-01, -2.8535e-02,  3.2431e-01, -2.2211e-01,\n",
      "          1.5887e-01,  2.8480e-01,  2.1065e-01, -3.2590e-01, -3.0512e-01,\n",
      "         -4.2174e-01, -2.1729e-01, -2.7202e-01, -4.9488e-01,  4.2156e-01,\n",
      "          4.1862e-01, -2.5510e-01, -1.7890e-01, -2.3873e-01, -1.4193e-01,\n",
      "          5.3084e-02,  1.0497e-01,  4.0349e-01, -4.5228e-01,  5.2142e-02,\n",
      "          4.7570e-01, -1.9244e-01, -2.6906e-01,  3.6564e-02,  2.8004e-01,\n",
      "          1.9182e-01, -3.6188e-01,  3.8665e-01,  2.9734e-01,  1.3272e-01,\n",
      "          2.8950e-01, -1.1843e-01,  1.8197e-01,  1.0729e-01, -1.2266e-01,\n",
      "          3.5865e-03, -3.2721e-01,  4.0710e-01,  2.1084e-01, -4.9324e-01,\n",
      "         -4.5889e-01,  4.4462e-01,  1.4339e-01,  6.2129e-02, -3.7913e-01,\n",
      "         -6.1301e-02, -5.8408e-02,  2.0735e-01, -2.0772e-01, -4.4801e-01,\n",
      "          8.8660e-02, -3.2614e-01,  5.8857e-02, -3.6563e-03, -1.2983e-01,\n",
      "          2.8123e-01, -3.0251e-01, -2.0165e-01, -2.1545e-01,  3.0909e-01,\n",
      "         -1.9715e-01, -3.4939e-01, -3.8061e-01,  4.6676e-01,  4.4117e-01,\n",
      "         -1.2104e-01,  1.7660e-01, -2.4349e-02,  2.1922e-01, -4.1842e-01,\n",
      "         -4.8203e-01,  6.6405e-02, -2.5446e-01, -2.6192e-01, -3.6002e-01,\n",
      "          3.5817e-01, -5.3533e-02, -1.7330e-01, -2.6523e-01, -4.1151e-01,\n",
      "          2.4367e-01, -1.5724e-01,  1.0179e-01,  1.8709e-02,  2.6733e-01,\n",
      "         -3.9686e-01,  1.9645e-01,  1.2850e-01, -1.6648e-01, -2.5955e-02,\n",
      "          2.8239e-01, -3.5465e-01, -8.4585e-02, -1.6311e-01,  2.4559e-01,\n",
      "          3.4939e-01,  2.8058e-01,  2.2911e-01, -1.5656e-01, -3.8567e-01,\n",
      "          4.8251e-02,  5.4339e-02,  2.4236e-03,  4.1418e-02,  1.3403e-01,\n",
      "          4.6479e-01, -3.6504e-01, -3.9382e-01,  8.7580e-02,  3.4661e-01,\n",
      "          3.6698e-01,  3.1494e-01,  2.8518e-01,  3.6290e-01, -1.9336e-01,\n",
      "         -3.9900e-01, -3.0189e-01,  4.7716e-01, -5.7343e-02, -3.9568e-01,\n",
      "          4.7925e-01, -2.9701e-01,  1.6367e-01, -2.7030e-01,  1.7530e-01,\n",
      "         -4.8403e-01, -2.8215e-01, -1.4787e-01, -9.4221e-02,  1.7540e-02,\n",
      "         -2.8544e-01,  3.6671e-01,  4.9354e-01,  2.5809e-01,  4.5043e-01,\n",
      "          4.0915e-01,  3.4615e-01, -2.8886e-01, -1.0766e-01,  2.1052e-01,\n",
      "          6.3936e-02, -1.3789e-01,  3.7345e-01,  2.8598e-01, -4.5961e-01,\n",
      "         -2.7873e-01,  2.4984e-01, -3.3217e-01, -3.3710e-01, -3.4967e-01,\n",
      "         -6.8175e-02,  4.5880e-01,  1.7332e-01, -2.4107e-01, -2.8240e-01,\n",
      "         -2.8811e-01,  2.5626e-01, -4.9741e-01,  8.1723e-02,  2.7604e-01,\n",
      "          4.7153e-01, -3.5504e-01, -3.8385e-01, -3.3069e-01, -2.1225e-01,\n",
      "          1.5297e-01,  4.7800e-01, -1.8034e-02, -3.5942e-01,  4.7031e-01,\n",
      "         -3.1677e-01,  1.2753e-01,  4.5037e-01,  2.8465e-01,  1.2980e-01,\n",
      "         -2.7024e-01,  2.7878e-01, -4.7501e-01,  4.2285e-01,  4.5226e-01,\n",
      "         -7.5834e-02, -2.7833e-01],\n",
      "        [ 3.6166e-01, -2.9039e-01, -1.9532e-01,  2.1463e-01,  3.0336e-01,\n",
      "          3.4968e-01,  1.7080e-01, -3.1044e-01, -2.6154e-01, -1.7996e-01,\n",
      "         -3.9154e-01,  4.5309e-01,  2.6656e-01,  3.2655e-01,  4.4053e-01,\n",
      "         -3.8769e-01,  4.5635e-01, -1.7444e-01,  2.8878e-01,  2.6081e-01,\n",
      "         -3.5004e-01,  1.7178e-01,  4.8178e-01, -2.5152e-01, -1.7255e-01,\n",
      "         -4.2525e-01, -5.4256e-02, -2.4009e-01, -2.3620e-01, -1.2440e-01,\n",
      "          3.0858e-01, -4.4050e-01,  1.7219e-01, -1.5976e-01,  3.5247e-01,\n",
      "          2.9060e-01, -3.8982e-01,  5.3928e-02,  2.4100e-01,  6.9047e-03,\n",
      "          4.1114e-01,  3.0978e-02,  2.3816e-01, -2.6534e-01, -9.1487e-02,\n",
      "         -1.2970e-01, -1.3459e-01,  4.5638e-01,  4.2021e-01, -2.6263e-01,\n",
      "          6.4170e-02,  2.1780e-01, -1.9102e-01, -3.3325e-02, -4.4954e-01,\n",
      "          2.6437e-01, -3.3372e-01,  4.9230e-01, -3.5328e-01, -2.8936e-01,\n",
      "         -1.1743e-01, -1.5401e-02,  3.7282e-01,  1.4672e-01,  7.0350e-02,\n",
      "          3.2993e-01,  2.1661e-01, -1.4682e-01,  4.2327e-01, -2.8551e-01,\n",
      "         -1.3827e-01,  2.2878e-01, -7.2087e-02,  4.5573e-01,  4.1544e-02,\n",
      "          1.9417e-01, -1.5592e-01, -2.4566e-01,  2.5795e-01,  1.7242e-03,\n",
      "          3.9895e-01,  6.4041e-02, -3.9480e-01, -1.9869e-01,  2.9795e-02,\n",
      "          7.1546e-02, -3.7036e-02, -1.2344e-01,  4.1349e-01, -1.3400e-01,\n",
      "          2.2914e-01,  4.0769e-01,  2.6082e-01, -3.1787e-01, -1.5388e-01,\n",
      "          4.6755e-01,  3.8927e-01,  3.0885e-01, -1.5936e-01,  1.8015e-01,\n",
      "         -1.3583e-01, -2.4389e-01, -3.9084e-01,  3.8825e-01, -1.7214e-01,\n",
      "          2.9319e-01,  4.8597e-01,  3.2779e-01,  2.0918e-01,  1.6785e-01,\n",
      "         -4.5960e-01, -4.2303e-01, -2.9061e-01,  4.8491e-01, -1.8013e-01,\n",
      "         -4.4329e-01,  1.1073e-03, -3.2739e-01, -1.9347e-01, -2.4275e-01,\n",
      "          1.8868e-01, -2.4091e-01, -6.2331e-02, -1.4281e-01,  4.4504e-01,\n",
      "         -4.6353e-01,  1.3425e-01, -4.4999e-01, -4.0042e-01,  1.3937e-01,\n",
      "         -2.3713e-01, -2.1600e-01, -4.5663e-01,  2.7168e-02,  2.3453e-01,\n",
      "         -3.6210e-01,  2.8875e-01,  3.8881e-01,  1.2422e-01,  4.6177e-01,\n",
      "          4.4899e-01,  4.2099e-01, -2.2689e-01,  2.3584e-01, -1.8267e-01,\n",
      "          2.2487e-01, -3.3422e-01, -4.5962e-01, -4.3136e-01, -4.5188e-01,\n",
      "         -2.5392e-01,  2.1336e-01,  5.7722e-02, -5.0130e-02, -4.0297e-01,\n",
      "          1.9646e-01, -2.6501e-01,  3.6008e-01,  1.8912e-01, -2.5685e-01,\n",
      "         -3.4995e-01, -2.0231e-01, -4.7444e-01, -7.5289e-02, -4.8484e-01,\n",
      "          2.4525e-01,  2.8355e-01, -4.7662e-01,  2.2617e-01,  4.4863e-01,\n",
      "          2.4027e-01, -1.9471e-01, -2.4254e-01, -1.4281e-01,  4.4090e-01,\n",
      "          1.5124e-01,  3.8078e-01, -7.7263e-02, -2.4661e-01, -3.7154e-01,\n",
      "         -2.3260e-01, -4.9303e-01,  4.7423e-02, -2.7226e-01, -3.4987e-01,\n",
      "          1.2225e-01,  4.7308e-01, -3.5899e-01, -4.5405e-01, -1.4199e-01,\n",
      "         -2.3627e-01,  4.9589e-01]], requires_grad=True)\n",
      "Wq: torch.Size([4, 128])\n",
      "tensor([[-3.2097e-01, -1.5344e-02, -1.1510e-01, -3.0874e-01,  3.3117e-01,\n",
      "         -2.5896e-01,  6.9617e-02,  4.6040e-01, -4.1660e-01,  4.1727e-01,\n",
      "          4.1405e-01, -2.2532e-01,  4.6028e-01,  7.0296e-02, -4.5145e-01,\n",
      "         -4.9349e-01,  8.6677e-02,  2.9951e-01,  3.9258e-01, -4.0547e-01,\n",
      "          4.8854e-01, -4.5012e-01,  2.7701e-01,  3.6282e-01, -2.8834e-01,\n",
      "          5.2543e-02, -4.6092e-01,  1.8264e-01, -4.3262e-01, -4.9215e-01,\n",
      "          4.2615e-01, -3.8198e-01,  1.2274e-01,  2.0562e-01,  3.0310e-01,\n",
      "          2.8698e-01, -8.8241e-02, -1.6075e-01,  2.5161e-01,  4.2444e-01,\n",
      "         -8.2624e-02, -4.0507e-01, -1.2071e-01, -4.2426e-01, -2.8454e-01,\n",
      "          2.0912e-01,  2.4156e-01, -3.9879e-01, -2.2591e-01,  3.3003e-01,\n",
      "          3.2374e-01, -1.6253e-02,  2.8714e-01, -3.8530e-01, -3.8436e-01,\n",
      "         -2.4212e-01,  3.6687e-02, -4.5855e-01,  4.8830e-01,  4.7009e-01,\n",
      "         -1.6423e-01, -3.2546e-01,  4.4032e-01,  3.6316e-01, -8.7626e-02,\n",
      "         -4.4174e-01, -4.6600e-01, -4.8568e-03, -4.8418e-01,  6.4661e-02,\n",
      "          4.8494e-01, -1.6592e-01, -2.0181e-01,  4.4469e-01,  9.5466e-02,\n",
      "          4.8366e-01,  4.4880e-01, -1.9130e-01,  1.5374e-01,  2.2177e-01,\n",
      "          4.9499e-01,  4.8626e-01, -3.4601e-01, -3.4760e-01,  8.6108e-02,\n",
      "          4.1357e-01, -2.4427e-01, -2.5268e-01,  2.0449e-01, -4.9356e-01,\n",
      "         -3.6677e-01, -1.0875e-01,  4.0741e-01, -4.2069e-01, -3.3176e-01,\n",
      "          2.1056e-01, -8.8283e-02, -1.0314e-01,  4.6487e-01, -3.7954e-01,\n",
      "         -2.3864e-01, -4.2001e-01, -1.2691e-01,  1.9158e-01,  1.1451e-01,\n",
      "          1.6561e-01,  1.7231e-01,  1.7429e-01, -1.8123e-01,  3.6120e-01,\n",
      "          3.1331e-01, -9.2029e-02,  3.9661e-01,  8.8137e-02,  1.8981e-01,\n",
      "          1.9692e-02,  3.0192e-01, -2.1089e-01, -3.5772e-02, -4.6871e-01,\n",
      "         -4.8495e-01, -3.9506e-01,  1.8029e-01,  3.0469e-01, -6.6937e-02,\n",
      "         -1.2337e-01, -4.9423e-01,  4.1953e-01],\n",
      "        [-1.4990e-01,  2.7819e-01,  7.4129e-02, -4.4660e-02,  4.3282e-01,\n",
      "         -3.0581e-01,  1.8844e-01,  4.4872e-01, -1.1302e-01,  1.4516e-01,\n",
      "         -4.8995e-05,  4.3458e-01,  5.3082e-02,  4.4512e-01,  1.2523e-01,\n",
      "         -1.1507e-01,  3.1780e-01, -3.1342e-01, -4.4965e-01, -3.3284e-02,\n",
      "          1.5604e-02, -1.6669e-01,  1.8346e-01, -2.4034e-01, -4.7407e-01,\n",
      "         -7.6704e-02,  6.7073e-02,  1.2652e-01, -3.9627e-01,  3.7724e-01,\n",
      "         -1.9206e-01,  3.0627e-01,  4.2555e-02, -3.0911e-01,  4.3753e-01,\n",
      "         -2.9667e-01, -1.4520e-02, -1.0416e-02, -3.2493e-01,  4.7091e-01,\n",
      "          2.3998e-01,  3.4873e-01,  5.5291e-02,  9.5433e-02,  1.9302e-01,\n",
      "          3.8875e-01,  2.3504e-01, -1.4209e-01,  2.1127e-01,  2.3719e-01,\n",
      "          3.7475e-01, -2.1212e-01, -2.0359e-01, -4.3030e-01,  8.8165e-02,\n",
      "         -2.9098e-01,  7.8429e-02, -3.4094e-01,  4.3696e-01,  1.9249e-01,\n",
      "         -4.8272e-01, -3.3708e-01, -2.7591e-01, -2.7684e-01, -3.7848e-01,\n",
      "         -4.9171e-01, -4.6506e-01, -1.9350e-01,  3.7812e-01,  3.6263e-02,\n",
      "         -1.0092e-01, -3.0463e-01, -5.2127e-02, -2.3248e-01, -1.8609e-01,\n",
      "         -4.9422e-01,  4.5411e-01,  2.4784e-01,  1.7444e-01,  2.7136e-01,\n",
      "         -3.0759e-01, -2.0130e-01,  2.6875e-01, -4.1371e-01,  4.6615e-01,\n",
      "          1.8902e-01, -2.4082e-01, -4.6193e-01, -6.2983e-02, -2.6200e-01,\n",
      "          3.7065e-01,  4.9508e-01, -5.5899e-03,  1.6553e-01,  2.9573e-01,\n",
      "         -2.0170e-01, -5.1177e-03,  3.8824e-01, -3.7151e-01, -1.4505e-01,\n",
      "         -3.5091e-01, -4.1308e-01, -4.3970e-01,  2.8640e-01, -7.5188e-02,\n",
      "         -1.1953e-01,  2.8627e-01,  2.9312e-01,  4.1545e-02,  3.4332e-01,\n",
      "          5.6798e-02,  2.4595e-02,  1.1926e-01,  4.9348e-02,  1.0625e-01,\n",
      "          3.0282e-01,  1.3777e-01,  4.6769e-01, -4.9020e-01,  3.9159e-01,\n",
      "          6.4323e-02,  3.1386e-01,  1.8567e-01, -3.6389e-01, -6.6978e-02,\n",
      "          3.4887e-01, -4.4969e-01,  4.6964e-01],\n",
      "        [-3.6494e-01, -1.9819e-01,  1.2391e-01,  1.4987e-01, -4.8445e-01,\n",
      "         -2.4220e-01,  1.5516e-01, -2.8535e-02,  3.2431e-01, -2.2211e-01,\n",
      "          1.5887e-01,  2.8480e-01,  2.1065e-01, -3.2590e-01, -3.0512e-01,\n",
      "         -4.2174e-01, -2.1729e-01, -2.7202e-01, -4.9488e-01,  4.2156e-01,\n",
      "          4.1862e-01, -2.5510e-01, -1.7890e-01, -2.3873e-01, -1.4193e-01,\n",
      "          5.3084e-02,  1.0497e-01,  4.0349e-01, -4.5228e-01,  5.2142e-02,\n",
      "          4.7570e-01, -1.9244e-01, -2.6906e-01,  3.6564e-02,  2.8004e-01,\n",
      "          1.9182e-01, -3.6188e-01,  3.8665e-01,  2.9734e-01,  1.3272e-01,\n",
      "          2.8950e-01, -1.1843e-01,  1.8197e-01,  1.0729e-01, -1.2266e-01,\n",
      "          3.5865e-03, -3.2721e-01,  4.0710e-01,  2.1084e-01, -4.9324e-01,\n",
      "         -4.5889e-01,  4.4462e-01,  1.4339e-01,  6.2129e-02, -3.7913e-01,\n",
      "         -6.1301e-02, -5.8408e-02,  2.0735e-01, -2.0772e-01, -4.4801e-01,\n",
      "          8.8660e-02, -3.2614e-01,  5.8857e-02, -3.6563e-03, -1.2983e-01,\n",
      "          2.8123e-01, -3.0251e-01, -2.0165e-01, -2.1545e-01,  3.0909e-01,\n",
      "         -1.9715e-01, -3.4939e-01, -3.8061e-01,  4.6676e-01,  4.4117e-01,\n",
      "         -1.2104e-01,  1.7660e-01, -2.4349e-02,  2.1922e-01, -4.1842e-01,\n",
      "         -4.8203e-01,  6.6405e-02, -2.5446e-01, -2.6192e-01, -3.6002e-01,\n",
      "          3.5817e-01, -5.3533e-02, -1.7330e-01, -2.6523e-01, -4.1151e-01,\n",
      "          2.4367e-01, -1.5724e-01,  1.0179e-01,  1.8709e-02,  2.6733e-01,\n",
      "         -3.9686e-01,  1.9645e-01,  1.2850e-01, -1.6648e-01, -2.5955e-02,\n",
      "          2.8239e-01, -3.5465e-01, -8.4585e-02, -1.6311e-01,  2.4559e-01,\n",
      "          3.4939e-01,  2.8058e-01,  2.2911e-01, -1.5656e-01, -3.8567e-01,\n",
      "          4.8251e-02,  5.4339e-02,  2.4236e-03,  4.1418e-02,  1.3403e-01,\n",
      "          4.6479e-01, -3.6504e-01, -3.9382e-01,  8.7580e-02,  3.4661e-01,\n",
      "          3.6698e-01,  3.1494e-01,  2.8518e-01,  3.6290e-01, -1.9336e-01,\n",
      "         -3.9900e-01, -3.0189e-01,  4.7716e-01],\n",
      "        [ 3.6166e-01, -2.9039e-01, -1.9532e-01,  2.1463e-01,  3.0336e-01,\n",
      "          3.4968e-01,  1.7080e-01, -3.1044e-01, -2.6154e-01, -1.7996e-01,\n",
      "         -3.9154e-01,  4.5309e-01,  2.6656e-01,  3.2655e-01,  4.4053e-01,\n",
      "         -3.8769e-01,  4.5635e-01, -1.7444e-01,  2.8878e-01,  2.6081e-01,\n",
      "         -3.5004e-01,  1.7178e-01,  4.8178e-01, -2.5152e-01, -1.7255e-01,\n",
      "         -4.2525e-01, -5.4256e-02, -2.4009e-01, -2.3620e-01, -1.2440e-01,\n",
      "          3.0858e-01, -4.4050e-01,  1.7219e-01, -1.5976e-01,  3.5247e-01,\n",
      "          2.9060e-01, -3.8982e-01,  5.3928e-02,  2.4100e-01,  6.9047e-03,\n",
      "          4.1114e-01,  3.0978e-02,  2.3816e-01, -2.6534e-01, -9.1487e-02,\n",
      "         -1.2970e-01, -1.3459e-01,  4.5638e-01,  4.2021e-01, -2.6263e-01,\n",
      "          6.4170e-02,  2.1780e-01, -1.9102e-01, -3.3325e-02, -4.4954e-01,\n",
      "          2.6437e-01, -3.3372e-01,  4.9230e-01, -3.5328e-01, -2.8936e-01,\n",
      "         -1.1743e-01, -1.5401e-02,  3.7282e-01,  1.4672e-01,  7.0350e-02,\n",
      "          3.2993e-01,  2.1661e-01, -1.4682e-01,  4.2327e-01, -2.8551e-01,\n",
      "         -1.3827e-01,  2.2878e-01, -7.2087e-02,  4.5573e-01,  4.1544e-02,\n",
      "          1.9417e-01, -1.5592e-01, -2.4566e-01,  2.5795e-01,  1.7242e-03,\n",
      "          3.9895e-01,  6.4041e-02, -3.9480e-01, -1.9869e-01,  2.9795e-02,\n",
      "          7.1546e-02, -3.7036e-02, -1.2344e-01,  4.1349e-01, -1.3400e-01,\n",
      "          2.2914e-01,  4.0769e-01,  2.6082e-01, -3.1787e-01, -1.5388e-01,\n",
      "          4.6755e-01,  3.8927e-01,  3.0885e-01, -1.5936e-01,  1.8015e-01,\n",
      "         -1.3583e-01, -2.4389e-01, -3.9084e-01,  3.8825e-01, -1.7214e-01,\n",
      "          2.9319e-01,  4.8597e-01,  3.2779e-01,  2.0918e-01,  1.6785e-01,\n",
      "         -4.5960e-01, -4.2303e-01, -2.9061e-01,  4.8491e-01, -1.8013e-01,\n",
      "         -4.4329e-01,  1.1073e-03, -3.2739e-01, -1.9347e-01, -2.4275e-01,\n",
      "          1.8868e-01, -2.4091e-01, -6.2331e-02, -1.4281e-01,  4.4504e-01,\n",
      "         -4.6353e-01,  1.3425e-01, -4.4999e-01]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wk: torch.Size([4, 32])\n",
      "tensor([[-0.1553, -0.3907, -0.1353, -0.1834,  0.3281,  0.3566,  0.0125,  0.3220,\n",
      "         -0.4728,  0.4053, -0.1106,  0.2006, -0.3140,  0.0940, -0.0235, -0.4638,\n",
      "         -0.2144,  0.3576,  0.2637, -0.1129,  0.1342,  0.2356,  0.2626, -0.2528,\n",
      "         -0.4116,  0.2929, -0.3678,  0.0282, -0.4102, -0.2127,  0.1234, -0.4159],\n",
      "        [ 0.0832, -0.0981, -0.4366, -0.3156, -0.4994,  0.2409, -0.0746, -0.0859,\n",
      "         -0.4486,  0.2031,  0.1667, -0.3426, -0.3101,  0.2331,  0.2109,  0.1308,\n",
      "         -0.2913, -0.2992, -0.1622,  0.3290, -0.3057, -0.2059,  0.3593, -0.2645,\n",
      "         -0.1815, -0.4256, -0.2721, -0.3370,  0.0057, -0.0099, -0.2970, -0.4733],\n",
      "        [-0.0573, -0.3957,  0.4792, -0.2970,  0.1637, -0.2703,  0.1753, -0.4840,\n",
      "         -0.2821, -0.1479, -0.0942,  0.0175, -0.2854,  0.3667,  0.4935,  0.2581,\n",
      "          0.4504,  0.4091,  0.3461, -0.2889, -0.1077,  0.2105,  0.0639, -0.1379,\n",
      "          0.3734,  0.2860, -0.4596, -0.2787,  0.2498, -0.3322, -0.3371, -0.3497],\n",
      "        [-0.4004,  0.1394, -0.2371, -0.2160, -0.4566,  0.0272,  0.2345, -0.3621,\n",
      "          0.2887,  0.3888,  0.1242,  0.4618,  0.4490,  0.4210, -0.2269,  0.2358,\n",
      "         -0.1827,  0.2249, -0.3342, -0.4596, -0.4314, -0.4519, -0.2539,  0.2134,\n",
      "          0.0577, -0.0501, -0.4030,  0.1965, -0.2650,  0.3601,  0.1891, -0.2569]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wv: torch.Size([4, 32])\n",
      "tensor([[-0.2297, -0.2896,  0.1119, -0.2086, -0.1285, -0.1827, -0.0931, -0.2413,\n",
      "         -0.4642, -0.1566, -0.2637, -0.0271, -0.1591, -0.2213, -0.1838,  0.3740,\n",
      "         -0.3194,  0.0239, -0.2505, -0.2104, -0.2773, -0.2960,  0.4809, -0.4548,\n",
      "          0.3085,  0.1078,  0.0915, -0.4350, -0.2034,  0.0133, -0.1576,  0.1814],\n",
      "        [-0.0579, -0.2175,  0.2943,  0.3645, -0.0325,  0.2580, -0.4358, -0.0848,\n",
      "          0.0054,  0.3866, -0.4527, -0.2348,  0.3268,  0.0179,  0.0364,  0.2810,\n",
      "          0.4152, -0.2728, -0.1507, -0.3673,  0.0660, -0.1682,  0.0075, -0.4935,\n",
      "          0.3933,  0.1655,  0.3943,  0.3843,  0.0243, -0.3108,  0.3287,  0.4750],\n",
      "        [-0.0682,  0.4588,  0.1733, -0.2411, -0.2824, -0.2881,  0.2563, -0.4974,\n",
      "          0.0817,  0.2760,  0.4715, -0.3550, -0.3838, -0.3307, -0.2122,  0.1530,\n",
      "          0.4780, -0.0180, -0.3594,  0.4703, -0.3168,  0.1275,  0.4504,  0.2846,\n",
      "          0.1298, -0.2702,  0.2788, -0.4750,  0.4229,  0.4523, -0.0758, -0.2783],\n",
      "        [-0.3499, -0.2023, -0.4744, -0.0753, -0.4848,  0.2453,  0.2835, -0.4766,\n",
      "          0.2262,  0.4486,  0.2403, -0.1947, -0.2425, -0.1428,  0.4409,  0.1512,\n",
      "          0.3808, -0.0773, -0.2466, -0.3715, -0.2326, -0.4930,  0.0474, -0.2723,\n",
      "         -0.3499,  0.1222,  0.4731, -0.3590, -0.4541, -0.1420, -0.2363,  0.4959]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wq spliced: torch.Size([2, 64])\n",
      "tensor([[-0.2173, -0.2720, -0.4949,  0.4216,  0.4186, -0.2551, -0.1789, -0.2387,\n",
      "         -0.1419,  0.0531,  0.1050,  0.4035, -0.4523,  0.0521,  0.4757, -0.1924,\n",
      "          0.2108, -0.4932, -0.4589,  0.4446,  0.1434,  0.0621, -0.3791, -0.0613,\n",
      "         -0.0584,  0.2073, -0.2077, -0.4480,  0.0887, -0.3261,  0.0589, -0.0037,\n",
      "         -0.4820,  0.0664, -0.2545, -0.2619, -0.3600,  0.3582, -0.0535, -0.1733,\n",
      "         -0.2652, -0.4115,  0.2437, -0.1572,  0.1018,  0.0187,  0.2673, -0.3969,\n",
      "          0.0024,  0.0414,  0.1340,  0.4648, -0.3650, -0.3938,  0.0876,  0.3466,\n",
      "          0.3670,  0.3149,  0.2852,  0.3629, -0.1934, -0.3990, -0.3019,  0.4772],\n",
      "        [ 0.4563, -0.1744,  0.2888,  0.2608, -0.3500,  0.1718,  0.4818, -0.2515,\n",
      "         -0.1725, -0.4252, -0.0543, -0.2401, -0.2362, -0.1244,  0.3086, -0.4405,\n",
      "          0.4202, -0.2626,  0.0642,  0.2178, -0.1910, -0.0333, -0.4495,  0.2644,\n",
      "         -0.3337,  0.4923, -0.3533, -0.2894, -0.1174, -0.0154,  0.3728,  0.1467,\n",
      "          0.3989,  0.0640, -0.3948, -0.1987,  0.0298,  0.0715, -0.0370, -0.1234,\n",
      "          0.4135, -0.1340,  0.2291,  0.4077,  0.2608, -0.3179, -0.1539,  0.4676,\n",
      "         -0.2906,  0.4849, -0.1801, -0.4433,  0.0011, -0.3274, -0.1935, -0.2427,\n",
      "          0.1887, -0.2409, -0.0623, -0.1428,  0.4450, -0.4635,  0.1343, -0.4500]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Wk spliced: torch.Size([2, 16])\n",
      "tensor([[ 0.4504,  0.4091,  0.3461, -0.2889, -0.1077,  0.2105,  0.0639, -0.1379,\n",
      "          0.3734,  0.2860, -0.4596, -0.2787,  0.2498, -0.3322, -0.3371, -0.3497],\n",
      "        [-0.1827,  0.2249, -0.3342, -0.4596, -0.4314, -0.4519, -0.2539,  0.2134,\n",
      "          0.0577, -0.0501, -0.4030,  0.1965, -0.2650,  0.3601,  0.1891, -0.2569]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Wv spliced: torch.Size([2, 16])\n",
      "tensor([[ 0.4780, -0.0180, -0.3594,  0.4703, -0.3168,  0.1275,  0.4504,  0.2846,\n",
      "          0.1298, -0.2702,  0.2788, -0.4750,  0.4229,  0.4523, -0.0758, -0.2783],\n",
      "        [ 0.3808, -0.0773, -0.2466, -0.3715, -0.2326, -0.4930,  0.0474, -0.2723,\n",
      "         -0.3499,  0.1222,  0.4731, -0.3590, -0.4541, -0.1420, -0.2363,  0.4959]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Wqkv_spliced: torch.Size([2, 96])\n",
      "tensor([[-0.2173, -0.2720, -0.4949,  0.4216,  0.4186, -0.2551, -0.1789, -0.2387,\n",
      "         -0.1419,  0.0531,  0.1050,  0.4035, -0.4523,  0.0521,  0.4757, -0.1924,\n",
      "          0.2108, -0.4932, -0.4589,  0.4446,  0.1434,  0.0621, -0.3791, -0.0613,\n",
      "         -0.0584,  0.2073, -0.2077, -0.4480,  0.0887, -0.3261,  0.0589, -0.0037,\n",
      "         -0.4820,  0.0664, -0.2545, -0.2619, -0.3600,  0.3582, -0.0535, -0.1733,\n",
      "         -0.2652, -0.4115,  0.2437, -0.1572,  0.1018,  0.0187,  0.2673, -0.3969,\n",
      "          0.0024,  0.0414,  0.1340,  0.4648, -0.3650, -0.3938,  0.0876,  0.3466,\n",
      "          0.3670,  0.3149,  0.2852,  0.3629, -0.1934, -0.3990, -0.3019,  0.4772,\n",
      "          0.4504,  0.4091,  0.3461, -0.2889, -0.1077,  0.2105,  0.0639, -0.1379,\n",
      "          0.3734,  0.2860, -0.4596, -0.2787,  0.2498, -0.3322, -0.3371, -0.3497,\n",
      "          0.4780, -0.0180, -0.3594,  0.4703, -0.3168,  0.1275,  0.4504,  0.2846,\n",
      "          0.1298, -0.2702,  0.2788, -0.4750,  0.4229,  0.4523, -0.0758, -0.2783],\n",
      "        [ 0.4563, -0.1744,  0.2888,  0.2608, -0.3500,  0.1718,  0.4818, -0.2515,\n",
      "         -0.1725, -0.4252, -0.0543, -0.2401, -0.2362, -0.1244,  0.3086, -0.4405,\n",
      "          0.4202, -0.2626,  0.0642,  0.2178, -0.1910, -0.0333, -0.4495,  0.2644,\n",
      "         -0.3337,  0.4923, -0.3533, -0.2894, -0.1174, -0.0154,  0.3728,  0.1467,\n",
      "          0.3989,  0.0640, -0.3948, -0.1987,  0.0298,  0.0715, -0.0370, -0.1234,\n",
      "          0.4135, -0.1340,  0.2291,  0.4077,  0.2608, -0.3179, -0.1539,  0.4676,\n",
      "         -0.2906,  0.4849, -0.1801, -0.4433,  0.0011, -0.3274, -0.1935, -0.2427,\n",
      "          0.1887, -0.2409, -0.0623, -0.1428,  0.4450, -0.4635,  0.1343, -0.4500,\n",
      "         -0.1827,  0.2249, -0.3342, -0.4596, -0.4314, -0.4519, -0.2539,  0.2134,\n",
      "          0.0577, -0.0501, -0.4030,  0.1965, -0.2650,  0.3601,  0.1891, -0.2569,\n",
      "          0.3808, -0.0773, -0.2466, -0.3715, -0.2326, -0.4930,  0.0474, -0.2723,\n",
      "         -0.3499,  0.1222,  0.4731, -0.3590, -0.4541, -0.1420, -0.2363,  0.4959]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "xqkv: torch.Size([1, 3, 96])\n",
      "tensor([[[-3.4402e-01,  4.2502e-01,  7.7734e-02, -6.4775e-01,  5.5599e-02,\n",
      "           1.3767e-02, -4.0493e-01,  4.8596e-01,  3.1531e-01,  4.4407e-01,\n",
      "          -2.4515e-02, -5.8045e-02,  6.4491e-01,  9.9639e-02, -7.4732e-01,\n",
      "           6.6453e-01, -6.5647e-01,  7.0909e-01,  3.0571e-01, -6.1746e-01,\n",
      "           1.0066e-01, -1.3119e-02,  8.2924e-01, -2.5268e-01,  4.3121e-01,\n",
      "          -7.3630e-01,  5.7708e-01,  7.0237e-01,  6.1457e-02,  2.8727e-01,\n",
      "          -4.7645e-01, -1.6533e-01, -5.9308e-02, -1.2838e-01,  6.6336e-01,\n",
      "           4.4450e-01,  2.6342e-01, -3.7817e-01,  8.6751e-02,  2.8490e-01,\n",
      "          -2.5521e-01,  4.9394e-01, -4.6436e-01, -3.3782e-01, -3.8342e-01,\n",
      "           3.4927e-01, -4.4420e-02, -2.0843e-01,  3.3146e-01, -5.9065e-01,\n",
      "           9.5895e-02,  1.2444e-01,  3.0049e-01,  7.0121e-01,  1.4960e-01,\n",
      "          -7.9808e-03, -5.1987e-01,  1.6086e-02, -1.6422e-01, -1.3612e-01,\n",
      "          -3.5083e-01,  8.6171e-01,  9.5509e-02,  1.2190e-01, -1.6274e-01,\n",
      "          -5.9625e-01,  9.7363e-02,  7.6618e-01,  5.8396e-01,  3.4448e-01,\n",
      "           2.3851e-01, -1.3083e-01, -3.7494e-01, -1.7888e-01,  8.4233e-01,\n",
      "           4.9865e-03,  9.7550e-02, -1.3859e-01,  6.1656e-02,  5.8378e-01,\n",
      "          -8.3206e-01,  1.0356e-01,  5.8008e-01,  3.7550e-02,  5.2875e-01,\n",
      "           4.6031e-01, -4.2671e-01,  7.7098e-02,  2.9416e-01,  8.3122e-02,\n",
      "          -7.7329e-01,  8.0458e-01,  1.7145e-01, -2.1093e-01,  3.3379e-01,\n",
      "          -3.3893e-01],\n",
      "         [ 3.2802e-01, -4.2915e-01, -9.8160e-02,  6.5434e-01, -3.6065e-02,\n",
      "          -2.4785e-02,  3.8950e-01, -4.8748e-01, -3.1573e-01, -4.3347e-01,\n",
      "           2.8698e-02,  7.4798e-02, -6.5291e-01, -9.5483e-02,  7.5445e-01,\n",
      "          -6.6068e-01,  6.5358e-01, -7.1771e-01, -3.2031e-01,  6.2564e-01,\n",
      "          -9.2450e-02,  1.5621e-02, -8.3058e-01,  2.4527e-01, -4.2578e-01,\n",
      "           7.3178e-01, -5.7554e-01, -7.0912e-01, -5.6397e-02, -2.9635e-01,\n",
      "           4.7020e-01,  1.6209e-01,  3.6903e-02,  1.2893e-01, -6.6228e-01,\n",
      "          -4.4782e-01, -2.7444e-01,  3.8697e-01, -8.7504e-02, -2.8726e-01,\n",
      "           2.3874e-01, -5.0294e-01,  4.6650e-01,  3.2459e-01,  3.8079e-01,\n",
      "          -3.4195e-01,  5.5409e-02,  1.8702e-01, -3.2519e-01,  5.8151e-01,\n",
      "          -8.8189e-02, -1.0159e-01, -3.1104e-01, -7.0558e-01, -1.4295e-01,\n",
      "           2.3150e-02,  5.2642e-01, -1.8684e-03,  1.7377e-01,  1.4963e-01,\n",
      "           3.3576e-01, -8.6333e-01, -1.0708e-01, -9.8550e-02,  1.7962e-01,\n",
      "           6.0325e-01, -8.0256e-02, -7.6470e-01, -5.7787e-01, -3.2877e-01,\n",
      "          -2.3125e-01,  1.2231e-01,  3.8448e-01,  1.8820e-01, -8.4699e-01,\n",
      "          -1.7212e-02, -8.4696e-02,  1.2133e-01, -7.5408e-02, -5.8838e-01,\n",
      "           8.3772e-01, -1.0244e-01, -5.8518e-01, -1.6068e-02, -5.3292e-01,\n",
      "          -4.4612e-01,  4.3868e-01, -6.3085e-02, -2.8296e-01, -9.3520e-02,\n",
      "           7.7124e-01, -8.1062e-01, -1.4958e-01,  2.2699e-01, -3.3094e-01,\n",
      "           3.2033e-01],\n",
      "         [-7.0005e-01, -3.5462e-02, -7.4801e-01,  6.5705e-02,  7.5201e-01,\n",
      "          -4.1143e-01, -6.9596e-01,  7.9871e-02,  7.3436e-02,  5.2483e-01,\n",
      "           1.5077e-01,  6.1511e-01, -1.1968e-01,  1.8478e-01,  5.8043e-02,\n",
      "           3.3247e-01, -2.9389e-01, -1.2483e-01, -4.6402e-01,  1.3388e-01,\n",
      "           3.3778e-01,  9.0605e-02,  1.8334e-01, -3.5043e-01,  3.2646e-01,\n",
      "          -3.7816e-01,  2.2108e-01, -5.6085e-02,  2.0809e-01, -2.6099e-01,\n",
      "          -3.7017e-01, -1.6856e-01, -8.6128e-01, -1.5538e-02,  2.2800e-01,\n",
      "           4.9326e-04, -3.4088e-01,  2.2501e-01, -3.9270e-03, -8.7122e-03,\n",
      "          -6.9264e-01, -2.0011e-01, -5.0415e-02, -5.9392e-01, -2.0723e-01,\n",
      "           3.7440e-01,  4.0168e-01, -8.6594e-01,  3.2977e-01, -5.1145e-01,\n",
      "           3.1751e-01,  8.9656e-01, -3.1281e-01,  3.3055e-02,  2.9291e-01,\n",
      "           5.6956e-01,  1.0045e-01,  5.4045e-01,  3.1369e-01,  4.7077e-01,\n",
      "          -6.6687e-01,  1.8214e-01, -4.0905e-01,  9.1468e-01,  5.9043e-01,\n",
      "           9.5637e-02,  6.7231e-01,  2.7174e-01,  3.9453e-01,  6.8923e-01,\n",
      "           3.4090e-01, -3.5827e-01,  2.5365e-01,  3.0061e-01,  6.2130e-02,\n",
      "          -4.5943e-01,  5.1207e-01, -6.8954e-01, -5.0096e-01, -8.8056e-03,\n",
      "          -2.1410e-02,  7.1732e-02, -2.8679e-02,  8.2037e-01, -8.0675e-03,\n",
      "           6.6480e-01,  3.3091e-01,  5.4995e-01,  5.0531e-01, -3.6850e-01,\n",
      "          -2.9552e-01, -6.1326e-04,  8.7290e-01,  5.4611e-01,  2.0170e-01,\n",
      "          -7.9673e-01]]], grad_fn=<UnsafeViewBackward0>)\n",
      "xq: torch.Size([1, 3, 64])\n",
      "tensor([[[-3.4402e-01,  4.2502e-01,  7.7734e-02, -6.4775e-01,  5.5599e-02,\n",
      "           1.3767e-02, -4.0493e-01,  4.8596e-01,  3.1531e-01,  4.4407e-01,\n",
      "          -2.4515e-02, -5.8045e-02,  6.4491e-01,  9.9639e-02, -7.4732e-01,\n",
      "           6.6453e-01, -6.5647e-01,  7.0909e-01,  3.0571e-01, -6.1746e-01,\n",
      "           1.0066e-01, -1.3119e-02,  8.2924e-01, -2.5268e-01,  4.3121e-01,\n",
      "          -7.3630e-01,  5.7708e-01,  7.0237e-01,  6.1457e-02,  2.8727e-01,\n",
      "          -4.7645e-01, -1.6533e-01, -5.9308e-02, -1.2838e-01,  6.6336e-01,\n",
      "           4.4450e-01,  2.6342e-01, -3.7817e-01,  8.6751e-02,  2.8490e-01,\n",
      "          -2.5521e-01,  4.9394e-01, -4.6436e-01, -3.3782e-01, -3.8342e-01,\n",
      "           3.4927e-01, -4.4420e-02, -2.0843e-01,  3.3146e-01, -5.9065e-01,\n",
      "           9.5895e-02,  1.2444e-01,  3.0049e-01,  7.0121e-01,  1.4960e-01,\n",
      "          -7.9808e-03, -5.1987e-01,  1.6086e-02, -1.6422e-01, -1.3612e-01,\n",
      "          -3.5083e-01,  8.6171e-01,  9.5509e-02,  1.2190e-01],\n",
      "         [ 3.2802e-01, -4.2915e-01, -9.8160e-02,  6.5434e-01, -3.6065e-02,\n",
      "          -2.4785e-02,  3.8950e-01, -4.8748e-01, -3.1573e-01, -4.3347e-01,\n",
      "           2.8698e-02,  7.4798e-02, -6.5291e-01, -9.5483e-02,  7.5445e-01,\n",
      "          -6.6068e-01,  6.5358e-01, -7.1771e-01, -3.2031e-01,  6.2564e-01,\n",
      "          -9.2450e-02,  1.5621e-02, -8.3058e-01,  2.4527e-01, -4.2578e-01,\n",
      "           7.3178e-01, -5.7554e-01, -7.0912e-01, -5.6397e-02, -2.9635e-01,\n",
      "           4.7020e-01,  1.6209e-01,  3.6903e-02,  1.2893e-01, -6.6228e-01,\n",
      "          -4.4782e-01, -2.7444e-01,  3.8697e-01, -8.7504e-02, -2.8726e-01,\n",
      "           2.3874e-01, -5.0294e-01,  4.6650e-01,  3.2459e-01,  3.8079e-01,\n",
      "          -3.4195e-01,  5.5409e-02,  1.8702e-01, -3.2519e-01,  5.8151e-01,\n",
      "          -8.8189e-02, -1.0159e-01, -3.1104e-01, -7.0558e-01, -1.4295e-01,\n",
      "           2.3150e-02,  5.2642e-01, -1.8684e-03,  1.7377e-01,  1.4963e-01,\n",
      "           3.3576e-01, -8.6333e-01, -1.0708e-01, -9.8550e-02],\n",
      "         [-7.0005e-01, -3.5462e-02, -7.4801e-01,  6.5705e-02,  7.5201e-01,\n",
      "          -4.1143e-01, -6.9596e-01,  7.9871e-02,  7.3436e-02,  5.2483e-01,\n",
      "           1.5077e-01,  6.1511e-01, -1.1968e-01,  1.8478e-01,  5.8043e-02,\n",
      "           3.3247e-01, -2.9389e-01, -1.2483e-01, -4.6402e-01,  1.3388e-01,\n",
      "           3.3778e-01,  9.0605e-02,  1.8334e-01, -3.5043e-01,  3.2646e-01,\n",
      "          -3.7816e-01,  2.2108e-01, -5.6085e-02,  2.0809e-01, -2.6099e-01,\n",
      "          -3.7017e-01, -1.6856e-01, -8.6128e-01, -1.5538e-02,  2.2800e-01,\n",
      "           4.9326e-04, -3.4088e-01,  2.2501e-01, -3.9270e-03, -8.7122e-03,\n",
      "          -6.9264e-01, -2.0011e-01, -5.0415e-02, -5.9392e-01, -2.0723e-01,\n",
      "           3.7440e-01,  4.0168e-01, -8.6594e-01,  3.2977e-01, -5.1145e-01,\n",
      "           3.1751e-01,  8.9656e-01, -3.1281e-01,  3.3055e-02,  2.9291e-01,\n",
      "           5.6956e-01,  1.0045e-01,  5.4045e-01,  3.1369e-01,  4.7077e-01,\n",
      "          -6.6687e-01,  1.8214e-01, -4.0905e-01,  9.1468e-01]]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "xk: torch.Size([1, 3, 16])\n",
      "tensor([[[-0.1627, -0.5962,  0.0974,  0.7662,  0.5840,  0.3445,  0.2385,\n",
      "          -0.1308, -0.3749, -0.1789,  0.8423,  0.0050,  0.0976, -0.1386,\n",
      "           0.0617,  0.5838],\n",
      "         [ 0.1796,  0.6032, -0.0803, -0.7647, -0.5779, -0.3288, -0.2313,\n",
      "           0.1223,  0.3845,  0.1882, -0.8470, -0.0172, -0.0847,  0.1213,\n",
      "          -0.0754, -0.5884],\n",
      "         [ 0.5904,  0.0956,  0.6723,  0.2717,  0.3945,  0.6892,  0.3409,\n",
      "          -0.3583,  0.2536,  0.3006,  0.0621, -0.4594,  0.5121, -0.6895,\n",
      "          -0.5010, -0.0088]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xv: torch.Size([1, 3, 16])\n",
      "tensor([[[-8.3206e-01,  1.0356e-01,  5.8008e-01,  3.7550e-02,  5.2875e-01,\n",
      "           4.6031e-01, -4.2671e-01,  7.7098e-02,  2.9416e-01,  8.3122e-02,\n",
      "          -7.7329e-01,  8.0458e-01,  1.7145e-01, -2.1093e-01,  3.3379e-01,\n",
      "          -3.3893e-01],\n",
      "         [ 8.3772e-01, -1.0244e-01, -5.8518e-01, -1.6068e-02, -5.3292e-01,\n",
      "          -4.4612e-01,  4.3868e-01, -6.3085e-02, -2.8296e-01, -9.3520e-02,\n",
      "           7.7124e-01, -8.1062e-01, -1.4958e-01,  2.2699e-01, -3.3094e-01,\n",
      "           3.2033e-01],\n",
      "         [-2.1410e-02,  7.1732e-02, -2.8679e-02,  8.2037e-01, -8.0675e-03,\n",
      "           6.6480e-01,  3.3091e-01,  5.4995e-01,  5.0531e-01, -3.6850e-01,\n",
      "          -2.9552e-01, -6.1326e-04,  8.7290e-01,  5.4611e-01,  2.0170e-01,\n",
      "          -7.9673e-01]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xq reshaped: torch.Size([1, 3, 4, 16])\n",
      "tensor([[[[-3.4402e-01,  4.2502e-01,  7.7734e-02, -6.4775e-01,  5.5599e-02,\n",
      "            1.3767e-02, -4.0493e-01,  4.8596e-01,  3.1531e-01,  4.4407e-01,\n",
      "           -2.4515e-02, -5.8045e-02,  6.4491e-01,  9.9639e-02, -7.4732e-01,\n",
      "            6.6453e-01],\n",
      "          [-6.5647e-01,  7.0909e-01,  3.0571e-01, -6.1746e-01,  1.0066e-01,\n",
      "           -1.3119e-02,  8.2924e-01, -2.5268e-01,  4.3121e-01, -7.3630e-01,\n",
      "            5.7708e-01,  7.0237e-01,  6.1457e-02,  2.8727e-01, -4.7645e-01,\n",
      "           -1.6533e-01],\n",
      "          [-5.9308e-02, -1.2838e-01,  6.6336e-01,  4.4450e-01,  2.6342e-01,\n",
      "           -3.7817e-01,  8.6751e-02,  2.8490e-01, -2.5521e-01,  4.9394e-01,\n",
      "           -4.6436e-01, -3.3782e-01, -3.8342e-01,  3.4927e-01, -4.4420e-02,\n",
      "           -2.0843e-01],\n",
      "          [ 3.3146e-01, -5.9065e-01,  9.5895e-02,  1.2444e-01,  3.0049e-01,\n",
      "            7.0121e-01,  1.4960e-01, -7.9808e-03, -5.1987e-01,  1.6086e-02,\n",
      "           -1.6422e-01, -1.3612e-01, -3.5083e-01,  8.6171e-01,  9.5509e-02,\n",
      "            1.2190e-01]],\n",
      "\n",
      "         [[ 3.2802e-01, -4.2915e-01, -9.8160e-02,  6.5434e-01, -3.6065e-02,\n",
      "           -2.4785e-02,  3.8950e-01, -4.8748e-01, -3.1573e-01, -4.3347e-01,\n",
      "            2.8698e-02,  7.4798e-02, -6.5291e-01, -9.5483e-02,  7.5445e-01,\n",
      "           -6.6068e-01],\n",
      "          [ 6.5358e-01, -7.1771e-01, -3.2031e-01,  6.2564e-01, -9.2450e-02,\n",
      "            1.5621e-02, -8.3058e-01,  2.4527e-01, -4.2578e-01,  7.3178e-01,\n",
      "           -5.7554e-01, -7.0912e-01, -5.6397e-02, -2.9635e-01,  4.7020e-01,\n",
      "            1.6209e-01],\n",
      "          [ 3.6903e-02,  1.2893e-01, -6.6228e-01, -4.4782e-01, -2.7444e-01,\n",
      "            3.8697e-01, -8.7504e-02, -2.8726e-01,  2.3874e-01, -5.0294e-01,\n",
      "            4.6650e-01,  3.2459e-01,  3.8079e-01, -3.4195e-01,  5.5409e-02,\n",
      "            1.8702e-01],\n",
      "          [-3.2519e-01,  5.8151e-01, -8.8189e-02, -1.0159e-01, -3.1104e-01,\n",
      "           -7.0558e-01, -1.4295e-01,  2.3150e-02,  5.2642e-01, -1.8684e-03,\n",
      "            1.7377e-01,  1.4963e-01,  3.3576e-01, -8.6333e-01, -1.0708e-01,\n",
      "           -9.8550e-02]],\n",
      "\n",
      "         [[-7.0005e-01, -3.5462e-02, -7.4801e-01,  6.5705e-02,  7.5201e-01,\n",
      "           -4.1143e-01, -6.9596e-01,  7.9871e-02,  7.3436e-02,  5.2483e-01,\n",
      "            1.5077e-01,  6.1511e-01, -1.1968e-01,  1.8478e-01,  5.8043e-02,\n",
      "            3.3247e-01],\n",
      "          [-2.9389e-01, -1.2483e-01, -4.6402e-01,  1.3388e-01,  3.3778e-01,\n",
      "            9.0605e-02,  1.8334e-01, -3.5043e-01,  3.2646e-01, -3.7816e-01,\n",
      "            2.2108e-01, -5.6085e-02,  2.0809e-01, -2.6099e-01, -3.7017e-01,\n",
      "           -1.6856e-01],\n",
      "          [-8.6128e-01, -1.5538e-02,  2.2800e-01,  4.9326e-04, -3.4088e-01,\n",
      "            2.2501e-01, -3.9270e-03, -8.7122e-03, -6.9264e-01, -2.0011e-01,\n",
      "           -5.0415e-02, -5.9392e-01, -2.0723e-01,  3.7440e-01,  4.0168e-01,\n",
      "           -8.6594e-01],\n",
      "          [ 3.2977e-01, -5.1145e-01,  3.1751e-01,  8.9656e-01, -3.1281e-01,\n",
      "            3.3055e-02,  2.9291e-01,  5.6956e-01,  1.0045e-01,  5.4045e-01,\n",
      "            3.1369e-01,  4.7077e-01, -6.6687e-01,  1.8214e-01, -4.0905e-01,\n",
      "            9.1468e-01]]]], grad_fn=<ViewBackward0>)\n",
      "xk reshaped: torch.Size([1, 3, 1, 16])\n",
      "tensor([[[[-0.1627, -0.5962,  0.0974,  0.7662,  0.5840,  0.3445,  0.2385,\n",
      "           -0.1308, -0.3749, -0.1789,  0.8423,  0.0050,  0.0976, -0.1386,\n",
      "            0.0617,  0.5838]],\n",
      "\n",
      "         [[ 0.1796,  0.6032, -0.0803, -0.7647, -0.5779, -0.3288, -0.2313,\n",
      "            0.1223,  0.3845,  0.1882, -0.8470, -0.0172, -0.0847,  0.1213,\n",
      "           -0.0754, -0.5884]],\n",
      "\n",
      "         [[ 0.5904,  0.0956,  0.6723,  0.2717,  0.3945,  0.6892,  0.3409,\n",
      "           -0.3583,  0.2536,  0.3006,  0.0621, -0.4594,  0.5121, -0.6895,\n",
      "           -0.5010, -0.0088]]]], grad_fn=<ViewBackward0>)\n",
      "xv reshaped: torch.Size([1, 3, 1, 16])\n",
      "tensor([[[[-8.3206e-01,  1.0356e-01,  5.8008e-01,  3.7550e-02,  5.2875e-01,\n",
      "            4.6031e-01, -4.2671e-01,  7.7098e-02,  2.9416e-01,  8.3122e-02,\n",
      "           -7.7329e-01,  8.0458e-01,  1.7145e-01, -2.1093e-01,  3.3379e-01,\n",
      "           -3.3893e-01]],\n",
      "\n",
      "         [[ 8.3772e-01, -1.0244e-01, -5.8518e-01, -1.6068e-02, -5.3292e-01,\n",
      "           -4.4612e-01,  4.3868e-01, -6.3085e-02, -2.8296e-01, -9.3520e-02,\n",
      "            7.7124e-01, -8.1062e-01, -1.4958e-01,  2.2699e-01, -3.3094e-01,\n",
      "            3.2033e-01]],\n",
      "\n",
      "         [[-2.1410e-02,  7.1732e-02, -2.8679e-02,  8.2037e-01, -8.0675e-03,\n",
      "            6.6480e-01,  3.3091e-01,  5.4995e-01,  5.0531e-01, -3.6850e-01,\n",
      "           -2.9552e-01, -6.1326e-04,  8.7290e-01,  5.4611e-01,  2.0170e-01,\n",
      "           -7.9673e-01]]]], grad_fn=<ViewBackward0>)\n",
      "rotated xq: torch.Size([1, 3, 4, 16])\n",
      "tensor([[[[-0.3440,  0.4250,  0.0777, -0.6477,  0.0556,  0.0138, -0.4049,\n",
      "            0.4860,  0.3153,  0.4441, -0.0245, -0.0580,  0.6449,  0.0996,\n",
      "           -0.7473,  0.6645],\n",
      "          [-0.6565,  0.7091,  0.3057, -0.6175,  0.1007, -0.0131,  0.8292,\n",
      "           -0.2527,  0.4312, -0.7363,  0.5771,  0.7024,  0.0615,  0.2873,\n",
      "           -0.4765, -0.1653],\n",
      "          [-0.0593, -0.1284,  0.6634,  0.4445,  0.2634, -0.3782,  0.0868,\n",
      "            0.2849, -0.2552,  0.4939, -0.4644, -0.3378, -0.3834,  0.3493,\n",
      "           -0.0444, -0.2084],\n",
      "          [ 0.3315, -0.5907,  0.0959,  0.1244,  0.3005,  0.7012,  0.1496,\n",
      "           -0.0080, -0.5199,  0.0161, -0.1642, -0.1361, -0.3508,  0.8617,\n",
      "            0.0955,  0.1219]],\n",
      "\n",
      "         [[ 0.4429, -0.1319, -0.1022,  0.6308,  0.0293, -0.0194,  0.3654,\n",
      "           -0.4757,  0.1054, -0.5955, -0.0033,  0.1894, -0.6532, -0.0967,\n",
      "            0.7664, -0.6692],\n",
      "          [ 0.7114, -0.9973, -0.1254,  0.7412, -0.0864,  0.0323, -0.8450,\n",
      "            0.2424,  0.3199,  0.2364, -0.6466, -0.5873, -0.0653, -0.2950,\n",
      "            0.4437,  0.1664],\n",
      "          [-0.1810,  0.3772, -0.7745, -0.4982, -0.3111,  0.4056, -0.0892,\n",
      "           -0.2905,  0.1600, -0.3568,  0.2374,  0.2403,  0.3515, -0.3197,\n",
      "            0.0526,  0.1819],\n",
      "          [-0.6187,  0.4930, -0.1379, -0.1265, -0.3430, -0.6559, -0.1395,\n",
      "            0.0249,  0.0108,  0.3085,  0.1377,  0.1293,  0.3030, -0.9016,\n",
      "           -0.1115, -0.0981]],\n",
      "\n",
      "         [[ 0.2245, -0.4888, -0.6925, -0.1526,  0.7608, -0.4296, -0.6982,\n",
      "            0.0680, -0.6671,  0.1945, -0.3206,  0.5995,  0.0321,  0.1374,\n",
      "            0.0139,  0.3351],\n",
      "          [-0.1746,  0.2873, -0.5050,  0.1450,  0.2897,  0.1193,  0.2064,\n",
      "           -0.3442, -0.4031, -0.2758, -0.0960, -0.0060,  0.2710, -0.2492,\n",
      "           -0.3578, -0.1809],\n",
      "          [ 0.9882,  0.1738,  0.2137,  0.2073, -0.2929,  0.1816, -0.0293,\n",
      "            0.0221, -0.4949, -0.1004,  0.0941, -0.5566, -0.2708,  0.3973,\n",
      "            0.4006, -0.8657],\n",
      "          [-0.2286, -0.7082,  0.0707,  0.6765, -0.1741,  0.0124,  0.3182,\n",
      "            0.5367,  0.2581, -0.2282,  0.4407,  0.7535, -0.7157,  0.1847,\n",
      "           -0.3897,  0.9344]]]], grad_fn=<TransposeBackward0>)\n",
      "rotated xk: torch.Size([1, 3, 1, 16])\n",
      "tensor([[[[-0.1627, -0.5962,  0.0974,  0.7662,  0.5840,  0.3445,  0.2385,\n",
      "           -0.1308, -0.3749, -0.1789,  0.8423,  0.0050,  0.0976, -0.1386,\n",
      "            0.0617,  0.5838]],\n",
      "\n",
      "         [[-0.2265,  0.4100,  0.1871, -0.7496, -0.5665, -0.3351, -0.2288,\n",
      "            0.1328,  0.3589,  0.4808, -0.8299, -0.1522, -0.1420,  0.1027,\n",
      "           -0.0827, -0.5861]],\n",
      "\n",
      "         [[-0.4763, -0.2299,  0.5055,  0.4147,  0.2849,  0.7623,  0.3719,\n",
      "           -0.3577,  0.4313,  0.2160,  0.4475, -0.3361,  0.5802, -0.6078,\n",
      "           -0.4784, -0.0215]]]], grad_fn=<TransposeBackward0>)\n",
      "repeat_interleaved xk: torch.Size([1, 3, 4, 16])\n",
      "tensor([[[[-0.1627, -0.5962,  0.0974,  0.7662,  0.5840,  0.3445,  0.2385,\n",
      "           -0.1308, -0.3749, -0.1789,  0.8423,  0.0050,  0.0976, -0.1386,\n",
      "            0.0617,  0.5838],\n",
      "          [-0.1627, -0.5962,  0.0974,  0.7662,  0.5840,  0.3445,  0.2385,\n",
      "           -0.1308, -0.3749, -0.1789,  0.8423,  0.0050,  0.0976, -0.1386,\n",
      "            0.0617,  0.5838],\n",
      "          [-0.1627, -0.5962,  0.0974,  0.7662,  0.5840,  0.3445,  0.2385,\n",
      "           -0.1308, -0.3749, -0.1789,  0.8423,  0.0050,  0.0976, -0.1386,\n",
      "            0.0617,  0.5838],\n",
      "          [-0.1627, -0.5962,  0.0974,  0.7662,  0.5840,  0.3445,  0.2385,\n",
      "           -0.1308, -0.3749, -0.1789,  0.8423,  0.0050,  0.0976, -0.1386,\n",
      "            0.0617,  0.5838]],\n",
      "\n",
      "         [[-0.2265,  0.4100,  0.1871, -0.7496, -0.5665, -0.3351, -0.2288,\n",
      "            0.1328,  0.3589,  0.4808, -0.8299, -0.1522, -0.1420,  0.1027,\n",
      "           -0.0827, -0.5861],\n",
      "          [-0.2265,  0.4100,  0.1871, -0.7496, -0.5665, -0.3351, -0.2288,\n",
      "            0.1328,  0.3589,  0.4808, -0.8299, -0.1522, -0.1420,  0.1027,\n",
      "           -0.0827, -0.5861],\n",
      "          [-0.2265,  0.4100,  0.1871, -0.7496, -0.5665, -0.3351, -0.2288,\n",
      "            0.1328,  0.3589,  0.4808, -0.8299, -0.1522, -0.1420,  0.1027,\n",
      "           -0.0827, -0.5861],\n",
      "          [-0.2265,  0.4100,  0.1871, -0.7496, -0.5665, -0.3351, -0.2288,\n",
      "            0.1328,  0.3589,  0.4808, -0.8299, -0.1522, -0.1420,  0.1027,\n",
      "           -0.0827, -0.5861]],\n",
      "\n",
      "         [[-0.4763, -0.2299,  0.5055,  0.4147,  0.2849,  0.7623,  0.3719,\n",
      "           -0.3577,  0.4313,  0.2160,  0.4475, -0.3361,  0.5802, -0.6078,\n",
      "           -0.4784, -0.0215],\n",
      "          [-0.4763, -0.2299,  0.5055,  0.4147,  0.2849,  0.7623,  0.3719,\n",
      "           -0.3577,  0.4313,  0.2160,  0.4475, -0.3361,  0.5802, -0.6078,\n",
      "           -0.4784, -0.0215],\n",
      "          [-0.4763, -0.2299,  0.5055,  0.4147,  0.2849,  0.7623,  0.3719,\n",
      "           -0.3577,  0.4313,  0.2160,  0.4475, -0.3361,  0.5802, -0.6078,\n",
      "           -0.4784, -0.0215],\n",
      "          [-0.4763, -0.2299,  0.5055,  0.4147,  0.2849,  0.7623,  0.3719,\n",
      "           -0.3577,  0.4313,  0.2160,  0.4475, -0.3361,  0.5802, -0.6078,\n",
      "           -0.4784, -0.0215]]]], grad_fn=<ViewBackward0>)\n",
      "repeat_interleaved xv: torch.Size([1, 3, 4, 16])\n",
      "tensor([[[[-8.3206e-01,  1.0356e-01,  5.8008e-01,  3.7550e-02,  5.2875e-01,\n",
      "            4.6031e-01, -4.2671e-01,  7.7098e-02,  2.9416e-01,  8.3122e-02,\n",
      "           -7.7329e-01,  8.0458e-01,  1.7145e-01, -2.1093e-01,  3.3379e-01,\n",
      "           -3.3893e-01],\n",
      "          [-8.3206e-01,  1.0356e-01,  5.8008e-01,  3.7550e-02,  5.2875e-01,\n",
      "            4.6031e-01, -4.2671e-01,  7.7098e-02,  2.9416e-01,  8.3122e-02,\n",
      "           -7.7329e-01,  8.0458e-01,  1.7145e-01, -2.1093e-01,  3.3379e-01,\n",
      "           -3.3893e-01],\n",
      "          [-8.3206e-01,  1.0356e-01,  5.8008e-01,  3.7550e-02,  5.2875e-01,\n",
      "            4.6031e-01, -4.2671e-01,  7.7098e-02,  2.9416e-01,  8.3122e-02,\n",
      "           -7.7329e-01,  8.0458e-01,  1.7145e-01, -2.1093e-01,  3.3379e-01,\n",
      "           -3.3893e-01],\n",
      "          [-8.3206e-01,  1.0356e-01,  5.8008e-01,  3.7550e-02,  5.2875e-01,\n",
      "            4.6031e-01, -4.2671e-01,  7.7098e-02,  2.9416e-01,  8.3122e-02,\n",
      "           -7.7329e-01,  8.0458e-01,  1.7145e-01, -2.1093e-01,  3.3379e-01,\n",
      "           -3.3893e-01]],\n",
      "\n",
      "         [[ 8.3772e-01, -1.0244e-01, -5.8518e-01, -1.6068e-02, -5.3292e-01,\n",
      "           -4.4612e-01,  4.3868e-01, -6.3085e-02, -2.8296e-01, -9.3520e-02,\n",
      "            7.7124e-01, -8.1062e-01, -1.4958e-01,  2.2699e-01, -3.3094e-01,\n",
      "            3.2033e-01],\n",
      "          [ 8.3772e-01, -1.0244e-01, -5.8518e-01, -1.6068e-02, -5.3292e-01,\n",
      "           -4.4612e-01,  4.3868e-01, -6.3085e-02, -2.8296e-01, -9.3520e-02,\n",
      "            7.7124e-01, -8.1062e-01, -1.4958e-01,  2.2699e-01, -3.3094e-01,\n",
      "            3.2033e-01],\n",
      "          [ 8.3772e-01, -1.0244e-01, -5.8518e-01, -1.6068e-02, -5.3292e-01,\n",
      "           -4.4612e-01,  4.3868e-01, -6.3085e-02, -2.8296e-01, -9.3520e-02,\n",
      "            7.7124e-01, -8.1062e-01, -1.4958e-01,  2.2699e-01, -3.3094e-01,\n",
      "            3.2033e-01],\n",
      "          [ 8.3772e-01, -1.0244e-01, -5.8518e-01, -1.6068e-02, -5.3292e-01,\n",
      "           -4.4612e-01,  4.3868e-01, -6.3085e-02, -2.8296e-01, -9.3520e-02,\n",
      "            7.7124e-01, -8.1062e-01, -1.4958e-01,  2.2699e-01, -3.3094e-01,\n",
      "            3.2033e-01]],\n",
      "\n",
      "         [[-2.1410e-02,  7.1732e-02, -2.8679e-02,  8.2037e-01, -8.0675e-03,\n",
      "            6.6480e-01,  3.3091e-01,  5.4995e-01,  5.0531e-01, -3.6850e-01,\n",
      "           -2.9552e-01, -6.1326e-04,  8.7290e-01,  5.4611e-01,  2.0170e-01,\n",
      "           -7.9673e-01],\n",
      "          [-2.1410e-02,  7.1732e-02, -2.8679e-02,  8.2037e-01, -8.0675e-03,\n",
      "            6.6480e-01,  3.3091e-01,  5.4995e-01,  5.0531e-01, -3.6850e-01,\n",
      "           -2.9552e-01, -6.1326e-04,  8.7290e-01,  5.4611e-01,  2.0170e-01,\n",
      "           -7.9673e-01],\n",
      "          [-2.1410e-02,  7.1732e-02, -2.8679e-02,  8.2037e-01, -8.0675e-03,\n",
      "            6.6480e-01,  3.3091e-01,  5.4995e-01,  5.0531e-01, -3.6850e-01,\n",
      "           -2.9552e-01, -6.1326e-04,  8.7290e-01,  5.4611e-01,  2.0170e-01,\n",
      "           -7.9673e-01],\n",
      "          [-2.1410e-02,  7.1732e-02, -2.8679e-02,  8.2037e-01, -8.0675e-03,\n",
      "            6.6480e-01,  3.3091e-01,  5.4995e-01,  5.0531e-01, -3.6850e-01,\n",
      "           -2.9552e-01, -6.1326e-04,  8.7290e-01,  5.4611e-01,  2.0170e-01,\n",
      "           -7.9673e-01]]]], grad_fn=<ViewBackward0>)\n",
      "transposed xq: torch.Size([1, 4, 3, 16])\n",
      "tensor([[[[-0.3440,  0.4250,  0.0777, -0.6477,  0.0556,  0.0138, -0.4049,\n",
      "            0.4860,  0.3153,  0.4441, -0.0245, -0.0580,  0.6449,  0.0996,\n",
      "           -0.7473,  0.6645],\n",
      "          [ 0.4429, -0.1319, -0.1022,  0.6308,  0.0293, -0.0194,  0.3654,\n",
      "           -0.4757,  0.1054, -0.5955, -0.0033,  0.1894, -0.6532, -0.0967,\n",
      "            0.7664, -0.6692],\n",
      "          [ 0.2245, -0.4888, -0.6925, -0.1526,  0.7608, -0.4296, -0.6982,\n",
      "            0.0680, -0.6671,  0.1945, -0.3206,  0.5995,  0.0321,  0.1374,\n",
      "            0.0139,  0.3351]],\n",
      "\n",
      "         [[-0.6565,  0.7091,  0.3057, -0.6175,  0.1007, -0.0131,  0.8292,\n",
      "           -0.2527,  0.4312, -0.7363,  0.5771,  0.7024,  0.0615,  0.2873,\n",
      "           -0.4765, -0.1653],\n",
      "          [ 0.7114, -0.9973, -0.1254,  0.7412, -0.0864,  0.0323, -0.8450,\n",
      "            0.2424,  0.3199,  0.2364, -0.6466, -0.5873, -0.0653, -0.2950,\n",
      "            0.4437,  0.1664],\n",
      "          [-0.1746,  0.2873, -0.5050,  0.1450,  0.2897,  0.1193,  0.2064,\n",
      "           -0.3442, -0.4031, -0.2758, -0.0960, -0.0060,  0.2710, -0.2492,\n",
      "           -0.3578, -0.1809]],\n",
      "\n",
      "         [[-0.0593, -0.1284,  0.6634,  0.4445,  0.2634, -0.3782,  0.0868,\n",
      "            0.2849, -0.2552,  0.4939, -0.4644, -0.3378, -0.3834,  0.3493,\n",
      "           -0.0444, -0.2084],\n",
      "          [-0.1810,  0.3772, -0.7745, -0.4982, -0.3111,  0.4056, -0.0892,\n",
      "           -0.2905,  0.1600, -0.3568,  0.2374,  0.2403,  0.3515, -0.3197,\n",
      "            0.0526,  0.1819],\n",
      "          [ 0.9882,  0.1738,  0.2137,  0.2073, -0.2929,  0.1816, -0.0293,\n",
      "            0.0221, -0.4949, -0.1004,  0.0941, -0.5566, -0.2708,  0.3973,\n",
      "            0.4006, -0.8657]],\n",
      "\n",
      "         [[ 0.3315, -0.5907,  0.0959,  0.1244,  0.3005,  0.7012,  0.1496,\n",
      "           -0.0080, -0.5199,  0.0161, -0.1642, -0.1361, -0.3508,  0.8617,\n",
      "            0.0955,  0.1219],\n",
      "          [-0.6187,  0.4930, -0.1379, -0.1265, -0.3430, -0.6559, -0.1395,\n",
      "            0.0249,  0.0108,  0.3085,  0.1377,  0.1293,  0.3030, -0.9016,\n",
      "           -0.1115, -0.0981],\n",
      "          [-0.2286, -0.7082,  0.0707,  0.6765, -0.1741,  0.0124,  0.3182,\n",
      "            0.5367,  0.2581, -0.2282,  0.4407,  0.7535, -0.7157,  0.1847,\n",
      "           -0.3897,  0.9344]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xk: torch.Size([1, 4, 3, 16])\n",
      "tensor([[[[-0.1627, -0.5962,  0.0974,  0.7662,  0.5840,  0.3445,  0.2385,\n",
      "           -0.1308, -0.3749, -0.1789,  0.8423,  0.0050,  0.0976, -0.1386,\n",
      "            0.0617,  0.5838],\n",
      "          [-0.2265,  0.4100,  0.1871, -0.7496, -0.5665, -0.3351, -0.2288,\n",
      "            0.1328,  0.3589,  0.4808, -0.8299, -0.1522, -0.1420,  0.1027,\n",
      "           -0.0827, -0.5861],\n",
      "          [-0.4763, -0.2299,  0.5055,  0.4147,  0.2849,  0.7623,  0.3719,\n",
      "           -0.3577,  0.4313,  0.2160,  0.4475, -0.3361,  0.5802, -0.6078,\n",
      "           -0.4784, -0.0215]],\n",
      "\n",
      "         [[-0.1627, -0.5962,  0.0974,  0.7662,  0.5840,  0.3445,  0.2385,\n",
      "           -0.1308, -0.3749, -0.1789,  0.8423,  0.0050,  0.0976, -0.1386,\n",
      "            0.0617,  0.5838],\n",
      "          [-0.2265,  0.4100,  0.1871, -0.7496, -0.5665, -0.3351, -0.2288,\n",
      "            0.1328,  0.3589,  0.4808, -0.8299, -0.1522, -0.1420,  0.1027,\n",
      "           -0.0827, -0.5861],\n",
      "          [-0.4763, -0.2299,  0.5055,  0.4147,  0.2849,  0.7623,  0.3719,\n",
      "           -0.3577,  0.4313,  0.2160,  0.4475, -0.3361,  0.5802, -0.6078,\n",
      "           -0.4784, -0.0215]],\n",
      "\n",
      "         [[-0.1627, -0.5962,  0.0974,  0.7662,  0.5840,  0.3445,  0.2385,\n",
      "           -0.1308, -0.3749, -0.1789,  0.8423,  0.0050,  0.0976, -0.1386,\n",
      "            0.0617,  0.5838],\n",
      "          [-0.2265,  0.4100,  0.1871, -0.7496, -0.5665, -0.3351, -0.2288,\n",
      "            0.1328,  0.3589,  0.4808, -0.8299, -0.1522, -0.1420,  0.1027,\n",
      "           -0.0827, -0.5861],\n",
      "          [-0.4763, -0.2299,  0.5055,  0.4147,  0.2849,  0.7623,  0.3719,\n",
      "           -0.3577,  0.4313,  0.2160,  0.4475, -0.3361,  0.5802, -0.6078,\n",
      "           -0.4784, -0.0215]],\n",
      "\n",
      "         [[-0.1627, -0.5962,  0.0974,  0.7662,  0.5840,  0.3445,  0.2385,\n",
      "           -0.1308, -0.3749, -0.1789,  0.8423,  0.0050,  0.0976, -0.1386,\n",
      "            0.0617,  0.5838],\n",
      "          [-0.2265,  0.4100,  0.1871, -0.7496, -0.5665, -0.3351, -0.2288,\n",
      "            0.1328,  0.3589,  0.4808, -0.8299, -0.1522, -0.1420,  0.1027,\n",
      "           -0.0827, -0.5861],\n",
      "          [-0.4763, -0.2299,  0.5055,  0.4147,  0.2849,  0.7623,  0.3719,\n",
      "           -0.3577,  0.4313,  0.2160,  0.4475, -0.3361,  0.5802, -0.6078,\n",
      "           -0.4784, -0.0215]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xv: torch.Size([1, 4, 3, 16])\n",
      "tensor([[[[-8.3206e-01,  1.0356e-01,  5.8008e-01,  3.7550e-02,  5.2875e-01,\n",
      "            4.6031e-01, -4.2671e-01,  7.7098e-02,  2.9416e-01,  8.3122e-02,\n",
      "           -7.7329e-01,  8.0458e-01,  1.7145e-01, -2.1093e-01,  3.3379e-01,\n",
      "           -3.3893e-01],\n",
      "          [ 8.3772e-01, -1.0244e-01, -5.8518e-01, -1.6068e-02, -5.3292e-01,\n",
      "           -4.4612e-01,  4.3868e-01, -6.3085e-02, -2.8296e-01, -9.3520e-02,\n",
      "            7.7124e-01, -8.1062e-01, -1.4958e-01,  2.2699e-01, -3.3094e-01,\n",
      "            3.2033e-01],\n",
      "          [-2.1410e-02,  7.1732e-02, -2.8679e-02,  8.2037e-01, -8.0675e-03,\n",
      "            6.6480e-01,  3.3091e-01,  5.4995e-01,  5.0531e-01, -3.6850e-01,\n",
      "           -2.9552e-01, -6.1326e-04,  8.7290e-01,  5.4611e-01,  2.0170e-01,\n",
      "           -7.9673e-01]],\n",
      "\n",
      "         [[-8.3206e-01,  1.0356e-01,  5.8008e-01,  3.7550e-02,  5.2875e-01,\n",
      "            4.6031e-01, -4.2671e-01,  7.7098e-02,  2.9416e-01,  8.3122e-02,\n",
      "           -7.7329e-01,  8.0458e-01,  1.7145e-01, -2.1093e-01,  3.3379e-01,\n",
      "           -3.3893e-01],\n",
      "          [ 8.3772e-01, -1.0244e-01, -5.8518e-01, -1.6068e-02, -5.3292e-01,\n",
      "           -4.4612e-01,  4.3868e-01, -6.3085e-02, -2.8296e-01, -9.3520e-02,\n",
      "            7.7124e-01, -8.1062e-01, -1.4958e-01,  2.2699e-01, -3.3094e-01,\n",
      "            3.2033e-01],\n",
      "          [-2.1410e-02,  7.1732e-02, -2.8679e-02,  8.2037e-01, -8.0675e-03,\n",
      "            6.6480e-01,  3.3091e-01,  5.4995e-01,  5.0531e-01, -3.6850e-01,\n",
      "           -2.9552e-01, -6.1326e-04,  8.7290e-01,  5.4611e-01,  2.0170e-01,\n",
      "           -7.9673e-01]],\n",
      "\n",
      "         [[-8.3206e-01,  1.0356e-01,  5.8008e-01,  3.7550e-02,  5.2875e-01,\n",
      "            4.6031e-01, -4.2671e-01,  7.7098e-02,  2.9416e-01,  8.3122e-02,\n",
      "           -7.7329e-01,  8.0458e-01,  1.7145e-01, -2.1093e-01,  3.3379e-01,\n",
      "           -3.3893e-01],\n",
      "          [ 8.3772e-01, -1.0244e-01, -5.8518e-01, -1.6068e-02, -5.3292e-01,\n",
      "           -4.4612e-01,  4.3868e-01, -6.3085e-02, -2.8296e-01, -9.3520e-02,\n",
      "            7.7124e-01, -8.1062e-01, -1.4958e-01,  2.2699e-01, -3.3094e-01,\n",
      "            3.2033e-01],\n",
      "          [-2.1410e-02,  7.1732e-02, -2.8679e-02,  8.2037e-01, -8.0675e-03,\n",
      "            6.6480e-01,  3.3091e-01,  5.4995e-01,  5.0531e-01, -3.6850e-01,\n",
      "           -2.9552e-01, -6.1326e-04,  8.7290e-01,  5.4611e-01,  2.0170e-01,\n",
      "           -7.9673e-01]],\n",
      "\n",
      "         [[-8.3206e-01,  1.0356e-01,  5.8008e-01,  3.7550e-02,  5.2875e-01,\n",
      "            4.6031e-01, -4.2671e-01,  7.7098e-02,  2.9416e-01,  8.3122e-02,\n",
      "           -7.7329e-01,  8.0458e-01,  1.7145e-01, -2.1093e-01,  3.3379e-01,\n",
      "           -3.3893e-01],\n",
      "          [ 8.3772e-01, -1.0244e-01, -5.8518e-01, -1.6068e-02, -5.3292e-01,\n",
      "           -4.4612e-01,  4.3868e-01, -6.3085e-02, -2.8296e-01, -9.3520e-02,\n",
      "            7.7124e-01, -8.1062e-01, -1.4958e-01,  2.2699e-01, -3.3094e-01,\n",
      "            3.2033e-01],\n",
      "          [-2.1410e-02,  7.1732e-02, -2.8679e-02,  8.2037e-01, -8.0675e-03,\n",
      "            6.6480e-01,  3.3091e-01,  5.4995e-01,  5.0531e-01, -3.6850e-01,\n",
      "           -2.9552e-01, -6.1326e-04,  8.7290e-01,  5.4611e-01,  2.0170e-01,\n",
      "           -7.9673e-01]]]], grad_fn=<TransposeBackward0>)\n",
      "scores: torch.Size([1, 4, 3, 3])\n",
      "tensor([[[[-0.1592,  0.2050,  0.1090],\n",
      "          [ 0.0778, -0.1665, -0.1230],\n",
      "          [ 0.0801, -0.1360, -0.3680]],\n",
      "\n",
      "         [[-0.0436,  0.0139,  0.1518],\n",
      "          [ 0.0528, -0.0466, -0.0610],\n",
      "          [ 0.0691, -0.1170,  0.1012]],\n",
      "\n",
      "         [[-0.0243,  0.1365, -0.0530],\n",
      "          [-0.0712, -0.0238,  0.0198],\n",
      "          [-0.1213,  0.0453, -0.2145]],\n",
      "\n",
      "         [[ 0.2083, -0.1990, -0.0669],\n",
      "          [-0.1546,  0.2025,  0.0674],\n",
      "          [ 0.4095, -0.3830,  0.0297]]]], grad_fn=<MulBackward0>)\n",
      "mask: torch.Size([1, 1, 3, 3])\n",
      "tensor([[[[ 0.0000e+00, -2.3820e+38, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]]]])\n",
      "masked scores: torch.Size([1, 4, 3, 3])\n",
      "tensor([[[[-1.5918e-01, -2.3820e+38, -2.3820e+38],\n",
      "          [ 7.7802e-02, -1.6654e-01, -2.3820e+38],\n",
      "          [ 8.0074e-02, -1.3597e-01, -3.6801e-01]],\n",
      "\n",
      "         [[-4.3565e-02, -2.3820e+38, -2.3820e+38],\n",
      "          [ 5.2797e-02, -4.6558e-02, -2.3820e+38],\n",
      "          [ 6.9121e-02, -1.1701e-01,  1.0120e-01]],\n",
      "\n",
      "         [[-2.4350e-02, -2.3820e+38, -2.3820e+38],\n",
      "          [-7.1196e-02, -2.3765e-02, -2.3820e+38],\n",
      "          [-1.2134e-01,  4.5264e-02, -2.1455e-01]],\n",
      "\n",
      "         [[ 2.0828e-01, -2.3820e+38, -2.3820e+38],\n",
      "          [-1.5464e-01,  2.0248e-01, -2.3820e+38],\n",
      "          [ 4.0951e-01, -3.8300e-01,  2.9747e-02]]]], grad_fn=<AddBackward0>)\n",
      "softmaxed scores: torch.Size([1, 4, 3, 3])\n",
      "tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [0.5608, 0.4392, 0.0000],\n",
      "          [0.4091, 0.3296, 0.2613]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.5248, 0.4752, 0.0000],\n",
      "          [0.3493, 0.2900, 0.3607]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.4881, 0.5119, 0.0000],\n",
      "          [0.3234, 0.3820, 0.2946]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.4117, 0.5883, 0.0000],\n",
      "          [0.4680, 0.2119, 0.3201]]]], grad_fn=<SoftmaxBackward0>)\n",
      "attention: torch.Size([1, 4, 3, 16])\n",
      "tensor([[[[-8.3206e-01,  1.0356e-01,  5.8008e-01,  3.7550e-02,  5.2875e-01,\n",
      "            4.6031e-01, -4.2671e-01,  7.7098e-02,  2.9416e-01,  8.3122e-02,\n",
      "           -7.7329e-01,  8.0458e-01,  1.7145e-01, -2.1093e-01,  3.3379e-01,\n",
      "           -3.3893e-01],\n",
      "          [-9.8666e-02,  1.3085e-02,  6.8278e-02,  1.4000e-02,  6.2447e-02,\n",
      "            6.2192e-02, -4.6616e-02,  1.5527e-02,  4.0681e-02,  5.5380e-03,\n",
      "           -9.4909e-02,  9.5159e-02,  3.0451e-02, -1.8585e-02,  4.1831e-02,\n",
      "           -4.9372e-02],\n",
      "          [-6.9865e-02,  2.7350e-02,  3.6930e-02,  2.2446e-01,  3.8544e-02,\n",
      "            2.1500e-01,  5.6508e-02,  1.5447e-01,  1.5913e-01, -9.3123e-02,\n",
      "           -1.3937e-01,  6.1801e-02,  2.4896e-01,  1.3125e-01,  8.0182e-02,\n",
      "           -2.4129e-01]],\n",
      "\n",
      "         [[-8.3206e-01,  1.0356e-01,  5.8008e-01,  3.7550e-02,  5.2875e-01,\n",
      "            4.6031e-01, -4.2671e-01,  7.7098e-02,  2.9416e-01,  8.3122e-02,\n",
      "           -7.7329e-01,  8.0458e-01,  1.7145e-01, -2.1093e-01,  3.3379e-01,\n",
      "           -3.3893e-01],\n",
      "          [-3.8613e-02,  5.6766e-03,  2.6369e-02,  1.2072e-02,  2.4264e-02,\n",
      "            2.9592e-02, -1.5492e-02,  1.0485e-02,  1.9925e-02, -8.1493e-04,\n",
      "           -3.9360e-02,  3.7069e-02,  1.8905e-02, -2.8356e-03,  1.7924e-02,\n",
      "           -2.5662e-02],\n",
      "          [-5.5443e-02,  3.2345e-02,  2.2589e-02,  3.0436e-01,  2.7249e-02,\n",
      "            2.7122e-01,  9.7516e-02,  2.0700e-01,  2.0296e-01, -1.3100e-01,\n",
      "           -1.5306e-01,  4.5760e-02,  3.3137e-01,  1.8913e-01,  9.3382e-02,\n",
      "           -3.1288e-01]],\n",
      "\n",
      "         [[-8.3206e-01,  1.0356e-01,  5.8008e-01,  3.7550e-02,  5.2875e-01,\n",
      "            4.6031e-01, -4.2671e-01,  7.7098e-02,  2.9416e-01,  8.3122e-02,\n",
      "           -7.7329e-01,  8.0458e-01,  1.7145e-01, -2.1093e-01,  3.3379e-01,\n",
      "           -3.3893e-01],\n",
      "          [ 2.2625e-02, -1.8782e-03, -1.6365e-02,  1.0106e-02, -1.4672e-02,\n",
      "           -3.6503e-03,  1.6245e-02,  5.3444e-03, -1.2400e-03, -7.2931e-03,\n",
      "            1.7284e-02, -2.2168e-02,  7.1311e-03,  1.3225e-02, -6.4542e-03,\n",
      "           -1.4843e-03],\n",
      "          [ 4.4635e-02,  1.5492e-02, -4.4406e-02,  2.4769e-01, -3.4969e-02,\n",
      "            1.7429e-01,  1.2708e-01,  1.6285e-01,  1.3590e-01, -1.1741e-01,\n",
      "           -4.2511e-02, -4.9657e-02,  2.5547e-01,  1.7939e-01,  4.0942e-02,\n",
      "           -2.2196e-01]],\n",
      "\n",
      "         [[-8.3206e-01,  1.0356e-01,  5.8008e-01,  3.7550e-02,  5.2875e-01,\n",
      "            4.6031e-01, -4.2671e-01,  7.7098e-02,  2.9416e-01,  8.3122e-02,\n",
      "           -7.7329e-01,  8.0458e-01,  1.7145e-01, -2.1093e-01,  3.3379e-01,\n",
      "           -3.3893e-01],\n",
      "          [ 1.5034e-01, -1.7634e-02, -1.0549e-01,  6.0046e-03, -9.5876e-02,\n",
      "           -7.2980e-02,  8.2436e-02, -5.3777e-03, -4.5382e-02, -2.0804e-02,\n",
      "            1.3542e-01, -1.4571e-01, -1.7423e-02,  4.6720e-02, -5.7297e-02,\n",
      "            4.8940e-02],\n",
      "          [-2.1878e-01,  4.9729e-02,  1.3832e-01,  2.7679e-01,  1.3197e-01,\n",
      "            3.3373e-01, -8.2707e-04,  1.9877e-01,  2.3948e-01, -9.8879e-02,\n",
      "           -2.9311e-01,  2.0461e-01,  3.2799e-01,  1.2420e-01,  1.5067e-01,\n",
      "           -3.4581e-01]]]], grad_fn=<UnsafeViewBackward0>)\n",
      "reshaped attention: torch.Size([1, 3, 64])\n",
      "tensor([[[-8.3206e-01,  1.0356e-01,  5.8008e-01,  3.7550e-02,  5.2875e-01,\n",
      "           4.6031e-01, -4.2671e-01,  7.7098e-02,  2.9416e-01,  8.3122e-02,\n",
      "          -7.7329e-01,  8.0458e-01,  1.7145e-01, -2.1093e-01,  3.3379e-01,\n",
      "          -3.3893e-01, -8.3206e-01,  1.0356e-01,  5.8008e-01,  3.7550e-02,\n",
      "           5.2875e-01,  4.6031e-01, -4.2671e-01,  7.7098e-02,  2.9416e-01,\n",
      "           8.3122e-02, -7.7329e-01,  8.0458e-01,  1.7145e-01, -2.1093e-01,\n",
      "           3.3379e-01, -3.3893e-01, -8.3206e-01,  1.0356e-01,  5.8008e-01,\n",
      "           3.7550e-02,  5.2875e-01,  4.6031e-01, -4.2671e-01,  7.7098e-02,\n",
      "           2.9416e-01,  8.3122e-02, -7.7329e-01,  8.0458e-01,  1.7145e-01,\n",
      "          -2.1093e-01,  3.3379e-01, -3.3893e-01, -8.3206e-01,  1.0356e-01,\n",
      "           5.8008e-01,  3.7550e-02,  5.2875e-01,  4.6031e-01, -4.2671e-01,\n",
      "           7.7098e-02,  2.9416e-01,  8.3122e-02, -7.7329e-01,  8.0458e-01,\n",
      "           1.7145e-01, -2.1093e-01,  3.3379e-01, -3.3893e-01],\n",
      "         [-9.8666e-02,  1.3085e-02,  6.8278e-02,  1.4000e-02,  6.2447e-02,\n",
      "           6.2192e-02, -4.6616e-02,  1.5527e-02,  4.0681e-02,  5.5380e-03,\n",
      "          -9.4909e-02,  9.5159e-02,  3.0451e-02, -1.8585e-02,  4.1831e-02,\n",
      "          -4.9372e-02, -3.8613e-02,  5.6766e-03,  2.6369e-02,  1.2072e-02,\n",
      "           2.4264e-02,  2.9592e-02, -1.5492e-02,  1.0485e-02,  1.9925e-02,\n",
      "          -8.1493e-04, -3.9360e-02,  3.7069e-02,  1.8905e-02, -2.8356e-03,\n",
      "           1.7924e-02, -2.5662e-02,  2.2625e-02, -1.8782e-03, -1.6365e-02,\n",
      "           1.0106e-02, -1.4672e-02, -3.6503e-03,  1.6245e-02,  5.3444e-03,\n",
      "          -1.2400e-03, -7.2931e-03,  1.7284e-02, -2.2168e-02,  7.1311e-03,\n",
      "           1.3225e-02, -6.4542e-03, -1.4843e-03,  1.5034e-01, -1.7634e-02,\n",
      "          -1.0549e-01,  6.0046e-03, -9.5876e-02, -7.2980e-02,  8.2436e-02,\n",
      "          -5.3777e-03, -4.5382e-02, -2.0804e-02,  1.3542e-01, -1.4571e-01,\n",
      "          -1.7423e-02,  4.6720e-02, -5.7297e-02,  4.8940e-02],\n",
      "         [-6.9865e-02,  2.7350e-02,  3.6930e-02,  2.2446e-01,  3.8544e-02,\n",
      "           2.1500e-01,  5.6508e-02,  1.5447e-01,  1.5913e-01, -9.3123e-02,\n",
      "          -1.3937e-01,  6.1801e-02,  2.4896e-01,  1.3125e-01,  8.0182e-02,\n",
      "          -2.4129e-01, -5.5443e-02,  3.2345e-02,  2.2589e-02,  3.0436e-01,\n",
      "           2.7249e-02,  2.7122e-01,  9.7516e-02,  2.0700e-01,  2.0296e-01,\n",
      "          -1.3100e-01, -1.5306e-01,  4.5760e-02,  3.3137e-01,  1.8913e-01,\n",
      "           9.3382e-02, -3.1288e-01,  4.4635e-02,  1.5492e-02, -4.4406e-02,\n",
      "           2.4769e-01, -3.4969e-02,  1.7429e-01,  1.2708e-01,  1.6285e-01,\n",
      "           1.3590e-01, -1.1741e-01, -4.2511e-02, -4.9657e-02,  2.5547e-01,\n",
      "           1.7939e-01,  4.0942e-02, -2.2196e-01, -2.1878e-01,  4.9729e-02,\n",
      "           1.3832e-01,  2.7679e-01,  1.3197e-01,  3.3373e-01, -8.2707e-04,\n",
      "           1.9877e-01,  2.3948e-01, -9.8879e-02, -2.9311e-01,  2.0461e-01,\n",
      "           3.2799e-01,  1.2420e-01,  1.5067e-01, -3.4581e-01]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "self.Wo: torch.Size([128, 4])\n",
      "Parameter containing:\n",
      "tensor([[-0.0079,  0.0757,  0.0749,  0.0462],\n",
      "        [ 0.0460, -0.0546,  0.0502, -0.0218],\n",
      "        [-0.0014, -0.0434, -0.0075,  0.0641],\n",
      "        [ 0.0671, -0.0485, -0.0218,  0.0060],\n",
      "        [ 0.0309,  0.0527,  0.0630, -0.0393],\n",
      "        [ 0.0365, -0.0782,  0.0736, -0.0832],\n",
      "        [-0.0593,  0.0654, -0.0045, -0.0195],\n",
      "        [-0.0409,  0.0541, -0.0687,  0.0108],\n",
      "        [ 0.0058,  0.0848,  0.0145,  0.0689],\n",
      "        [ 0.0026, -0.0821,  0.0007, -0.0826],\n",
      "        [ 0.0520,  0.0327, -0.0099,  0.0035],\n",
      "        [-0.0113, -0.0376, -0.0707,  0.0649],\n",
      "        [ 0.0840,  0.0234, -0.0376,  0.0552],\n",
      "        [ 0.0851, -0.0157,  0.0635,  0.0348],\n",
      "        [-0.0805,  0.0577, -0.0686,  0.0417],\n",
      "        [ 0.0427, -0.0689, -0.0836, -0.0075],\n",
      "        [-0.0264,  0.0699,  0.0436, -0.0784],\n",
      "        [-0.0033,  0.0120, -0.0278,  0.0845],\n",
      "        [ 0.0211,  0.0219, -0.0064,  0.0622],\n",
      "        [ 0.0116, -0.0601, -0.0351,  0.0438],\n",
      "        [ 0.0529, -0.0174,  0.0657, -0.0547],\n",
      "        [ 0.0491,  0.0206,  0.0715,  0.0095],\n",
      "        [-0.0055, -0.0811,  0.0734, -0.0154],\n",
      "        [-0.0363,  0.0509,  0.0742,  0.0262],\n",
      "        [-0.0246, -0.0447,  0.0577, -0.0400],\n",
      "        [ 0.0389,  0.0033,  0.0518, -0.0246],\n",
      "        [-0.0094,  0.0168,  0.0019,  0.0794],\n",
      "        [ 0.0543, -0.0045, -0.0197, -0.0204],\n",
      "        [ 0.0335,  0.0870, -0.0398,  0.0823],\n",
      "        [-0.0363,  0.0104, -0.0642, -0.0105],\n",
      "        [ 0.0579,  0.0792,  0.0132, -0.0749],\n",
      "        [-0.0677,  0.0264,  0.0142,  0.0011],\n",
      "        [ 0.0827,  0.0024, -0.0406,  0.0068],\n",
      "        [-0.0444, -0.0071, -0.0194,  0.0284],\n",
      "        [ 0.0008, -0.0484, -0.0254,  0.0153],\n",
      "        [-0.0116,  0.0578,  0.0749, -0.0239],\n",
      "        [ 0.0216, -0.0516, -0.0301, -0.0757],\n",
      "        [-0.0307,  0.0636, -0.0593, -0.0068],\n",
      "        [-0.0570, -0.0767, -0.0239, -0.0010],\n",
      "        [ 0.0309, -0.0854,  0.0383,  0.0767],\n",
      "        [-0.0815, -0.0579,  0.0037,  0.0585],\n",
      "        [-0.0627, -0.0090, -0.0607,  0.0304],\n",
      "        [-0.0340, -0.0255, -0.0437,  0.0423],\n",
      "        [-0.0613, -0.0223, -0.0705,  0.0365],\n",
      "        [-0.0433, -0.0027, -0.0013,  0.0185],\n",
      "        [-0.0285,  0.0573,  0.0227,  0.0772],\n",
      "        [ 0.0165,  0.0765,  0.0751, -0.0699],\n",
      "        [-0.0342, -0.0433,  0.0554, -0.0370],\n",
      "        [-0.0358, -0.0385,  0.0709, -0.0257],\n",
      "        [ 0.0039,  0.0320, -0.0702, -0.0271],\n",
      "        [-0.0589, -0.0542, -0.0375, -0.0203],\n",
      "        [-0.0016, -0.0322,  0.0168,  0.0031],\n",
      "        [ 0.0460,  0.0677, -0.0761, -0.0260],\n",
      "        [ 0.0072,  0.0692, -0.0434,  0.0824],\n",
      "        [-0.0196, -0.0401, -0.0728, -0.0267],\n",
      "        [-0.0574,  0.0809,  0.0224,  0.0883],\n",
      "        [-0.0287,  0.0293, -0.0042,  0.0625],\n",
      "        [-0.0607,  0.0780, -0.0487, -0.0470],\n",
      "        [ 0.0747,  0.0287,  0.0232, -0.0343],\n",
      "        [-0.0560,  0.0061, -0.0046,  0.0253],\n",
      "        [-0.0705, -0.0466, -0.0435,  0.0327],\n",
      "        [-0.0161,  0.0762, -0.0001, -0.0089],\n",
      "        [ 0.0236,  0.0732,  0.0577, -0.0353],\n",
      "        [ 0.0087, -0.0584,  0.0357, -0.0773],\n",
      "        [-0.0670, -0.0439, -0.0023,  0.0128],\n",
      "        [ 0.0493,  0.0366,  0.0490,  0.0567],\n",
      "        [ 0.0221,  0.0582, -0.0552, -0.0022],\n",
      "        [ 0.0296,  0.0399,  0.0145, -0.0225],\n",
      "        [ 0.0603, -0.0110, -0.0349,  0.0332],\n",
      "        [-0.0803, -0.0851,  0.0214, -0.0223],\n",
      "        [-0.0452,  0.0069,  0.0462, -0.0463],\n",
      "        [-0.0643, -0.0237, -0.0822,  0.0790],\n",
      "        [ 0.0303,  0.0251, -0.0693,  0.0408],\n",
      "        [ 0.0185,  0.0506,  0.0176,  0.0865],\n",
      "        [-0.0782, -0.0380, -0.0346, -0.0512],\n",
      "        [-0.0452, -0.0101,  0.0294,  0.0438],\n",
      "        [-0.0496,  0.0350,  0.0099,  0.0243],\n",
      "        [-0.0701, -0.0437, -0.0308,  0.0103],\n",
      "        [-0.0842, -0.0148, -0.0684, -0.0078],\n",
      "        [-0.0701, -0.0414,  0.0810, -0.0062],\n",
      "        [-0.0146,  0.0005,  0.0533,  0.0344],\n",
      "        [ 0.0085,  0.0402,  0.0674, -0.0017],\n",
      "        [-0.0191, -0.0351,  0.0825,  0.0132],\n",
      "        [-0.0615,  0.0387,  0.0302, -0.0637],\n",
      "        [ 0.0858, -0.0366,  0.0106,  0.0591],\n",
      "        [-0.0817, -0.0733, -0.0238,  0.0241],\n",
      "        [-0.0592, -0.0499, -0.0003,  0.0668],\n",
      "        [-0.0703, -0.0642,  0.0084, -0.0529],\n",
      "        [ 0.0316, -0.0261, -0.0091,  0.0567],\n",
      "        [-0.0310, -0.0424,  0.0673,  0.0006],\n",
      "        [-0.0438, -0.0581,  0.0070,  0.0240],\n",
      "        [ 0.0063, -0.0861, -0.0742,  0.0079],\n",
      "        [-0.0030,  0.0009,  0.0563, -0.0348],\n",
      "        [ 0.0190,  0.0472, -0.0052, -0.0382],\n",
      "        [-0.0308, -0.0811, -0.0136,  0.0697],\n",
      "        [ 0.0380, -0.0230, -0.0244, -0.0466],\n",
      "        [-0.0628, -0.0160, -0.0716,  0.0577],\n",
      "        [ 0.0544, -0.0765,  0.0275, -0.0681],\n",
      "        [ 0.0318,  0.0341,  0.0013, -0.0111],\n",
      "        [-0.0799,  0.0441, -0.0169, -0.0087],\n",
      "        [-0.0725,  0.0369, -0.0701,  0.0023],\n",
      "        [ 0.0854, -0.0224, -0.0534, -0.0877],\n",
      "        [ 0.0283, -0.0878, -0.0075, -0.0721],\n",
      "        [-0.0322,  0.0820,  0.0631,  0.0723],\n",
      "        [ 0.0064,  0.0323,  0.0624,  0.0100],\n",
      "        [ 0.0456,  0.0870, -0.0688,  0.0531],\n",
      "        [ 0.0388, -0.0071,  0.0018, -0.0191],\n",
      "        [ 0.0644, -0.0875, -0.0701,  0.0224],\n",
      "        [-0.0434, -0.0629, -0.0451, -0.0093],\n",
      "        [ 0.0819,  0.0873,  0.0458, -0.0539],\n",
      "        [ 0.0413, -0.0432, -0.0012, -0.0310],\n",
      "        [ 0.0508,  0.0221, -0.0217, -0.0304],\n",
      "        [-0.0855,  0.0643,  0.0014, -0.0692],\n",
      "        [-0.0104,  0.0773,  0.0658, -0.0545],\n",
      "        [ 0.0044, -0.0092, -0.0831,  0.0229],\n",
      "        [ 0.0332, -0.0195, -0.0683,  0.0217],\n",
      "        [-0.0628, -0.0243, -0.0171,  0.0723],\n",
      "        [-0.0556, -0.0618, -0.0275,  0.0095],\n",
      "        [ 0.0450,  0.0690, -0.0316,  0.0098],\n",
      "        [-0.0299, -0.0863,  0.0291, -0.0333],\n",
      "        [ 0.0593, -0.0456,  0.0017,  0.0872],\n",
      "        [ 0.0687,  0.0754,  0.0252, -0.0636],\n",
      "        [ 0.0395, -0.0375, -0.0368,  0.0031],\n",
      "        [ 0.0429,  0.0580, -0.0204, -0.0299],\n",
      "        [-0.0047, -0.0014, -0.0368, -0.0616],\n",
      "        [-0.0147,  0.0073, -0.0368,  0.0620],\n",
      "        [ 0.0585, -0.0117, -0.0788,  0.0392],\n",
      "        [ 0.0628,  0.0042,  0.0403,  0.0434]], requires_grad=True)\n",
      "spliced Wo: torch.Size([64, 2])\n",
      "tensor([[ 0.0436, -0.0784],\n",
      "        [-0.0278,  0.0845],\n",
      "        [-0.0064,  0.0622],\n",
      "        [-0.0351,  0.0438],\n",
      "        [ 0.0657, -0.0547],\n",
      "        [ 0.0715,  0.0095],\n",
      "        [ 0.0734, -0.0154],\n",
      "        [ 0.0742,  0.0262],\n",
      "        [ 0.0577, -0.0400],\n",
      "        [ 0.0518, -0.0246],\n",
      "        [ 0.0019,  0.0794],\n",
      "        [-0.0197, -0.0204],\n",
      "        [-0.0398,  0.0823],\n",
      "        [-0.0642, -0.0105],\n",
      "        [ 0.0132, -0.0749],\n",
      "        [ 0.0142,  0.0011],\n",
      "        [ 0.0709, -0.0257],\n",
      "        [-0.0702, -0.0271],\n",
      "        [-0.0375, -0.0203],\n",
      "        [ 0.0168,  0.0031],\n",
      "        [-0.0761, -0.0260],\n",
      "        [-0.0434,  0.0824],\n",
      "        [-0.0728, -0.0267],\n",
      "        [ 0.0224,  0.0883],\n",
      "        [-0.0042,  0.0625],\n",
      "        [-0.0487, -0.0470],\n",
      "        [ 0.0232, -0.0343],\n",
      "        [-0.0046,  0.0253],\n",
      "        [-0.0435,  0.0327],\n",
      "        [-0.0001, -0.0089],\n",
      "        [ 0.0577, -0.0353],\n",
      "        [ 0.0357, -0.0773],\n",
      "        [ 0.0533,  0.0344],\n",
      "        [ 0.0674, -0.0017],\n",
      "        [ 0.0825,  0.0132],\n",
      "        [ 0.0302, -0.0637],\n",
      "        [ 0.0106,  0.0591],\n",
      "        [-0.0238,  0.0241],\n",
      "        [-0.0003,  0.0668],\n",
      "        [ 0.0084, -0.0529],\n",
      "        [-0.0091,  0.0567],\n",
      "        [ 0.0673,  0.0006],\n",
      "        [ 0.0070,  0.0240],\n",
      "        [-0.0742,  0.0079],\n",
      "        [ 0.0563, -0.0348],\n",
      "        [-0.0052, -0.0382],\n",
      "        [-0.0136,  0.0697],\n",
      "        [-0.0244, -0.0466],\n",
      "        [ 0.0014, -0.0692],\n",
      "        [ 0.0658, -0.0545],\n",
      "        [-0.0831,  0.0229],\n",
      "        [-0.0683,  0.0217],\n",
      "        [-0.0171,  0.0723],\n",
      "        [-0.0275,  0.0095],\n",
      "        [-0.0316,  0.0098],\n",
      "        [ 0.0291, -0.0333],\n",
      "        [ 0.0017,  0.0872],\n",
      "        [ 0.0252, -0.0636],\n",
      "        [-0.0368,  0.0031],\n",
      "        [-0.0204, -0.0299],\n",
      "        [-0.0368, -0.0616],\n",
      "        [-0.0368,  0.0620],\n",
      "        [-0.0788,  0.0392],\n",
      "        [ 0.0403,  0.0434]], grad_fn=<CatBackward0>)\n",
      "projected output: torch.Size([1, 3, 2])\n",
      "tensor([[[-0.2495,  0.2301],\n",
      "         [ 0.0064, -0.0043],\n",
      "         [-0.0873,  0.1302]]], grad_fn=<UnsafeViewBackward0>)\n",
      "----------------- END MultiQueryAttention.forwardTensor() --------------------\n",
      "x in layer after MQA & resid connection and before MLP:\n",
      "tensor([[[-1.2760, -1.1947],\n",
      "         [ 0.9788,  1.2757],\n",
      "         [ 1.2380, -1.6208]]], grad_fn=<AddBackward0>)\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 3, 2])\n",
      "tensor([[[-1.2760, -1.1947],\n",
      "         [ 0.9788,  1.2757],\n",
      "         [ 1.2380, -1.6208]]], grad_fn=<AddBackward0>)\n",
      "normed x: torch.Size([1, 3, 2])\n",
      "tensor([[[-1.0323, -0.9666],\n",
      "         [ 0.8609,  1.1220],\n",
      "         [ 0.8585, -1.1239]]], grad_fn=<MulBackward0>)\n",
      "dim: 2\n",
      "skip: 2\n",
      "spliced scale: torch.Size([2])\n",
      "tensor([0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 3, 2])\n",
      "tensor([[[-1.0323, -0.9666],\n",
      "         [ 0.8609,  1.1220],\n",
      "         [ 0.8585, -1.1239]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "---------- MLP Input: torch.Tensor ------------\n",
      "------------- MLP.forwardTensor() ------------\n",
      "x: torch.Size([1, 3, 2])\n",
      "tensor([[[-1.0323, -0.9666],\n",
      "         [ 0.8609,  1.1220],\n",
      "         [ 0.8585, -1.1239]]], grad_fn=<MulBackward0>)\n",
      "d_dim: 2\n",
      "d_skip: 2\n",
      "i_dim: 8\n",
      "i_skip: 8\n",
      "Wgate: torch.Size([4, 16])\n",
      "Parameter containing:\n",
      "tensor([[-0.4711, -0.2294,  0.3826,  0.2413,  0.0221, -0.1754, -0.4362,  0.4694,\n",
      "         -0.2813,  0.0714, -0.1090,  0.2667, -0.4410, -0.4892, -0.3989,  0.2439],\n",
      "        [ 0.2152, -0.2464, -0.1261, -0.0181,  0.3651,  0.3879,  0.2535, -0.3582,\n",
      "          0.0528,  0.2415,  0.1849,  0.3567, -0.0384,  0.4340, -0.3725,  0.3105],\n",
      "        [-0.2154,  0.2649,  0.3705, -0.4557,  0.1628, -0.2772, -0.1900,  0.4409,\n",
      "          0.3852, -0.0915, -0.4160,  0.0106,  0.2830,  0.1615,  0.0159,  0.4351],\n",
      "        [ 0.0065, -0.4006,  0.0773, -0.0895, -0.2334, -0.0861,  0.0079,  0.1160,\n",
      "          0.4893, -0.4880,  0.0563, -0.4693, -0.4824,  0.1443, -0.1036,  0.1029]],\n",
      "       requires_grad=True)\n",
      "Wgate spliced: torch.Size([2, 8])\n",
      "tensor([[ 0.3852, -0.0915, -0.4160,  0.0106,  0.2830,  0.1615,  0.0159,  0.4351],\n",
      "        [ 0.4893, -0.4880,  0.0563, -0.4693, -0.4824,  0.1443, -0.1036,  0.1029]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Bgate: torch.Size([16])\n",
      "Parameter containing:\n",
      "tensor([ 0.1358,  0.2152,  0.2748, -0.3331, -0.1813,  0.0264,  0.4762, -0.4854,\n",
      "        -0.2410, -0.1014, -0.0148,  0.3180, -0.3845, -0.0458,  0.2054,  0.4083],\n",
      "       requires_grad=True)\n",
      "Bgate spliced: torch.Size([8])\n",
      "tensor([-0.2410, -0.1014, -0.0148,  0.3180, -0.3845, -0.0458,  0.2054,  0.4083],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xgate: torch.Size([1, 3, 8])\n",
      "tensor([[[-1.1115,  0.4648,  0.3602,  0.7607, -0.2104, -0.3520,  0.2891,\n",
      "          -0.1403],\n",
      "         [ 0.6396, -0.7277, -0.3097, -0.1994, -0.6822,  0.2552,  0.1028,\n",
      "           0.8983],\n",
      "         [-0.4602,  0.3686, -0.4351,  0.8546,  0.4005, -0.0693,  0.3354,\n",
      "           0.6662]]], grad_fn=<AddBackward0>)\n",
      "GeLU'ed Xgate: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.1480,  0.3156,  0.2308,  0.5908, -0.0877, -0.1276,  0.1774,\n",
      "          -0.0623],\n",
      "         [ 0.4725, -0.1698, -0.1172, -0.0840, -0.1689,  0.1533,  0.0556,\n",
      "           0.7325],\n",
      "         [-0.1485,  0.2373, -0.1443,  0.6868,  0.2626, -0.0327,  0.2117,\n",
      "           0.4979]]], grad_fn=<GeluBackward0>)\n",
      "Wup: torch.Size([4, 16])\n",
      "Parameter containing:\n",
      "tensor([[ 0.1108,  0.3057,  0.1222,  0.4416,  0.1918,  0.4734,  0.3690, -0.1281,\n",
      "         -0.3413, -0.3456,  0.0640, -0.0212, -0.4335, -0.2666, -0.1306,  0.3194],\n",
      "        [-0.2082, -0.3736,  0.3005,  0.1933, -0.4231, -0.1596,  0.4303, -0.1602,\n",
      "          0.3515, -0.4712, -0.0116,  0.4635,  0.3738, -0.3957, -0.0436, -0.2250],\n",
      "        [-0.3199,  0.3075,  0.1510,  0.2626, -0.4115, -0.2712,  0.0805, -0.4850,\n",
      "         -0.3383, -0.3757, -0.2884, -0.4095, -0.4162, -0.1790,  0.4855, -0.1969],\n",
      "        [ 0.1019,  0.3763, -0.1884, -0.0300,  0.1595,  0.4114, -0.3694, -0.1927,\n",
      "         -0.0307,  0.0315,  0.1099, -0.3822, -0.0468,  0.1110, -0.2350,  0.2922]],\n",
      "       requires_grad=True)\n",
      "Wup spliced: torch.Size([2, 8])\n",
      "tensor([[-0.3383, -0.3757, -0.2884, -0.4095, -0.4162, -0.1790,  0.4855, -0.1969],\n",
      "        [-0.0307,  0.0315,  0.1099, -0.3822, -0.0468,  0.1110, -0.2350,  0.2922]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Bup: torch.Size([16])\n",
      "Parameter containing:\n",
      "tensor([ 0.2493, -0.0109, -0.0733, -0.2945, -0.1111,  0.2164, -0.0506,  0.2475,\n",
      "        -0.1203, -0.3330,  0.2976, -0.1972,  0.0952, -0.0163, -0.4097,  0.3306],\n",
      "       requires_grad=True)\n",
      "Bup spliced: torch.Size([8])\n",
      "tensor([-0.1203, -0.3330,  0.2976, -0.1972,  0.0952, -0.0163, -0.4097,  0.3306],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xup: torch.Size([1, 3, 8])\n",
      "tensor([[[ 0.2586,  0.0243,  0.4891,  0.5950,  0.5700,  0.0613, -0.6838,\n",
      "           0.2514],\n",
      "         [-0.4460, -0.6210,  0.1728, -0.9785, -0.3155, -0.0459, -0.2554,\n",
      "           0.4890],\n",
      "         [-0.3763, -0.6910, -0.0735, -0.1191, -0.2095, -0.2947,  0.2712,\n",
      "          -0.1668]]], grad_fn=<AddBackward0>)\n",
      "Xfuse: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.0383,  0.0077,  0.1129,  0.3515, -0.0500, -0.0078, -0.1213,\n",
      "          -0.0157],\n",
      "         [-0.2107,  0.1055, -0.0202,  0.0822,  0.0533, -0.0070, -0.0142,\n",
      "           0.3582],\n",
      "         [ 0.0559, -0.1640,  0.0106, -0.0818, -0.0550,  0.0096,  0.0574,\n",
      "          -0.0831]]], grad_fn=<MulBackward0>)\n",
      "Wdown: torch.Size([16, 4])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0279, -0.2160, -0.1735,  0.1518],\n",
      "        [ 0.1084, -0.0126, -0.1621, -0.0115],\n",
      "        [-0.2072, -0.2054,  0.0458,  0.1223],\n",
      "        [-0.2420, -0.0571,  0.2103, -0.2155],\n",
      "        [-0.2034, -0.1359,  0.0111, -0.0423],\n",
      "        [ 0.1759, -0.2092, -0.2173, -0.0898],\n",
      "        [-0.2167, -0.1977,  0.2118,  0.1070],\n",
      "        [-0.1025,  0.1586, -0.2437, -0.1566],\n",
      "        [-0.1593,  0.2311, -0.2201, -0.1509],\n",
      "        [-0.2218, -0.0825, -0.0995,  0.2268],\n",
      "        [-0.1547, -0.2075, -0.0174,  0.0752],\n",
      "        [ 0.0670,  0.1068, -0.2171, -0.0432],\n",
      "        [ 0.2249, -0.1833, -0.0199, -0.0505],\n",
      "        [-0.0075, -0.1590,  0.1165,  0.0302],\n",
      "        [ 0.1515,  0.2459,  0.0666,  0.1779],\n",
      "        [ 0.0220,  0.1881,  0.0740, -0.2126]], requires_grad=True)\n",
      "Wdown spliced: torch.Size([8, 2])\n",
      "tensor([[-0.2201, -0.1509],\n",
      "        [-0.0995,  0.2268],\n",
      "        [-0.0174,  0.0752],\n",
      "        [-0.2171, -0.0432],\n",
      "        [-0.0199, -0.0505],\n",
      "        [ 0.1165,  0.0302],\n",
      "        [ 0.0666,  0.1779],\n",
      "        [ 0.0740, -0.2126]], grad_fn=<SliceBackward0>)\n",
      "Bdown: torch.Size([4])\n",
      "Parameter containing:\n",
      "tensor([-0.0769,  0.1017,  0.2368,  0.0229], requires_grad=True)\n",
      "Bdown spliced: torch.Size([2])\n",
      "tensor([0.2368, 0.0229], grad_fn=<SliceBackward0>)\n",
      "outputs: torch.Size([1, 3, 2])\n",
      "tensor([[[ 0.1570,  0.0077],\n",
      "         [ 0.2789, -0.0081],\n",
      "         [ 0.2583,  0.0125]]], grad_fn=<AddBackward0>)\n",
      "------------- END MLP.forwardTensor() ------------\n",
      "layer's final residual state:\n",
      "tensor([[[-1.1190, -1.1870],\n",
      "         [ 1.2576,  1.2676],\n",
      "         [ 1.4963, -1.6083]]], grad_fn=<AddBackward0>)\n",
      "----------------- END Layer.forwardTensor() --------------------\n",
      "forwardTensor() output: torch.Size([1, 3, 2])\n",
      "tensor([[[-1.1190, -1.1870],\n",
      "         [ 1.2576,  1.2676],\n",
      "         [ 1.4963, -1.6083]]], grad_fn=<AddBackward0>)\n",
      "final output: ((tensor([[[ 0.1550,  0.2258, -1.7857, -0.2692],\n",
      "         [-0.7763, -1.1361, -1.2054,  2.1558],\n",
      "         [-0.4130,  0.0375,  1.0142,  0.9613]]], grad_fn=<AddBackward0>),), (tensor([[[-0.3527, -0.3661],\n",
      "         [ 2.2496,  0.2749],\n",
      "         [-0.4138,  1.3496]]], grad_fn=<AddBackward0>), tensor([[[-1.1190, -1.1870],\n",
      "         [ 1.2576,  1.2676],\n",
      "         [ 1.4963, -1.6083]]], grad_fn=<AddBackward0>)))\n",
      "------------- END Layer.forwardTuple() ------------\n",
      "out: ((tensor([[[ 0.1550,  0.2258, -1.7857, -0.2692],\n",
      "         [-0.7763, -1.1361, -1.2054,  2.1558],\n",
      "         [-0.4130,  0.0375,  1.0142,  0.9613]]], grad_fn=<AddBackward0>),), (tensor([[[-0.3527, -0.3661],\n",
      "         [ 2.2496,  0.2749],\n",
      "         [-0.4138,  1.3496]]], grad_fn=<AddBackward0>), tensor([[[-1.1190, -1.1870],\n",
      "         [ 1.2576,  1.2676],\n",
      "         [ 1.4963, -1.6083]]], grad_fn=<AddBackward0>)))\n",
      "---------- RESET CONFIG --------\n",
      "model_count:  [1, 2, 4]\n",
      "model_dim_list:  [128, 64, 32]\n"
     ]
    }
   ],
   "source": [
    "# Testing our Layer's forwardTuple()\n",
    "verbose = True\n",
    "\n",
    "print(\"--------- Micro Hyperparameters -------\")\n",
    "hold1, hold2, hold3, hold4 = config.hidden_size, config.levels, config.max_position_embeddings, config.hidden_size\n",
    "config.hidden_size = 4\n",
    "config.levels = 2\n",
    "config.max_position_embeddings = 3\n",
    "config.vocab_size = 5\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "\n",
    "embedding = torch.randn(config.vocab_size, config.hidden_size)\n",
    "print(f\"embedding: {embedding.shape}\\n{embedding}\")\n",
    "\n",
    "layer = Layer(config)\n",
    "# we need to make sure to send in a tuple of the expected size. above we set hidden_size=4 and levels=2\n",
    "x = ((torch.randn((1,3,config.hidden_size)),),\n",
    "     (torch.randn((1,3,config.hidden_size//config.split)),torch.randn((1,3,config.hidden_size//config.split))))\n",
    "print(f\"x: {x}\")\n",
    "out = layer(x)\n",
    "print(f\"out: {out}\")\n",
    "\n",
    "verbose = False\n",
    "print(\"---------- RESET CONFIG --------\")\n",
    "config.hidden_size = hold1\n",
    "config.levels = hold2\n",
    "config.max_position_embeddings = hold3\n",
    "config.vocab_size = hold4\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec942733",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09e1d472",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FractalLoss(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        \"\"\"\n",
    "        input: \n",
    "            - logits are a tuple of tuples of tensors each of shape [batch_size, max_seq_len, vocab_size]\n",
    "            - target is a shape [batch_size, max_seq_len] tensor of the integer indices of the correct tokens\n",
    "        output: a tensor containing a single float of the loss value\n",
    "        \"\"\"\n",
    "        global verbose\n",
    "        if verbose: \n",
    "            print(\"------------- FractalLoss.forward() ------------\")\n",
    "            print(f\"logits:\\n{logits}\")\n",
    "            \n",
    "        assert type(x) == tuple # since this function should only be used during training\n",
    "            \n",
    "        # should only be used during training, so we assert input_len == max_position_embeddings\n",
    "        b,t,v = logits[0][0].shape\n",
    "        if verbose: print(f\"b:{b}, t:{t}, v:{v}, b*t:{b*t}\")\n",
    "        assert t == config.max_position_embeddings\n",
    "        \n",
    "        # Calculate losses for each output and stack them. i apologize for the weird format instead of regular for loops\n",
    "        loss = torch.stack([ # stacks across levels\n",
    "                            torch.stack( # stacks across models in level\n",
    "                                        [self.criterion(logits_ij.view(b*t, v), # reshapes for CELoss\n",
    "                                                        target.view(b*t)) \n",
    "                                         for logits_ij in logits[i]] # iterates across models in level\n",
    "                            ).sum() # sums across models in level\n",
    "                            for i in range(len(logits))] # iterates across levels\n",
    "                          ).sum() # sums across levels\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"final loss: {loss}\")\n",
    "            print(\"------------- END FractalLoss.forward() ------------\")\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "36754b4f-9dd4-4a61-a626-36b03d5d2217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Micro Hyperparameters -------\n",
      "model_count:  [1, 2]\n",
      "model_dim_list:  [4, 2]\n",
      "embedding: torch.Size([5, 4])\n",
      "tensor([[-1.3753, -0.7115, -1.8179, -0.5396],\n",
      "        [ 0.2276,  1.7926, -1.4529,  1.0655],\n",
      "        [ 0.5938,  0.5983,  0.4228,  0.5722],\n",
      "        [ 1.0451,  0.0733, -0.4143, -0.4269],\n",
      "        [ 0.0781, -0.5721, -0.1324, -0.7755]])\n",
      "logits: ((tensor([[[-1.8138,  1.2353,  0.5865,  0.6241, -0.8431],\n",
      "         [ 0.5524,  1.0414, -0.9325, -0.9413,  0.1517],\n",
      "         [ 2.0922, -1.1598,  0.0593, -0.7795,  0.3292]],\n",
      "\n",
      "        [[-0.6612,  1.0559,  0.7058,  0.1552,  0.9851],\n",
      "         [-2.2625, -0.3402, -0.6354,  0.8113,  1.6704],\n",
      "         [ 1.1679, -0.0254,  0.5453, -0.0048, -0.5645]]]),), (tensor([[[ 0.6266, -0.2598,  0.9878,  0.1268,  0.2202],\n",
      "         [-2.1192, -1.1076, -1.2282, -0.7977, -1.2385],\n",
      "         [ 0.7055, -0.5674,  1.9132,  0.1368,  1.4071]],\n",
      "\n",
      "        [[-0.8021, -1.8081, -0.6775, -1.6985, -0.7555],\n",
      "         [ 2.6726, -0.7442,  0.1797, -0.6538,  0.7061],\n",
      "         [ 1.1845,  0.6462, -0.3746,  1.0230,  0.3297]]]), tensor([[[-1.6289,  2.1398,  0.7261, -0.3258, -2.1022],\n",
      "         [-0.4493,  0.3945,  0.3095,  1.6578, -1.5057],\n",
      "         [-1.2101, -0.9721,  1.0917,  0.8529, -0.6524]],\n",
      "\n",
      "        [[ 0.0680, -0.2022, -0.9912,  0.1209,  0.4807],\n",
      "         [ 0.9899,  0.4919, -0.9623,  0.8022, -1.5263],\n",
      "         [ 0.7836,  2.0531, -0.4113,  0.3758, -1.4496]]])))\n",
      "target: tensor([[[0, 0, 3],\n",
      "         [1, 1, 2]]])\n",
      "------------- FractalLoss.forward() ------------\n",
      "logits:\n",
      "((tensor([[[-1.8138,  1.2353,  0.5865,  0.6241, -0.8431],\n",
      "         [ 0.5524,  1.0414, -0.9325, -0.9413,  0.1517],\n",
      "         [ 2.0922, -1.1598,  0.0593, -0.7795,  0.3292]],\n",
      "\n",
      "        [[-0.6612,  1.0559,  0.7058,  0.1552,  0.9851],\n",
      "         [-2.2625, -0.3402, -0.6354,  0.8113,  1.6704],\n",
      "         [ 1.1679, -0.0254,  0.5453, -0.0048, -0.5645]]]),), (tensor([[[ 0.6266, -0.2598,  0.9878,  0.1268,  0.2202],\n",
      "         [-2.1192, -1.1076, -1.2282, -0.7977, -1.2385],\n",
      "         [ 0.7055, -0.5674,  1.9132,  0.1368,  1.4071]],\n",
      "\n",
      "        [[-0.8021, -1.8081, -0.6775, -1.6985, -0.7555],\n",
      "         [ 2.6726, -0.7442,  0.1797, -0.6538,  0.7061],\n",
      "         [ 1.1845,  0.6462, -0.3746,  1.0230,  0.3297]]]), tensor([[[-1.6289,  2.1398,  0.7261, -0.3258, -2.1022],\n",
      "         [-0.4493,  0.3945,  0.3095,  1.6578, -1.5057],\n",
      "         [-1.2101, -0.9721,  1.0917,  0.8529, -0.6524]],\n",
      "\n",
      "        [[ 0.0680, -0.2022, -0.9912,  0.1209,  0.4807],\n",
      "         [ 0.9899,  0.4919, -0.9623,  0.8022, -1.5263],\n",
      "         [ 0.7836,  2.0531, -0.4113,  0.3758, -1.4496]]])))\n",
      "b:2, t:3, v:5, b*t:6\n",
      "final loss: 7.118621826171875\n",
      "------------- END FractalLoss.forward() ------------\n",
      "out: 7.118621826171875\n",
      "---------- RESET CONFIG --------\n",
      "model_count:  [1, 2, 4]\n",
      "model_dim_list:  [128, 64, 32]\n"
     ]
    }
   ],
   "source": [
    "# Testing our FractalLoss\n",
    "verbose = True\n",
    "\n",
    "print(\"--------- Micro Hyperparameters -------\")\n",
    "hold1, hold2, hold3, hold4 = config.hidden_size, config.levels, config.max_position_embeddings, config.hidden_size\n",
    "config.hidden_size = 4\n",
    "config.levels = 2\n",
    "config.max_position_embeddings = 3\n",
    "config.vocab_size = 5\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "\n",
    "embedding = torch.randn(config.vocab_size, config.hidden_size)\n",
    "print(f\"embedding: {embedding.shape}\\n{embedding}\")\n",
    "\n",
    "loss = FractalLoss(config)\n",
    "# we need to make sure to send in a tuple of the expected size. above we set hidden_size=4 and levels=2\n",
    "logits = ((torch.randn((2,3,config.vocab_size)),),\n",
    "     (torch.randn((2,3,config.vocab_size)),torch.randn((2,3,config.vocab_size))))\n",
    "print(f\"logits: {logits}\")\n",
    "target = torch.randint(config.vocab_size, (2,3)).unsqueeze(0)\n",
    "print(f\"target: {target}\")\n",
    "out = loss(logits, target)\n",
    "print(f\"out: {out}\")\n",
    "\n",
    "verbose = False\n",
    "print(\"---------- RESET CONFIG --------\")\n",
    "config.hidden_size = hold1\n",
    "config.levels = hold2\n",
    "config.max_position_embeddings = hold3\n",
    "config.vocab_size = hold4\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fb1d77",
   "metadata": {},
   "source": [
    "# ------------ BOOKMARK ----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4383399d",
   "metadata": {},
   "source": [
    "# The Model itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0018879",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FractalFormer(nn.Module):\n",
    "    def __init__(self, config: Config, tokenizer: tokenizer):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        # hyperparameters\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.max_seq_len = config.max_position_embeddings\n",
    "        self.head_dim = config.head_dim\n",
    "        self.vocab_size = config.vocab_size\n",
    "\n",
    "        ### FractalFormer-specific hyperparameters\n",
    "        self.num_levels = config.levels # the number of levels for sub-models to exist on\n",
    "        self.split = config.split # the number of splits to make at a given level\n",
    "        self.model_count = config.model_count # list of number of models at a given level\n",
    "        self.model_dim_list = config.model_dim_list # list of hidden dimensions corresponding to each given level\n",
    "        self.head_dim_list = config.head_dim_list # list of attention head dimensions corresponding to each given level    \n",
    "\n",
    "        # the embedding matrix. for converting tokens to the first residual state, and the last residual state to logits\n",
    "        self.embedder = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "\n",
    "        # for normalizing the initial embeddings\n",
    "        self.embedder_norm = RMSNorm(config.hidden_size)\n",
    "\n",
    "        # Initialize a sequence of DecoderLayer instances as specified by the number of hidden layers in the config\n",
    "        self.layers = nn.ModuleList(Layer(config) for _ in range(config.num_hidden_layers))\n",
    "\n",
    "        self.output_layer = OutputLayer(self.embedder.weight, config)\n",
    "        # i think i need to do this bc in the above version you can't use `self.` inside the init\n",
    "        #@property \n",
    "        #def output_layer(self):\n",
    "            #return OutputLayer(self.embedder.weight, config)\n",
    "\n",
    "        # the loss function\n",
    "        self.criterion = FractalLoss()\n",
    "\n",
    "    def forwardTensor(self,\n",
    "                      input_token_ids: torch.Tensor,\n",
    "                      level: int = 0, # integer designating the level of model to use. 0 is largest model, -1 is smallest\n",
    "                      model: int = 0, # integer designating the model in that level to use. 0 is top-left, -1 is bottom right\n",
    "                     ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        inputs: \n",
    "            - input_token_ids (torch.Tensor): a tensor of integers size (batch_size, sequence_length)\n",
    "            - level: integer designating the level of model to use. 0 is largest model, -1 is smallest\n",
    "            - model: integer designating the model in that level to use. 0 is top-left, -1 is bottom right\n",
    "        output: a torch.Tensor shape (batch_size, sequence_length, vocab_size)\n",
    "        \"\"\"\n",
    "        global verbose\n",
    "        if verbose: \n",
    "            print(\"------------- FractalFormer.forwardTensor() ------------\")\n",
    "            print(f\"input_token_ids: {input_token_ids.shape}\\n{input_token_ids}\")\n",
    "        \n",
    "        # adjusting everything to the specified level & model\n",
    "        d_dim = self.hidden_size / (2**level)\n",
    "        d_skip = model * h_dim\n",
    "        if verbose:\n",
    "            print(f\"d_dim: {d_dim}\")\n",
    "            print(f\"d_skip: {d_skip}\")\n",
    "        \n",
    "        # turn the input tokens into the first residual state using the embedding matrix\n",
    "        # (batch_size, input_len) & (vocab_size, hidden_size) -> (batch_size, input_len, hidden_size) -> (batch_size, input_len, h_dim)\n",
    "        x = self.embedder(input_token_ids)[:,:, d_skip:d_skip + d_dim]\n",
    "        if verbose: print(f\"initial x: {x.shape}\\n{x}\")\n",
    "        \n",
    "        # Gemma normalizes the embedding by sqrt(hidden_size)\n",
    "        # the question is, should I do this with the full sized hidden_size or do it at the splice size????\n",
    "        # imma do it at the splice size and change it later if i think the models aren't learning well\n",
    "        #x = x * (d_dim**0.5)\n",
    "        # alternatively i could just switch to doing a regular RMSNorm which would be more like me\n",
    "        # if i figure out this different sizes of hyperspheres thing it'd be more in line with that\n",
    "        x = self.embedder_norm(x, model)\n",
    "        if verbose: print(f\"normalized initial x: {x.shape}\\n{x}\")\n",
    "\n",
    "        # Iteratively process the input through each Layer\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if verbose: print(f\"begin layer {i}\")\n",
    "            x = layer(x, model)\n",
    "            if verbose: print(f\"output of layer {i}: {x.shape}\\n{x}\")\n",
    "\n",
    "        logits = self.output_layer(x, model)\n",
    "        if verbose: print(f\"output logits: {logits.shape}\\n{logits}\")\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def forwardTuple(self,\n",
    "                     input_token_ids: torch.Tensor,\n",
    "                     target_token_ids: torch.Tensor,\n",
    "                    ) -> torch.Tensor:\n",
    "        \n",
    "        x0 = ()\n",
    "        for i in range(self.num_levels):\n",
    "\n",
    "            # our splicing setup\n",
    "            h_dim = self.model_dim_list[i]\n",
    "            i_dim = h_dim * self.intermediate_multiplier\n",
    "            \n",
    "            x0_lvl = ()#, (), (), ()\n",
    "            for j in range(self.model_count[i]):\n",
    "\n",
    "                # splicing specific to this model\n",
    "                h_skip = j * h_dim\n",
    "                i_skip = j * i_dim\n",
    "\n",
    "                x0_lvl += (elf.embedder(input_token_ids)[:,:, h_skip:h_skip + h_dim] * (h_dim**0.5),)\n",
    "\n",
    "            x0 += (x0_lvl)\n",
    "    \n",
    "    def forward(self,\n",
    "                input_token_ids: torch.Tensor, # a shape (batch_size, input_seq_len OR max_seq_len)list of integer token ids\n",
    "                target_token_ids: torch.Tensor = None, # a shape (batch_size, max_seq_len) list of token ids to train on\n",
    "                level: int = 0, # integer designating the level of model to use. 0 is largest model\n",
    "                model: int = 0, # integer designating the model in that level to use. 0 is top-left model in level\n",
    "                ):\n",
    "        \n",
    "        if target_token_ids is None: # if we're not training, then we don't need to calculate loss\n",
    "            logits = self.forwardTensor(input_token_ids, level, mode)\n",
    "            loss = None\n",
    "        else:\n",
    "            # if we are training\n",
    "            # training uses a tuple of tuples of tensors\n",
    "            logits = self.forwardTuple(input_token_ids) # -> Tuple[Tuple[Tensor shape (batch_size, max_seq_len, vocab_size)]]\n",
    "            \n",
    "            # custom Fractal CE loss function\n",
    "            loss = self.criterion(logits, targets) \n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad() # no need to keep track of gradients during inference\n",
    "    def Sampler(\n",
    "        self,\n",
    "        logits: torch.Tensor, # shape (batch_size, input_len, vocab_size)\n",
    "        temperature: float, # controls how boring vs random the outputs should be\n",
    "        top_p: float, # the maximum cumulative probability of output options we're willing to consider\n",
    "        top_k: int, # the maximum number of output options we're willing to consider\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        The Sampler function is responsible for generating token predictions from Gemma's output.\n",
    "        It supports temperature scaling, top-p (nucleus) sampling, and top-k sampling \n",
    "        The class operates as follows:\n",
    "    \n",
    "        1. Selects the last hidden state for each sequence in the batch\n",
    "    \n",
    "        2. Computes logits by multiplying the selected hidden states with the transposed embedding matrix. \n",
    "    \n",
    "        3. Temperature is used to scale the logits, making the distribution over tokens sharper (lower temperature) \n",
    "        or flatter (higher temperature), which affects the randomness of the sampling (flatter -> more random)\n",
    "    \n",
    "        4. The softmax function is applied to the scaled logits to obtain a probability distribution over the vocabulary.\n",
    "    \n",
    "        5. For top-p sampling, the function computes the cumulative sum of the sorted probabilities and masks out tokens until the \n",
    "        cumulative probability exceeds the threshold defined by `top_ps`. This allows the model to focus on a subset of the most \n",
    "        probable tokens while ignoring the long tail of less likely tokens. \n",
    "        We to ignore long tail probabilities to avoid nonsensical output\n",
    "    \n",
    "        7. For top-k sampling, the function masks out all tokens except the `k` most likely ones, as specified by `top_ks`. \n",
    "        This ensures that the model only considers a fixed number of the most probable tokens for the next token prediction.\n",
    "    \n",
    "        8. After applying both the top-p and top-k masks, the probabilities are re-normalized so that they sum up to 1\n",
    "    \n",
    "        9. The function then samples from the re-normalized probability distribution to select the next token. \n",
    "        \"\"\"\n",
    "        # Select the last element for each sequence.\n",
    "        # (batch_size, input_len, vocab_size) -> (batch_size, vocab_size)\n",
    "        logits = logits[:,-1,:]\n",
    "        \n",
    "        # Apply temperature scaling\n",
    "        # (batch_size, vocab_size) / float -> (batch_size, vocab_size)\n",
    "        logits.div_(temperature) # div_ is an in-place operation which is ok since we don't record gradients during inference\n",
    "\n",
    "        # Calculate probabilities with softmax.\n",
    "        probs = torch.softmax(logits, dim=-1, dtype=torch.float) # dim=-1 is the vocab_size dimension that we calculate along\n",
    "\n",
    "        # sort the probabilities to for use in top-p & top-k\n",
    "        # both are (batch_size, vocab_size)\n",
    "        probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n",
    "        # probs_sort contains float probabilities while probs_idx contains integer indices\n",
    "\n",
    "        # calculating top-p\n",
    "        # creates same-size tensor of cumulatve probabilities instead of indivdiual probs\n",
    "        probs_sum = torch.cumsum(probs_sort, dim=-1) \n",
    "        # mask where 0's are top-p selections & 1's are to be excluded\n",
    "        top_ps_mask = (probs_sum - probs_sort) > top_p\n",
    "        # the original probabilities with excluded tokens changed to 0.0\n",
    "        probs_sort = torch.where(top_ps_mask, 0, probs_sort) \n",
    "\n",
    "        # calculating top_k\n",
    "        # create a shape (vocab_size) tensor that just iterates up by 1's\n",
    "        top_ks_mask = torch.arange(probs_idx.shape[-1], device=probs_idx.device) \n",
    "        # expand our mask along the batch_size dimension to become size (batch_size, vocab_size)\n",
    "        # \"expand\" means copy the original into this new size, so each length vocab_size row is the same\n",
    "        top_ks_mask = top_ks_mask.expand(probs_idx.shape[0], -1)\n",
    "        # top_ks is a list of integers. we keep whichever entries in top_ks_mask are greater than their corresponding entries in top_ks\n",
    "        top_ks_mask = top_ks_mask >= top_k\n",
    "\n",
    "        # we'll be combining top-p with top-k and using whichever gives us fewer tokens. a very conservative approach\n",
    "        # this trims probs_sort to also fit within our top_k requirement\n",
    "        probs_sort = torch.where(top_ks_mask, 0, probs_sort)\n",
    "\n",
    "        # Re-normalization so that total probabilities add up to 1\n",
    "        probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n",
    "        \n",
    "        # now we rearrange the modified probabilities in probs_sort back to their original order according to probs_idx\n",
    "        probs = torch.gather(probs_sort,\n",
    "                             dim=-1,\n",
    "                             index=torch.argsort(probs_idx, dim=-1))\n",
    "        \n",
    "        # samples from the distribution\n",
    "        next_token_id = torch.multinomial(probs, num_samples=1)\n",
    "        \n",
    "        return next_token_id # returns the predicted token\n",
    "        \n",
    "    def generate(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        output_len: int = 100, # the model will output 100 tokens\n",
    "        temperature: float = 0.95, # 0.95 is pretty close to not even using temperature at all (1.0 would be no effect)\n",
    "        top_p: float = 1.0, # defaulting to 1 means we essentially don't use top-p\n",
    "        top_k: int = 65, # setting top_k = vocab_size means we're effectively not using top_k at all\n",
    "    ) -> str: \n",
    "        \"\"\"Generates responses for given prompts using Gemma model.\"\"\"\n",
    "        \n",
    "        # encoding the prompt into token indices\n",
    "        tokens = self.tokenizer.encode(prompt)\n",
    "\n",
    "        # turning it into the right tensor shape\n",
    "        tokens = torch.tensor(tokens, device=config.device).unsqueeze(0)\n",
    "        \n",
    "        # we wouldn't want to go past the maximum context length we trained on\n",
    "        assert len(tokens) + output_len <= self.config.max_position_embeddings\n",
    "\n",
    "        for i in range(output_len):\n",
    "            # get the model's output logits and ignore the loss, which would be a NoneType object\n",
    "            logits, _ = self(tokens[:,:self.max_seq_len])\n",
    "            \n",
    "            next_token = self.Sampler(\n",
    "                logits = logits, # the actual output of the model\n",
    "                temperature = temperature,\n",
    "                top_p = top_p,\n",
    "                top_k = top_k\n",
    "            )\n",
    "\n",
    "            # add our new token to the sequence\n",
    "            tokens = torch.cat((tokens, next_token), dim=1)\n",
    "\n",
    "        # decode our list of tokens to an actual string\n",
    "        output = self.tokenizer.decode(tokens.squeeze(0).tolist())\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565e80c4",
   "metadata": {},
   "source": [
    "# Training-related Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "866442d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test splits\n",
    "data = torch.tensor(tokenizer.encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be our training dataset, the rest for validation\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ae69c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading for training which generates a small batch of data of inputs x and targets y\n",
    "def get_batch(split, batch_size):\n",
    "    # whether we grab from our training or validation dataset\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - config.max_position_embeddings, (batch_size,))\n",
    "    x = torch.stack([data[i:i+config.max_position_embeddings] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+config.max_position_embeddings+1] for i in ix])\n",
    "    x, y = x.to(config.device), y.to(config.device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2cd7dab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 88,   1,  72,  52,   0,  14,  43,   1,  54,  68,  44,  43,  41,  58,\n",
      "          85,  24,  33,  15,  21,  27,  71,  21,   1,  61,  77,  56,  70,  58,\n",
      "           1,  88,  56,   1,  46,  79, 127,  85,  16,  33,  23,  17,   1,  34,\n",
      "          21,  26,  15,  17,  26,  32,  21,  27,  71,  32,  87,   1,  61,  77,\n",
      "          56,  70,  58,  57,   1, 105,   1,  88,  56,  91,  50,  44, 125,  58,\n",
      "          39, 124,   1,  87, 113,   1,  84,   5,  58,  85,  21,  31,  13,  14,\n",
      "          17,  24,  24,  13,  71,  32, 102,  57,   1,  45,  76,  58,  99,  51,\n",
      "          70,   1,  84, 111,   1,  57,  53,  83,  61,  92,  58,   1,  94,   1,\n",
      "         101,   1,  58,  39,  99,   6,   7,   7,  75,  24,  33,  15,  21,  27,\n",
      "          71,  30,  47, 122,  58,  85,  16,  33,  23,  17,   1,  34,  21,  26,\n",
      "          15,  17,  26,  32,  21,  27,  71,  21,  58,   1,  51, 106,   1,  98,\n",
      "           1,  56,  47, 122,  58, 125,  40, 114,   1,  88,   1,  77,  43,   1,\n",
      "          47,   5,   1,  72,   1,  61, 115,  52,  45,   0,  32,  53,   1,  57,\n",
      "          54,  43,  39,  49,   1,  98, 105,  43,   1,  88,  56,   1,  58,  47,\n",
      "          83,   8,   1,  28, 115, 104, 113,  85,  21,  31,  13,  14,  17,  24,\n",
      "          24,  13,  71,  21,   1, 119,  52,  58,   0,  32,  53,   1,  65,  74,\n",
      "           1,  54,  68,  52,  47,  41,  47,  67,  57,   1,  41,  39,  96,  47,\n",
      "          44,  44,   1, 118,  54, 114,  63,   6,   7,   7,  75,  16,  33,  23,\n",
      "          17,   1,  34,  21]])\n",
      "you then\n",
      "Be perfect.\n",
      "\n",
      "LUCIO:\n",
      "I warrant your honour.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "The warrants for yourself; take heed to't.\n",
      "\n",
      "ISABELLA:\n",
      "This gentleman told somewhat of my tale,--\n",
      "\n",
      "LUCIO:\n",
      "Right.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "It may be right; but you are i' the wrong\n",
      "To speak before your time. Proceed.\n",
      "\n",
      "ISABELLA:\n",
      "I went\n",
      "To this pernicious caitiff deputy,--\n",
      "\n",
      "DUKE VI\n",
      "-------\n",
      "tensor([[  1,  72,  52,   0,  14,  43,   1,  54,  68,  44,  43,  41,  58,  85,\n",
      "          24,  33,  15,  21,  27,  71,  21,   1,  61,  77,  56,  70,  58,   1,\n",
      "          88,  56,   1,  46,  79, 127,  85,  16,  33,  23,  17,   1,  34,  21,\n",
      "          26,  15,  17,  26,  32,  21,  27,  71,  32,  87,   1,  61,  77,  56,\n",
      "          70,  58,  57,   1, 105,   1,  88,  56,  91,  50,  44, 125,  58,  39,\n",
      "         124,   1,  87, 113,   1,  84,   5,  58,  85,  21,  31,  13,  14,  17,\n",
      "          24,  24,  13,  71,  32, 102,  57,   1,  45,  76,  58,  99,  51,  70,\n",
      "           1,  84, 111,   1,  57,  53,  83,  61,  92,  58,   1,  94,   1, 101,\n",
      "           1,  58,  39,  99,   6,   7,   7,  75,  24,  33,  15,  21,  27,  71,\n",
      "          30,  47, 122,  58,  85,  16,  33,  23,  17,   1,  34,  21,  26,  15,\n",
      "          17,  26,  32,  21,  27,  71,  21,  58,   1,  51, 106,   1,  98,   1,\n",
      "          56,  47, 122,  58, 125,  40, 114,   1,  88,   1,  77,  43,   1,  47,\n",
      "           5,   1,  72,   1,  61, 115,  52,  45,   0,  32,  53,   1,  57,  54,\n",
      "          43,  39,  49,   1,  98, 105,  43,   1,  88,  56,   1,  58,  47,  83,\n",
      "           8,   1,  28, 115, 104, 113,  85,  21,  31,  13,  14,  17,  24,  24,\n",
      "          13,  71,  21,   1, 119,  52,  58,   0,  32,  53,   1,  65,  74,   1,\n",
      "          54,  68,  52,  47,  41,  47,  67,  57,   1,  41,  39,  96,  47,  44,\n",
      "          44,   1, 118,  54, 114,  63,   6,   7,   7,  75,  16,  33,  23,  17,\n",
      "           1,  34,  21,  26]])\n",
      " then\n",
      "Be perfect.\n",
      "\n",
      "LUCIO:\n",
      "I warrant your honour.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "The warrants for yourself; take heed to't.\n",
      "\n",
      "ISABELLA:\n",
      "This gentleman told somewhat of my tale,--\n",
      "\n",
      "LUCIO:\n",
      "Right.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "It may be right; but you are i' the wrong\n",
      "To speak before your time. Proceed.\n",
      "\n",
      "ISABELLA:\n",
      "I went\n",
      "To this pernicious caitiff deputy,--\n",
      "\n",
      "DUKE VIN\n"
     ]
    }
   ],
   "source": [
    "# a demonstration of what a batch with batch_size=1 looks like. Notice the one-token offset in characters\n",
    "xb, yb = get_batch('train', 1)\n",
    "print(xb)\n",
    "print(tokenizer.decode(xb.squeeze(0).tolist()))\n",
    "print(\"-------\")\n",
    "print(yb)\n",
    "print(tokenizer.decode(yb.squeeze(0).tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01205572",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, batch_size, eval_iters = 10): # to estimate loss during the training loop\n",
    "    out = {}\n",
    "    model.eval() # sets model to eval mode\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split, batch_size)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train() # just resets to training mode\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9254fc45",
   "metadata": {},
   "source": [
    "# Instantiating a brand new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee5ba532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "972.416 K parameters\n",
      "minGemma(\n",
      "  (embedder): Embedding(128, 128)\n",
      "  (model): Body(\n",
      "    (layers): ModuleList(\n",
      "      (0-3): 4 x Layer(\n",
      "        (self_attn): Attention(\n",
      "          (qkv_proj): Linear(in_features=128, out_features=192, bias=False)\n",
      "          (o_proj): Linear(in_features=128, out_features=128, bias=False)\n",
      "        )\n",
      "        (mlp): MLP(\n",
      "          (gate_proj): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (up_proj): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (down_proj): Linear(in_features=512, out_features=128, bias=True)\n",
      "        )\n",
      "        (input_layernorm): RMSNorm()\n",
      "        (post_attention_layernorm): RMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): RMSNorm()\n",
      "  )\n",
      "  (criterion): CrossEntropyLoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = minGemma(config, tokenizer).to(config.device)\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e3, 'K parameters')\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c01e3de",
   "metadata": {},
   "source": [
    "# Load a Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6bfcc9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "964.352 K parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "minGemma(\n",
       "  (embedder): Embedding(65, 128)\n",
       "  (model): GemmaBody(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaAttention(\n",
       "          (qkv_proj): Linear(in_features=128, out_features=192, bias=False)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=False)\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (up_proj): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (down_proj): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): RMSNorm()\n",
       "  )\n",
       "  (criterion): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize a blank model\n",
    "model = minGemma(config, tokenizer).to(config.device)  \n",
    "\n",
    "# here's the path to a minGemma model that i've trained with roughly 1m parameters\n",
    "path = 'models/minGemma-vocab_size128-max_position_embeddings256-num_hidden_layers4-num_attention_heads4-num_key_value_heads1-hidden_size128-intermediate_size512-head_dim32-rms_norm_eps1e-06-rope_theta100.0--2024-02-26|11-10-53.pth'\n",
    "\n",
    "# Load the saved state dictionary\n",
    "model.load_state_dict(torch.load(path))\n",
    "# REMEMBER TO CHANGE VALUES IN CONFIG TO MATCH THE MODEL YOU'VE LOADED\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e3, 'K parameters')\n",
    "\n",
    "# If you only plan to do inference, switch to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# If you plan to continue training the model, switch to training mode\n",
    "#model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5afc88",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "709c2430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "# this is not what they used, but this learning rate & weight decay work for our tiny minGemma\n",
    "learning_rate = 3e-4\n",
    "weight_decay = 0.01\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# how long we want to train for\n",
    "max_iters = 5000\n",
    "\n",
    "# how often we want to check & see how our loss is doing\n",
    "eval_interval = 250\n",
    "\n",
    "# batch size to use\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db20fc3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 130.0421, val loss 130.0103, time elapsed: 0.73 seconds\n",
      "step 250: train loss 4.8103, val loss 4.9765, time elapsed: 138.16 seconds\n",
      "step 500: train loss 3.6559, val loss 3.6816, time elapsed: 342.70 seconds\n",
      "step 750: train loss 3.2894, val loss 3.3416, time elapsed: 566.02 seconds\n",
      "step 1000: train loss 3.1266, val loss 3.1717, time elapsed: 769.44 seconds\n",
      "step 1250: train loss 3.0514, val loss 3.1126, time elapsed: 903.18 seconds\n",
      "step 1500: train loss 2.9887, val loss 3.0574, time elapsed: 1036.83 seconds\n",
      "step 1750: train loss 2.9147, val loss 3.0104, time elapsed: 1171.64 seconds\n",
      "step 2000: train loss 2.8687, val loss 2.9626, time elapsed: 1336.68 seconds\n",
      "step 2250: train loss 2.8162, val loss 2.9178, time elapsed: 1470.40 seconds\n",
      "step 2500: train loss 2.7705, val loss 2.8822, time elapsed: 1652.25 seconds\n",
      "step 2750: train loss 2.7071, val loss 2.8136, time elapsed: 1785.60 seconds\n",
      "step 3000: train loss 2.6603, val loss 2.7935, time elapsed: 1918.52 seconds\n",
      "step 3250: train loss 2.5937, val loss 2.7557, time elapsed: 2050.26 seconds\n",
      "step 3500: train loss 2.5603, val loss 2.7392, time elapsed: 2182.03 seconds\n",
      "step 3750: train loss 2.5329, val loss 2.7008, time elapsed: 2431.70 seconds\n",
      "step 4000: train loss 2.4875, val loss 2.6825, time elapsed: 2812.37 seconds\n",
      "step 4250: train loss 2.4370, val loss 2.6485, time elapsed: 2944.44 seconds\n",
      "step 4500: train loss 2.3834, val loss 2.6287, time elapsed: 3336.32 seconds\n",
      "step 4750: train loss 2.3948, val loss 2.6229, time elapsed: 3490.41 seconds\n",
      "step 4999: train loss 2.3185, val loss 2.5922, time elapsed: 3640.39 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Enable anomaly detection. uncomment these lines if you need to do extensive debugging\n",
    "#torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train', batch_size)\n",
    "    \n",
    "    # train\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        current_time = time.time()\n",
    "        elapsed_time = current_time - start_time\n",
    "        losses = estimate_loss(model, batch_size)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, time elapsed: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Disable anomaly detection after the training loop\n",
    "#torch.autograd.set_detect_anomaly(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3386f092",
   "metadata": {},
   "source": [
    "# Saving your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61635f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model currently held in memory\n",
    "# the filename specifies the model's class, hyperparameters, and date/time it was saved\n",
    "torch.save(model.state_dict(),\n",
    "           f'models/{model.__class__.__name__}'\n",
    "           f'-vocab_size{config.vocab_size}'\n",
    "           f'-max_position_embeddings{config.max_position_embeddings}'\n",
    "           f'-num_hidden_layers{config.num_hidden_layers}'\n",
    "           f'-num_attention_heads{config.num_attention_heads}'\n",
    "           f'-num_key_value_heads{config.num_key_value_heads}'\n",
    "           f'-hidden_size{config.hidden_size}'\n",
    "           f'-intermediate_size{config.intermediate_size}'\n",
    "           f'-head_dim{config.head_dim}'\n",
    "           f'-rms_norm_eps{config.rms_norm_eps}'\n",
    "           f'-rope_theta{config.rope_theta}'\n",
    "           f'--{time.strftime(\"%Y-%m-%d|%H-%M-%S\")}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e2a6cb",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "18529d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JULIET:\n",
      "O Romeo, Romeo! wherefore art thou lord,\n",
      "Bol, am the lad we vowerly her lastion greathe!\n",
      "\n",
      "Voddon:\n",
      "He his latter in my slould is fleeck frideed\n",
      "Or sperate so placel\n",
      "And mot to which conour barksag his light\n",
      "And see as please mene meanner.\n",
      "This scied what is ontued to my lead\n",
      "How I dod, me wit destrined have fain and\n",
      "do by \n"
     ]
    }
   ],
   "source": [
    "input_str = \"JULIET:\\nO Romeo, Romeo! wherefore art thou \" # the classic line\n",
    "max_useable_output_len = config.max_position_embeddings - len(input_str)\n",
    "output = model.generate(input_str, output_len = max_useable_output_len)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3042f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
